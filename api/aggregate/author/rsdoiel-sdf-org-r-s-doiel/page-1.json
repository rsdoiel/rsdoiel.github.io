{
  "page": 1,
  "total_pages": 1,
  "has_more": false,
  "next_page": null,
  "values": [
    {
      "content": "\nBrowser based site search\n=========================\n\nBy R. S. Doiel, 2022-11-18\n\nI recently read Brewster Kahleâ€™s 2015 post about his vision for a [distributed web](https://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/). Many of his ideas have carried over into [DWeb](https://wiki.mozilla.org/Dweb), [Indie Web](https://indieweb.org/), [Small Web](https://benhoyt.com/writings/the-small-web-is-beautiful/), [Small Internet](https://cafebedouin.org/2021/07/28/the-small-internet/) and the like. A point he touches on is site search running in the web browser.\n\nI've use this approach in my own website relying on [LunrJS](https://lunrjs.com) by Oliver Nightingale. It is a common approach for small sites built using Markdown and [Pandoc](https://pandoc.org).  In the Brewster article he mentions [js-search](https://github.com/cebe/js-search), an implementation I was not familiar with. Like LunrJS the query engine runs in the browser via JavaScript but unlike LunrJS the indexes are built using PHP rather than JavaScript. The last couple of years I've used [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) to generating indexes for my own website site while using LunrJS for the browser side query engine. Today I check to see what the [Hugo](https://gohugo.io/tools/search/) community is using and found [Pagefind](https://github.com/cloudcannon/pagefind). Pagefind looks impressive. There was a presentation on at [Hugo Conference 2022](https://hugoconf.io/). It takes building a Lucene-like index several steps further. I appears to handle much larger indexes without requiring the full indexes to be downloaded into the browser.  It seems like a good candidate for prototyping personal search engine.\n\nHow long have been has browser side search been around? I do not remember when I started using. I explored seven projects on GitHub that implemented browser side site search. This is an arbitrary selection projects but even then I had no idea that this approach dates back a over decade!\n\n| Project | Indexer | query engine | earliest commit[^1] | recent commit[^2] |\n|---------|---------|--------------|:-------------------:|:-----------------:|\n| [LunrJS](https://github.com/olivernn/lunr.js) | JavaScript | JavaScript | 2011 | 2020 |\n| [Fuse.io](https://github.com/krisk/Fuse) | JavaScript/Typescript | JavaScript/Typescript | 2012 | 2022 |\n| [search-index](https://github.com/fergiemcdowall/search-index) | JavaScript | JavaScript | 2013 | 2016 |\n| [js-search](https://github.com/cebe/js-search) (cebe) | PHP | JavaScript | 2014 | 2022 |\n| [js-search](https://github.com/bvaughn/js-search) (bvaughn)| JavaScript | JavaScript | 2015 | 2022 |\n| [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) | Python | Python or JavaScript | 2018 | 2022 |\n| [Pagefind](https://github.com/cloudcannon/pagefind) | Rust | WASM and JavaScript | 2022 | 2022 |\n\n[^1]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n[^2]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-18",
        "keywords": [
          "search",
          "web browser",
          "dweb",
          "static site",
          "lunrjs",
          "pagefind"
        ],
        "pubDate": "2022-11-18",
        "title": "Browser based site search"
      },
      "url": "posts/2022/11/18/browser-side-site-search.json"
    },
    {
      "content": "\nTwitter's pending implosion\n===========================\n\nBy R. S. Doiel, 2022-11-11\n\nIt looks like Twitter continues to implode as layoffs and resignations continue. If bankers, investors and lenders call in the loans [bankruptcy appears to be possible](https://www.reuters.com/technology/twitter-information-security-chief-kissner-decides-leave-2022-11-10/). So what's next?\n\n\nThe problem\n-----------\n\nTwitter has been troubled for some time. The advertising model corrodes content. Twitter is effectively a massive RSS-like distribution system. It has stagnated as the APIs became more restrictive. The Advertising Business Model via [Ad-tech](https://pluralistic.net/tag/adtech/ \"per Cory Doctorow 'ad-fraud'\") encourages decay regardless of system.  Non-Twitter examples include commercial search engines (e.g. Google, Bing et el). Their usefulness usefulness declines over time. I believe this due to the increase in \"noise\" in the signal. The \"noise\" is driven be business models. That usually boils down to content who's function is to attract your attention so it can be sold for money. A corollary is [fear based journalism](https://medium.com/@oliviacadby/fear-mongering-journalisms-downfall-aac1f4f5756d). That has even caught the attention of a [Pope](https://www.9news.com.au/world/fear-based-journalism-is-terrorism-pope/4860b502-5dbb-4eef-abcf-57582445fc2c). Not fun.\n\nI suspect business models don't encourage great content. Business models are generally designed to turn a profit. They tend to get refined and tuned to that purpose. The evolution of Twitter and Google's search engine would make good case studies in that regard.\n\n\nA small hope\n------------\n\nI don't know what is next but I know what I find interesting. I've looked at Mastodon a number of times. It's not going away but the W3C activity pub spec is horribly complex. Complexity slows adoption. It reminds me of SGML. Conceptually interesting but in practice was too heavy. It did form inspiration for HTML though, and that has proven successful. What gives me hope is that Mastodon has survived. I think casting a wide net is interesting. The wider net is something I've heard called the \"small web\".\n\nThe small web\n-------------\n\nFor a number of years there has been a slowly growing  \"small web\" movement. I think it is relatively new term, I didn't find it in [Wikipedia](https://en.wikipedia.org/w/index.php?search=small+web&ns0=1 \"today is 2022-11-11\") when I looked today. As I see it the \"small web\" has been driven by a number of things. It is not a homogeneous movement but rather a collection of various efforts and communities.  I think it likely to continue to evolve. At times evolve rapidly. Perhaps it will coalesce at some point.  Here's what appears to me to be the common motivations as of 2022-11-11.\n\n- desire for simplicity\n- desire for authenticity\n- lower resource footprint\n- text as a primary but not exclusive medium\n- hyperlinks encouraged\n- a space where you're not a product\n- desire for decentralization (so you're not a product)\n- a desire to have room to grow (because you're not a product)\n\nThe \"small web\" is a term I've seen pop up in Gopherspace, among people who promote [Gemini](https://gemini.circumlunar.space/), [Micro blogging](https://micro.blog \"as an example of micro blogging\") and in the [Public Access Unix](https://sdf.org) communities.\"small web\" as a term does not return useful results in the commercial search engines I've checked. Elements seem to be part of [DWeb](https://getdweb.net/) which Mozilla is [championing](https://wiki.mozilla.org/Dweb). Curiously in spite of the hype and marketing I don't see \"small web\" in [web 3.0](https://www.forbes.com/advisor/investing/cryptocurrency/what-is-web-3-0/). I think blockchain has proven environmentally dangerous and tends to compile down to various forms of [grift](https://pluralistic.net/2022/05/27/voluntary-carbon-market/).\n\n\nSmall web\n---------\n\nWhat does \"small web\" mean to me?  I think it means\n\n- simple protocols that are flexible and friendly to tool creation\n- built on existing network protocols with a proven track record (e.g. IPv4, IPv6)\n- decentralized by design as was the early Internet\n- low barrier to participation\n    - e.g. a text editor, static site providing a URL to a twtxt file\n- text centric (at least for the moment)\n- integrated with the larger Internet, i.e. supports hyper links\n- friendly to distributed personal search engines (e.g. LunrJS running over curated set of JSONfeeds or twtxt urls)\n- \"feed\" oriented discovery based on simple formats (e.g. [RSS 2.0](https://cyber.harvard.edu/rss/rss.html), [JSONfeed](https://www.jsonfeed.org/), [twtxt](https://twtxt.readthedocs.io/en/latest/), [OPML](https://en.wikipedia.org/wiki/OPML), even [Gophermaps](https://en.wikipedia.org/wiki/Gopher_(protocol) \"see Source code of a menu title\"))\n- sustainable and preservation friendly\n    - example characteristics\n        - clone-able (e.g. as easy as cloning a Git Repo)\n        - push button update to Internet Archive's way back machine\n        - human and machine readable metadata\n\nI think the \"small web\" already exists. Examples include readable personal websites hosted as \"static pages\" via GitHub and S3 buckets are a good examples of prior art in a \"small web\".  Gopherspace is a good example of the \"small web\". I think the various [tilde communities](https://tilde.club) hosted on [Public Access Unix](https://en.wikipedia.org/wiki/SDF_Public_Access_Unix_System) are examples. Even the venerable \"bloggosphere\" of [Wordpress](https://wordpress.com) and the newer [Micro.blog](https://micro.blog/) is evidence that the \"small web\" already is hear. [Dave Winer](https://scripting.com)'s [Feedland](http://feedland.org/) is a good example of innovation in the \"small web\" happen today.  [Yarn.social](https://yarn.social) built on twtxt file format is very promising. I would argue right now the \"small web\" is the internet that already exists outside the walled gardens of Google, Meta/Facebook, Twitter, TikTok, Pinterest, Slack, Trello, Discord, etc.\n\nI think it is significant that the \"small web\" existed before the Pandemic. It continued to thrive during it. It is likely to evolve beyond it. The pending shift has already happening as it is already populated by \"early adopters\" and appears to be growing into larger community participation.  For the \"main stream\" it is waiting to be \"discovered\" or perhaps \"re-discovered\" depending on your point of view.\n\nHow do you participate?\n-----------------------\n\nYou may already be participating in the \"small web\".  Do you blog? Do your read feeds? Do you use a non-soloed social media platform like Mastodon? Do you use Gopher? The \"small web\" is defined by choice and is characterized by simplicity. It is a general term. You're the navigator not an algorithm tuned to tune someone a profit. If you are not sure where to start you can join a communities like [sdf.org](https://sdf.org) and get started there. You can explore [Gopherspace](https://floodgap.com) via a WWW proxy. You can create a static website and host a [twtxt](https://twtxt.readthedocs.io/en/latest/) file on GitHub or a [Yarn Pod](https://yarn.social). You can create a site via [Micro.blog](https://micro.blog) or [Feedland](http://feedland.org). You can blog. You can read RSS feeds or read twtxt feed with [twtxt](https://twtxt.readthedocs.io/en/latest/user/intro.html), [twet](https://github.com/quite/twet) or [yarn.social](https://yarn.social). You participate by stepping outside the walled gardens and seeing the larger \"Internet\".\n\nI think the important thing is to realize the alternatives are already here, you don't need to wait for invention, invitation or permission. You can move beyond the silos today. You don't need to have your attention captured then bought and sold. It's not so much a matter of \"giving up\" a silo but rather stepping outside one and breathing some fresh air.\n\nThings to watch\n---------------\n\n- [Feedland](https://feedland.org)\n- [yarn.social](https://yarn.social) and [twtxt](https://twtxt.readthedocs.io/en/latest/)\n- [Micro.blog](https://micro.blog/)\n- [Mastodon](https://joinmastodon.org/)\n- [Gopherspace](http://gopher.floodgap.com/gopher/gw?a=gopher%3A%2F%2Fgopher.floodgap.com%2F1%2Fworld), see [Gopherspace in 2020](https://cheapskatesguide.org/articles/gopherspace.html) as a nice orientation to see the internet through lynx and text\n- Even [Project Gemini](https://gemini.circumlunar.space/)\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "keywords": [
          "small web",
          "twtxt",
          "micro blogging",
          "social networks"
        ],
        "pubDate": "2022-11-11",
        "title": "Twitter's pending implosion"
      },
      "url": "posts/2022/11/11/Twitter-implosion.json"
    },
    {
      "content": "\n# Initial Impression of Pagefind\n\nBy R. S. Doiel, 2022-11-21\n\nI'm interested in site search that does not require using server side services (e.g. Solr/Elasticsearch/Opensearch). I've used [LunrJS](https://lunrjs.com) on my person blog site for several years.  The challenge with LunrJS is indexes become large and that limits how much your can index and still have a quick loading page. [Pagefind](https://pagefind.app) addresses the large index problem. The search page only downloads the portion of the indexes it needs. The index and search functionality are compiled down to WASM files. This does raise challenges if you're targeting older web browsers.\n\nPagefind is a [rust](https://www.rust-lang.org/) application build using `cargo` and `rustc`. Unlike the documentation on the [Pagefind](https://pagefind.app) website which suggests installing via `npm` and `npx` I recommend installing it from sources using the latest release of cargo/rustic.  For me I found getting the latest cargo/rustc is easiest using [rustup](https://rustup.rs/). Pagefind will not compile using older versions of cargo/rustc (e.g. the example currently available from Mac Ports for M1 Macs).\n\nHere's the steps I took to bring Pagefind up on my M1 Mac.\n\n1. Install cargo/rust using rustup\n2. Make sure `$HOME/.cargo/bin` is in my PATH\n3. Clone the Pagefind Git repository\n4. Change to the repository directory\n5. Build and install pagefind\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\ngit clone git@github.com git@github.com:CloudCannon/pagefind.git src/github.com/CloudCannon/pagefind\ncd src/github.com/CloudCannon/pagefind\ncargo install pagefind --features extended\n```\n\nNext steps were\n\n1. Switch to my local copy of my website\n2. Build my site in the usual page\n3. Update my `search.html` page to use pagefind\n4. Index my site using pagefind\n5. Test my a local web server\n\nTo get the HTML/JavaScript needed to embed pagefind in your search page see [Getting Started](https://pagefind.app/docs/). The HTML/JavaScript fragment is at the top of the page. After updating `search.html` I ran the pagefind command[^1].\n\n```\npagefind --verbose --bundle-dir ./pagefind --source .\n```\n\nThe indexing is wicked fast and it gives you nice details. I verified everything worked as expected using `pttk ws` static site web server. I then published my website. You can see the results at <http://rsdoiel.sdf.org/search.html> and <https://rsdoiel.github.io/search.html>\n\n[^1]: I specified the bundle directory because GitHub pages had a problem with the default `_pagefind`.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-21",
        "keywords": [
          "site search",
          "pagefind",
          "rust",
          "cargo",
          "rustup",
          "M1",
          "macOS"
        ],
        "pubDate": "2022-11-21",
        "title": "Initial Impressions of Pagefind"
      },
      "url": "posts/2022/11/21/initial-impressions-pagefind.json"
    },
    {
      "content": "\nCompiling Pandoc from source\n============================\n\nBy R. S. Doiel, 2022-11-07\n\nI started playing around with Pandoc's __pandoc-server__ last Friday. I want to play with the latest version of Pandoc.  When I gave it a try this weekend I found that my Raspberry Pi 400's SD card was too small. This lead me to giving the build process a try on my Ubuntu desktop. These are my notes about how I going about building from scratch.  I am not a Haskell programmer and don't know the tool chain or language. Take everything that follows with a good dose of salt but this is what I did to get everything up and running. I am following the compile from source instructions in Pandoc's [INSTALL.md](https://github.com/jgm/pandoc/blob/master/INSTALL.md)\n\nI'm running this first on an Intel Ubuntu box because I have the disk space available there. If it works then I'll try it directly on my Raspberry Pi 400 with an upgrade SD card and running the 64bit version of Raspberry Pi OS.\n\nI did not have Haskell or Cabal installed when I started this process.\n\nSteps\n-----\n\n1. Install __stack__ (it will install GHC)\n2. Clone the GitHub repo for [Pandoc](https://github.com/jgm/pandoc)\n3. Setup __stack__ for Pandoc\n4. Build and test with __stack__\n5. Install __stack__ install\n6. Make a symbolic link from __pandoc__ to __pandoc-server__\n\n```\nsudo apt update\nsudo apt search \"haskell-stack\"\nsudo apt install \"haskell-stack\"\nstack upgrade\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup \nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nThis step takes a long time and on the Raspberry Pi it'll take allot longer.\n\nThe final installation of Pandoc was in my `$HOME/.local/bin` directory. Assuming this is early in your path this can allow you to experiment with a different version of Pandoc from the one installed on your system. \n\nI also wanted to try the latest of __pandoc-server__.  This was not automatically installed and is not mentioned in the INSTALL.md file explicitly. But looking at the discussion of running __pandoc-server__ in CGI mode got me thinking. I then checked the installation on my Ubuntu box for the packaged version of pandoc-server and saw that is was a symbolic link.  Adding a `ln` command to my build instruction solved the problem.\n\nI decided to try compiling Pandoc on my M1 mac.  First I needed to get __stack__ installed. I use Mac Ports but it wasn't in the list of available packages.  Fortunately the Haskell Stack website provides a shell script for installation on Unixes. I wanted to install __stack__ in my home `bin` directory not `/usr/bin/slack`. So after reviewing the downloaded install script I found the `-d` option for changing where it installs to. It indicated I need to additional work with __xcode__.\n\n```\ncurl -sSL https://get.haskellstack.org/ > stack-install.sh\nmore stack-install.sh\nsh stack-install.sh -d $HOME/bin\n```\n\nThe __stack__ installation resulted in a message in this form.\n\n```\nStack has been installed to: $HOME/bin/stack\n\nNOTE: You may need to run 'xcode-select --install' and/or\n      'open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg'\n      to set up the Xcode command-line tools, which Stack uses.\n\nWARNING: '$HOME/.local/bin' is not on your PATH.\n    Stack will place the binaries it builds in '$HOME/.local/bin' so\n    for best results, please add it to the beginning of PATH in your profile.\n```\n\nI already had xcode setup for compiling Go so those addition step was not needed.  I only needed to add `$HOME/.local/bin` to my search path.\n\nI then followed the steps I used on my Ubuntu Intel box.\n\n```\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup\nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nNow when I have a chance to update my Raspberry Pi 400 to a suitable sized SD Card (or external drive) I'll be ready to compile a current version of Pandoc from source.\n\nAdditional notes\n----------------\n\n[stack](https://docs.haskellstack.org/en/stable/) is a Haskell build tool. It setups up an Haskell environment per project. If a project requires a specific version of the Haskell compiler it'll be installed and made accessible for the project. In this way it's a bit like having a specific environment for Python. The stack website indicates that it targets cross platform development in Haskell which is nice.  Other features of stack remind me of Go \"go\" command in that it can build things or Rust's \"cargo\" command. Like __cargo__ it can update itself which is nice. That is what I did after installing the Debian package version used by Ubuntu. Configuration of a \"stack\" project uses YAML files. Stack uses __cabal__, Haskell's older build tool but subsumes __cabal-install__ for setting up __cabal__ and __ghc__. It appears from my reading that __stack__ addresses some of the short falls __cabal__ originally had and specifically focusing on reproducible compiles. This is important in sharing code as well as if you want to integrate automated compilation and testing. It maintains a project with \"cabal files\" so there is the ability to work with older non-stack code if I read the documentation correctly. Both __cabal__ and __stack__ seem to be evolving in parallel taking different approaches but influencing one another. Both systems use \"cabal files\" for describing projects and dependencies as of 2022. The short version of [Why Stack](https://docs.haskellstack.org/en/stable/#why-stack) can be found the __stack__ website.\n\n[Hackage](https://hackage.haskell.org/) is a central repository of Haskell packages. \n\n[Stackage](https://www.stackage.org/) is a curated subset of Hackage packages. It appears to be the preferred place for __stack__ to pull from.\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-07",
        "keywords": [
          "pandoc",
          "pandoc-server",
          "pandoc-citeproc",
          "haskell-stack",
          "cabal",
          "ghc"
        ],
        "pubDate": "2022-11-07",
        "title": "Compiling Pandoc from source"
      },
      "url": "posts/2022/11/07/compiling-pandoc-from-source.json"
    },
    {
      "content": "\n\nGetting things setup\n====================\n\nBy R. S. Doiel, 2022-10-09\n\nI'm digging my [gopherhole on sdf.org](gopher://sdf.org:70/0/users/rsdoiel)\nas I wait for my validation to go through.  The plan is to migrate content\nfrom rsdoiel.github.io to here and host it in a Gopher context.  It's\ninteresting learning my way around sdf.org. Reminds me of my student days\nwhen I first had access to a Unix system.  Each Unix has it own flavors and\neven for the same Unix type/version each system has it's own particular\nvariation. Unix lends itself to customization and that why one system can\n\"feel\" or \"look\" different than the next.\n\nI'm trying to remember how to use Pico (vi isn't available yet).\nDiscovering how far \"mkgopher\" can go (very far it turns out).\n\nI'm looking forward to validation so I can have access to Git and\n\"move in\" to this gopherspace in a more sustainable way.\n\nThings to read and do\n---------------------\n\n- wait to be validated\n- learn [gitia](https://git.sdf.org) and setup up a mirror my personal projects and blog\n- read up on [gophernicus](https://www.gophernicus.org/) (the gopher server used by sdf.org)\n- [twenex project](https://www.twenex.org/), sounds interesting,\n  I remember accessing a TOPS-20 system at Whitesands in New Mexico\n  once upon a time.\n- figure out to access comp.lang.oberon if it is available on sdf.org\n- figure out, after validation, if I can compile OBNC for working on\n  Artemis and Oberon-07 code projects\n\nUpdates\n-------\n\n7:30 - 7:30; Gopher; Setup; Account verified, Yippee!\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-10-09",
        "keywords": [
          "gopher",
          "public unix"
        ],
        "pubDate": "2022-10-09",
        "title": "Getting things setup"
      },
      "url": "posts/2022/10/09/getting-things-setup.json"
    },
    {
      "content": "\n# Progress and time remaining\n\nBy R. S. Doiel, 2022-11-05\n\nI often find myself logging output when I'm developing tools.  This is typically the case where I am iterating over data and transforming it. Overtime I've come to realize I really want a few specific pieces of information for non-error logging (e.g. `-verbose` which monitors progress as well as errors).\n\n- percentage completed\n- estimated time allocated (i.e. time remaining)\n\nTo do that I need three pieces of information.\n\n1. the count of the current iteration(e.g. `i`)\n2. the total number of iterations required (e.g. `tot`)\n3. The time just before I started iterating(e.g. `t0`)\n\nThe values for `i` and `tot` let me compute the percent completed. The percent completed is trivial `(i/tot) * 100.0`. Note on the first pass (i.e. `i == 0`) you can skip the percentage calculation.\n\n\n```golang\nimport (\n\t\"time\"\n\t\"fmt\"\n)\n\n// Show progress with amount of time running\nfunc progress(t0 time.Time, i int, tot int) string {\n    if i == 0 {\n        return \"\"\n    }\n\tpercent := (float64(i) / float64(tot)) * 100.0\n\tt1 := time.Now()\n\t// NOTE: Truncating the duration to seconds\n\treturn fmt.Sprintf(\"%.2f%% %v\", percent, t1.Sub(t0).Truncate(time.Second))\n}\n```\n\nHere's how you might use it.\n\n```golang\n\ttot := len(ids)\n\tt0 := time.Now()\n\tfor i, id := range ids {\n\t\t// ... processing stuff here ... and display progress every 1000 records\n\t\tif (i % 1000) == 0 {\n\t\t\tlog.Printf(\"%s records processed\", progress(t0, i, tot))\n\t\t}\n\t}\n```\n\nAn improvement on this is to include an time remaining. I need to calculated the estimated time allocation (i.e. ETA). I know `t0` so I can estimate that with this formula `estimated time allocation = (((current running time since t0)/ the number of items processed) * total number of items)`[^1]. ETA adjusted for time running gives us time remaining[^2]. The first pass of the function progress has a trivial optimization since we don't have enough delta t0 to compute an estimate. Calls after that are computed using our formula.\n\n[^1]: In code `(rt/i)*tot` is estimated time allocation\n\n[^2]: Estimated Time Remaining, in code `((rt/i)*tot) - rt`\n\n```golang\nfunc progress(t0 time.Time, i int, tot int) string {\n\tif i == 0 {\n\t\treturn \"0.00 ETR Unknown\"\n\t}\n\t// percent completed\n\tpercent := (float64(i) / float64(tot)) * 100.0\n\t// running time\n    rt := time.Now().Sub(t0)\n    // estimated time allocation - running time = time remaining\n    eta := time.Duration((float64(rt)/float64(i)*float64(tot)) - float64(rt))\n    return fmt.Sprintf(\"%.2f%% ETR %v\", percent, eta.Truncate(time.Second))\n}\n```\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "programming",
          "golang",
          "log info"
        ],
        "pubDate": "2022-12-05",
        "title": "Progress and time remaining"
      },
      "url": "posts/2022/12/05/progress-and-time-remaining.json"
    },
    {
      "content": "\n# Go and MySQL timestamps\n\nBy R. S. Doiel, 2022-12-12\n\nThe Go [sql](https://pkg.go.dev/database/sql) package provides a nice abstraction for working with SQL databases. The underlying drivers and DBMS can present some quirks that are SQL dialect and driver specific such as the [MySQL driver](github.com/go-sql-driver/mysql).  Sometimes that is not a big deal. [MySQL](https://dev.mysql.com) can maintain a creation timestamp as well as a modified timestamp easily via the SQL schema definition for the field. Unfortunately if you need to work with the MySQL timestamp at a Go level (e.g. display the timestamp in a useful way) the int64 provided via the driver isn't compatible with the `int64` used in Go's `time.Time`. To work around this limitation I've found it necessary to convert the MySQL timestamp to a formatted string using [DATE_FORMAT](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_date-format \"DATE_FORMAT is a MySQL date/time function returning a string value\") and from the Go side convert the formatted string into a `time.Time` using `time.Parse()`. Below is some Golang pseudo code showing this approach.\n\n```\n// Format used by MySQL strings representing date/times\nconst MySQLTimestamp = \"2006-01-02 15:04:05\"\n\n// GetRecordUpdate takes a configuration with a db attribute previously\n// opened and an id string returning a record populated with id and updated values where updated is an attribute of type time.Time. We use MySQL's\n// `DATE_FORMAT()` function to convert the timestamp into a string and\n// Go's `time.Parse()` to convert the string into a `time.Time` value.\nfunc GetRecordUpdate(cfg, id string) {\n\tstmt := `SELECT id, DATE_FORMAT(updated, \"%Y-%m-%d %H:%i:%s\") FROM some_tabl WHERE id = ?`\n\trow, err := cfg.db.Query(stmt, id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer row.Close()\n\trecord := new(Record)\n\tif row.Next() {\n\t\tvar updated string\n\t\tif err := row.Scan(&record.ID, &updated); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trecord.Updated, err = time.Parse(MySQLTimestamp, updated)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\terr = row.Err()\n\treturn record, err\n}\n```\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "golang",
          "sql",
          "timestamps"
        ],
        "pubDate": "2022-12-12",
        "title": "Go and MySQL timestamps"
      },
      "url": "posts/2022/12/12/Go-and-MySQL-Timestamps.json"
    },
    {
      "content": "\n# SQL query to CSV, a missing datatool\n\nBy R. S. Doiel, 2023-01-13\n\nUpdate: 2023-03-13\n\nAt work we maintain allot of metadata related academic and research publications in SQL databases. We use SQL to query the database and export what we need in tab delimited files. Often the exported data includes a column containing publication or article titles.  Titles in library metadata can be a bit messy. They contain a wide set of UTF-8 characters include math symbols and various types of quotation marks. The exported tab delimited data usually needs clean up before you can import it successfully into a spreadsheet.\n\nIn the worst cases we debug what the problem is then write a Python script to handle the tweak to fix things.  This results in allot of extra work and slows down the turn around for getting reports out quickly. This is particularly true of data stored in MySQL 8 (though we also use SQLite 3 and Postgres).\n\nThis got me thinking about how to get a clean export (tab or CSV) from our SQL databases today.  It would be nice if you provided a command line tool with a data source string (e.g. in a config file or the environment), a SQL query and the tool would use that to render a CSV or tab delimited file to standard out or a output file. It would work something like this.\n\n```\n    sql2csv -o eprint_status_report.csv -config=$HOME/.my.cnf \\\n\t    'SELECT eprintid, title, eprint_status FROM eprint' \n```\n\nThe `sql2csv` would take the results of the query and write to the CSV file.\n\nThe nice thing about this approach is that I could support the three relational databases we use -- i.e. MySQL 8, Postgres and SQLite3 with one common tool so my Bash scripts that run the reports would be very simple rather than specialized to one database system or the other.\n\nI hope to experiment with this approach in the next release of [datatools](https://github.com/caltechlibrary/datatools), an open source project maintained at work.\n\n## update\n\nJon Woodring pointed out to me today that both SQLite3 and PostgreSQL clients can output to CSV without need of an external tool. Wish MySQL client did that! Instead MySQL client supports tab delimited output. I'm still concidering sql2csv due to the ammount work I do with MySQL database but I'm not sure if it will make it into to the datatools project or now since I suspect our MySQL usage will decline overtime as more projects are built with PostgreSQL and SQLite3.\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "sql",
          "csv",
          "tab delimited"
        ],
        "pubDate": "2023-01-03",
        "title": "SQL query to CSV, a missing datatool",
        "updateDate": "2023-03-13"
      },
      "url": "posts/2023/01/03/sql-to-csv-a-missing-datatool.json"
    }
  ]
}