{
  "page": 1,
  "total_pages": 1,
  "has_more": false,
  "next_page": null,
  "values": [
    {
      "content": "\nPttk and STN\n============\n\nBy R. S. Doiel, started 2022-08-15\n(updated: 2022-09-26, pdtk was renamed pttk)\n\nThis log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the \"blogit\" action that will translates the simple timesheet notation (aka STN) to a short blog post.  My \"short post\" interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.\n\n2022-08-15\n\n16:45 - 17:45; Golang; ptdk, stngo; Thinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n2022-08-14\n\n14:00 - 17:00; Golang; pdtk, stngo; Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n2022-08-12\n\n16:00 - 16:30; Golang; stngo; A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n2022-09-26\n\n6:30 - 7:30; Golang; pttk; renamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2022-08-15",
        "pubDate": "2022-08-15",
        "title": "PTTK and STN",
        "updated": "2022-09-26"
      },
      "url": "posts/2022/08/15/golang-development.json"
    },
    {
      "content": "\n# Building Lagrange on Raspberry Pi OS\n\nThese are my quick notes on building the Lagrange Gemini browser on Raspberry Pi OS. They are based on instructions I found at <gemini://home.gegeweb.org/install_lagrange_linux.gmi>. These are in French and I don't speak or read French. My loss. The author kindly provided the specific command sequence in shell that I could read those. That was all I needed. When I read the site today I had to click through an expired certificate. That's why I think it is a good idea to capture the instructions here for the next time I need them.  I made single change to his instructions. I have cloned the repository from <https://github.com/skyjake/lagrange>.\n\n## Steps to build\n\n1. Install the programs and libraries in Raspberry Pi OS to build Lagrange\n2. Create a directory to hold the repository, then change into it\n3. Clone the repository\n4. Add a \"build\" directory to the repository and change into it\n5. Run \"cmake\" to build the release\n6. Run \"make\" in the build directory and install\n7. Test it out.\n\nWhen you clone the repository you want to clone recursively and get the release branch. Below is a transcript of the commands I typed in my shell to build Lagrange on my Raspberry Pi 4.\n\n~~~shell\nsudo apt install build-essential cmake \\\n     libsdl2-dev libssl-dev libpcre3-dev \\\n     zlib1g-dev libunistring-dev git\nmkdir -p src/github.com/skyjake && cd src/github.com/skyjake \ngit clone --recursive --branch release git@github.com:skyjake/lagrange.git\nmkdir -p lagrange/build && lagrange/build\ncmake ../ -DCMAKE_BUILD_TYPE=Release\nsudo make install\nlagrange &\n~~~\n\nThat's about it. It worked without a hitch. I'd like to thank GÃ©rald Niel who I think created the page on gegeweb.org. I attempted to leave a thank you via the web form but couldn't get past the spam screener since I didn't understand the instructions. C'est la vie.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-05-10",
        "pubDate": "2024-05-10",
        "title": "Building Lagrange on Raspberry Pi OS"
      },
      "url": "posts/2024/05/10/building-lagrange-on-pi-os.json"
    },
    {
      "content": "\n# A Text oriented web\n\nBy R. S. Doiel, 2024-02-25\n\nThe web is a busy place. There seems to be a gestalt resonant at the moment on the web that can be summarized by two phrases, \"back to basics\" and \"simplification\". It is not the first time I've seen this nor is it likely the last. This blog post describes a thought experiment about a simplification with minimal invention and focus on feature elimination. It's a way to think about the web status quo a little differently. My intention is to explore the implications of a more text centered web experience that could coexist as a subset of today's web.\n\n## The web's \"good stuff\"\n\nI think the following could form the \"good stuff\" in a Crockford[^1] sense of pairing things down to the essential.\n\n- the transport layer should remain HTTP but be limited to a few methods (GET, POST and HEAD) and the common header elements (e.g. length, content-type come to mind)\n- The trio of HTML, CSS and JavaScript is really complex, swap this out for Markdown augmented with YAML (Markdown and YAML already have a synergy in Markdown processors like Pandoc)\n- A Web form is expressed using GHYITS[^2], it is delimited in the Markdown document by the familiar \"`^---$`\" block element, web form content would be encoded as YAML in the body of the POST using the content type \"`application/x-yaml`\".\n- Content would be served using the `text/markdown; charset: utf-8` content type already commonly used to identify Markdown content distinct from `plain/text`\n\nI need a nice name for describing the arrangement of Markdown+YAML over HTTP arrangement. Is the descriptive acronym for \"text oriented web\", i.e. \"tow\", enough? Does it already have a meaning in software or the web? Would the protocol be \"tow://\"? I really need something a bit more clever and catchy if this is going to proceed beyond a thought experiment.\n\n[^1]: Douglas Crockford \"discovered\" JSON, see <https://en.wikipedia.org/wiki/Douglas_Crockford>\n\n[^2]: GHYITS, acronym, GitHub YAML Issue Template Syntax, see <https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/syntax-for-issue-forms>\n\n## Prototyping Options\n\nA proof of concept could be possible using off the self web server and web browser. The missing parts would be setting up the web server to add the `text/markdown; charset: utf-8` header for \"`.md`\" files and to handle processing POST with a content type of `application/x-yaml`. Client side could be implemented in a static web page via JavaScript or WASM module. The JS/WASM could convert the Markdown+YAML into HTML rendering the \"tow\" content. The web form on submit would be intercepted by the JavaScript event handler and reformulated as a POST with a content type of `application/x-yaml`.\n\nBuilding a \"tow\" server and client should be straight forward in Go (probably many languages). The the standard \"http\" package can be used to implement the specialized http server. The `yaml.v3` package to process the YAML POST data. Similar you should be able to create a text client for the command line or even a GUI client via [Fyne](https://fyne.io)\n\n## Exploratory Questions\n\n- What does it mean to have a more text oriented web?\n- What advantages could a text user interface have over a graphical user interface?\n- Can \"tow\" provide enough simple interactivity to support interactive fiction?\n- Could a simple specification be stated clearly in a few pages of text?\n- What possibilities open up when a web browser can send a data structure via YAML to a service?\n- Can we live with a simpler client than a modern evergreen web browser?\n- With a conversation interaction model of \"listener\" and a \"speaker\", does it make sense thinking in terms of client server architecture?\n- How hard is it to support both traditional website and this minimal \"tow\" site using the same corpus?\n- Can this be done sustainably?\n\n## Extrapolations\n\nFrom a thought experiment I can see how to implement this both from a proof of concept level but also from a service and viewer level. I think it even offers an opportunity to function in a peer to peer manner.  If we're focusing primarily on text then the storage requirements can be minimal and the service could even live in a database system like SQLite3 as a form of sandbox of content.  Leveraging HTTP/HTTPS means we don't need any special support for content traveling across the net. With a much smaller foot print you can scratch the itch of a simpler textual experience without the trackers, JavaScript ping backs, etc. It could re-emphasize the conversion versus broadcast metaphor popularized by the walled gardens.  It might provide a more satisifying experience on Mobile since the payloads delivered to the web browser could be much smaller.\n\n## What is needed to demonstrate a standalone \"tow\"?\n\n- A modified HTTP web server (easy to implement in Go and other languages)\n- A viewer/browser, possible to implement via Fyne in Go or as a text application/command line interface in Go\n\n## Why not Gopher or Gemini?\n\nTow is not there to replace anything, not Gopher, Not Gemini, the WWW. It is an exploration of a subset of the WWW protocols with a specific focused on textual interaction. I don't see why a server or browser couldn't support Gopher and Gemini as well as Tow. Given that Markdown can easily be rendered into Gem Text, and Markdown can be treated as plain text I suspect you should be able to support all three text rich systems from the same copy and easily derive a full HTML results if desired too.\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-02-25",
        "createdDate": "2024-02-25",
        "keywords": [
          "web",
          "text"
        ],
        "pubDate": "2024-02-25",
        "title": "A Text Oriented Web"
      },
      "url": "posts/2024/02/25/text_oriented_web.json"
    },
    {
      "content": "\n# Two missing features from HTML5, an enhanced form.enctype and a list input type\n\n## Robert's wish list for browsers and web forms handling\n\nBy R. S. Doiel, 2024-02-23\n\nI wish the form element supported a `application/json` encoding type and there was such a thing as a `list-input` element.\n\nI've been thinking about how we can get back to basic HTML documents and move away from JavaScript required to render richer web forms. When web forms arrived on scene in the early 1990s they included a few basic input types. Over the years a few have been added but by and large the data model has remained relatively flat. The exception being the select element with `multiple` attribute set. I believe we are being limited by the original choice of urlencoding web forms and then resort to JavaScript to address it's limitations.\n\nWhat does the encoding of a web form actually look like?  The web generally encodes the form using urlencoding. It presents a stream of key value pairs where the keys are the form's input names and the values are the value of the input element. With a multi-select element the browser simply repeats the key and adds the next value in the selection list to that key.  In Go you can describe this simple data structure as a `map[string][]string`[^1]. Most of the time a key points to a single element array of string but sometimes it can have multiple elements using that key and then the array expands to accommodate. Most of the time we don't think about this as web developers. The library provided with your programming language decodes the form into a more programmer friendly representation. But still I believe this simple urlencoding has held us back. Let me illustrate the problem through a series of simple form examples.\n\n[^1]: In English this could be described as \"a map using a string to point at a list of strings\" with \"string\" being a sequence of letters or characters.\n\nHere's an example of a simple form with a multi select box. It is asking for your choice of ice cream flavors.\n\n~~~html\n<form method=\"POST\">\n  <label for=\"ice-cream-flavors\">Choose your ice cream flavors:</label>\n  <select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple >\n    <option value=\"Chocolate\">Chocolate</option>\n    <option value=\"Coconut\">Cocunut</option>\n    <option value=\"Mint\">Mint</option>\n    <option value=\"Strawberry\">Strawberry</option>\n    <option value=\"Vanilla\">Vanilla</option>\n    <option value=\"Banana\">Banana</option>\n    <option value=\"Peanut\">Peanut</option>\n  </select>\n  <p>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nBy default your web browser will packaged this up and send it using \"application/x-www-form-urlencoded\". If you select \"Coconut\" and \"Strawberry\" then the service receiving your data will get an encoded document that looks like this.\n\n~~~urlencoding\nice-cream-flavors=Coconut&ice-cream-flavors=Strawberry\n~~~\n\nThe ampersands separate the key value pairs. The fact that \"ice-cream-flavors\" name repeats means that the key \"ice-cream-flavors\" will point to an array of values.  In pretty printed JSON representation is a little clearer.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Coconut\", \"Strawberry\" ]\n}\n~~~\n\nSo far so good. Zero need to enhance the spec. It works and has worked for a very long time. Stability is a good thing. Let's elaborate a little further.  I've added a dish choice for the ice cream, \"Sugar Cone\" and \"Waffle Bowl\". That web form looks like.\n\n~~~html\n<form method=\"POST\">\n<label for=\"ice-cream-flavors\">Select the flavor for each scoop of ice cream:</label>\n<select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple>\n  <option value=\"Chocolate\">Chocolate</option>\n  <option value=\"Coconut\">Cocunut</option>\n  <option value=\"Mint\">Mint</option>\n  <option value=\"Strawberry\">Strawberry</option>\n  <option value=\"Vanilla\">Vanilla</option>\n  <option value=\"Banana\">Banana</option>\n  <option value=\"Peanut\">Peanut</option>\n</select>\n<p>\n<fieldset>\n  <legend>Pick your delivery dish</legend>\n  <div>\n    <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n    <label for=\"sugar-cone\">Sugar Cone</label>\n  </div>\n  <div>\n    <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n    <label for=\"waffle-bowl\">Waffle Bowl</label>\n  </div>\n</fieldset>\n<input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nIf we select \"Banana\" and \"Peanut\" flavors served in a \"Waffle Bowl\" the encoded document would reach the web service looking something like this.\n\n~~~urlencoded\nice-cream-flavors=Banana&ice-cream-flavors=Peanut&ice-cream-dish=waffle-cone\n~~~\n\nThat's not too bad. Again this is the state of web form for ages now. In JSON it could be represented as the following.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Banana\", \"Peanut\" ],\n    \"ice-cream-dish\": \"waffle-cone\"\n}\n~~~\n\nThis is great we have a simple web form that can collect a single ice cream order.  But what if we want to actually place several individual ice cream orders as one order? Today we have two choices, multiple web forms that accumulate the orders (circa 2000) or use JavaScript create a web UI that can handle list of form elements. Both have their drawbacks.\n\nIn the case of the old school approach changing web pages just to update an order can be slow and increase uncertainty about your current order. That is why the JavaScript approach has come to be more common. But that JavaScript approach comes at a huge price. It's much more complex, we've seen a dozens of libraries and frameworks that have come and gone trying to manage that complexity in various ways.\n\nIf we supported JSON encoded from submission directly in the web browser I think we'd make a huge step forward. It could decouple the JavaScript requirement. That would avoid much of the cruft that we ship down to the web browser today because we can't manage lists of things without resorting to JavaScript.\n\nLet's pretend there was a new input element type called \"list-input\". A \"list-input\" element can contain any combination of today's basic form elements. Here's my hypothetical `list-input` based from example. In it we're going to select the ice cream flavors and the dish format (cone, bowl) as before but have them accumulate in a list. That form could be expressed in HTML similar to my mock up below.\n\n~~~html\n<form>\n  <label for=\"ice-cream-order\">Place your next order, press submit when you have all of them.</label>\n  <list-input id=\"ice-cream-order\" name=\"ice-cream-order\">\n    <label for=\"ice-cream-flavor\">Select the flavor for each scoop of ice cream:</label>\n    <select id=\"ice-cream-flavor\" name=\"ice-cream-flavor\" multiple>\n      <option value=\"Chocolate\">Chocolate</option>\n      <option value=\"Coconut\">Cocunut</option>\n      <option value=\"Mint\">Mint</option>\n      <option value=\"Strawberry\">Strawberry</option>\n      <option value=\"Vanilla\">Vanilla</option>\n      <option value=\"Banana\">Banana</option>\n      <option value=\"Peanut\">Peanut</option>\n    </select>\n  <p>\n  <fieldset>\n    <legend>Pick your delivery dish</legend>\n    <div>\n      <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n      <label for=\"sugar-cone\">Sugar Cone</label>\n    </div>\n    <div>\n      <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n      <label for=\"waffle-bowl\">Waffle Bowl</label>\n    </div>\n  </fieldset>\n  </list-input>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nWith two additional lines of HTML the input form can now support a list of individual ice cream orders. Assuming only urlencoding is supported then how does that get encoded and sent to the web server? Here is an example set of orders\n\n1. vanilla ice cream with a sugar cone\n2. chocolate with a waffle bowl\n\n~~~urlencoded\nice-cream-flavors=Vanilla&ice-cream-flavors=Chocolate&ice-cream-dish=sugar-cone&ice-cream-dish=waffle-bowl\n~~~\n\nWhich flavor goes with which dish?  That's the problem with urlencoding a list in your web form. We just can't keep the data alignment manageable.  What if the web browser used JSON encoding? \n\n~~~json\n[\n  {\n      \"ice-cream-flavors\": [ \"Vanilla\" ],\n      \"ice-cream-dish\": \"sugar-cone\"\n  },\n  {\n      \"ice-cream-flavors\": [ \"Chocolate\" ],\n      \"ice-cream-dish\": \"waffle-bowl\"\n  }\n~~~\n\nSuddenly the alignment problem goes away. There is precedence for controlling behavior of the web browser submission through the `enctype` attribute. File upload was addressed by adding support for `multipart/form-data`.  In 2024 and for over the last decade it has been common practice in web services to support JSON data submission. I believe it is time that the web browser also supports this directly. This would allow us to decouple the necessity of using JavaScript in browser as we require today. The form elements already map well to a JSON encoding. If JSON encoding was enabled then adding a element like my \"list-input\" would make sense.  Otherwise we remain stuck in a world where hypertext markup language remains very limited and can't live without JavaScript.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "html",
          "web forms",
          "encoding"
        ],
        "title": "Two missing features from HTML5, an enhanced form.enctype and a list input type"
      },
      "url": "posts/2024/02/23/enhanced_form_handling.json"
    },
    {
      "content": "\n# Updated recipe, compile PostgREST 12.0.2 (M1)\n\nby R. S. Doiel, 2024-01-04\n\nThese are my updated \"quick notes\" for compiling PostgREST v12.0.2 on a M1 Mac Mini using the current recommended\nversions of ghc, cabal and stack supplied [GHCup](https://www.haskell.org/ghcup).  When I recently tried to use\nmy previous [quick recipe](/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.md) I was disappointed it failed with errors like \n\n~~~\nResolving dependencies...\nError: cabal: Could not resolve dependencies:\n[__0] trying: postgrest-9.0.1 (user goal)\n[__1] next goal: optparse-applicative (dependency of postgrest)\n[__1] rejecting: optparse-applicative-0.18.1.0 (conflict: postgrest =>\noptparse-applicative>=0.13 && <0.17)\n[__1] skipping: optparse-applicative-0.17.1.0, optparse-applicative-0.17.0.0\n(has the same characteristics that caused the previous version to fail:\nexcluded by constraint '>=0.13 && <0.17' from 'postgrest')\n[__1] trying: optparse-applicative-0.16.1.0\n[__2] next goal: directory (dependency of postgrest)\n[__2] rejecting: directory-1.3.7.1/installed-1.3.7.1 (conflict: postgrest =>\nbase>=4.9 && <4.16, directory => base==4.17.2.1/installed-4.17.2.1)\n[__2] trying: directory-1.3.8.2\n[__3] next goal: base (dependency of postgrest)\n[__3] rejecting: base-4.17.2.1/installed-4.17.2.1 (conflict: postgrest =>\nbase>=4.9 && <4.16)\n\n...\n\n~~~\n\nSo this type of output means GHC and Cabal are not finding the versions of things they need\nto compile PostgREST. I then tried picking ghc 9.2.8 since the default.nix file indicated\na minimum of ghc 9.2.4.  The `ghcup tui` makes it easy to grab a listed version then set it\nas the active one.\n\nI made sure I was working in the v12.0.2 tagged release version of the Git repo for PostgREST.\nThen ran the usual suspects for compiling the project. I really wish PostgREST came with \ninstall from source documentation. It took me a while to think about looking in the default.nix\nfile for a minimum GHC version. That's why I am writing this update.\n\nA similar recipe can be used for building PostgREST on Linux.\n\n1. Upgrade [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)\n    a. Use the curl example command to install it or `gchup upgrade`\n    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)\n2. Make sure GHCup is pointing at the version PostgREST v12.0.2 needs, i.e. ghc v9.2.8. I chose to keep \"recommended\" versions of Cabal and Stack\n3. Clone <https://github.com/PostgREST/postgrest> to my local machine\n4. Check out the version you want to build, i.e. v12.0.2\n5. Run the \"usual\" Haskell build sequence with cabal\n    a. `cabal clean`\n    b. `cabal update`\n    c. `cabal build`\n    d. `cabal install` (I use the `--overwrite-policy=always` option to overwrite my old v11 postgrest install)\n\nHere's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).\n\n~~~\nghcup upgrade\nghcup tui\nmkdir -p src/github.com/PostgREST\ncd src/github.com/PostgREST\ngit clone git@github.com:PostgREST/postgrest\ncd postgrest\ngit checkout v12.0.2\ncabal clean\ncabal update\ncabal build\ncabal install --overwrite-policy=always\n~~~\n\nThis will install PostgREST in your `$HOME/.cabal/bin` directory. Make sure\nit is in your path (it should be if you've sourced the GHCup environment after you installed GHCup).\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "PostgREST",
          "M1"
        ],
        "pubDate": "2024-01-04",
        "title": "Updated recipe, compiling PostgREST 12.0.2 (M1)"
      },
      "url": "posts/2024/01/04/updated-recipe-compiling-postgrest_v12.0.2.json"
    },
    {
      "content": "\n# gsettings command\n\nOne of the things I find annoying about Ubuntu Desktop defaults is that when I open a new application it opens in the upper left corner. I then drag it to the center screen and start working. It's amazing how a small inconvenience can grind on you over time.  When I've search the net for changing this behavior the usual suggestions are \"install gnome-tweaks\". This seems ham-handed. I think continue searching and eventually find the command below. So I am making a note of the command here in my blog so I can find it latter.\n\n~~~\ngsettings set org.gnome.mutter center-new-window true\n~~~\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "Ubuntu Desktop",
          "Gnome",
          "gsettings"
        ],
        "pubDate": "2023-05-20",
        "title": "gsettings command"
      },
      "url": "posts/2023/05/20/gsettings-commands.json"
    },
    {
      "content": "\n# Quick recipe, compile Pandoc (M1)\n\nThese are my quick notes for compiling Pandoc on a M1 Mac Mini. I use a similar recipe for building Pandoc on Linux (NOTE: the challenges with libiconv and Mac Ports' libiconv below if you get a build error).\n\n1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)\n    a. Use the curl example command to install it\n    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)\n2. Make sure GHCup is pointing at the \"recommended\" versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)\n3. Clone <https://github.com/jgm/pandoc> to your local machine\n4. Check out the version you want to build (e.g. 3.1.4)\n5. Run the \"usual\" Haskell build sequence with cabal per Pandoc's installation documentation for building from source\n    a. `cabal clean`\n    b. `cabal update`\n    c. `cabal install pandoc-cli`\n\nHere's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).\n\n~~~\ncurl --proto '=https' --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh\nsource $HOME/.gchup/env\nghcup tui\nmkdir -p src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\ngit clone git@github.com:jgm/pandoc\ncd pandoc\ngit checkout 3.1.4\ncabal clean\ncabal update\ncabal install pandoc-cli\n~~~\n\nThis will install Pandoc in your `$HOME/.cabal/bin` directory. Make sure\nit is in your path (it should be if you've sourced the GHCup environment after you installed GHCup).\n\n## libiconv compile issues\n\nIf you use Mac Ports it can confuse Cabal/Haskell which one to link to. You'll get an error talking about undefined symbols and iconv.  To get a clean compile I've typically worked around this issue by removing the Mac Ports installed libiconv temporarily (e.g. `sudo port uninstall libiconv`, an using the \"all\" option when prompted).  After I've got a clean install of Pandoc then I re-install libiconv for those Ports based applications that need it. Putting libiconv back is important, as Mac Ports version of Git expects it.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "Pandoc",
          "GHCup",
          "M1"
        ],
        "pubDate": "2023-07-05",
        "title": "Quick recipe, compiling Pandoc (M1)"
      },
      "url": "posts/2023/07/05/quick-recipe-compiling-Pandoc-M1.json"
    },
    {
      "content": "\n# Quick recipe, compile PostgREST (M1)\n\nThese are my quick notes for compiling PostgREST on a M1 Mac Mini. I use a similar recipe for building PostgREST on Linux.\n\n1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)\n    a. Use the curl example command to install it\n    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)\n2. Make sure GHCup is pointing at the \"recommended\" versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)\n3. Clone <https://github.com/PostgREST/postgrest> to your local machine\n4. Check out the version you want to build (e.g. v11.1.0)\n5. Run the \"usual\" Haskell build sequence with cabal\n    a. `cabal clean`\n    b. `cabal update`\n    c. `cabal build`\n    d. `cabal install`\n\nHere's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).\n\n~~~\ncurl --proto '=https' --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh\nsource $HOME/.gchup/env\nghcup tui\nmkdir -p src/github.com/PostgREST\ncd src/github.com/PostgREST\ngit clone git@github.com:PostgREST/postgrest\ncd postgrest\ncabal clean\ncabal update\ncabal build\ncabal install\n~~~\n\nThis will install PostgREST in your `$HOME/.cabal/bin` directory. Make sure\nit is in your path (it should be if you've sourced the GHCup environment after you installed GHCup).\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "PostgREST",
          "M1"
        ],
        "pubDate": "2023-07-05",
        "title": "Quick recipe, compiling PostgREST (M1)"
      },
      "url": "posts/2023/07/05/quick-recipe-compiling-PostgREST-M1.json"
    },
    {
      "content": "\n# First Personal Search Engine Prototype\n\nBy R. S. Doiel, 2023-08-10\n\nI've implemented a first prototype of my personal search engine which\nI will abbreviate as \"pse\" from here on out. I implemented it using \nthree [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) scripts\nrelying on [sqlite3](https://sqlite.org), [wget](https://en.wikipedia.org/wiki/Wget) and [PageFind](https://pagefind.app) to do the heavy lifting.\n\nBoth Firefox and newsboat store useful information in sqlite databases.  Firefox's `moz_places.sqlite` holds both all the URLs visited as well as those that are associated with bookmarks (i.e. the SQLite database `moz_bookmarks.sqlite`).  I had about 2000 bookmarks, less than I thought with many being stale from link rot. Stale page URLs really slow down the harvest process because of the need for wget to wait on various timeouts (e.g. DNS, server response, download times).  The \"history\" URLs would make an interesting collection to spider but you'd probably want to have an exclude list (e.g. there's no point in saving queries to search engines, web mail, shopping sites). Exploring that will wait for another prototype.\n\nThe `cache.db` associated with Newsboat provided a rich resource of content and much fewer stale links (not surprising because I maintain that URL list more much activity then reviewing my bookmarks).  Between the two I had 16,000 pages. I used SQLite 3 to query the url values from the various DB into sorting for unique URLs into a single text file one URL per line.\n\nThe next thing after creating a list of pages I wanted to search was to download them into a directory using wget.  Wget has many options, I choose to enable timestamping, create a protocol directory and then a domain and path directory for each item. This has the advantage of being able to transform the Path into a URL later.\n\nOnce the content was harvested I then used PageFind to index the all the harvested content. Since I started using PageFind originally the tool has gained an option called `--serve` which provides a localhost web service on port 1414.  All I needed to do was add an index.html file to the directory where I harvested the content and saved the PageFind indexes. Then I used PageFind to again to provide a localhost based personal search engine.\n\nWhile the total number of pages was small (16k pages) I did find interesting results just trying out random words. This makes the prototype look promising.\n\n## Current prototype components\n\nI have simple Bash script that gets the URLs from both Firefox bookmarks and Newsboat's cache then generates a single text file of unique URLs I've named \"pages.txt\".\n\nI then use the \"pages.txt\" file to harvest content with wget into a tree structure like \n\n- htdocs\n    - http (all the http based URLs I harvest go in here)\n    - https (all the https based URLs I harvest go in here)\n    - pagefind (this holds the PageFind indexes and JavaScript to implement the search UI)\n    - index.html (this holds the webpage for the search UI using the libraries in `pagefind`)\n\nSince I'm only downloaded the HTML the 16k pages does not take up significant disk space yet.\n\n## Prototype Implementation\n\nHere's the bash scripts I use to get the URLs, harvest content and launch my localhost search engine based on PageFind.\n\nGet the URLs I want to be searchable. I use to environment variables\nfor finding the various SQLite 3 databases (i.e. PSE_MOZ_PLACES, PSE_NEWSBOAT).\n\n~~~\n#!/bin/bash\n\nif [ \"$PSE_MOZ_PLACES\" = \"\" ]; then\n    printf \"the PSE_MOZ_PLACES environment variable is not set.\"\n    exit 1\nfi\nif [ \"$PSE_NEWSBOAT\" = \"\" ]; then\n    printf \"the PSE_NEWSBOAT environment variable is not set.\"\n    exit 1\nfi\n\nsqlite3 \"$PSE_MOZ_PLACES\" \\\n    'SELECT moz_places.url AS url FROM moz_bookmarks JOIN moz_places ON moz_bookmarks.fk = moz_places.id WHERE moz_bookmarks.type = 1 AND moz_bookmarks.fk IS NOT NULL' \\\n    >moz_places.txt\nsqlite3 \"$PSE_NEWSBOAT\" 'SELECT url FROM rss_item' >newsboat.txt\ncat moz_places.txt newsboat.txt |\n    grep -E '^(http|https):' |\n    grep -v '://127.0.' |\n    grep -v '://192.' |\n    grep -v 'view-source:' |\n    sort -u >pages.txt\n~~~\n\nThe next step is to have the pages. I use wget for that.\n\n~~~\n#!/bin/bash\n#\nif [ ! -f \"pages.txt\" ]; then\n    echo \"missing pages.txt, skipping harvest\"\n    exit 1\nfi\necho \"Output is logged to pages.log\"\nwget --input-file pages.txt \\\n    --timestamping \\\n    --append-output pages.log \\\n    --directory-prefix htdocs \\\n    --max-redirect=5 \\\n    --force-directories \\\n    --protocol-directories \\\n    --convert-links \\\n    --no-cache --no-cookies\n~~~\n\nFinally I have a bash script that generates the index.html page, an Open Search Description XML file, indexes the harvested sites and launches PageFind in server mode.\n\n~~~\n#!/bin/bash\nmkdir -p htdocs\n\ncat <<OPENSEARCH_XML >htdocs/pse.osdx\n<OpenSearchDescription xmlns=\"http://a9.com/-/spec/opensearch/1.1/\"\n                       xmlns:moz=\"http://www.mozilla.org/2006/browser/search/\">\n  <ShortName>PSE</ShortName>\n  <Description>A Personal Search Engine implemented via wget and PageFind</Description>\n  <InputEncoding>UTF-8</InputEncoding>\n  <Url rel=\"self\" type=\"text/html\" method=\"get\" template=\"http://localhost:1414/index.html?q={searchTerms}\" />\n  <moz:SearchForm>http://localhost:1414/index.html</moz:SearchForm>\n</OpenSearchDescription>\nOPENSEARCH_XML\n\ncat <<HTML >htdocs/index.html\n<html>\n<head>\n<link\n  rel=\"search\"\n  type=\"application/opensearchdescription+xml\"\n  title=\"A Personal Search Engine\"\n  href=\"http://localhost:1414/pse.osdx\" />\n<link href=\"/pagefind/pagefind-ui.css\" rel=\"stylesheet\">\n</head>\n<body>\n<h1>A personal search engine</h1>\n<div id=\"search\"></div>\n<script src=\"/pagefind/pagefind-ui.js\" type=\"text/javascript\"></script>\n<script>\n    window.addEventListener('DOMContentLoaded', function(event) {\n\t\tlet page_url = new URL(window.location.href),\n    \t    query_string = page_url.searchParams.get('q'),\n      \t\tpse = new PagefindUI({ element: \"#search\" });\n\t\tif (query_string !== null) {\n\t\t\tpse.triggerSearch(query_string);\n\t\t}\n    });\n</script>\n</body>\n</html>\nHTML\n\npagefind \\\n--source htdocs \\\n--serve\n~~~\n\nThen I just language my web browser pointing at `http://localhost:1414/index.html`. I can even pass the URL a `?q=...` query string if I want.\n\nFrom a functionality point of view this is very bare bones and I don't think 16K pages is enough to make it compelling (I think I need closer to 100K for that).\n\n## What I learned from the prototype so far\n\nThis prototype suffers from several limitations.\n\n1. Stale links in my pages.txt make the harvest process really really slow, I need to have a way to avoid stale links getting into the pages.txt or have them removed from the pages.txt\n2. PageFind's result display uses the pages I downloaded to my local machine. It would be better if the result link was translated to point at the actual source of the pages, I think this can be done via JavaScript in my index.html when I setup the PageFind search/results element. Needs more exploration\n\n16K pages is a very tiny corpus. I get interesting results from my testing but not good enough to make me use first.  I'm guessing I need a corpus of at least 100K pages to be compelling for first search use.\n\nIt is really nice having a localhost personal search engine. It means that I can keep working with my home network connection is problematic. I like that. Since the website generated for my localhost system is a \"static site\" I could easily replicate that to net and make it available to other machines.\n\nRight now the big time sync is harvesting content to index. I'm not certain yet how much space disk space will be needed for my 100K page target corpus.\n\nSetting up indexing and the search UI were the easiest part of the process.  PageFind is so easy to work with compare to enterprise search applications.\n\n## Things to explore\n\nI can think of several ways to enlarge my search corpus. The first is there are a few websites I use for reference that are small enough to mirror. Wget provides a mirror function. Working from a \"sites.txt\" list I could mirror those sites periodically and have their content available for indexing.\n\nWhen experimenting with the mirror option I notice I wind up with PDF that are linked in the pages being mirrored.  If I used the Unix find command to locate all the PDF I could use another tool to extract there text.  Doing that would enlarge my search beyond plain text and HTML.  I would need to think this through as ultimately I'd want to be able to recover the path to the PDF when those results are displayed.\n\nAnother approach would be to work with my full web browsers' history as\nwell as it's bookmarks. This would significantly expand the corpus. If I did this I could also check the \"head\" of the HTML for references to feeds that could be folded into my feed link harvests. This would have the advantage of capture content from sources I find useful to read but would catch blog posts I might have skipped due to limited reading time.\n\nI use Pocket to read the pages I find interesting in my feed reader.  Pocket has an API and I could get some additional interesting pages from it. Pocket also has various curated lists and they might have interesting pages to harvest and index. I think the trick would be to use those suggests against an exclude list of some sort. E.g. Makes not sense to try to harvest paywall stuff or commercial sites more generally. One of the values I see in pse is that it is a personal search engine not a replacement for commercial search engines generally.\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 2,
        "pubDate": "2023-03-10",
        "series": "Personal Search Engine",
        "title": "First Personal Search Engine Prototype",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/10/first-prototype-pse.json"
    },
    {
      "content": "\n# Prototyping a personal search engine\n\nBy R. S. Doiel, 2023-03-07\n\n> Do we really need a search engine to index the \"whole web\"? Maybe a curated subset is better.\n\nAlex Schreoder's post [A Vision for Search](https://alexschroeder.ch/wiki/2023-03-07_A_vision_for_search) prompted me to write up an idea I call a \"personal search engine\".   I've been thinking about a \"a personal search engine\" for years, maybe a decade.\n\nWith the current state of brokenness in commercial search engines, especially with the implosion of the commercial social media platforms, we have an opportunity to re-think search on a more personal level.\n\nThe tooling around static site generation where a personal search is an extension of your own website suggests a path out of the quagmire of commercial search engines.  Can techniques I use for my own site search, be extended into a personal search engine?\n\n## A Broken Cycle\n\nSearch engines happened pretty early on in the web. If my memory is correct they showed up with the arrival of support for [CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface \"Common Gateway Interface\") in early web server software. Remembering back through the decades I see a pattern.\n\n1. Someone comes up with a clever way to index web content and determine relevancy\n2. They index enough the web to be interesting and attract early adopters\n3. They go mainstream, this compels them to have a business model, usually some form of ad-tech\n4. They index an ever larger portion of the web, the results from the search engine starts to degrade\n5. The business model becomes the primary focus of the company, the indexing gets exploited (e.g. content farms, page hijacking), the search results degrade.\n\nStage four and five can be summed up as the \"bad search engine stage\". When things get bad enough a new search engine comes on the scene and the early adopters jump ship and the cycle repeats. This was well established by the time some graduate students at Stanford invented page rank. I think it is happening now with search integrated ChatGPT.\n\nI think we're at the point in the cycle where there is an opportunity for something new. Maybe even break the loop entirely.\n\n## How do I use search?\n\nMy use of search engines can be described in four broad categories.\n\n1. Look for a specific answer queries\n    - `spelling of <VALUE>`\n    - `meaning of <VALUE>`\n    - `location of <VALUE>`\n    - `convert <UNIT> from <VALUE> to <UNIT>`\n2. Shopping queries\n    - pricing an item\n    - finding an item\n    - finding marketing material on an item\n3. Look for subject information\n    - a topic search\n    - news event\n    - algorithms\n4. Look for information I know exists\n    - technical documentation\n    - an article I read\n    - an article I want to read next\n\nMost of my searches are either subject information or retrieving something I know exists. Both are particularly susceptible to degradation when the business model comes to dominate the search results.\n\nA personal search engine for me would address these four types of searches before I reach for alternatives. In the mean time I'm stuck attempting to mitigate the bad search experience as best I can.\n\n## Mitigating the bad search engine experience\n\n> As commercial search engines degrade I rely on a given website's own search more\n\nI've noticed the follow search behavior practice changes in my own web usage.  For shopping I tend to go to the vendors I trust and use their searches on their websites.  To learn about a place, it's Wikipedia and if I trying to get a sense of going there I'll probably rely on an Open Street Map to avoid the ad-tech in commercial services. I dread using the commercial maps because usage correlates so strongly with the spam I encounter the next time I use an internet connected device.\n\nFor spelling and dictionary I can use Wiktionary. Location information I use Wikipedia and Open Street Maps. Weather info I have links into [NOAA](https://www.weather.gov/) website. Do I really need to use the commercial services?\n\nIt seems obvious that the commercial services for me are at best a fallback experience. They are no longer the \"go to\" place on the web to find stuff. I miss the convenience of using my web browsers URL box as a search box but the noise of the commercial search engines means the convenience is not worth the cost.\n\nWhat I would really like is a search service that integrated **my trusted sources** with a single search box but without the noise of the commercial sites. Is this possible? How much work would it be?\n\nI think a personal (or even small group) search engine is plausible and desirable. I think we can build a prototype with some off the shelf parts.\n\n## Observations\n\n1. I only need to index a tiny subset of the web, I don't want a web crawler that needs to be monitored and managed\n2. The audience of the search engine is me and possibly some friends\n3. There are a huge number of existing standards, protocols and structured data formats and practices I could leverage to mange building a search corpus and for indexing.\n4. Static site generators have moved site search from services (often outsourced to commercial search engines) to browser based solutions (e.g. [PageFind](https://pagefind.app), [LunrJS](https://lunrjs.com))\n5. A localhost site could stage pages for indexing and I could leverage my personal website to expose my indexes to my web devices (e.g. my phone).\n6. Tools like wget can mirror websites and that could also be used to stage content for a personal search engine\n7. There is a growing body of Open Access data, journal articles and books, these could be indexed and made available in a personal search engine with some effort\n\n## Exploring a personal search engine concept\n\nWhen I've brought up the idea of \"a personal search engine\" over the years with colleagues I've been consistently surprise at the opposition I encounter.  There are so many of reasons not to build something, including a personal search engine. That has left me thinking more deeply about the problem, a good thing in my experience.  I've synthesized that resistance into three general questions. Keeping those questions in mind will be helpful in evaluating the costs in time for prototyping a personal search engine and ultimately if the prototype should turn into an open source project.\n\n1. How would a personal search engine know/discover \"new\" content to include?\n2. Search engines are hard to setup and maintain (e.g. Solr, Opensearch), why would I want to spend time doing that?\n3. Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nConstraints can be a good thing to consider as well. Here's some constraints I think will be helpful when considering a prototype implementation.\n\n- I maintain my personal website using a Raspberry Pi 400. The personal search engine needs to respect the limitations of that device.\n- I'd like to be able to access my personal search engine from all my networked devices (e.g. my phone when I am away from home)\n- I have little time to prototype or code anything\n- I need to explain the prototype easily if I want others to help expand on the ideas\n- If it breaks I need to easily fix it\n\nI believe recent evolution of static site generation and site search offer an adjacent technology that can be leverage to demonstrate a personal search engine as a prototype. The prototype of a personal search engine could be an extension of my existing website.\n\n## Addressing challenges\n\n### How can a personal search engine know about new things?\n\nThe first challenge boils down to discovering content you want to index. What I'm describing is a personal search engine. I'm not trying to \"index the whole web\" or even a large part of it. I suspect the corpus I regularly search probably is in the neighborhood of a 100,000 pages or so. Too big for a bookmark list but magnitudes smaller than search engine deployments commonly see in an enterprise setting. I also am NOT suggesting a personal search engine will replace commercial search engines or even compete with them. What I'm thinking of is an added tool, not a replacement.\n\nCurating content is labor intensive. This is why Yahoo evolved from a curated web directory to become a hybrid web directory plus search engine before its final demise.  I don't want to have to change how I currently find content on the web. When I do stumble on something interesting I need a mechanism to easily add it to my personal search engine. Fortunately I think my current web reading habits can function like a [mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk).\n\nMost \"new\" content I find isn't from using a commercial search engine. When I look at my habits I find two avenues for content discovery dominate. I come across something via social media (today that's RSS feeds provided via Mastodon and Yarn Social/Twtxt) or from RSS, Atom and JSON feeds of blogs or websites I follow. Since the social media platforms I track support RSS I typically read all this content via newsboat which is a terminal based feed reader. I still find myself using the web browser's bookmark feature. It's just the bookmarks aren't helpful if they remain only in my web browser.  I also use [Pocket](https://getpocket.com) to read things later. I think all these can serve as a \"link discovery\" mechanism for a personal search engine. It's just a matter of collecting the URLs into a list of content I want to index, staging the content, index it and publish the resulting indexes on my personal website using a browser based search engine to query them.\n\nThis link discovery approach is different from how commercial search engines work.  Commercial engines rely on crawlers that retrieve a web page, analyze the content, find new links in the page then recursively follows those to scan whole domains and websites.  Recursive crawlers aren't automatic. It's easy for them to get trapped in link loops and often they can be a burden to the sites they are crawling (hence robot.txt files suggesting to crawlers what needs to be avoided).  I don't need to index the whole web, usually not even whole websites.  I'm interested in page level content and I can get a list of web pages from by bookmarks and the feeds I follow.\n\n\nA Quick digression:\n\nBlogs, in spite of media hype, haven't \"gone away\".  Presently we're seeing a bit of a renaissance with projects like [Micro.blog](https://micro.blog) and [FeedLand](http://docs.feedland.org/about.opml \"this is a cool project from Dave Winer\"). The \"big services\" like [WordPress](https://wordpress.com), [Medium](https://medium.com), [Substack](https://substack.com) and [Mailchimp](https://mailchimp.com/) provide RSS feeds for their content. RSS/Atom/JSON feed syndication all are alive and well at least for the sites I track and content I read. I suspect this is the case for others.  What is a challenge is knowing how to find the feed URL.  But even that I've notice is becoming increasingly predictable. I suspect given a list of blog sites I could come up with a way of guessing the feed URL in many cases even without an advertised URL in the HTML head or RSS link in the footer.\n\n### Search engines are hard to setup and maintain, how can that be made easier?\n\nI think this can be addressed in several ways. First is splitting the problem of content retrieval, indexing and search UI.  [PageFind](https://pagefind.app) is the current static site search I use on my blog.  It does a really good job at indexing blog content will little configuration. PageFind is clever about the indexes it builds.  When PageFind indexes a site is builds a partitioned index. Each partition is loaded by the web browser only when the current search string suggests it is needed. This means you can index a large number of pages (e.g. 100,000 pages) before it starts to feel sluggish. Indexing is fast and can be done on demand after harvesting the new pages you come across in your feeds. If the PageFind indexes are saved in my static site directory (a Git repository) I can implement the search UI there implementing the personal search engine prototype. The web browser is the search engine and PageFind tool is the indexer. The harvester is built by extracting interesting URLs from the feeds I follow and the current state of my web browsers' bookmarks and potentially from content in Pocket. Note the web browser bookmarks are synchronized across my devices so if I encounter an interesting URL in the physical world I can easily add it my personal search engine too the next time I process the synchronized bookmark file.\n\n### Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nEnterprise Search Engine Software is complicated to setup, very resource intensive and requires upkeep. For me Solr, Elasticsearch, Opensearch falls into the category \"day job\" duty and I do not want that burden for my personal search engine. Fortunately I don't need to run Solr, Elasticsearch or Opensearch. I can build a decent search engine using [PageFind](https://pagefind.app).  PageFind is simple to configured, simple to index with and it's indexes scale superbly for a browser based search engine UI. Hosting is reduced to the existing effort I put into updating my personal blog and automating the link extraction from the feeds I follow and my web browsers' current bookmark file.\n\nI currently use PageFind for web content I mirror to a search directory locally for off line reading. From that experience I know it can handle at least 100,000 pages. I know it will work on my Raspberry Pi 400. I don't see a problem in a prototype personal search engine assuming a corpus in the neighborhood of 100,000 pages.\n\n\n## Sketching the prototype\n\nHere's a sketch of a prototype of \"a personal search engine\" built on PageFind.\n\n1. Generate a list of URLs pointing at pages I want to index (this can be done by mining my bookmarks and feed reader content).\n2. Harvest and stage the pages on my local file system, maintaining a way to associated their actual URL with the staged copy\n3. Index with PageFind and save the resulting indexes my local copy of my personal website\n4. Have a page on my personal website use these indexes to implement a search and results page\n\nThe code that I would need to be implemented is mostly around extracting URL from my browser's bookmark file and my the feeds managed in my feed reader. Since newsboat is open source and it stores it cached feeds in a SQLite3 database in principle I could use the tables in that database to generate a list of content to harvest for indexing. I could write a script that combines the content from my bookmarks file and newsboat database rendering a flat list to harvest, stage and then index with PageFind. A prototype could be done in Bash or Python without too much of an effort.\n\nOne challenge remains after harvesting and staging is solved. It would be nice to use my personal search engine as my default search engine. After all I am already curating the content. I think this can be done by supporting the [Open Search Description](https://developer.mozilla.org/en-US/docs/Web/OpenSearch) to make my personal search engine a first class citizen in my browser URL bar. Similarly I could turn the personal search engine page into a PWA so I can have it on my phone's desktop along the other apps I commonly use.\n\nObservations that maybe helpful for a successful prototype\n\n1. I don't need to crawl the whole web just the pages that interest me\n2. I don't need to create or monitor a recursive web crawler\n3. I avoid junk because I'm curating the sources through my existing web reading practices\n4. I am targeting a small search corpus, approximately 100,000 pages or so\n5. I am only indexing HTML, Pagefind can limit the elements it indexes\n\nA prototype of a personal search engine seems possible. The challenge will be finding the time to implement it.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 1,
        "pubDate": "2023-03-07",
        "series": "Personal Search Engine",
        "title": "Prototyping a personal search engine",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/07/prototyping-a-personal-search-engine.json"
    }
  ]
}