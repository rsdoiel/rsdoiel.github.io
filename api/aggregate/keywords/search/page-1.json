{
  "page": 1,
  "total_pages": 1,
  "has_more": false,
  "next_page": null,
  "values": [
    {
      "content": "\nBrowser based site search\n=========================\n\nBy R. S. Doiel, 2022-11-18\n\nI recently read Brewster Kahleâ€™s 2015 post about his vision for a [distributed web](https://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/). Many of his ideas have carried over into [DWeb](https://wiki.mozilla.org/Dweb), [Indie Web](https://indieweb.org/), [Small Web](https://benhoyt.com/writings/the-small-web-is-beautiful/), [Small Internet](https://cafebedouin.org/2021/07/28/the-small-internet/) and the like. A point he touches on is site search running in the web browser.\n\nI've use this approach in my own website relying on [LunrJS](https://lunrjs.com) by Oliver Nightingale. It is a common approach for small sites built using Markdown and [Pandoc](https://pandoc.org).  In the Brewster article he mentions [js-search](https://github.com/cebe/js-search), an implementation I was not familiar with. Like LunrJS the query engine runs in the browser via JavaScript but unlike LunrJS the indexes are built using PHP rather than JavaScript. The last couple of years I've used [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) to generating indexes for my own website site while using LunrJS for the browser side query engine. Today I check to see what the [Hugo](https://gohugo.io/tools/search/) community is using and found [Pagefind](https://github.com/cloudcannon/pagefind). Pagefind looks impressive. There was a presentation on at [Hugo Conference 2022](https://hugoconf.io/). It takes building a Lucene-like index several steps further. I appears to handle much larger indexes without requiring the full indexes to be downloaded into the browser.  It seems like a good candidate for prototyping personal search engine.\n\nHow long have been has browser side search been around? I do not remember when I started using. I explored seven projects on GitHub that implemented browser side site search. This is an arbitrary selection projects but even then I had no idea that this approach dates back a over decade!\n\n| Project | Indexer | query engine | earliest commit[^1] | recent commit[^2] |\n|---------|---------|--------------|:-------------------:|:-----------------:|\n| [LunrJS](https://github.com/olivernn/lunr.js) | JavaScript | JavaScript | 2011 | 2020 |\n| [Fuse.io](https://github.com/krisk/Fuse) | JavaScript/Typescript | JavaScript/Typescript | 2012 | 2022 |\n| [search-index](https://github.com/fergiemcdowall/search-index) | JavaScript | JavaScript | 2013 | 2016 |\n| [js-search](https://github.com/cebe/js-search) (cebe) | PHP | JavaScript | 2014 | 2022 |\n| [js-search](https://github.com/bvaughn/js-search) (bvaughn)| JavaScript | JavaScript | 2015 | 2022 |\n| [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) | Python | Python or JavaScript | 2018 | 2022 |\n| [Pagefind](https://github.com/cloudcannon/pagefind) | Rust | WASM and JavaScript | 2022 | 2022 |\n\n[^1]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n[^2]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-18",
        "keywords": [
          "search",
          "web browser",
          "dweb",
          "static site",
          "lunrjs",
          "pagefind"
        ],
        "pubDate": "2022-11-18",
        "title": "Browser based site search"
      },
      "url": "posts/2022/11/18/browser-side-site-search.json"
    },
    {
      "content": "\n# First Personal Search Engine Prototype\n\nBy R. S. Doiel, 2023-08-10\n\nI've implemented a first prototype of my personal search engine which\nI will abbreviate as \"pse\" from here on out. I implemented it using \nthree [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) scripts\nrelying on [sqlite3](https://sqlite.org), [wget](https://en.wikipedia.org/wiki/Wget) and [PageFind](https://pagefind.app) to do the heavy lifting.\n\nBoth Firefox and newsboat store useful information in sqlite databases.  Firefox's `moz_places.sqlite` holds both all the URLs visited as well as those that are associated with bookmarks (i.e. the SQLite database `moz_bookmarks.sqlite`).  I had about 2000 bookmarks, less than I thought with many being stale from link rot. Stale page URLs really slow down the harvest process because of the need for wget to wait on various timeouts (e.g. DNS, server response, download times).  The \"history\" URLs would make an interesting collection to spider but you'd probably want to have an exclude list (e.g. there's no point in saving queries to search engines, web mail, shopping sites). Exploring that will wait for another prototype.\n\nThe `cache.db` associated with Newsboat provided a rich resource of content and much fewer stale links (not surprising because I maintain that URL list more much activity then reviewing my bookmarks).  Between the two I had 16,000 pages. I used SQLite 3 to query the url values from the various DB into sorting for unique URLs into a single text file one URL per line.\n\nThe next thing after creating a list of pages I wanted to search was to download them into a directory using wget.  Wget has many options, I choose to enable timestamping, create a protocol directory and then a domain and path directory for each item. This has the advantage of being able to transform the Path into a URL later.\n\nOnce the content was harvested I then used PageFind to index the all the harvested content. Since I started using PageFind originally the tool has gained an option called `--serve` which provides a localhost web service on port 1414.  All I needed to do was add an index.html file to the directory where I harvested the content and saved the PageFind indexes. Then I used PageFind to again to provide a localhost based personal search engine.\n\nWhile the total number of pages was small (16k pages) I did find interesting results just trying out random words. This makes the prototype look promising.\n\n## Current prototype components\n\nI have simple Bash script that gets the URLs from both Firefox bookmarks and Newsboat's cache then generates a single text file of unique URLs I've named \"pages.txt\".\n\nI then use the \"pages.txt\" file to harvest content with wget into a tree structure like \n\n- htdocs\n    - http (all the http based URLs I harvest go in here)\n    - https (all the https based URLs I harvest go in here)\n    - pagefind (this holds the PageFind indexes and JavaScript to implement the search UI)\n    - index.html (this holds the webpage for the search UI using the libraries in `pagefind`)\n\nSince I'm only downloaded the HTML the 16k pages does not take up significant disk space yet.\n\n## Prototype Implementation\n\nHere's the bash scripts I use to get the URLs, harvest content and launch my localhost search engine based on PageFind.\n\nGet the URLs I want to be searchable. I use to environment variables\nfor finding the various SQLite 3 databases (i.e. PSE_MOZ_PLACES, PSE_NEWSBOAT).\n\n~~~\n#!/bin/bash\n\nif [ \"$PSE_MOZ_PLACES\" = \"\" ]; then\n    printf \"the PSE_MOZ_PLACES environment variable is not set.\"\n    exit 1\nfi\nif [ \"$PSE_NEWSBOAT\" = \"\" ]; then\n    printf \"the PSE_NEWSBOAT environment variable is not set.\"\n    exit 1\nfi\n\nsqlite3 \"$PSE_MOZ_PLACES\" \\\n    'SELECT moz_places.url AS url FROM moz_bookmarks JOIN moz_places ON moz_bookmarks.fk = moz_places.id WHERE moz_bookmarks.type = 1 AND moz_bookmarks.fk IS NOT NULL' \\\n    >moz_places.txt\nsqlite3 \"$PSE_NEWSBOAT\" 'SELECT url FROM rss_item' >newsboat.txt\ncat moz_places.txt newsboat.txt |\n    grep -E '^(http|https):' |\n    grep -v '://127.0.' |\n    grep -v '://192.' |\n    grep -v 'view-source:' |\n    sort -u >pages.txt\n~~~\n\nThe next step is to have the pages. I use wget for that.\n\n~~~\n#!/bin/bash\n#\nif [ ! -f \"pages.txt\" ]; then\n    echo \"missing pages.txt, skipping harvest\"\n    exit 1\nfi\necho \"Output is logged to pages.log\"\nwget --input-file pages.txt \\\n    --timestamping \\\n    --append-output pages.log \\\n    --directory-prefix htdocs \\\n    --max-redirect=5 \\\n    --force-directories \\\n    --protocol-directories \\\n    --convert-links \\\n    --no-cache --no-cookies\n~~~\n\nFinally I have a bash script that generates the index.html page, an Open Search Description XML file, indexes the harvested sites and launches PageFind in server mode.\n\n~~~\n#!/bin/bash\nmkdir -p htdocs\n\ncat <<OPENSEARCH_XML >htdocs/pse.osdx\n<OpenSearchDescription xmlns=\"http://a9.com/-/spec/opensearch/1.1/\"\n                       xmlns:moz=\"http://www.mozilla.org/2006/browser/search/\">\n  <ShortName>PSE</ShortName>\n  <Description>A Personal Search Engine implemented via wget and PageFind</Description>\n  <InputEncoding>UTF-8</InputEncoding>\n  <Url rel=\"self\" type=\"text/html\" method=\"get\" template=\"http://localhost:1414/index.html?q={searchTerms}\" />\n  <moz:SearchForm>http://localhost:1414/index.html</moz:SearchForm>\n</OpenSearchDescription>\nOPENSEARCH_XML\n\ncat <<HTML >htdocs/index.html\n<html>\n<head>\n<link\n  rel=\"search\"\n  type=\"application/opensearchdescription+xml\"\n  title=\"A Personal Search Engine\"\n  href=\"http://localhost:1414/pse.osdx\" />\n<link href=\"/pagefind/pagefind-ui.css\" rel=\"stylesheet\">\n</head>\n<body>\n<h1>A personal search engine</h1>\n<div id=\"search\"></div>\n<script src=\"/pagefind/pagefind-ui.js\" type=\"text/javascript\"></script>\n<script>\n    window.addEventListener('DOMContentLoaded', function(event) {\n\t\tlet page_url = new URL(window.location.href),\n    \t    query_string = page_url.searchParams.get('q'),\n      \t\tpse = new PagefindUI({ element: \"#search\" });\n\t\tif (query_string !== null) {\n\t\t\tpse.triggerSearch(query_string);\n\t\t}\n    });\n</script>\n</body>\n</html>\nHTML\n\npagefind \\\n--source htdocs \\\n--serve\n~~~\n\nThen I just language my web browser pointing at `http://localhost:1414/index.html`. I can even pass the URL a `?q=...` query string if I want.\n\nFrom a functionality point of view this is very bare bones and I don't think 16K pages is enough to make it compelling (I think I need closer to 100K for that).\n\n## What I learned from the prototype so far\n\nThis prototype suffers from several limitations.\n\n1. Stale links in my pages.txt make the harvest process really really slow, I need to have a way to avoid stale links getting into the pages.txt or have them removed from the pages.txt\n2. PageFind's result display uses the pages I downloaded to my local machine. It would be better if the result link was translated to point at the actual source of the pages, I think this can be done via JavaScript in my index.html when I setup the PageFind search/results element. Needs more exploration\n\n16K pages is a very tiny corpus. I get interesting results from my testing but not good enough to make me use first.  I'm guessing I need a corpus of at least 100K pages to be compelling for first search use.\n\nIt is really nice having a localhost personal search engine. It means that I can keep working with my home network connection is problematic. I like that. Since the website generated for my localhost system is a \"static site\" I could easily replicate that to net and make it available to other machines.\n\nRight now the big time sync is harvesting content to index. I'm not certain yet how much space disk space will be needed for my 100K page target corpus.\n\nSetting up indexing and the search UI were the easiest part of the process.  PageFind is so easy to work with compare to enterprise search applications.\n\n## Things to explore\n\nI can think of several ways to enlarge my search corpus. The first is there are a few websites I use for reference that are small enough to mirror. Wget provides a mirror function. Working from a \"sites.txt\" list I could mirror those sites periodically and have their content available for indexing.\n\nWhen experimenting with the mirror option I notice I wind up with PDF that are linked in the pages being mirrored.  If I used the Unix find command to locate all the PDF I could use another tool to extract there text.  Doing that would enlarge my search beyond plain text and HTML.  I would need to think this through as ultimately I'd want to be able to recover the path to the PDF when those results are displayed.\n\nAnother approach would be to work with my full web browsers' history as\nwell as it's bookmarks. This would significantly expand the corpus. If I did this I could also check the \"head\" of the HTML for references to feeds that could be folded into my feed link harvests. This would have the advantage of capture content from sources I find useful to read but would catch blog posts I might have skipped due to limited reading time.\n\nI use Pocket to read the pages I find interesting in my feed reader.  Pocket has an API and I could get some additional interesting pages from it. Pocket also has various curated lists and they might have interesting pages to harvest and index. I think the trick would be to use those suggests against an exclude list of some sort. E.g. Makes not sense to try to harvest paywall stuff or commercial sites more generally. One of the values I see in pse is that it is a personal search engine not a replacement for commercial search engines generally.\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 2,
        "pubDate": "2023-03-10",
        "series": "Personal Search Engine",
        "title": "First Personal Search Engine Prototype",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/10/first-prototype-pse.json"
    },
    {
      "content": "\n# Prototyping a personal search engine\n\nBy R. S. Doiel, 2023-03-07\n\n> Do we really need a search engine to index the \"whole web\"? Maybe a curated subset is better.\n\nAlex Schreoder's post [A Vision for Search](https://alexschroeder.ch/wiki/2023-03-07_A_vision_for_search) prompted me to write up an idea I call a \"personal search engine\".   I've been thinking about a \"a personal search engine\" for years, maybe a decade.\n\nWith the current state of brokenness in commercial search engines, especially with the implosion of the commercial social media platforms, we have an opportunity to re-think search on a more personal level.\n\nThe tooling around static site generation where a personal search is an extension of your own website suggests a path out of the quagmire of commercial search engines.  Can techniques I use for my own site search, be extended into a personal search engine?\n\n## A Broken Cycle\n\nSearch engines happened pretty early on in the web. If my memory is correct they showed up with the arrival of support for [CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface \"Common Gateway Interface\") in early web server software. Remembering back through the decades I see a pattern.\n\n1. Someone comes up with a clever way to index web content and determine relevancy\n2. They index enough the web to be interesting and attract early adopters\n3. They go mainstream, this compels them to have a business model, usually some form of ad-tech\n4. They index an ever larger portion of the web, the results from the search engine starts to degrade\n5. The business model becomes the primary focus of the company, the indexing gets exploited (e.g. content farms, page hijacking), the search results degrade.\n\nStage four and five can be summed up as the \"bad search engine stage\". When things get bad enough a new search engine comes on the scene and the early adopters jump ship and the cycle repeats. This was well established by the time some graduate students at Stanford invented page rank. I think it is happening now with search integrated ChatGPT.\n\nI think we're at the point in the cycle where there is an opportunity for something new. Maybe even break the loop entirely.\n\n## How do I use search?\n\nMy use of search engines can be described in four broad categories.\n\n1. Look for a specific answer queries\n    - `spelling of <VALUE>`\n    - `meaning of <VALUE>`\n    - `location of <VALUE>`\n    - `convert <UNIT> from <VALUE> to <UNIT>`\n2. Shopping queries\n    - pricing an item\n    - finding an item\n    - finding marketing material on an item\n3. Look for subject information\n    - a topic search\n    - news event\n    - algorithms\n4. Look for information I know exists\n    - technical documentation\n    - an article I read\n    - an article I want to read next\n\nMost of my searches are either subject information or retrieving something I know exists. Both are particularly susceptible to degradation when the business model comes to dominate the search results.\n\nA personal search engine for me would address these four types of searches before I reach for alternatives. In the mean time I'm stuck attempting to mitigate the bad search experience as best I can.\n\n## Mitigating the bad search engine experience\n\n> As commercial search engines degrade I rely on a given website's own search more\n\nI've noticed the follow search behavior practice changes in my own web usage.  For shopping I tend to go to the vendors I trust and use their searches on their websites.  To learn about a place, it's Wikipedia and if I trying to get a sense of going there I'll probably rely on an Open Street Map to avoid the ad-tech in commercial services. I dread using the commercial maps because usage correlates so strongly with the spam I encounter the next time I use an internet connected device.\n\nFor spelling and dictionary I can use Wiktionary. Location information I use Wikipedia and Open Street Maps. Weather info I have links into [NOAA](https://www.weather.gov/) website. Do I really need to use the commercial services?\n\nIt seems obvious that the commercial services for me are at best a fallback experience. They are no longer the \"go to\" place on the web to find stuff. I miss the convenience of using my web browsers URL box as a search box but the noise of the commercial search engines means the convenience is not worth the cost.\n\nWhat I would really like is a search service that integrated **my trusted sources** with a single search box but without the noise of the commercial sites. Is this possible? How much work would it be?\n\nI think a personal (or even small group) search engine is plausible and desirable. I think we can build a prototype with some off the shelf parts.\n\n## Observations\n\n1. I only need to index a tiny subset of the web, I don't want a web crawler that needs to be monitored and managed\n2. The audience of the search engine is me and possibly some friends\n3. There are a huge number of existing standards, protocols and structured data formats and practices I could leverage to mange building a search corpus and for indexing.\n4. Static site generators have moved site search from services (often outsourced to commercial search engines) to browser based solutions (e.g. [PageFind](https://pagefind.app), [LunrJS](https://lunrjs.com))\n5. A localhost site could stage pages for indexing and I could leverage my personal website to expose my indexes to my web devices (e.g. my phone).\n6. Tools like wget can mirror websites and that could also be used to stage content for a personal search engine\n7. There is a growing body of Open Access data, journal articles and books, these could be indexed and made available in a personal search engine with some effort\n\n## Exploring a personal search engine concept\n\nWhen I've brought up the idea of \"a personal search engine\" over the years with colleagues I've been consistently surprise at the opposition I encounter.  There are so many of reasons not to build something, including a personal search engine. That has left me thinking more deeply about the problem, a good thing in my experience.  I've synthesized that resistance into three general questions. Keeping those questions in mind will be helpful in evaluating the costs in time for prototyping a personal search engine and ultimately if the prototype should turn into an open source project.\n\n1. How would a personal search engine know/discover \"new\" content to include?\n2. Search engines are hard to setup and maintain (e.g. Solr, Opensearch), why would I want to spend time doing that?\n3. Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nConstraints can be a good thing to consider as well. Here's some constraints I think will be helpful when considering a prototype implementation.\n\n- I maintain my personal website using a Raspberry Pi 400. The personal search engine needs to respect the limitations of that device.\n- I'd like to be able to access my personal search engine from all my networked devices (e.g. my phone when I am away from home)\n- I have little time to prototype or code anything\n- I need to explain the prototype easily if I want others to help expand on the ideas\n- If it breaks I need to easily fix it\n\nI believe recent evolution of static site generation and site search offer an adjacent technology that can be leverage to demonstrate a personal search engine as a prototype. The prototype of a personal search engine could be an extension of my existing website.\n\n## Addressing challenges\n\n### How can a personal search engine know about new things?\n\nThe first challenge boils down to discovering content you want to index. What I'm describing is a personal search engine. I'm not trying to \"index the whole web\" or even a large part of it. I suspect the corpus I regularly search probably is in the neighborhood of a 100,000 pages or so. Too big for a bookmark list but magnitudes smaller than search engine deployments commonly see in an enterprise setting. I also am NOT suggesting a personal search engine will replace commercial search engines or even compete with them. What I'm thinking of is an added tool, not a replacement.\n\nCurating content is labor intensive. This is why Yahoo evolved from a curated web directory to become a hybrid web directory plus search engine before its final demise.  I don't want to have to change how I currently find content on the web. When I do stumble on something interesting I need a mechanism to easily add it to my personal search engine. Fortunately I think my current web reading habits can function like a [mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk).\n\nMost \"new\" content I find isn't from using a commercial search engine. When I look at my habits I find two avenues for content discovery dominate. I come across something via social media (today that's RSS feeds provided via Mastodon and Yarn Social/Twtxt) or from RSS, Atom and JSON feeds of blogs or websites I follow. Since the social media platforms I track support RSS I typically read all this content via newsboat which is a terminal based feed reader. I still find myself using the web browser's bookmark feature. It's just the bookmarks aren't helpful if they remain only in my web browser.  I also use [Pocket](https://getpocket.com) to read things later. I think all these can serve as a \"link discovery\" mechanism for a personal search engine. It's just a matter of collecting the URLs into a list of content I want to index, staging the content, index it and publish the resulting indexes on my personal website using a browser based search engine to query them.\n\nThis link discovery approach is different from how commercial search engines work.  Commercial engines rely on crawlers that retrieve a web page, analyze the content, find new links in the page then recursively follows those to scan whole domains and websites.  Recursive crawlers aren't automatic. It's easy for them to get trapped in link loops and often they can be a burden to the sites they are crawling (hence robot.txt files suggesting to crawlers what needs to be avoided).  I don't need to index the whole web, usually not even whole websites.  I'm interested in page level content and I can get a list of web pages from by bookmarks and the feeds I follow.\n\n\nA Quick digression:\n\nBlogs, in spite of media hype, haven't \"gone away\".  Presently we're seeing a bit of a renaissance with projects like [Micro.blog](https://micro.blog) and [FeedLand](http://docs.feedland.org/about.opml \"this is a cool project from Dave Winer\"). The \"big services\" like [WordPress](https://wordpress.com), [Medium](https://medium.com), [Substack](https://substack.com) and [Mailchimp](https://mailchimp.com/) provide RSS feeds for their content. RSS/Atom/JSON feed syndication all are alive and well at least for the sites I track and content I read. I suspect this is the case for others.  What is a challenge is knowing how to find the feed URL.  But even that I've notice is becoming increasingly predictable. I suspect given a list of blog sites I could come up with a way of guessing the feed URL in many cases even without an advertised URL in the HTML head or RSS link in the footer.\n\n### Search engines are hard to setup and maintain, how can that be made easier?\n\nI think this can be addressed in several ways. First is splitting the problem of content retrieval, indexing and search UI.  [PageFind](https://pagefind.app) is the current static site search I use on my blog.  It does a really good job at indexing blog content will little configuration. PageFind is clever about the indexes it builds.  When PageFind indexes a site is builds a partitioned index. Each partition is loaded by the web browser only when the current search string suggests it is needed. This means you can index a large number of pages (e.g. 100,000 pages) before it starts to feel sluggish. Indexing is fast and can be done on demand after harvesting the new pages you come across in your feeds. If the PageFind indexes are saved in my static site directory (a Git repository) I can implement the search UI there implementing the personal search engine prototype. The web browser is the search engine and PageFind tool is the indexer. The harvester is built by extracting interesting URLs from the feeds I follow and the current state of my web browsers' bookmarks and potentially from content in Pocket. Note the web browser bookmarks are synchronized across my devices so if I encounter an interesting URL in the physical world I can easily add it my personal search engine too the next time I process the synchronized bookmark file.\n\n### Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nEnterprise Search Engine Software is complicated to setup, very resource intensive and requires upkeep. For me Solr, Elasticsearch, Opensearch falls into the category \"day job\" duty and I do not want that burden for my personal search engine. Fortunately I don't need to run Solr, Elasticsearch or Opensearch. I can build a decent search engine using [PageFind](https://pagefind.app).  PageFind is simple to configured, simple to index with and it's indexes scale superbly for a browser based search engine UI. Hosting is reduced to the existing effort I put into updating my personal blog and automating the link extraction from the feeds I follow and my web browsers' current bookmark file.\n\nI currently use PageFind for web content I mirror to a search directory locally for off line reading. From that experience I know it can handle at least 100,000 pages. I know it will work on my Raspberry Pi 400. I don't see a problem in a prototype personal search engine assuming a corpus in the neighborhood of 100,000 pages.\n\n\n## Sketching the prototype\n\nHere's a sketch of a prototype of \"a personal search engine\" built on PageFind.\n\n1. Generate a list of URLs pointing at pages I want to index (this can be done by mining my bookmarks and feed reader content).\n2. Harvest and stage the pages on my local file system, maintaining a way to associated their actual URL with the staged copy\n3. Index with PageFind and save the resulting indexes my local copy of my personal website\n4. Have a page on my personal website use these indexes to implement a search and results page\n\nThe code that I would need to be implemented is mostly around extracting URL from my browser's bookmark file and my the feeds managed in my feed reader. Since newsboat is open source and it stores it cached feeds in a SQLite3 database in principle I could use the tables in that database to generate a list of content to harvest for indexing. I could write a script that combines the content from my bookmarks file and newsboat database rendering a flat list to harvest, stage and then index with PageFind. A prototype could be done in Bash or Python without too much of an effort.\n\nOne challenge remains after harvesting and staging is solved. It would be nice to use my personal search engine as my default search engine. After all I am already curating the content. I think this can be done by supporting the [Open Search Description](https://developer.mozilla.org/en-US/docs/Web/OpenSearch) to make my personal search engine a first class citizen in my browser URL bar. Similarly I could turn the personal search engine page into a PWA so I can have it on my phone's desktop along the other apps I commonly use.\n\nObservations that maybe helpful for a successful prototype\n\n1. I don't need to crawl the whole web just the pages that interest me\n2. I don't need to create or monitor a recursive web crawler\n3. I avoid junk because I'm curating the sources through my existing web reading practices\n4. I am targeting a small search corpus, approximately 100,000 pages or so\n5. I am only indexing HTML, Pagefind can limit the elements it indexes\n\nA prototype of a personal search engine seems possible. The challenge will be finding the time to implement it.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 1,
        "pubDate": "2023-03-07",
        "series": "Personal Search Engine",
        "title": "Prototyping a personal search engine",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/07/prototyping-a-personal-search-engine.json"
    },
    {
      "content": "\n\n# Android, Termux and Dev Environment\n\nBy R. S. Doiel 2016-09-20\n\nRecently I got a new Android 6 tablet. I got a case with a tiny Bluetooth keyboard. I started wondering if I could use it as a development device when on the road. So this is my diary of that test.\n\n## Challenges\n\n1. Find a way to run Bash without rooting my device\n2. See if I could use my normal web toolkit\n\t+ curl\n\t+ jq\n\t+ sed\n\t+ grep\n3. See if I could compile or add my own custom Golang programs\n4. Test setup by running a local static file server, mkpage and update my website\n\n## Searching for Android packages and tools of my toolbox\n\nAfter searching with Duck Duck Go and Google I came across the [termux](https://termux.com). Termux provides a minimal Bash shell environment with support for adding\npackages with _apt_ and _dpkg_.  The repositories visible to *termux* include\nmost of the C tool chain (e.g. clang, make, autoconf, etc) as well as my old Unix favorites _curl_, _grep_, _sed_, _gawk_ and a new addition to my toolkit _jq_.  Additionally you'll find recent versions (as of Sept. 2016) versions of _Golang_, _PHP_, _python_, and _Ruby_.\n\nThis quickly brought me through step 3.  Installing _go_, _git_, and _openssh_ completed what I needed to test static site development with some of the tools in our incubator at [Caltech Library](https://caltechlibrary.github.io).\n\n## Setting up for static site development\n\nAfter configuring _git_, adding my public key to GitHub and running _go get_ on my\ncustom static site tools I confirmed I could build and test static websites from my Android tablet using *Termux*.\n\nHere's the list of packages I installed under *Termux* to provide a suitable shell environment for writing and website constructions.\n\n```shell\n    apt install autoconf automake bash-completion bc binutils-dev bison \\\n        bzip2 clang cmake coreutils ctags curl dialog diffutils dos2unix \\\n        expect ffmpeg findutils gawk git gnutls golang grep gzip \\\n\timagemagick jq less lynx m4 make-dev man-dev nano nodejs \\\n        openssh patch php-dev python readline-dev rlwrap rsync ruby-dev \\\n        sed sensible-utils sharutils sqlite tar texinfo tree unzip vim \\\n        w3m wget zip\n```\n\nThis then allowed me to setup my *golang* environment variables and install\nmy typical custom written tools\n\n```shell\n    export PATH=$HOME/bin:$PATH\n    export GOPATH=$HOME\n    export GOBIN=$HOME/bin\n    go get github.com/rsdoiel/shelltools/...\n    go get github.com/caltechlibrary/mkpage/...\n    go get github.com/caltechlibrary/md2slides/...\n    go get github.com/caltechlibrary/ws/...\n```\n\nFinally pulled down some content to test.\n\n```shell\n    cd\n    mkdir Sites\n    git clone https://github.com/rsdoiel/rsdoiel.github.io.git Sites/rsdoiel.github.io\n    cd  Sites/rsdoiel.github.io\n    ws\n```\n\nThis started the local static site webserver and I pointed by Firefox for Android at http://localhost:8000 and saw a local copy of my personal website. From there I wrote this article and updated it just as if I was working on a Raspberry Pi or standard Linux laptop.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-09-20",
        "keywords": [
          "Bash",
          "cURL",
          "jq",
          "sed",
          "grep",
          "search",
          "golang",
          "Android"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Android, Termux and Dev Environment"
      },
      "url": "posts/2016/09/20/Android-Termux-Dev-environment.json"
    },
    {
      "content": "\n\n# Go, Bleve and Library oriented software\n\nBy R. S. Doiel, 2018-02-19\n(updated: 2018-02-22)\n\nIn 2016, Stephen Davison, asked me, \"Why use Go and Blevesearch for\nour library projects?\" After our conversation I wrote up some notes so\nI would remember. It is now 2018 and I am revising these notes. I\nthink our choice paid off.  What follows is the current state of my\nreflection on the background, rational, concerns, and risk mitigation\nstrategies so far for using [Go](https://golang.org) and\n[Blevesearch](https://blevesearch.com) for Caltech Library projects.\n\n## Background\n\nI first came across Go a few years back when it was announced as an\nOpen Source project by Google at an Google I/O event (2012). The\noriginal Go authors were Robert Griesemer, Rob Pike, and Ken\nThompson. What I remember from that presentation was Go was a rather\nconsistent language with the features you need but little else.  Go\ndeveloped at Google as a response to high development costs for C/C++\nand Java in addition to challenges with performance and slow\ncompilation times.  As a language I would put Go between C/C++ and\nJava. It comes the ease of writing and reading you find in languages\nlike Python. Syntax is firmly in the C/C++ family but heavily\nsimplified. Like Java it provides many modern features including rich basic\ndata structures and garbage collection. It has a very complete standard\nlibrary and provides very good tooling.  This makes it easy to\ngenerate code level documentation, format code, test, efficiently profile, \nand debug.\n\nOften programming languages develop around a specific set of needs.\nThis is true for Go. Given the Google origin it should not be\nsurprising to find that Go's primary strengths are working with \nstructured data, I/O and concurrency. The rich standard\nlibrary is organized around a package concept. These include packages\nsupporting network protocols, file and socket I/O as well as various\nencoding and compression scheme. It has particularly strong support\nfor XML, JSON, CSV formatted data out of the box. It has a template\nlibrary for working with plain text formats as well as generating safe\nHTML. You can browse Go's standard library https://golang.org/pkg/.\n\nAn additional feature is Go's consistency. Go code that compiles under\nversion 1.0 still compiles under 1.10. Even before 1.0 code changes\nthat were breaking came with tooling to automatically updates existing\ncode.  Running code is a strong element of Go's evolution.\n\nGo is unsurprising and has even been called boring.  This turns out to\nbe a strength when building sustainable projects in a small team.\n\n\n## Why do I write Go?\n\nFor me Go is a good way to write web services, assemble websites,\ncreate search appliances and write command line (cli) utilities. When\na shell script becomes unwieldy Go is often what I turn to.  Go is\nwell suited to building tools as well as systems.  Go based command\nline tools are very easy to orchestrate with shell and Python.\n\nGo runs on all the platforms I actively use - Windows, Mac OS X, Linux\non both Intel and ARM (e.g. Raspberry Pi, Pine64). It has experimental\nsupport for Android and iOS.  I've used a tool called\n[GopherJS](http://gopherjs.org) to write web browser applications that\ntransform my command line tools into web tools with a friendlier user\ninterface (see our [BibTeX Tools](https://caltechlibrary.github.io/bibtex/webapp/)).\n\nGo supports cross compiling out of the box. This means a production\nsystem running on AWS, Google's compute engine or Microsoft's Azure\ncan be compiled from Windows, Mac OS or even a Raspberry Pi!\nDeployment is a matter of copying the (self contained) compiled binary\nonto the production system. This contrasts with other\nplatforms like Perl, PHP, Python, NodeJS and Ruby where you need to\ninstall not only your application code but all dependencies. While\ninterpretive languages retain an advantage of having a REPL, Go\nbased programs have advantages of fast compile times and easy deployment.\n\nIn many of the projects I've written in Go I've only required a few\n(if any) 3rd party libraries (packages in Go's nomenclature). This is\nquite a bit different from my experience with Perl, PHP, Python,\nNodeJS and Ruby. This is in large part a legacy of having grown up at\nGoogle before become an open source project. While the Go standard\npackages are very good there is a rich ecosystem for 3rd party\npackages for specialized needs. I've found I tend to rely only on a\nfew of them. The one I've used the most is\n[Bleve](http://blevesearch.com).\n\nBleve is a Go package for building search engines. When I originally\ncame across Bleve (around 2014), it was described as \"Lucene lite\". \n\"Lucene lite\" was an apt description, but I find it easier\nto use than Lucene. When I first used Bleve I embedded its\nfunctionality into the tools I used to process data and present web\nservices. It did not have much in the way of stand alone command line\ntooling.  Today I increasingly think of Bleve as \"Elastic Search\nlite\". It ships with a set of command line tools that include support\nfor building Bleve's indexes.  My current practice is to only embed the search\nportion of the packages. I can use the Bleve command line for the\nrest.  In 2018, Bleve is being actively developed, has a small vibrant\ncommunity and is used by [Couchbase](https://couchbase.com), a well\nestablished NoSQL player.\n\n\n## Who is using Go?\n\nMany companies use Go. The short list includes\nGoogle, Amazon, Netflix, Dropbox, Box, eBay, Pearsons and even\nWalmart and Microsoft. This came to my attention at developer conferences\nback in 2014.  People from many of these companies started\npresenting at conferences on pilot projects that had been successful\nand moved to production. Part of what drove adoption was the ease\nof development in Go along with good system performance. I also think\nthere was a growing disenchantment with alternatives like C++, C sharp\nand Java as well as the weight of the LAMP, Tomcat, and OpenStack.\n\nHighly visible Go based projects include\n\n+ [Docker](http://docker.org) and [Rocket](http://www.docker.com) - Containerization for running process in the cloud\n+ [Kubernettes](http://kubernetes.io/) and [Terraform](https://www.terraform.io/) - Container orchestration systems\n+ [Hugo](http://hugo.io) - the fast/popular static website generator, an alternative to Jekyll, for those who want speed\n+ [Caddy](https://caddyserver.com/) - a Go based web server trying to unseat Apache/NGinX focusing on easy of use plus speed\n+ [IPFS](http://ipfs.io) - a cutting edge distributed storage system based on block chains\n\n\n### Who is using Blevesearch?\n\nHere's some larger projects using Bleve.\n\n+ [Couchbase](http://www.couchbase.com), a NoSQL database platform are replacing Lucene with Bleve.  Currently the creator of Bleve works for them.\n+ [Hugo](http://hugo.io) can integrate with Bleve for search and index generation\n+ [Caddy](https://caddyserver.com/) integrates with Bleve to provide an embedded search capability\n\n\n## Managing risks\n\nIn 2014 Go was moving from bleeding to leading edge. Serious capital\nwas behind its adoption and it stopped being an exotic conference\nitem. In 2014 Bleve was definitely bleeding edge. By late 2015 and early\n2016 the program level API stabilized. People were piloting projects\nwith it. This included our small group at Caltech Library. In 2015\nnon-English language support appeared followed by a growing list\nof non-European languages in 2016. By mid 2016 we started to see \nmissing features like alternative sorting added. While Bleve isn't\nyet 1.0 (Feb. 2018) it is reliable. The primary challenge for the Bleve\nproject is documentation targeting the novice and non-Programmer users.\nBleve has proven effective as an indexing and search platform for \narchival, library, and data repository content.\n\nAdopting new software comes with risk. We have mitigated this in two ways.\n\n1. Identify alternative technology (a plan B)\n2. Architect our systems for easy decomposition and re-composition\n\nIn the case of Go, packages can be compiled to a C-Shared\nlibrary. This allows us to share working Go packages with languages\nlike Python, R, and PHP. We have included shared Go/Python modules\non our current road map for projects.\n\nFor Blevesearch the two alternatives are Solr and Elastic\nSearch. Both are well known, documented, and solid.  The costs would be\nrecommitting to a Java stack and its resource requirements. We have\nalready identified what we want to index and that could be converted\nto either platform if needed.  If we stick with Go but dropped \nBlevesearch we would swap out the Bleve specific code for Go packages \nsupporting Solr and Elastic Search.\n\n\nThe greatest risk in adopting Go for library and archive projects was \nknowledge transfer. We addressed this \nby knowledge sharing and insuring the Go codebase can \nbe used via command line programs.  Additionally \nwe are adding support for Go based Python modules.\nTraining also is available in the form of books, websites and\nonline courses ([lynda.com](https://www.lynda.com/Go-tutorials/Up-Running-Go/412378-2.html) offers a \"Up Running Go\" course).\n\n\n## What are the benefits?\n\nFor library and archives software we have found Go's benefits include\nimproved back end systems performance at a lower cost, ease of development, \nease of deployment, a rich standard library focused on the types of things \nneeded in library and archival software.  Go plays nice with\nother systems (e.g. I create an API based service in Go that can easily\nbe consumed by a web browser running JavaScript or Perl/PHP/Python\ncode running under LAMP). In the library and archives setting Go \ncan become a high performance duck tape. We get the performance and \nreliability of C/Java type systems with code simplicity \nsimilar to Python.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-02-19",
        "keywords": [
          "Golang",
          "Bleve",
          "search"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Go, Bleve and Library oriented software"
      },
      "url": "posts/2018/02/19/go-bleve-and-libraries.json"
    }
  ]
}