{
  "page": 4,
  "total_pages": 5,
  "has_more": true,
  "next_page": "posts/all/page-5.json",
  "values": [
    {
      "content": "\n\nSetting up a RetroFlag GPi Case\n===============================\n\nBy R. S. Doiel, 2020-12-24\n\nThese are my notes for setting up a RetroFlag GPi case using Recalbox\ndistribution for retro gaming.\n\n+ RetroFlag GPi Case Kit (including a Raspberry Pi Zero W and blank SD Card)\n+ A computer to setup the SD Card  and the Raspberry Pi Imager v1.5\n\nWe will be installing [Recalbox](https://www.recalbox.com/ \"the all-in-one retro gaming console\")\nv7.7.x for Raspberry Pi Zero W and GPi case.  Recalbox which is a Retro\nGaming Linux distribution.\n\nSteps\n=====\n\nPreparing the 32 GiB SD Card\n---------------------------\n\n1. Download the appropriate Raspberry Pi Imager 1.5 from \n   https://www.raspberrypi.org/software/ for your system\n2. Install and launch the Raspberry Pi Imager\n3. Click \"Operating System\"\n  a. Select \"Emulation and game OS\"\n  b. Select \"Recalbox\"\n  c. Select \"Recalbox 7.1.1-Reloaded (Pi 0/1/GPi Case)\"\n4. Click \"SD Card\" \", then select the bank 32 GiB SD Card\n5. Click \"Write\"\n6. You will be asked remove the SD Card when done, do so and and exit \n   Raspberry Pi Imager\n\nNOTE: The current release of Recalbox (7.7.1) doesn't require patching\nwith \"GPI_Case_patch.zip\" or installing the shutdown scripts as suggested\non the RetroFlag website. Applying the patches will prevent the GPi\nfrom booting. The website instructions appear to be for an earlier release\nof Recalbox.\n\n\nInstalling the Raspberry Pi Zero W in the GPi Case\n--------------------------------------------------\n\nThe RetroFlag comes with instructions to install the Raspberry Pi Zero W\nin the case. I found the pictorial instructions confusing. Doing a search\nfor \"RetroFlag GPi Case Setup\" yielded a link to [Howchoo's YouTube\nvideo](https://www.youtube.com/watch?v=NyJUlNifN1I&feature=youtu.be \"RetroFlag GPi CASE Setup and Usage\"),  This video also talks about setting up Retro Pi software,\nGPi case patches. Skip these. The instructions are now for software that\nis out of date (the video dates back to 2019). \n\nNOTE: Howchoo describes installing RetroPie not Recalbox. Don't install a\n\"wpa_supplicant.conf\" file or \"ssh\" file on the SD Card as suggested.\nIt is not needed and will cause problems.\n\nThe GPi case looks very much like a Game Boy. It includes a \"Game Pack\"\ntype module which will hold our Raspberry Pi once installed. I found the\nassembly instructions confusing but searching YouTube for \"RetroFlag GPi\nCase Setup\" listed several videos which describe the process of putting\nthe case together as well as how to install RetroPie or\nRecalbox Linux Distributions.\n\nBooting the Pi Zero W with the SD Card\n--------------------------------------\n\n1. Make sure the GPi Zero Case **IS NOT CONNECTED TO POWER**\n  a. the switch the case off\n  b. Disconnect the barrel to USB cable from a power source\n2. Remove the \"game pack\" element where you've installed the Raspberry Pi Zero W\n3. Insert the SD Card into the SD Card slot under the soft cover on the side of\n   the Game Pack case\n4. Re-insert \"Game Pack\" into side of the GPi case\n5. Plug the barrel USB cable into a USB Power supply , \n6. Turn the power switch to \"ON\" on the top of the GPi case\n7. Wait patiently, it's going to take several minutes to boot the first time\n\n\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-12-24",
        "keywords": [
          "raspberry pi",
          "retro games",
          "case"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "RetroFlag GPi Case Setup"
      },
      "url": "posts/2020/12/24/gpi-case-setup.json"
    },
    {
      "content": "\n\n# Instant Articles, Accelerated Mobile Pages, Twitter Cards and Open Graph\n\nBy R. S. Doiel 2016-05-30\n\n## The problem\n\nThe web has gotten slow. In [2016](http://httparchive.org/trends.php) the \naverage page weight is in multi-megabytes and the average number of network \nrequests needed to deliver the content is counted in \nthe hundreds. In the mix are saturated networks and a continued public \nexpectation of responsiveness (web wisdom suggests you have about 3 seconds \nbefore people give up).  The odd thing is we've known how to build fast \nwebsites for a [decade](https://www.stevesouders.com/) or so.  \nCollectively we don't build them [fast](https://www.sitepoint.com/average-page-weight-increased-another-16-2015/). \n\n\n## Meet the new abstractions\n\nCorporations believe they have the answer and they are providing us \nwith another set of abstractions. In a few years maybe these will \nget distilled down to a shared common view but in the mean time disc \ncosts remain reasonably priced and generating these new forms of \npages or feeds is a template or so away.\n\n+ [Twitter Cards](https://dev.twitter.com/cards/overview) and [Open Graph](http://ogp.me/)\n  + Exposing your content via social media, search results or embedded in pages via an aside element\n+ [Accelerated Mobile Pages](https://www.ampproject.org/) (also called AMP)\n  + A simplification in content delivery to improve web reading experience\n  + Its usefulness is it proscribes an approach to leverage what we have\n  + AMP works well with Twitter Cards, Open Graph and can leverage Web Components\n+ [Instant Articles](https://instantarticles.fb.com/)\n  + a format play to feed the walled garden of Facebook for iOS and Android devices\n\n\n## The players \n\n### Twitter Cards and Open Graph\n\nTwitter's Titter Cards and Facebook's Open Graph offer approaches to \nbuild off of our existing meta elements in an HTML page's document \nhead.  They are named space to avoid collisions but supporting both \nwill still result in some content duplication. The k-weight \ndifference in the resulting HTML pages isn't too bad. \n\nAdopting either or both is a matter of adjusting how your render your \nweb page's head block.  It is easy enough to do manually but easier \nstill using some sort of template system that renders the appropriate \nmeta elements based on the content type and contents in the page \nbeing rendered.  \n\nGoogle and other search engines can leverage this richer meta \ndata and integrate it into their results. Google's Now application can \nrender content cards based on either semantic. It also appears that \ncontent cards are being leverage selectively for an aside and related \ncontent on Google search results pages. You could even built this into \nyour own indexing process for use with the Solr or Elasticsearch.\n\nContent Cards offer intriguing opportunity for web crawlers and search \nengines.  This is particularly true when combined with mature feed \nformats like RSS, OPML, Atom and the maturing efforts in the linked \ndata community around JSON-LD.\n\n\n### AMP - Accelerated Mobile Pages\n\nThe backers of AMP (not to be confused with Apache+MySQL+PHP) are largely\npublishers including major news outlets and web media\ncompanies in the US and Europe. This is an abridged list from 2015--\n\n+ BBC\n+ Atlantic Media\n+ Vox Media\n+ Conde Nast\n+ New York Times\n+ Wall Street Journal\n+ The Daily Mail\n+ Huffington Post\n+ Gannet\n+ The Guardian\n+ The Economist\n+ The Financial Times\n\nIn additional to the publishers there is participation by tech companies\nsuch as Google, Pinterest, Twitter, LinkedIn and Wordpress.com.  Accelerated\nMobile Pages offer benefits for web crawlers and search engines supporting\nsurfacing content is clearly and enabling easier distinction from \nadvertisements. \n\n\n### Instant Articles\n\nIn additional to Open Graph Facebook has put forward [Instant Articles](https://developers.facebook.com/docs/instant-articles).\nLike AMP it is targeting content delivery for mobile. Unlike AMP Instant Articles is an\nexplicit binding into Facebook's walled garden only exposing the content on supported\nversions of iOS and Android. You don't see Instant Articles in your Facebook timeline or when  \nyou browse from a desktop web browser.  Unlike the previous\nexamples you actually need to sign up to participate in the Instant Article publishing\nprocess.  Sign up cost is having a Facebook account, being approved by Facebook and compliance\nwith their terms of service. Facebook does provide some publishing tools, publishing controls\nas well as some analytics. They do allow 3rd party ads as well as encourage access to\ntheir advertising network.  Once approved the burden on your content manage process \nappears manageable.  \n\nYou can submit Instant Articles via a modified RSS feed or directly through their API. \nIn this sense the overhead is about the same as that for implementing support for Twitter Cards\nOpen Graph, and AMP. Facebook does a good job of quickly propagating changes to your\nInstant Articles across their platform. That's nice.\n\nWhy go through the trouble? If you're a content producer and your audience lives on Facebook\nFacebook commands the attention of a lot of eye balls.  Instant Articles provides \nanother avenue to reach them.  For some Facebook effectively controls the public view of the \nweb much as America Online and Prodigy did decades ago. [Dave Winer](https://twitter.com/davewiner) \nhas written extensively on how he implemented Instant Article support along with \nsome very reasoned pros and cons for doing so. The landscape is evolving and \n[Dave's river of news](http://scripting.com) is worth following.\n\n\n## Impact on building content\n\nThese approaches require changes in your production of your HTML and RSS sent to the browser.\nTwitter Cards and Open Graph change what you put in the HEAD element of the HTML\npages.  AMP proscribes what you should put in the BODY element of the webpage.\nInstant Articles tweaks your RSS output.  Not surprisingly the major content management \nsystems Wordpress and Drupal have plugins for this.  All can be implemented via your template \nsystem or page generation process.\n\n\n## Whither adopt?\n\nBecause these approaches boil down to content assembly the adoption risk \nis low.  If your audience views Twitter, Facebook or Google search results \nthen it is probably worth doing.  All allow you to continue to publish your \nown content and own your URLs as opposed to being a tenant on one or another \nplatform. That benefits the open web.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-05-30",
        "keywords": [
          "structured data",
          "amp",
          "opengraph",
          "twitter",
          "google",
          "facebook",
          "instant pages"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Instant Articles, Accelerated Mobile Pages, Twitter Cards and Open Graph"
      },
      "url": "posts/2016/05/30/amp-cards-and-open-graph.json"
    },
    {
      "content": "\n\nOPML to Markdown and back\n=========================\n\nBy R. S. Doiel 2016-05-28\n\n## Overview\n\nI wrote a Go language package to sort [OPML](http://dev.opml.org/spec2.html) outlines. \nI wrote this because my preferred [feed reader ](http://goread.io) supports manual \nsorting but not automatic alpha sorting by the _outline_ element's _text_ attribute. \n\n## Observations\n\nOut of the box the OPML 2 Spec provides attributes indicating inclusion of other OPML files,\nscripts, basic metadata (create, modified, authorship), and even directory structures.\n\n[Fargo](http://fargo.io) allows user defined attributes to be applied to the _outline_ \nelement in OPML. This could be used in support some of the \n[Scrivener](https://www.literatureandlatte.com/scrivener.php)\nfeatures I miss such as describing how to render a project to various formats such as\nrtf, pdf, ePub, web pages or even [Final Draft fdx](https://www.finaldraft.com/) files.\n\nI write allot of Markdown formatted text.  Markdown is simple to index, \nsearch and convert into useful formats. Markdown is not good at expressing more\ncomplex structures such as metadata. Website generators that use markdown often\nrequire a preamble or _front matter_ in the markdown to provide any metadata. This\nleaves your document head cluttered and less human readable.\n\nAnother approach is to include a parallel document with the metadata.  It occurred to me \nthat an OPML file could easily hold that metadata. It can even hold Markdown content.\nThe trouble with OPML is that it is not quick to edit by hand.\n\n    Is there a round trip semantic mapping between OPML and Markdown?\n\n\n## Germination of an idea\n\nEntering a web link in Fargo the link is URL encoded and saved in the _text_ attribute of the \n_outline_ element.\n\nThe source view of a web links in Fargo's _outline_ element looks like\n\n```OPML\n    <outline text=\"&gt; href=&quot;http://example.org&quot;&lt;My example.org&gt;/a&lt;\" />\n```\n\nThat _outline_ element might render in Markdown as\n\n```\n    + [My element.org](http://example.org)\n```\n\nThe steps to create the Markdown view are simple\n\n1. URL decode the _text_ attribute\n2. Convert HTML to Markdown\n\nMaking a round trip could be done by\n\n3. Convert Markdown into HTML\n4. For each _li_ element covert to an _outline_ element URL encoding the inner HTML of the _li_\n\nSo far so good. What about something more complex?\n\n\nHere's an _outline_ element example from http://hosting.opml.org/dave/spec/directory.opml \n\n```OPML\n    <outline text=\"Scripting News sites\" created=\"Sun, 16 Oct 2005 05:56:10 GMT\" type=\"link\" url=\"http://hosting.opml.org/dave/mySites.opml\"/>\n```\n\nTo me that should look like \n\n```\n    + [Scripting News Sites](http://hosting.opml.org/dave/mySites.opml)\n```\n\nWhat about the _created_ attribute? Could we render this case as an additional set of anchors using data uri?\n\nThis suggest a rule like\n\n+ if the _text_ attribute contains HTML markup\n    + URL decode into HTML\n    + Convert HTML to Markdown\n+ else render attributes as additional anchors using data URI\n\nThis might work as follows. \n\n```OPML\n    <outline text=\"Scripting News sites\" \n        created=\"Sun, 16 Oct 2005 05:56:10 GMT\" \n        type=\"link\" \n        url=\"http://hosting.opml.org/dave/mySites.opml\"/>\n```\n\nWould become \n\n```Markdown\n    + [Scripting News Sites](http://hosting.opml.org/dave/mySites.opml) [type](data:text/plain;link) [created](data:text/date;Sun, 16 Oct 2005 05:56:10 GMT)\n```\n\nIn HTML this would look like\n\n```HTML\n    <li><a href=\"http://histing.opml.org/dave/mySites.opml\">Scripting News Sites</a>\n        <a href=\"data:text/plain;link\">type</a>\n        <a href=\"data:text/date;Sun, 16 Oct 2005 05:56:10 GMT\">created</a></li>\n```\n\n### Markdown to OPML\n\nComing back to OPML from Markdown then becomes\n\n+ Convert Markdown to HTML\n+ For each _li_ element inspect anchors, \n    + if anchors contain data URI then map _outline_ element\n    + else URL encode and embed in _outline_ _text_ attribute\n\nIs this viable? Does it have any advantages?\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-05-28",
        "keywords": [
          "golang",
          "opml",
          "markdown"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "OPML to Markdown and back"
      },
      "url": "posts/2016/05/28/OPML-to-Markdown-and-back.json"
    },
    {
      "content": "\n\n# Exploring Bash for Windows 10 Pro\n\nBy R. S. Doiel 2016-08-15\n\n    UPDATE (2016-10-27, RSD): Today trying to compile Go 1.7.3 under \n    Windows 10 Pro I've am getting compile errors when the \n    assembler is being built.  I can compile go1.4.3 but see errors \n    in some of the tests results.\n\n## Initial Setup and configuration\n\nI am running Windows 10 Pro (64bit) Anniversary edition under Virtual Box. The VM was upgraded from an earlier version of Windows 10 Pro (64bit). The VM was allocated 4G or ram, 200G disc and simulating 2 cores.  After the upgrade I took the following steps\n\n+ Search with Bing for \"Bash for Windows\" \n    + Bing returns http://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/\n+ Switch on developer mode for Windows\n+ Turned on Linux Subsystem Beta (searched for \"Turning on Features\")\n+ Reboot\n+ Search for \"Bash\" and clicked on \"Run Bash command\"\n+ Answered \"y\"\n+ Waited for download and extracted file system\n+ When prompted setup developer account with username/password\n    + Documentation can be found at https://aka.ms/wsldocs\n+ Exit root install shell\n+ Search for \"Bash\" locally\n+ Launched \"Bash on Ubuntu on Windows\"\n+ Authenticate with your username/password\n\n\n## Setting up Go under Bash for Windows 10\n\nWith Bash installed these are the steps I took to compile Go\nunder Bash on Ubuntu on Windows.\n\n```shell\n    sudo apt-get update && sudo apt-get upgrade -y\n    sudo apt-get autoremove\n    sudo apt-get install build-essential clang git-core unzip zip -y\n    export CGO_ENABLE=0\n    git clone https://github.com/golang/go go1.4\n    git clone https://github.com/golang/go go\n    cd go1.4\n    git checkout go1.4.3\n    cd src\n    ./all.bash\n    cd\n    export PATH=$PATH:$HOME/go1.4/bin\n    cd go\n    git checkout go1.7\n    cd src\n    ./all.bash\n    cd\n    export PATH=$HOME/go/bin:$HOME/bin:$PATH\n    export GOPATH=$HOME\n```\n\nNote some tests failing during compilation in both 1.4.3 and 1.7. They mostly failed\naround network sockets.  This is probably a result of the limitations in the Linux subsystem\nunder Windows.\n\nIf successful you should be able to run `go version` as well as install additional Go based software\nwith the usual `go get ...` syntax.\n\nIn your `.bashrc` or `.profile` add the following\n\n```shell\n    export PATH=$HOME/go/bin:$HOME/bin:$PATH\n    export GOPATH=$HOME\n```\n\n\n## Improved vim setup\n\nI like the vim-go packages for editing Go code in vim. They are easy to setup.\n\n```shell\n     mkdir -p ~/.vim/autoload ~/.vim/bundle \n     curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim\n     git clone https://github.com/fatih/vim-go.git ~/.vim/bundle/vim-go\n```\n\nExample $HOME/.vimrc\n\n```vimrc\n    execute pathogen#infect()\n    syntax on\n    filetype plugin on\n    set ai\n    set nu\n    set smartindent\n    set tabstop=4\n    set shiftwidth=4\n    set expandtab\n    let &background = ( &background == \"dark\"? \"light\" : \"dark\" )\n    let g:vim_markdown_folding_disabled=1\n```\n\nColor schemes are browsable at [vimcolors.com](http://vimcolors.com). They can be installed in\n$HOME/.vim/colors.\n\n1. git clone and place the colorscheme\n2. place the *.vim file holding the color scheme into $HOME/.vim/colors\n3. start vim and at the : do colorscheme NAME where NAME is the scheme you want to try\n\nYou can find the default shipped color schemes in /usr/share/vim/vimNN/colors where vimNN is the version number\ne.g. /usr/share/vim/vim74/colors.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-08-15",
        "keywords": [
          "Golang",
          "Windows",
          "Bash",
          "Linux Subsystem"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Exploring Bash for Windows 10 Pro"
      },
      "url": "posts/2016/08/15/Setting-up-Go-under-Bash-for-Windows-10.json"
    },
    {
      "content": "\n\nFrom Markdown and Bash to mkpage\n================================\n\nBy R. S. Doiel 2016-08-16\n\nWhen I started maintaining a website on GitHub a few years ago my needs\nwere so simple I hand coded the HTML.  Eventually I adopted \na markdown processor for maintaining the prose. My \"theme\" was a\nCSS file and some HTML fragments to wrap the markdown output. If I needed \ninteractivity I used JavaScript to access content via a web API. \nLife was simple, all I had to learn to get started was Git and how to\npopulate a branch called \"gh-pages\".\n\n\n## Deconstructing Content Management Systems\n\nRecently my website needs have grown. I started experimenting with static\nsite generators thinking an existing system would be the right fit. \nWhat I found were feature rich systems that varied primarily in \nimplementation language and template engine. Even though I wasn't\nrequired to run Apache, MySQL and PHP/Perl/Python/Ruby/Tomcat it felt \nlike the static site generators were racing to fill a complexity \nvacuum. In the end they were interesting to explore but far more\nthan I was willing to run. I believe modern content management systems can\nbe deconstruct into something simpler.\n\nSome of the core elements of modern content management systems are\n\n+ creation and curation of data sources (including metadata)\n+ transforming data sources if needed\n+ mapping a data source to appropriate template set\n+ rendering template sets to produce a final website\n\nModern static site generators leave creation and curation to your \ntext editor and revision control system (e.g. vi and git). \n\nMost static site generators use a simplified markup. A populate one is\ncalled [Markdown](https://en.wikipedia.org/wiki/Markdown). This \"markup\"\nis predictable enough that you can easily convert the results to HTML and\nother useful formats with tools like [pandoc](http://pandoc.org/). In most \nstatic site generators your content is curated in Markdown and when the \npages are built it is rendered to HTML for injection into your website's \ntemplate and theme.\n\nMapping the data sources to templates, combining the templates and rendering \nthe final website is where most systems introduce a large amount of complexity.\nThis is true of static site generators like [Jekill](https://jekyllrb.com) and \n[Hugo](https://gohugo.io).\n\n\n## An experimental deconstruction\n\nI wanted a simple command line tool that would make a single web page.\nIt would take a few data sources and formats and run them through a\ntemplate system. The template system needed to be simple but support\nthe case where data might not be available. It would be nice if it handled\nthe case of repetitious data like that used in tables or lists. Ideally\nI could render many pages from a single template assuming a simple website\nand layout.\n\n### A single page generator\n\n[mkpage](https://github.com/rsdoiel/mkpage) started as an experiment in\nbuilding a simple single page generator. It's responsibilities\ninclude mapping data sources to the template, transforming data if needed\nand rendering the results. After reviewing the websites I've setup in\nthe last year or two I realized I had three common types of data.\n\n1. Plain text or content that did not need further processing\n2. Markdown content (e.g. page content, navigation lists)\n3. Occasionally I include content from JSON feeds\n\nI also realized I only needed to handle three data sources.\n\n1. strings\n2. files\n3. web resources\n\nEach of these sources might provide plain text, markdown or JSON data formats.\n\nThat poses the question of how to express the data format and the data \nsource when mapping the content into a template. The web resources are\neasy in the sense that the web responses include content type information.\nFiles can be simple too as the file extension indicates their\nformat (e.g. \".md\" for Markdown, \".json\" for JSON documents). What remained\nwas how to identify a text string's format.  I opted for a prefix ending in \na colon (e.g. \"text:\" for plain text, \"markdown:\" for markdown \nand \"json:\" for JSON). This mapping allows for a simple key/value\nrelationship to be expressed easily on the command line.\n\n### mkpage in action\n\nDescribing how to build \"mypage.html\" from \"mypage.md\" and \"nav.md\" \n(containing links for navigating the website) is as easy as typing\n\n```shell\n    mkpage \"content=mypage.md\" \"navigation=nav.md\" page.tmpl > mypage.html\n```\n\nIn this example the template is called \"page.tmpl\" and we redirect the \noutput to \"mypage.html\".\n\n\nAdding a custom page title is easy too.\n\n```shell\n    mkpage \"title=text:My Page\" \\\n        \"content=mypage.md' \"navigation=nav.md\" \n        page.tmpl \\\n        > mypage.html\n```\n\nLikewise integrating some JSON data from weather.gov is relatively straight\nforward. The hardest part is discovering the [URL](http://forecast.weather.gov/MapClick.php?lat=34.0522&lon=118.2437&DFcstType=json) \nthat returns JSON!  Notice I have added a weather field and the URL. When data\nis received back from weather.gov it is JSON decoded and then passed to the\ntemplate for rendering using the \"range\" template function.\n\n```shell\n    mkpage \"title=My Page\" \\\n        \"content=mypage.md\" \\\n        \"navigation=nav.md\" \\\n        \"weather=http://forecast.weather.gov/MapClick.php?lat=34.0522&lon=118.2437&DFcstType=json\" \\\n        page.tmpl \\\n        > mypage.html\n```\n\nWhat is *mkpage* doing?\n\n1. Reading the data sources and formats from the command line\n2. Transforming the Markdown and JSON content appropriately\n3. Applying them to the template (e.g. page.tmpl)\n4. Render the results to stdout\n\nBuilding a website then is only a matter of maintaining navigation in\n*nav.md* and identifying the pages needing to be created. I can easily \nautomated that using the Unix find, grep, cut and sort. Also with find \nI can iteratively process each markdown file applying a \ntemplate and rendering the related HTML file.  This can be done for a site \nof a few pages (e.g. about, resume and cv) to more complex websites like \nblogs and repository activities.\n\nHere's an example template that would be suitable for the previous\ncommand line example. It's mostly just HTML and some curly bracket notation \nsprinkled in.\n\n```html\n    <!DOCTYPE html>\n    <html>\n    <head>\n        {{with .title}}<title>{{- . -}}</title>{{end}}\n        <link rel=\"stylesheet\" href=\"css/site.css\">\n    </head>\n    <body>\n        <nav>\n        {{ .navigation }}\n        </nav>\n        <section>\n        {{ .content }}\n        </section>\n        <aside>\n        Weather Demo<br />\n        <ul>\n        {{range .weather.data.text}}\n            <li>{{ . }}</li>\n        {{end}}\n        </ul>\n        </aside>\n\n    </body>\n    </html>\n```\n\nYou can find out more about [mkpage](https://github.com/rsdoiel/mkpage)\n[rsdoiel.github.io/mkpage](https://rsdoiel.github.io/mkpage).\n\nTo learn more about Go's text templates see \n[golang.org/pkg/text/template](https://golang.org/pkg/text/template/). \n\nIf your site generator needs are more than *mkpage* I suggest [Hugo](https://gohugo.io). \nIt's what I would probably reach for if I was building a large complex organizational\nsite or news site.\n\nIf you're looking for an innovative and rich author centric content system\nI suggest Dave Winer's [Fargo](http://fargo.io) outliner and [1999.io](https://1999.io).\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-08-16",
        "keywords": [
          "Bash",
          "Markdown",
          "site generator",
          "mkpage",
          "pandoc"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "From Markdown and Bash to mkpage"
      },
      "url": "posts/2016/08/16/From-Markdown-and-Bash-to-mkpage.json"
    },
    {
      "content": "\n\nHow to make a Pi-Top more Raspbian\n==================================\n\nBy R. S. Doiel, 2016-07-04\n\nI have a first generation Pi-Top.  I like the idea but found I didn't use it much due to a preference for\nbasic Raspbian. With the recent Pi-TopOS upgrades I realized getting back to basic Raspbian was relatively\nstraight forward.\n\n## The recipe\n\n1. Make sure you're running the latest Pi-TopOS based on Jessie\n2. Login into your Pi-Top normally\n3. From the Pi-Top dashboard select the \"Desktop\" icon\n4. When you see the familiar Raspbian desktop click on the following things\n\t+ Click on the Raspberry Menu (upper left corner)\n\t+ Click on Preferences\n\t+ Click on Raspberry Pi Configuration\n5. I made the following changes to my System configuration\n\t+ Under *Boot* I selected \"To CLI\"\n\t+ I unchecked *login as user \"pi\"*\n6. Restart your Pi Top\n\t+ Click on Raspberry Menu in the upper left of the desktop\n\t+ Click on shutdown\n\t+ Select *reboot*\n7. When you restart you'll see an old school console login, login as the pi user using your Pi-Top password\n8. Remove the following program use the *apt* command\n\t+ ceed-universe\n\t+ pt-dashboard\n\t+ pt-splashscreen\n\n```\n    sudo apt purge ceed-universe pt-dashboard pt-splashscreen\n```\n\nNote: pi-battery, pt-hub-controller, pt-ipc, pt-speaker are hardware drivers specific to your Pi-Top so you probably\nwant to keep them.\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-07-04",
        "keywords": [
          "Raspberry Pi",
          "Pi-Top",
          "Rasbian",
          "Raspberry Pi OS",
          ":operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "How to make a Pi-Top more Raspbian"
      },
      "url": "posts/2016/07/04/How-To-Make-A-PiTop-More-Raspbian.json"
    },
    {
      "content": "\n\n# Android, Termux and Dev Environment\n\nBy R. S. Doiel 2016-09-20\n\nRecently I got a new Android 6 tablet. I got a case with a tiny Bluetooth keyboard. I started wondering if I could use it as a development device when on the road. So this is my diary of that test.\n\n## Challenges\n\n1. Find a way to run Bash without rooting my device\n2. See if I could use my normal web toolkit\n\t+ curl\n\t+ jq\n\t+ sed\n\t+ grep\n3. See if I could compile or add my own custom Golang programs\n4. Test setup by running a local static file server, mkpage and update my website\n\n## Searching for Android packages and tools of my toolbox\n\nAfter searching with Duck Duck Go and Google I came across the [termux](https://termux.com). Termux provides a minimal Bash shell environment with support for adding\npackages with _apt_ and _dpkg_.  The repositories visible to *termux* include\nmost of the C tool chain (e.g. clang, make, autoconf, etc) as well as my old Unix favorites _curl_, _grep_, _sed_, _gawk_ and a new addition to my toolkit _jq_.  Additionally you'll find recent versions (as of Sept. 2016) versions of _Golang_, _PHP_, _python_, and _Ruby_.\n\nThis quickly brought me through step 3.  Installing _go_, _git_, and _openssh_ completed what I needed to test static site development with some of the tools in our incubator at [Caltech Library](https://caltechlibrary.github.io).\n\n## Setting up for static site development\n\nAfter configuring _git_, adding my public key to GitHub and running _go get_ on my\ncustom static site tools I confirmed I could build and test static websites from my Android tablet using *Termux*.\n\nHere's the list of packages I installed under *Termux* to provide a suitable shell environment for writing and website constructions.\n\n```shell\n    apt install autoconf automake bash-completion bc binutils-dev bison \\\n        bzip2 clang cmake coreutils ctags curl dialog diffutils dos2unix \\\n        expect ffmpeg findutils gawk git gnutls golang grep gzip \\\n\timagemagick jq less lynx m4 make-dev man-dev nano nodejs \\\n        openssh patch php-dev python readline-dev rlwrap rsync ruby-dev \\\n        sed sensible-utils sharutils sqlite tar texinfo tree unzip vim \\\n        w3m wget zip\n```\n\nThis then allowed me to setup my *golang* environment variables and install\nmy typical custom written tools\n\n```shell\n    export PATH=$HOME/bin:$PATH\n    export GOPATH=$HOME\n    export GOBIN=$HOME/bin\n    go get github.com/rsdoiel/shelltools/...\n    go get github.com/caltechlibrary/mkpage/...\n    go get github.com/caltechlibrary/md2slides/...\n    go get github.com/caltechlibrary/ws/...\n```\n\nFinally pulled down some content to test.\n\n```shell\n    cd\n    mkdir Sites\n    git clone https://github.com/rsdoiel/rsdoiel.github.io.git Sites/rsdoiel.github.io\n    cd  Sites/rsdoiel.github.io\n    ws\n```\n\nThis started the local static site webserver and I pointed by Firefox for Android at http://localhost:8000 and saw a local copy of my personal website. From there I wrote this article and updated it just as if I was working on a Raspberry Pi or standard Linux laptop.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-09-20",
        "keywords": [
          "Bash",
          "cURL",
          "jq",
          "sed",
          "grep",
          "search",
          "golang",
          "Android"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Android, Termux and Dev Environment"
      },
      "url": "posts/2016/09/20/Android-Termux-Dev-environment.json"
    },
    {
      "content": "\n\n# NodeJS, NPM, Electron\n\nBy R. S. Doiel 2017-10-20\n\nElectron is an app platform leveraging web technologies. Conceptually it is a\nmashup of NodeJS and Chrome browser. [Electron](https://electron.atom.io/) site\nhas a nice starter app. It displays a window with Electron version info and\n'hello world'.\n\nBefore you can get going with _Electron_ you need to have a\nworking _NodeJS_ and _NPM_. I usually compile from source and this\nwas my old recipe (adjusted for v8.7.0).\n\n```shell\n    cd\n    git clone https://github.com/nodejs/node.git\n    cd node\n    git checkout v8.7.0\n    ./configure --prefix=$HOME\n    make && make install\n```\n\nTo install an _Electron Quick Start_ I added the additional steps.\n\n```shell\n    cd\n    git clone https://github.com/electron/electron-quick-start\n    cd electron-quick-start\n    npm install\n    npm start\n```\n\nNotice _Electron_ depends on a working _node_ and _npm_.  When I\ntried this recipe it failed on `npm install` with errors regarding\ninternal missing node modules.\n\nAfter some fiddling I confirmed my node/npm install failed because\nI had install the new version of over a partially installed previous\nversion. This causes the node_modules to be populated with various\nconflicting versions of internal modules.\n\nSorting that out allowed me to test the current version of\n*electron-quick-start* cloned on 2017-10-20 under _NodeJS_ v8.7.0.\n\n## Avoiding Setup Issues in the future\n\nThe *Makefile* for _NodeJS_ includes an 'uninstall' option. Revising\nmy _NodeJS_ install recipe above I now do the following to setup a machine\nto work with _NodeJS_ or _Electron_.\n\n```shell\n    git clone git@github.com:nodejs/node.git\n    cd node\n    ./configure --prefix=$HOME\n    make uninstall\n    make clean\n    make -j 5\n    make install\n```\n\nIf I am on a device with a multi-core CPU (most of the time) you can speed\nup the make process using a `-j CPU_CORE_COUNT_PLUS_ONE` option (e.g. `-j 5`\nfor my 4 core x86 laptop).\n\nOnce _node_ and _npm_ were working normally the instructions in the\n*electron-quick-start* worked flawlessly on my x86.\n\nI have tested the node install recipe change on my Pine64 Pinebook, on \nseveral Raspberry Pi 3s as well as my x86 Ubuntu Linux laptop.\n\nI have not gotten Electron up on my Pine64 Pinebook or Raspberry Pi's yet. \n`npm install` outputs errors suggesting that it is expecting an x86 architecture.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2017, R. S. Doiel",
        "date": "2017-10-20",
        "keywords": [
          "Javascript",
          "NodeJS",
          "Electron"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "NodeJS, NPM, Electron"
      },
      "url": "posts/2017/10/20/node-npm-electron.json"
    },
    {
      "content": "\n\n# Cross compiling Go 1.8.3 for Pine64 Pinebook\n\nBy R. S. Doiel 2017-06-16\n\nPine64's Pinebook has a 64-bit Quad-Core ARM Cortex A53 which is \nnot the same ARM processor found on a Raspberry Pi 3. As a \nresult it needs its own compiled version of Go. Fortunately cross \ncompiling Go is very straight forward. I found two helpful Gists\non GitHub discussing compiling Go for a 64-Bit ARM processor. \n\n+ [conoro's gist](https://gist.github.com/conoro/4fca191fad018b6e47922a21fab499ca)\n+ [truedat101's gist](https://gist.github.com/truedat101/5898604b1f7a1ec42d65a75fa6a0b802)\n\nI am using a Raspberry Pi 3, raspberrypi.local, as my cross compile \nhost. Go 1.8.3 is already compiled and available.  Inspired by the \ngists I worked up with this recipe to bring a Go 1.8.3 to my Pinebook.\n\n```shell\n    cd\n    mkdir -p gobuild\n    cd gobuild\n    git clone https://github.com/golang/go.git go1.8.3\n    cd go1.8.3\n    git checkout go1.8.3\n    export GOHOSTARCH=arm\n    export GOARCH=arm64\n    export GOOS=linux\n    cd src\n    ./bootstrap.bash\n```\n\nAfter the bootstrap compile is finished I switch to my Pinebook,\ncopy the bootstrap compiler to my Pinebook and use it to compile\na new go1.8.3 for Pine64.\n\n```shell\n    cd\n    scp -r raspberrypi.local:gobuild/*.tbz ./\n    tar jxvf go-linux-arm64-bootstrap.tbz\n    export GOROOT=go-linux-arm64-bootstrap\n    go-linux-arm64-bootstrap/bin/go version\n    unset GOROOT\n    git clone https://github.com/golang/go\n    cd go\n    git checkout go1.8.3\n    export GOROOT_BOOTSTRAP=$HOME/go-linux-arm64-bootstrap\n    cd src\n    ./all.bash\n```\n\n_all.bash_ will successfully compile _go_ and _gofmt_ but fail on \nthe tests. It's not perfect but appears to work as I explore\nbuilding Go applications on my Pinebook. Upcoming Go releases should\nprovide better support for 64 bit ARM.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-06-16",
        "keywords": [
          "Golang",
          "Pine64",
          "ARM"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Cross compiling Go 1.8.3 for Pine64 Pinebook"
      },
      "url": "posts/2017/06/16/cross-compiling-go.json"
    },
    {
      "content": "\n\n# Harvesting my Gists from GitHub\n\nBy R. S. Doiel 2017-12-10\n\nThis is a just quick set of notes on harvesting my Gists on GitHub so I\nhave an independent copy for my own website. \n\n## Assumptions\n\nIn this gist I assume you are using Bash on a POSIX system (e.g. Raspbian \non a Raspberry Pi) with the standard compliment of Unix utilities (e.g. cut, \nsed, curl). I also use Stephen Dolan's [jq](https://github.com/stedolan/jq)\nas well as Caltech Library's [datatools](https://github.com/caltechlibrary/datatools).\nSee the respective GitHub repositories for installation instructions.\nThe gist harvest process was developed against GitHub's v3 API\n(see developer.github.com). \n\nIn the following examples \"USER\" is assumed to hold your GitHub user id \n(e.g. rsdoiel for https://github.com/rsdoiel).\n\n## Getting my basic profile\n\nThis retrieves the public view of your profile.\n\n```shell\n    curl -o USER \"https://api.github.com/users/USER\"\n```\n\n## Find the urL for your gists\n\nGet the gists url from `USER.json.\n\n```shell\n    GISTS_URL=$(jq \".gists_url\" \"USER.json\" | sed -E 's/\"//g' | cut -d '{' -f 1)\n    curl -o gists.json \"${GISTS_URL}\"\n```\n\nNow `gists.json` should hold a JSON array of objects representing your Gists.\n\n## Harvesting the individual Gists.\n\nWhen you look at _gists.json_ you'll see a multi-level JSON structure.  It has been\nformatted by the API so be easy to scrape.  But since this data is JSON and Caltech Library\nhas some nice utilities for working with JSON I'll use *jsonrange* and *jq* to pull out a list\nof individual Gists URLS.\n\n```shell\n    jsonrange -i gists.json | while read I; do \n        jq \".[$I].files\" gists.json | sed -E 's/\"//g'\n    done\n```\n\nExpanding this we can now curl each individual gist metadata to find URL to the raw file.\n\n\n```shell\n    jsonrange -i gists.json | while read I; do \n        jq \".[$I].files\" gists.json | jsonrange -i - | while read FNAME; do\n            jq \".[$I].files[\\\"$FNAME\\\"].raw_url\" gists.json | sed -E 's/\"//g'; \n        done;\n    done\n```\n\nNow that we have URLs to the raw gist files we can use curl again to fetch each.\n\nWhat do we want to store with our harvested gists?  The raw files, metadata\nabout the Gist (e.g. when it was created), the Gist ID. Putting it all together\nwe have the following script.\n\n```shell\n    #!/bin/bash\n    if [[ \"$1\" = \"\" ]]; then\n        echo \"USAGE: $(basename \"$0\") GITHUB_USERNAME\"\n        exit 1\n    fi\n\n    USER=\"$1\"\n    curl -o \"$USER.json\" \"https://api.github.com/users/$USER\"\n    if [[ ! -s \"$USER.json\" ]]; then\n        echo \"Someting went wrong getting https://api.github.cm/users/${USER}\"\n        exit 1\n    fi\n\n    GISTS_URL=$(jq \".gists_url\" \"$USER.json\" | sed -E 's/\"//g' | cut -d '{' -f 1)\n    curl -o gists.json \"${GISTS_URL}\"\n    if [[ ! -s gists.json ]]; then\n        echo \"Someting went wrong getting ${GISTS_URL}\"\n        exit 1\n    fi\n\n    # For each gist harvest our file\n    jsonrange -i gists.json | while read I; do\n        GIST_ID=$(jq \".[$I].id\" gists.json | sed -E 's/\"//g')\n        mkdir -p \"gists/$GIST_ID\"\n        echo \"Saving gists/$GIST_ID/metadata.json\"\n        jq \".[$I]\" gists.json > \"gists/$GIST_ID/metadata.json\"\n        jq \".[$I].files\" gists.json | jsonrange -i - | while read FNAME; do\n            URL=$(jq \".[$I].files[\\\"$FNAME\\\"].raw_url\" gists.json | sed -E 's/\"//g')\n            echo \"Saving gist/$GIST_ID/$FNAME\"\n            curl -o \"gists/$GIST_ID/$FNAME\" \"$URL\"\n        done;\n    done\n```\n\n\n\n\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2017, R. S. Doiel",
        "date": "2017-12-10",
        "keywords": [
          "GitHub",
          "Gists",
          "JSON"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Harvesting my Gists from GitHub"
      },
      "url": "posts/2017/12/10/harvesting-my-gists-from-github.json"
    },
    {
      "content": "\n\n# Raspbian Stretch on DELL E4310 Laptop\n\nby R. S. Doiel 2017-12-18\n\nToday I bought a used Dell E4310 laptop. The E4310 is an \"old model\" now\nbut certainly not vintage yet.  It has a nice keyboard and reasonable \nscreen size and resolution. I bought it as a writing machine. I mostly\nwrite in Markdown or Fountain depending on what I am writing these days.\n\n## Getting the laptop setup\n\nThe machine came with a minimal bootable Windows 7 CD and an blank \ninternal drive. Windows 7 installed fine but was missing the network \ndrivers for WiFi.  I had previously copied the new [Raspbian Stretch](https://www.raspberrypi.org/blog/raspbian-stretch/) ISO to a USB drive. While\nthe E4310 didn't support booting from the USB drive Windows 7 does make\nit easy to write to a DVRW. After digging around and finding a blank disc\nI could write to it was a couple of mouse clicks and a bit of waiting \nand I had new bootable Raspbian Stretch CD.\n\nBooting from the Raspbian Stretch CD worked like a charm. I selected \nthe graphical install which worked well though initially the trackpad \nwasn't visible so I just used keyboard navigation to setup the install.\nAfter the installation was complete and I rebooted without the install\ndisc everything worked except the internal WiFi adapter.\n\nI had several WiFi dongles that I use with my Raspberry Pis so I \nborrowed one and with that was able to run the usual `sudo apt update \n&& sudo apt upgrade`.\n\nWhile waiting for the updates I did a little web searching and found \nwhat I needed to know on the Debian Wiki (see\nhttps://wiki.debian.org/iwlwifi?action=show&redirect=iwlagn).  Following\nthe instructions for *Debian 9 \"Stretch\"* ---\n\n```shell\n    sudo vi /etc/apt/sources.list.d/non-free.list \n    # adding the deb source line from the web page\n    sudo apt update && sudo apt install fireware-iwlwifi\n    sudo modprobe -r iwlwifi; sudo modprobe iwlwifi\n    sudo shutdown -r now\n```\n\nAfter that everything came up fine.\n\n## First Impressions\n\nFirst, I like Raspbian Pixel. It was fun on my Pi but on an Intel box\nwith 4Gig RAM it is wicked fast.  Pixel is currently my favorite flavor \nof Debian GNU/Linux. It is simple, minimal with a consistent UI for \nan X based system. Quite lovely. \n\nIf you've got an old laptop you'd like to breath some life into \nRaspbian Stretch is the way to go.\n\n\n### steps for my install process\n\n+ Booted from a minimal Windows 7 CD to get a basic OS minus networking\n+ Used Windows 7 and the internal DVD-RW to create a Raspbian Stretch CD\n+ Booted from the Raspbian Stretch CD and installed Raspbian replacing Windows 7\n+ Used a spare WiFi dongle initially to fetch the non-free iwlwifi modules\n+ Updated my source list, re-run apt update and upgrade\n+ Rebooted and everything came up and is running\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2017, R. S. Doiel",
        "date": "2017-12-18",
        "keywords": [
          "Raspbian",
          "Raspberry Pi OS",
          "amd64",
          "i386",
          "operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Raspbian Stretch on DELL E4310 Laptop"
      },
      "url": "posts/2017/12/18/raspbian-stretch-on-amd64.json"
    },
    {
      "content": "\nRevealing the Pandoc AST\n========================\n\nI've used Pandoc for a number of years, probably a decade. It's been wonderful\nwatching it grow in capability. When Pandoc started accepting JSON documents as\na support metadata file things really started to click for me. Pandoc became\nmy go to tool for rendering content in my writing and documentation projects.\n\nRecently I've decided I want a little bit more from Pandoc. I've become curious\nabout prototyping some document conversion via Pandoc's filter mechanism. To do\nthat you need to understand the AST, aka abstract syntax tree. \nHow is the AST structure? \n\nIt turns out I just wasn't thinking simply enough (or maybe just not paying\nenough attention while I skimmed Pandoc's documentation). Pandoc's processing\nmodel looks like\n\n```\n\tINPUT --reader--> AST --filter AST --writer--> OUTPUT\n```\n\nI've \"known\" this forever. The missing piece for me was understanding \nthe AST can be an output format.  Use the `--to` option with the value\n\"native\" you get the Haskell representation of the AST. It's that simple.\n\n```\n\tpandoc --from=markdown --to=native \\\n\t   learning-to-write-a-pandoc-filter.md | \\\n\t   head -n 20\n```\n\nOutput\n\n```\n[ Header\n    1\n    ( \"learning-to-write-a-pandoc-filter\" , [] , [] )\n    [ Str \"Learning\"\n    , Space\n    , Str \"to\"\n    , Space\n    , Str \"write\"\n    , Space\n    , Str \"a\"\n    , Space\n    , Str \"Pandoc\"\n    , Space\n    , Str \"filter\"\n    ]\n, Para\n    [ Str \"I\\8217ve\"\n    , Space\n    , Str \"used\"\n    , Space\n```\n\nIf you prefer JSON over Haskell use `--to=json` for similar effect. Here's\nan example piping through [jq](https://stedolan.github.io/jq/).\n\n```\n\tpandoc --from=markdown --to=json \\\n\t   learning-to-write-a-pandoc-filter.md | jq .\n```\n\nWriting filters makes much sense to me now. I can see the AST and see\nhow the documentation describes writing hooks in Lua to process it.\n\n",
      "data": {
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "keywords": [
          "Pandoc",
          "filter"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 4,
        "series": "Pandoc Techniques",
        "title": "Revealing the Pandoc AST"
      },
      "url": "posts/2022/11/17/revealing-pandoc-ast.json"
    },
    {
      "content": "\n\nInstalling Golang from Source on RPi-OS for arm64\n==========================================\n\nBy R. S. Doiel, 2022-02-18\n\nThis are my quick notes on installing Golang from source on the Raspberry Pi OS 64 bit.\n\n1. Get a working compiler\n\ta. go to https://go.dev/dl/ and download go1.17.7.linux-arm64.tar.gz\n\tb. untar the tarball in your home directory (it'll unpack to $HOME/go)\n\tc. `cd go/src` and `make.bash`\n2. Move go directory to go1.17\n3. Clone go from GitHub\n4. Compile with the downloaded compiler\n\ta. `cd go/src`\n\tb. `env GOROOT_BOOTSTRAP=$HOME/go1.17 ./make.bash`\n\tc. Make sure `$HOME/go/bin` is in the path\n\td. `go version`\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-02-18",
        "keywords": [
          "raspberry pi",
          "Raspberry Pi OS",
          "arm64"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 1,
        "series": "Raspberry Pi",
        "title": "Installing Golang from source on RPi-OS for arm64"
      },
      "url": "posts/2022/02/18/Installing-Go-from-Source-RPiOS-arm64.json"
    },
    {
      "content": "\nTurbo Oberon, the dream\n=======================\n\nby R. S. Doiel, 2022-07-30\n\nSometimes I have odd dreams and that was true last night through early this morning. The dream was set in the future. I was already retired. It was a dream about \"Turbo Oberon\".\n\n\"Turbo Oberon\" was an Oberon language. The language compiler was named \"TO\" in my dream. A module's file extension was \".tom\", in honor of Tom Lopez (Meatball Fulton) of ZBS. There were allot of ZBS references in the dream.\n\n\"TO\" was very much a language in the Oberon-07 tradition with minor extensions when it came to bringing in modules. It allowed for a multi path search for module names. You could also express a Module import as a string allowing providing paths to the imported module.\n\nCompilation was similar to Go. Cross compilation was available out of the box by setting a few environment variables. I remember answering questions about the language and its evolution. I remember mentioning in the conversation about how I thought Go felling into the trap of complexity like Rust or C/C++ before it. The turning point for Go was generics. Complexity was the siren song to be resisted in \"Turbo Oberon\". Complexity is seductive to language designers and implementers. I was only an implementer.\n\nEvolution wise \"TO\" was built initially on the Go tool chain. As a result it featured easily cross-compiled binaries and had a rich standard set of Modules like Go but also included portable libraries for implementing graphic user interfaces. \"Turbo Oberon\" evolved as a conversation between Go and the clean simplicity of Oberon-07. Two example applications \"shipped\" with the \"TO\" compiler. They were an Oberon like Operating System (stand alone and hosted) and a Turbo Pascal like IDE. The IDE was called \"toe\" for Turbo Oberon Editor. I don't remember the name of the OS implementation but it might have been \"toos\". I remember \"TO\" caused problems for search engines and catalog systems.\n\nI remember remarking in the dream that programming in \"Turbo Oberon\" was a little like returning to my roots when I first learned to program in Turbo Pascal. Except I could run my programs regardless of the operating system or CPU architecture. \"\"TO\" compiler supported cross compilation for Unix, macOS, Windows on ARM, Intel, RISC-V. The targets were inherited from Go implementation roots.\n\nIn my dream I remember forking Go 1.18 and first replacing the front end of the compiler. I remember it was a challenge understanding the implementation and generate a Go compatible AST. The mapping between Oberon-07 and Go had its challenges. I remember first sticking to a strict Oberon-07 compiler targeting POSIX before enhancing module imports. I remember several failed attempts at getting module imports \"right\". I remember being on the fence about a map data type and going with a Maps module.  I don't remember how introspection worked but saying it was based on an ETH paper for Oberon 2.  I remember the compiler, like Go, eventually became self hosting. It supported a comments based DSL to annotating RECORD types making encoding and decoding convenient, an influence of Go and it's tool chain.\n\nI believe the \"Turbo Oberon Editor\" came first and that was followed by the operating system implementation.\n\nI remember talking about a book that influenced me called, \"Operating Systems through compilers\" but don't know who wrote it. I remember a discussion about the debt owed to Prof. Wirth. I remember that the book showed how once you really understood building the compile you could then build the OS. There was a joke riffing on the old Lisp joke but rephrased, \"all applications evolve not to a Lisp but to an embedded OS\".\n\nIt was a pleasant dream, in the dream I was older and already retired but still writing \"TO\" code and having fun with computers. I remember a closing video shot showing me typing away at what looked like the old Turbo Pascal IDE. As Mojo Sam said in **Somewhere Next Door to Reality**, \"it was a sorta a retro future\".\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2022-07-30",
        "keywords": [
          "Oberon",
          "Wirth",
          "ETH",
          "dreams",
          "compilers",
          "operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Turbo Oberon, the dream"
      },
      "url": "posts/2022/07/30/Turbo-Oberon.json"
    },
    {
      "content": "\nArtemis Project Status, 2022\n============================\n\nIt's been a while since I wrote an Oberon-07 post and even longer since I've worked on Artemis. Am I done with Oberon-07 and abandoning Artemis?  No. Life happens and free time to just hasn't been available. I don't know when that will change.\n\nWhat's the path forward?\n------------------------\n\nSince I plan to continue working Artemis I need to find a way forward in much less available time. Time to understand some of my constraints. \n\n1. I work on a variety of machines, OBNC is the only compiler I've consistently been able to use across all my machines\n2. Porting between compilers takes energy and time, and those compilers don't work across all my machines\n3. When I write Oberon-07 code I quickly hit a wall for the things I want to do, this is what original inspired Artemis, so there is still a need for a collection of modules\n4. Oberon/Oberon-07 on Wirth RISC virtual machine is not sufficient for my development needs\n5. A2, while very impressive, isn't working for me either (mostly because I need to work on ARM CPUs)\n\nThese constraints imply Artemis is currently too broadly scoped. I think I need to focus on what works in OBNC for now. Once I have a clear set of modules then I can revisit portability to other compilers.\n\nWhat modules do I think I need? If I look at my person projects I tend to work allot with text, often structured text (e.g. XML, JSON, CSV). I also tend to be working with network services. Occasionally I need to interact with database (e.g. SQLite3, MySQL, Postgres).  Artemis should provide modules to make it easy to write code in Oberon-07 that works in those areas. Some of that I can do by wrapping existing C libraries. Some I can simply write from scratch in Oberon-07 (e.g. a JSON encoder/decoder). That's going to me my focus as my hobby time becomes available and then.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-07-27",
        "keywords": [
          "Oberon",
          "Oberon-07",
          "Artemis"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 22,
        "series": "Mostly Oberon",
        "title": "Artemis Project Status, 2022"
      },
      "url": "posts/2022/07/27/Artemis-Status-Summer-2022.json"
    },
    {
      "content": "\n# Two missing features from HTML5, an enhanced form.enctype and a list input type\n\n## Robert's wish list for browsers and web forms handling\n\nBy R. S. Doiel, 2024-02-23\n\nI wish the form element supported a `application/json` encoding type and there was such a thing as a `list-input` element.\n\nI've been thinking about how we can get back to basic HTML documents and move away from JavaScript required to render richer web forms. When web forms arrived on scene in the early 1990s they included a few basic input types. Over the years a few have been added but by and large the data model has remained relatively flat. The exception being the select element with `multiple` attribute set. I believe we are being limited by the original choice of urlencoding web forms and then resort to JavaScript to address it's limitations.\n\nWhat does the encoding of a web form actually look like?  The web generally encodes the form using urlencoding. It presents a stream of key value pairs where the keys are the form's input names and the values are the value of the input element. With a multi-select element the browser simply repeats the key and adds the next value in the selection list to that key.  In Go you can describe this simple data structure as a `map[string][]string`[^1]. Most of the time a key points to a single element array of string but sometimes it can have multiple elements using that key and then the array expands to accommodate. Most of the time we don't think about this as web developers. The library provided with your programming language decodes the form into a more programmer friendly representation. But still I believe this simple urlencoding has held us back. Let me illustrate the problem through a series of simple form examples.\n\n[^1]: In English this could be described as \"a map using a string to point at a list of strings\" with \"string\" being a sequence of letters or characters.\n\nHere's an example of a simple form with a multi select box. It is asking for your choice of ice cream flavors.\n\n~~~html\n<form method=\"POST\">\n  <label for=\"ice-cream-flavors\">Choose your ice cream flavors:</label>\n  <select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple >\n    <option value=\"Chocolate\">Chocolate</option>\n    <option value=\"Coconut\">Cocunut</option>\n    <option value=\"Mint\">Mint</option>\n    <option value=\"Strawberry\">Strawberry</option>\n    <option value=\"Vanilla\">Vanilla</option>\n    <option value=\"Banana\">Banana</option>\n    <option value=\"Peanut\">Peanut</option>\n  </select>\n  <p>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nBy default your web browser will packaged this up and send it using \"application/x-www-form-urlencoded\". If you select \"Coconut\" and \"Strawberry\" then the service receiving your data will get an encoded document that looks like this.\n\n~~~urlencoding\nice-cream-flavors=Coconut&ice-cream-flavors=Strawberry\n~~~\n\nThe ampersands separate the key value pairs. The fact that \"ice-cream-flavors\" name repeats means that the key \"ice-cream-flavors\" will point to an array of values.  In pretty printed JSON representation is a little clearer.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Coconut\", \"Strawberry\" ]\n}\n~~~\n\nSo far so good. Zero need to enhance the spec. It works and has worked for a very long time. Stability is a good thing. Let's elaborate a little further.  I've added a dish choice for the ice cream, \"Sugar Cone\" and \"Waffle Bowl\". That web form looks like.\n\n~~~html\n<form method=\"POST\">\n<label for=\"ice-cream-flavors\">Select the flavor for each scoop of ice cream:</label>\n<select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple>\n  <option value=\"Chocolate\">Chocolate</option>\n  <option value=\"Coconut\">Cocunut</option>\n  <option value=\"Mint\">Mint</option>\n  <option value=\"Strawberry\">Strawberry</option>\n  <option value=\"Vanilla\">Vanilla</option>\n  <option value=\"Banana\">Banana</option>\n  <option value=\"Peanut\">Peanut</option>\n</select>\n<p>\n<fieldset>\n  <legend>Pick your delivery dish</legend>\n  <div>\n    <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n    <label for=\"sugar-cone\">Sugar Cone</label>\n  </div>\n  <div>\n    <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n    <label for=\"waffle-bowl\">Waffle Bowl</label>\n  </div>\n</fieldset>\n<input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nIf we select \"Banana\" and \"Peanut\" flavors served in a \"Waffle Bowl\" the encoded document would reach the web service looking something like this.\n\n~~~urlencoded\nice-cream-flavors=Banana&ice-cream-flavors=Peanut&ice-cream-dish=waffle-cone\n~~~\n\nThat's not too bad. Again this is the state of web form for ages now. In JSON it could be represented as the following.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Banana\", \"Peanut\" ],\n    \"ice-cream-dish\": \"waffle-cone\"\n}\n~~~\n\nThis is great we have a simple web form that can collect a single ice cream order.  But what if we want to actually place several individual ice cream orders as one order? Today we have two choices, multiple web forms that accumulate the orders (circa 2000) or use JavaScript create a web UI that can handle list of form elements. Both have their drawbacks.\n\nIn the case of the old school approach changing web pages just to update an order can be slow and increase uncertainty about your current order. That is why the JavaScript approach has come to be more common. But that JavaScript approach comes at a huge price. It's much more complex, we've seen a dozens of libraries and frameworks that have come and gone trying to manage that complexity in various ways.\n\nIf we supported JSON encoded from submission directly in the web browser I think we'd make a huge step forward. It could decouple the JavaScript requirement. That would avoid much of the cruft that we ship down to the web browser today because we can't manage lists of things without resorting to JavaScript.\n\nLet's pretend there was a new input element type called \"list-input\". A \"list-input\" element can contain any combination of today's basic form elements. Here's my hypothetical `list-input` based from example. In it we're going to select the ice cream flavors and the dish format (cone, bowl) as before but have them accumulate in a list. That form could be expressed in HTML similar to my mock up below.\n\n~~~html\n<form>\n  <label for=\"ice-cream-order\">Place your next order, press submit when you have all of them.</label>\n  <list-input id=\"ice-cream-order\" name=\"ice-cream-order\">\n    <label for=\"ice-cream-flavor\">Select the flavor for each scoop of ice cream:</label>\n    <select id=\"ice-cream-flavor\" name=\"ice-cream-flavor\" multiple>\n      <option value=\"Chocolate\">Chocolate</option>\n      <option value=\"Coconut\">Cocunut</option>\n      <option value=\"Mint\">Mint</option>\n      <option value=\"Strawberry\">Strawberry</option>\n      <option value=\"Vanilla\">Vanilla</option>\n      <option value=\"Banana\">Banana</option>\n      <option value=\"Peanut\">Peanut</option>\n    </select>\n  <p>\n  <fieldset>\n    <legend>Pick your delivery dish</legend>\n    <div>\n      <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n      <label for=\"sugar-cone\">Sugar Cone</label>\n    </div>\n    <div>\n      <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n      <label for=\"waffle-bowl\">Waffle Bowl</label>\n    </div>\n  </fieldset>\n  </list-input>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nWith two additional lines of HTML the input form can now support a list of individual ice cream orders. Assuming only urlencoding is supported then how does that get encoded and sent to the web server? Here is an example set of orders\n\n1. vanilla ice cream with a sugar cone\n2. chocolate with a waffle bowl\n\n~~~urlencoded\nice-cream-flavors=Vanilla&ice-cream-flavors=Chocolate&ice-cream-dish=sugar-cone&ice-cream-dish=waffle-bowl\n~~~\n\nWhich flavor goes with which dish?  That's the problem with urlencoding a list in your web form. We just can't keep the data alignment manageable.  What if the web browser used JSON encoding? \n\n~~~json\n[\n  {\n      \"ice-cream-flavors\": [ \"Vanilla\" ],\n      \"ice-cream-dish\": \"sugar-cone\"\n  },\n  {\n      \"ice-cream-flavors\": [ \"Chocolate\" ],\n      \"ice-cream-dish\": \"waffle-bowl\"\n  }\n~~~\n\nSuddenly the alignment problem goes away. There is precedence for controlling behavior of the web browser submission through the `enctype` attribute. File upload was addressed by adding support for `multipart/form-data`.  In 2024 and for over the last decade it has been common practice in web services to support JSON data submission. I believe it is time that the web browser also supports this directly. This would allow us to decouple the necessity of using JavaScript in browser as we require today. The form elements already map well to a JSON encoding. If JSON encoding was enabled then adding a element like my \"list-input\" would make sense.  Otherwise we remain stuck in a world where hypertext markup language remains very limited and can't live without JavaScript.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "html",
          "web forms",
          "encoding"
        ],
        "title": "Two missing features from HTML5, an enhanced form.enctype and a list input type"
      },
      "url": "posts/2024/02/23/enhanced_form_handling.json"
    },
    {
      "content": "\n# Postgres Quick Notes, take two\n\nBy R. S. Doiel, 2023-11-17\n\nWhat follows is some quick notes to remind me of the things I do when\nI setup a new instance of PostgreSQL on the various machines I work with.\n\n## Installation approach\n\nIf possible I install Postgres with the system's package manager or follow\nthe directions suggested for installation on the [Postgres website](https://postgres.org).\n\n### macOS and Postgres\n\nFor macOS that's not the route I take if possible is to install via [Postgres App](https://postgresapp.com/).\nThis provides a very nice setup of developing with Postgres on macOS and also allows you to easily\ntest multiple versions of Postgres.  It is not as convenient in the Mac Mini headless configuration\nI also use Postgres on macOS in. In that case I use Mac Ports' package manager to install Postgres.\nUnfortunately just using ports command isn't enough to get running. What follows is my notes on the\nadditional steps I've taken to get things working.\n\nInstall the version of Postgres you want (e.g. PostgreSQL 16) via ports\n\n1. install postgresql16, postgresql16-server, postgres_select\n2. make sure the postgres version is selected using the ports command\n3. make a directory for the default postgres db\n4. make sure the default db directory is owned by the postgres user\n5. run the initialization scripts provided by the posts installer\n6. use the ports command to load the plist\n7. start up the server, make sure the log file is writable\n\nHere's the commands I type in the shell\n\n~~~shell\nsudo port install postgresql16-server postgresql16 postgresql_select\n# Answer y to the prompt\n# After the install completes Ports will suggest the following to complete the process.\nsudo port select postgresql postgresql16\nsudo mkdir -p /opt/local/var/db/postgresql16/defaultdb\nsudo chown postgres:postgres /opt/local/var/db/postgresql16/defaultdb\nsudo -u postgres /bin/sh -c 'cd /opt/local/var/db/postgresql16 && /opt/local/lib/postgresql16/bin/initdb -D /opt/local/var/db/postgresql16/defaultdb'\nsudo port load postgresql16-server\nsudo -u postgres /bin/sh -c '/opt/local/lib/postgresql16/bin/pg_ctl -D /opt/local/var/db/postgresql16/defaultdb -l /opt/local/var/log/postgresql16/postgres.log start'\n~~~\n\n## Database users setup\n\nThis applies to most Postgres installations I do because I am using them to\ndevelop software solutions. In a production setting you'd want a more conservative\nsecurity approach.\n\n1. Make sure you can connect as the postgres user\n2.  For each developer\n    a. Use the Postgres createuser tool to create superuser account(s)\n    b. Use the Postgres createdb tool to create databases for those account(s)\n\nHere's the commands I type in the shell\n\n~~~shell\nsudo -u postgres psql\n~~~\n\nWhen in the psql shell you should be able to use the slash commands like\n\n\\\\l\n: list the databases\n\n\\\\dt\n: list the tables in the database\n\n\\\\d TABLE\\_NAME\n: list the schema for TABLE\\_NAME\n\n\\\\q\n: quit the psql shell\n\nAssuming we have a working Postgres I now create superuser accounts for\ndevelopment and databases that match the username.\n\n~~~shell\nsudo -u postgres createuser --interactive $USER\ncreatedb $USER\n~~~\n\nI should now be able to run the psql shell without specifying the\npostgres username.\n\n~~~shell\npsql\n~~~\n",
      "data": {
        "abstract": "A collection of quick notes for setting and Postgres for development.",
        "byline": "R. S. Doiel, 2023-11-17",
        "keywords": [
          "postgres"
        ],
        "title": "Postgres Quick Notes, take two"
      },
      "url": "posts/2023/11/17/PostgreSQL-Quick-Notes.json"
    },
    {
      "content": "\n# RSS and my web experience\n\nby R. S. Doiel, 2023-12-07\n\nI agree with [Dave Winer](http://scripting.com/2023/12/07/140505.html?title=whyWeWantFeedsInBluesky), Blue Sky should support RSS. Most web systems benefit from supporting RSS. RSS is a base level inter-opt for web software like HTML and JSON can be. While I may not be typical I am an example of a web user who experiences much of the web via RSS. I read blog and site content via RSS. I \"follow\" my friends and colleagues via RSS. This is true when they blog or when they post in a Mastodon community.  I track academic repositories content for [ETH Zurich Research](https://www.rc-blog.ethz.ch/en/feed) and [Caltech](https://feeds.library.caltech.edu/recent/combined.html) via RSS feeds. I check the weather via NOAA's [RSS feed](https://www.weather.gov/rss).  News sites often syndicate still via RSS and then Podcasts, if they are actual Podcasts are distributed via RSS.  All this is to say I think RSS is not dead. It remains easy to render can can be easy to consume.  If a website doesn't provide it it is possible to generate it yourself[1] or find a service to use that does[2]. RSS remains key to how I experience and use the web in 2023.\n\n[1]: Go libraries like [Colly](https://go-colly.org/) and [Gofeeds](https://github.com/mmcdole/gofeed) make it possible to roll your own like the one in skimmer\n\n[2]: https://firesky.tv/ is an example of a service that provides RSS for Bluesky via its raw API, [html2rss](https://html2rss.github.io/) is service that producing RSS feeds for popular sites that don't include them\n\nMy personal approach to feeds is very much tailored to me. It's probably overkill for most people but it works with my vision and cognitive limitations. He's the steps I take in feed reading. They essentially decompose a traditional feed reader and allow for more flexibility for my reading pleasure.\n\n1. Maintain a list of feeds in a simple text file\n2. Harvest those feeds with [skimmer](https://rsdoiel.github.io/skimmer), Skimmer stores the items in an [SQLite3](https://sqlite.org)\n3. I filter the items using SQL and SQLite3 or via an interactive mode provided by Skimmer\n4. Render saved items to Markdown with [skim2md](https://rsdoiel.github.io/skimmer/skim2md.1.html)\n5. Use [Pandoc](https://pandoc.org) to render the Markdown and view Firefox\n\nThe nice thing about this approach is that I can easily script it with Bash or even a Windows bat. I can easily maintain separate lists and separate databases for personal and work related material.  A bonus is the database items can also serve as a corpus for a personal search engine too. If you want to save maintain a public reading list this setup is ideal too. Of course the list of curated items can be transformed into their own RSS feed as well.\n\n[Skimmer](https://rsdoiel.github.io/skimmer/skimmer.1.html) is a deconstructed feed reader. Does that make it post modern feed reader?  Skimmer processes a list of feeds I follow and saves the results in an SQLite 3 database. That database can be used to filter the feeds and flag items as \"saved\". Typically I filter by timestamps. Saved items can be processed with `skim2md` to render a markdown document. `skim2md` has an option to include a \"save to pocket\" button for each item in the output. I use Pandoc to render the page then view that result in Firefox. At my leisure I read the web page and press the \"Save to pocket\" button any item I want to read later. It's a very comfortable experience.\n\nSkimmer lead me to think about a personal news page for myself and family. Skimmer lets me curate separate lists organized around themes. These can then be rendered to individual pages like pages of a newspaper. This has been captured in an experimental project I call [Antenna](https://rsdoiel.github.io/antenna). It even includes a feed search feature thanks to [PageFind](https://pagefind.app)\n",
      "data": {
        "abstract": "RSS is alive and kicking and Bluesky should support it too. Explore my recipe for reading web news.",
        "byline": "R. S. Doiel, 2023-12-07",
        "created": "2023-12-07",
        "keywords": [
          "RSS",
          "Feeds",
          "Social Media",
          "news"
        ],
        "numnber": 1,
        "series": "Simplification and the Web",
        "title": "RSS and my web experience"
      },
      "url": "posts/2023/12/07/rss-and-my-web-experience.json"
    },
    {
      "content": "\n# Find Bluesky RSS Feeds\n\nWith the update to [1.60](https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i) of Bluesky we can now follow people on Bluesky via RSS feeds. This makes things much more convienient for me. \nThe RSS feed is visible via the HTML markup on a person's profile page (which are now public). E.g. My Bluesky profile page is\nat <https://bsky.app/profile/rsdoiel.bsky.social> and if you look at that pages HTML markup you'll see a link element in the head\n\n```html\n <link rel=\"alternate\" type=\"application/rss+xml\" href=\"/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss\">\n```\n\nThat's the RSS feed. So now if you want to follow you can expand the URL to \n\n```\nhttps://bsky.app/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss\n```\n\nAnd use if via your feed reader. This is a sweat feature. It allows me to move my reading from visiting the website\nto getting updates via my feed reader.\n\n\n",
      "data": {
        "byline": "R. S. Doiel, 2023-12-23",
        "keywords": [
          "bluesky",
          "rss"
        ],
        "title": "Finding Bluesky RSS feeds"
      },
      "url": "posts/2023/12/23/finding-blue-sky-rss-feeds.json"
    },
    {
      "content": "\n\nFreeDOS to Oberon System 3\n==========================\n\nBy R. S. Doiel, 2019-07-28\n\n>    UPDATE: (2021-02-26, RSD) Under VirtualBox 6.1 these\n>    instructions still fail. My hope is to revise these \n>    instructions when I get it all sorted out.\n>\n>    Many links such as the ftp site at ETH Oberon are \n>    no more. I've updated this page to point at Wayback machine\n>    or included content in here where I cannot find it else where.\n>\n>    UPDATE: (2021-02-19, RSD) Under VirtualBox 6.1 these instructions \n>    fail. For VirtualBox I’ve used FreeDOS 1.3rc3 Live CD installing \n>    the “Plain DOS” without problems.\n>\n>    UPDATE: (2021-03-16, RSD) After reviewing my post, correcting\n>    some mistakes I finally was able to get FreeDOS up and running\n>    on VirtualBox 6.1. This allows NativeOberon 2.3.6 to be brought\n>    up by booting the \"oberon0.dsk\" virtual floppy and following\n>    the instructions included. You need to know how to use\n>    the Oberon mouse and the way commands work in Oberon.\n\nWhat follows are notes on getting a FreeDOS 1.2[^1] and \nthen Native Oberon[^2] running under VirtualBox 6.0. You might \nwonder why these two are together. While it was\neasy to run the Native Oberon installation process that process\nassumes you have a properly partitioned hard disk and VirtualBox\nseems to skip that process. I found taking advantage of FreeDOS\nsimplified things for me.\n\nMy goal was running Oberon System 3, but setting up a Virtual Box\nwith FreeDOS 1.2 gave me a virtual machine that functions like a \n1999 era PC. From there all the steps in the Oberon instructions\njust worked.\n\n## Creating FreeDOS 1.2 Virtual Box\n\nI've been doing a bit if computer history reading and decided to\nbring up some older systems as a means to understand where\nthings were.  The first computers I had access to were 8080, 8086\nmachines running MS DOS based. My first computer programming language\nwas Turbo Pascal. Feeling a bit nostalgic I thought it would be\ninteresting to use it again and see what I remembered from the days\nof old. While PC and MS DOS no longer exist as commercial productions\nan wonderful group of Open Source hackers have brought new life into\nDOS with FreeDOS 1.2[^3]. You'll find many of your old familiar commands\nbut also some nice improvements. You can even run it under VirtualBox\nwhich is what I proceeded to do.\n\n### VirtualBox 6.0 setup\n\nThe [FreeDOS](https://freedos.org) website includes a CD ROM image\nthat you can use to install it. There are couple small hitches though\nto get it working under VirtualBox. First go to the [download](https://freedos.org/download) page and download the [CDROM \"standard\" installer\"](http://www.freedos.org/download/download/FD12CD.iso).\n\nWhile that is downloading you can setup your VirtualBox machine.\nFirst to remember is DOS compared to today's operating systems is\nfrugal in its hardware requirements. As a result I picked very modest\nsettings for my virtual machine. \n\n1. Launch VirtualBox\n2. From the menu, pick Machine then pick new\n3. Name your machine (e.g. \"FreeDOS 1.2\"), select the type: \"Other\" and Operating system of \"DOS\"\n4. Set memory size as you like, I just accepted the default 32MB\n5. Hard disk, pick \"Create a virtual hard disc now\"\n6. Hard disk file type, pick \"VHD (Virtual Hard Disk)\"\n7. Storage on physical hard disk, I picked Dynamically allocated both either is fine\n8. File location and size, I accepted the default location and size\n9. Before starting my FreeDOS box I made a couple of changes using \"settings\" menu icon\n    a. Display, I picked bumped memory up to 128M and picked VBoxSVGA with 33D acceleration (for games)\n    b. Storage, I added a second floppy drive (empty)\n    c. Network, I picked attached to NAT\n10. When looking at my virtual machine's detail page I clicked on the Optical drive (empty), click \"choose disc image\" and pointed at the downloaded installed CD\n11. Click Start.\n12. At \"Welcome to FreeDOS 1.2\" blue screen, hit TAB key\n13. You will see a line that begins with a boot instruction. Add a space than add the word \"raw\" (without quotes) press enter\n14. Follow the install instructions, when you get to \"Drive C: does not appear to be partitioned\" dialog, pick \"Yes - Partition drive C:\"\n15. On the next screen pick \"Yes - Please reboot now\"\n16. When at the \"Welcome to FreeDOS 1.2\" screen hit TAB again\n17. Once again add a space and type \"raw\" to the command then press enter\n18. Pick \"Yes - continue with the installation\"\n19. Pick \"Yes - Please erase and format drive C:\"\n20. At this point its a normal FreeDOS install\n21. When the install is done and reboots \"eject\" the virtual CD form the \"Optical Drive\" in the VirtualBox panel, then choose \"boot from system disk\",you now should have a working FreeDOS on VirtualBox\n\n## Native Oberon System 3 on Virtual Box\n\nNative Oberon can be found at http://www.ethoberon.ethz.ch/native/.\nThere is a related ftp site[^4] where you can download the necessary\nfiles for the stand alone version. \n\nHere's the steps I used in my Mac to download Native Oberon and\ninto a file on my desktop called \"NativeOberon-Standalone\". Open\nthe macOS Terminal application. I assume you've got a Unix\ncommand called [wget](https://en.wikipedia.org/wiki/Wget)\nalready installed[^5].\n\n> NOTE: The ETH ftp server is no more. I've included Web Archive\n> links and links to my own copies of the files needed to\n> install Native Oberon 2.3.6 in the paragraphs that follow.\n> RSD, 2021-03-16\n\n```bash\n\n    cd\n    mkdir -p Desktop/NativeOberon-Standalone\n    cd Desktop/NativeOberon-Standalone\n    wget ftp://ftp.ethoberon.ethz.ch/ETHOberon/Native/StdAlone/\n\n```\n\nClone your FreeDOS Box first. You'll want to do a \"Full Clone\". You'll\nalso want to \"remove\" any optical disks or floppies. You do that from\nthe virtual boxes' detail page and clicking on the drive and picking the\n\"Remove disk from virtual drive\" in the popup menu.\n\nAt this point we have a a virtual machine that is very similar to an \n1999 era PC installed with MS DOS.  [Native Oberon](http://web.archive.org/web/20190929033749/http://www.ethoberon.ethz.ch/native/) Normally you'd\ninstall [Native Oberon via 1.44MB floppy disks](/blog/2019/07/28/NativeOberon-StnAlone-2.3.6.zip \"Zip file of individual floppies\"). \nWe can simulate that with our Virtual machine.\nIn the folder of you downloaded there is disc called \"oberon0.dsk\". That\ncan go in our first floppy drive. But how to we get the rest of the \nfiles onto a virtual floppies? This wasn't obvious to me at first.\n\nThe Oberon install disks were organized as follows\n\n| PACKAGE    | FILENAME     | SIZE  | DSK   |\n| ---------- | ------------ | ----- | ----- |\n| Oberon-0      | [oberon0.dsk](oberon0.dsk \"boot disk\")  |          | 0 | \n| Gadgets       | [gadgets.arc](gadgets1.arc \"a modified gadgets.arc to fit 1.4 floppy\")  | 1.4  2.9 | 1 | \n| Documentation | [docu.arc](docu.arc \"documentation\")     | 1.3  2.5 | 2 | \n| Applications  | [apps.arc](apps.arc \"applications\")     | 1.3  2.8 | 3 | \n| Tutorials     | [tutorial.arc](tutorial.arc \"tutorial\") | 0.3  0.8 | 4 | \n| Pr3Fonts      | [pr3fonts.arc](pr3fonts.arc \"fonts\") | 0.3  0.6 | 4 | \n| Pr6Fonts      | [pr6fonts.arc](pr6fonts.arc \"fonts\") | 0.5  1.8 | 4 | \n| Source1       | [source1.arc](source1.arc \"Source Code\")  | 0.9  2.5 | 5 | \n| Source2       | [source2.arc](source2.arc \"Source Code\")  | 1.2  3.5 | 6 | \n| Source3       | [source3.arc](source3.arc \"Source Code\")  | 0.6  1.7 | 7 | \n\n\nIt turns out you can create 1.44MB Fat16 disc images from the\nVirtual Box 6.0 floppy drive link.  When you click on the floppy\ndrive in the details page you have a choice that includes \"create a new floppy disc\". Select this, find the disc a filename like \"disk1\". Click\non the virtual floppy disk in the Virtual Box and \"remove\"\nthe disc then create disk2, disk3, etc. In each the empty disc image\nfiles places the files from the table above. These image files can then\nbe opened on your host operating system and files copied to them. \nIt's a tedious process but this gives you something the Oberon System \ncan read and install from. Originally I just put all the files into an \nISO CD ROM image but I could not figure out how to mount that from this\nversion of Oberon. Now when you start up your Oberon V3 virtual machine\nyou can install the rest of the software like Gadgets.\n\n\n[^1]: FreeDOS is an Open Source implementation of PC/MS DOC\n\n[^2]: Native Oberon is a 1990's version of Oberon System running on i386\n\n[^3]: Download FreeDOS from http://freedos.org/download\n\n[^4]: Download Native Oberon Stand Alone from [ftp://ftp.ethoberon.ethz.ch/ETHOberon/Native/StdAlone](NativeOberon-StdAlone-2.3.6.zip \"Zip of what used to be available in that directory at ftp.ethoberon.ethz.ch\")\n\n[^5]: wget is easily installed with [HomeBrew](https://brew.sh/) or [Mac Ports](https://www.macports.org/)\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2019-07-28",
        "keywords": [
          "FreeDOS",
          "Oberon System"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "FreeDOS 1.2 to Oberon System 3",
        "updated": "2021-03-16"
      },
      "url": "posts/2019/07/28/freedos-to-oberon-system-3.json"
    },
    {
      "content": "\nBeyond Oakwood, Modules and Aliases\n===================================\n\nBy R. S. Doiel, 2021-05-16\n\nOakwood is the name used to refer to an early Oberon language\nstandardization effort in the late 20th century.  It's the name\nof a hotel where compiler developers and the creators of Oberon\nand the Oberon System met to discuss compatibility. The lasting\ninfluence on the 21st century Oberon-07 language can be seen\nin the standard set of modules shipped with POSIX based Oberon-07\ncompilers like\n[OBNC](https://miasap.se/obnc/), [Vishap Oberon Compiler](https://github.com/vishaps/voc) and the \n[Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler).\n\nThe Oakwood guidelines described a minimum expectation for\na standard set of modules to be shipped with compilers.\nThe modules themselves are minimalist in implementation.\nMinimalism can assist in easing the learning curve\nand encouraging a deeper understanding of how things work.\n\nThe Oberon-07 language is smaller than the original Oberon language\nand the many dialects that followed.  I think of Oberon-07 as the\ndistillation of all previous innovation.  It embodies the\nspirit of \"Simple but not simpler than necessary\". Minimalism is\na fit description of the adaptions of the Oakwood modules for \nOberon-07 in the POSIX environment.\n\n\nWhen simple is too simple\n-------------------------\n\nSometimes I want more than the minimalist module.  A good example\nis standard [Strings](https://miasap.se/obnc/obncdoc/basic/Strings.def.html)\nmodule.  Thankfully you can augment the standard modules with your own.\nIf you are creative you can even create a drop in replacement.\nThis is what I wound up doing with my \"Chars\" module.\n\nIn the spirit of \"Simple but no simpler\" I originally kept Chars \nvery minimal. I only implemented what I missed most from Strings.\nI got down to a handful of functions for testing characters,\ntesting prefixes and suffixes as well as trim procedures. It was\nall I included in `Chars` was until recently.\n\nOver the last couple of weeks I have been reviewing my own Oberon-07\ncode in my personal projects.  I came to understand that\nin my quest for minimalism I had fallen for \"too simple\".\nThis was evidenced by two observations.  Everywhere I had used\nthe `Strings` module I also included `Chars`. It was boiler plate.\nThe IMPORT sequence was invariably a form of --\n\n~~~\n    IMPORT Strings, Chars, ....\n~~~\n\nOn top of that I found it distracting to see `Chars.*` and `Strings.*`\ncomingled and operating on the same data. If felt sub optimal. It\nfelt baroque. That got me thinking.\n\n> What if Chars included the functionality of Strings?\n\nI see two advantages to merging Chars and Strings. First I\nonly need to include one module instead of two. The second\nis my code becomes more readable. I think that is because\nexpanding Strings to include new procedures and constants allows\nfor both the familiar and for evolution. The problem is renaming\n`Chars.Mod` to `Strings.Mod` implies I'm supplying the standard\n`Strings` module. Fortunately Oberon provides a mechanism for\nsolving this problem. The solution Oberon provides is to allow\nmodule names to be aliased.  Look at my new import statement.\n\n~~~\n    IMPORT Strings := Chars, ...\n~~~\n\nIt is still minimal but at the same time shows `Chars` is going\nto be referenced as `Strings`. By implication `Chars` provides\nthe functionality `Strings` but is not the same as `Strings`.\nMy code reads nicely.  I don't loose the provenance of what\nis being referred to by `Strings` because it is clearly \nprovided in the IMPORT statement.\n\nIn my new [implementation](Chars.Mod) I support all the standard\nprocedures you'd find in an Oakwood compliant `Strings`.  I've\nincluded additional additional constants and functional procedures\nlike `StartsWith()` and `EndsWith()` and a complement of trim\nprocedures like `TrimLeft()`, `TrimRight()`, `Trim()`.\n`TrimPrefix()`, and `TrimSuffix()`.\n\nHere's how `Chars` definition stacks up as rendered by the\nobncdoc tool.\n\n```\n(* Chars.Mod - A module for working with CHAR and \n   ARRAY OF CHAR data types.\n\nCopyright (C) 2020, 2021 R. S. Doiel <rsdoiel@gmail.com>\nThis Source Code Form is subject to the terms of the\nMozilla PublicLicense, v. 2.0. If a copy of the MPL was\nnot distributed with thisfile, You can obtain one at\nhttp://mozilla.org/MPL/2.0/. *)\nDEFINITION Chars;\n\n(*\nChars.Mod provides a modern set of procedures for working\nwith CHAR and ARRAY OF CHAR. It is a drop in replacement\nfor the Oakwood definition \nStrings module.\n\nExample:\n\n    IMPORT Strings := Chars;\n\nYou now have a Strings compatible Chars module plus all the Chars\nextra accessible through the module alias of Strings. *)\n\nCONST\n  (* MAXSTR is exported so we can use a common\n     max string size easily *)\n  MAXSTR = 1024;\n  (* Character constants *)\n  EOT = 0X;\n  TAB = 9X;\n  LF  = 10X;\n  FF  = 11X;\n  CR  = 13X;\n  SPACE = \" \";\n  DASH  = \"-\";\n  LODASH = \"_\";\n  CARET = \"^\";\n  TILDE = \"~\";\n  QUOTE = 34X;\n\n  (* Constants commonly used characters to quote things.  *)\n  QUOT   = 34X;\n  AMP    = \"&\";\n  APOS   = \"'\";\n  LPAR   = \")\";\n  RPAR   = \"(\";\n  AST    = \"*\";\n  LT     = \"<\";\n  EQUALS = \"=\";\n  GT     = \">\";\n  LBRACK = \"[\";\n  RBRACK = \"]\";\n  LBRACE = \"}\";\n  RBRACE = \"{\";\n\nVAR\n  (* common cutsets, ideally these would be constants *)\n  spaces : ARRAY 6 OF CHAR;\n  punctuation : ARRAY 33 OF CHAR;\n\n(* InRange -- given a character to check and an inclusive range of\n    characters in the ASCII character set. Compare the ordinal values\n    for inclusively. Return TRUE if in range FALSE otherwise. *)\nPROCEDURE InRange(c, lower, upper : CHAR) : BOOLEAN;\n\n(* InCharList checks if character c is in list of chars *)\nPROCEDURE InCharList(c : CHAR; list : ARRAY OF CHAR) : BOOLEAN;\n\n(* IsUpper return true if the character is an upper case letter *)\nPROCEDURE IsUpper(c : CHAR) : BOOLEAN;\n\n(* IsLower return true if the character is a lower case letter *)\nPROCEDURE IsLower(c : CHAR) : BOOLEAN;\n\n(* IsDigit return true if the character in the range of \"0\" to \"9\" *)\nPROCEDURE IsDigit(c : CHAR) : BOOLEAN;\n\n(* IsAlpha return true is character is either upper or lower case letter *)\nPROCEDURE IsAlpha(c : CHAR) : BOOLEAN;\n\n(* IsAlphaNum return true is IsAlpha or IsDigit *)\nPROCEDURE IsAlphaNum (c : CHAR) : BOOLEAN;\n\n(* IsSpace returns TRUE if the char is a space, tab, carriage return or line feed *)\nPROCEDURE IsSpace(c : CHAR) : BOOLEAN;\n\n(* IsPunctuation returns TRUE if the char is a non-alpha non-numeral *)\nPROCEDURE IsPunctuation(c : CHAR) : BOOLEAN;\n\n(* Length returns the length of an ARRAY OF CHAR from zero to first\n    0X encountered. [Oakwood compatible] *)\nPROCEDURE Length(source : ARRAY OF CHAR) : INTEGER;\n\n(* Insert inserts a source ARRAY OF CHAR into a destination \n    ARRAY OF CHAR maintaining a trailing 0X and truncating if\n    necessary [Oakwood compatible] *)\nPROCEDURE Insert(source : ARRAY OF CHAR; pos : INTEGER; VAR dest : ARRAY OF CHAR);\n\n(* AppendChar - this copies the char and appends it to\n    the destination. Returns FALSE if append fails. *)\nPROCEDURE AppendChar(c : CHAR; VAR dest : ARRAY OF CHAR) : BOOLEAN;\n\n(* Append - copy the contents of source ARRAY OF CHAR to end of\n    dest ARRAY OF CHAR. [Oakwood complatible] *)\nPROCEDURE Append(source : ARRAY OF CHAR; VAR dest : ARRAY OF CHAR);\n\n(* Delete removes n number of characters starting at pos in an\n    ARRAY OF CHAR. [Oakwood complatible] *)\nPROCEDURE Delete(VAR source : ARRAY OF CHAR; pos, n : INTEGER);\n\n(* Replace replaces the characters starting at pos with the\n    source ARRAY OF CHAR overwriting the characters in dest\n    ARRAY OF CHAR. Replace will enforce a terminating 0X as\n    needed. [Oakwood compatible] *)\nPROCEDURE Replace(source : ARRAY OF CHAR; pos : INTEGER; VAR dest : ARRAY OF CHAR);\n\n(* Extract copies out a substring from an ARRAY OF CHAR into a dest\n    ARRAY OF CHAR starting at pos and for n characters\n    [Oakwood compatible] *)\nPROCEDURE Extract(source : ARRAY OF CHAR; pos, n : INTEGER; VAR dest : ARRAY OF CHAR);\n\n(* Pos returns the position of the first occurrence of a pattern\n    ARRAY OF CHAR starting at pos in a source ARRAY OF CHAR. If\n    pattern is not found then it returns -1 *)\nPROCEDURE Pos(pattern, source : ARRAY OF CHAR; pos : INTEGER) : INTEGER;\n\n(* Cap replaces each lower case letter within source by an uppercase one *)\nPROCEDURE Cap(VAR source : ARRAY OF CHAR);\n\n(* Equal - compares two ARRAY OF CHAR and returns TRUE\n    if the characters match up to the end of string,\n    FALSE otherwise. *)\nPROCEDURE Equal(a : ARRAY OF CHAR; b : ARRAY OF CHAR) : BOOLEAN;\n\n(* StartsWith - check to see of a prefix starts an ARRAY OF CHAR *)\nPROCEDURE StartsWith(prefix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR) : BOOLEAN;\n\n(* EndsWith - check to see of a prefix starts an ARRAY OF CHAR *)\nPROCEDURE EndsWith(suffix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR) : BOOLEAN;\n\n(* Clear - resets all cells of an ARRAY OF CHAR to 0X *)\nPROCEDURE Clear(VAR a : ARRAY OF CHAR);\n\n(* Shift returns the first character of an ARRAY OF CHAR and shifts the\n    remaining elements left appending an extra 0X if necessary *)\nPROCEDURE Shift(VAR source : ARRAY OF CHAR) : CHAR;\n\n(* Pop returns the last non-OX element of an ARRAY OF CHAR replacing\n    it with an OX *)\nPROCEDURE Pop(VAR source : ARRAY OF CHAR) : CHAR;\n\n(* TrimLeft - remove the leading characters in cutset\n    from an ARRAY OF CHAR *)\nPROCEDURE TrimLeft(cutset : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);\n\n(* TrimRight - remove tailing characters in cutset from\n    an ARRAY OF CHAR *)\nPROCEDURE TrimRight(cutset : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);\n\n(* Trim - remove leading and trailing characters in cutset\n    from an ARRAY OF CHAR *)\nPROCEDURE Trim(cutset : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);\n\n(* TrimLeftSpace - remove leading spaces from an ARRAY OF CHAR *)\nPROCEDURE TrimLeftSpace(VAR source : ARRAY OF CHAR);\n\n(* TrimRightSpace - remove the trailing spaces from an ARRAY OF CHAR *)\nPROCEDURE TrimRightSpace(VAR source : ARRAY OF CHAR);\n\n(* TrimSpace - remove leading and trailing space CHARS from an \n    ARRAY OF CHAR *)\nPROCEDURE TrimSpace(VAR source : ARRAY OF CHAR);\n\n(* TrimPrefix - remove a prefix ARRAY OF CHAR from a target \n    ARRAY OF CHAR *)\nPROCEDURE TrimPrefix(prefix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);\n\n(* TrimSuffix - remove a suffix ARRAY OF CHAR from a target\n    ARRAY OF CHAR *)\nPROCEDURE TrimSuffix(suffix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);\n\n(* TrimString - remove cutString from beginning and end of ARRAY OF CHAR *)\nPROCEDURE TrimString(cutString : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);\n\nEND Chars.\n```\n\nMy new `Chars` module has proven to be both more readable\nand more focused in my projects. I get all the functionality\nof `Strings` and the additional functionality I need in my own\nprojects. This improved the focus in my other modules and I think\nmaintained the spirit of \"Simple but not simpler\".\n\n+ [Chars.Mod](Chars.Mod)\n\nUPDATE: The current version of my `Chars` module can be found in \nmy [Artemis](https://github.com/rsdoiel/Artemis) repository. The\nrepository includes additional code and modules suitable to working\nwith Oberon-07 in a POSIX envinronment.\n\n### Next, Previous\n\n+ Next [Combining Oberon-07 with C using Obc-3](/blog/2021/06/14/Combining-Oberon-07-with-C-using-Obc-3.html)\n+ Prev [Dates & Clocks](/blog/2020/11/27/Dates-and-Clock.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2021, R. S. Doiel",
        "date": "2021-05-16",
        "keywords": [
          "Oberon",
          "Modules",
          "Oakwood",
          "Strings",
          "Chars"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 18,
        "title": "Beyond Oakwood, Modules and Aliases"
      },
      "url": "posts/2021/05/16/Beyond-Oakwood-Modules-and-Aliases.json"
    },
    {
      "content": "\nRevisiting Files\n================\n\nBy R. S. Doiel, 2021-11-22\n\nIn October I had an Email exchange with Algojack regarding a buggy example in [Oberon-07 and the file system](../../../2020/05/09/Oberon-07-and-the-filesystem.html). The serious bug was extraneous non-printable characters appearing a plain text file containing the string \"Hello World\". The trouble with the example was a result of my misreading the Oakwood guidelines and how **Files.WriteString()** is required to work. The **Files.WriteString()** procedure is supposed to write every element of a string to a file. This __includes the trailing Null character__. The problem for me is **Files.WriteString()** litters plain text files with tailing nulls. What I should have done was write my own **WriteString()** and **WriteLn()**. The program [HelloworldFile](./HelloworldFile.Mod) below is a more appropriate solution to writing strings and line endings than relying directly on **Files**. In a future post I will explorer making this more generalized in a revised \"Fmt\" module.\n\n~~~\nMODULE HelloworldFile;\n\nIMPORT Files, Strings;\n\nCONST OberonEOL = 1; UnixEOL = 2; WindowsEOL = 3;\n\nVAR\n  (* holds the eol marker type to use in WriteLn() *)\n  eolType : INTEGER;\n  (* Define a file handle *)\n    f : Files.File;\n  (* Define a file rider *)\n    r : Files.Rider;\n\nPROCEDURE WriteLn(VAR r : Files.Rider);\nBEGIN\n  IF eolType = WindowsEOL THEN\n    (* A DOS/Windows style line ending, LFCR *)\n    Files.Write(r, 13);\n    Files.Write(r, 10);\n  ELSIF eolType = UnixEOL THEN\n     (* Linux/macOS style line ending, LF *)\n     Files.Write(r, 10);\n  ELSE\n    (* Oberon, RISC OS style line ending, CR *)\n    Files.Write(r, 13);\n  END;\nEND WriteLn;\n\nPROCEDURE WriteString(VAR r : Files.Rider; s : ARRAY OF CHAR);\n  VAR i : INTEGER;\nBEGIN\n  i := 0;\n  WHILE i < Strings.Length(s) DO\n    Files.Write(r, ORD(s[i]));\n    INC(i);\n  END;\nEND WriteString;\n\nBEGIN\n  (* Set the desired eol type to use *)\n  eolType := UnixEOL;\n  (* Create our file, New returns a file handle *)\n  f := Files.New(\"helloworld.txt\"); ASSERT(f # NIL);\n  (* Register our file with the file system *)\n  Files.Register(f);\n  (* Set the position of the rider to the beginning *)\n  Files.Set(r, f, 0);\n  (* Use the rider to write out \"Hello World!\" followed by a end of line *)\n  WriteString(r, \"Hello World!\");\n  WriteLn(r);\n  (* Close our modified file *)\n  Files.Close(f);\nEND HelloworldFile.\n~~~\n\nI have two new procedures \"WriteString\" and \"WriteLn\". These mimic the parameters found in the Files module. The module body is a bit longer.\n\nCompare this to a simple example of \"Hello World\" using the **Out** module.\n\n~~~\nMODULE HelloWorld;\n\nIMPORT Out;\n\nBEGIN\n  Out.String(\"Hello World\");\n  Out.Ln;\nEND HelloWorld.\n~~~\n\nLook at the difference is in the module body. I need to setup our file and rider as well as pick the type of line ending to use in \"WriteLn\". The procedures doing the actual work look very similar, \"String\" versus \"WriteString\" and \"Ln\" versus \"WriteLn\".  \n\n\nLine ends vary between operating systems. Unix-like systems usually use a line feed. DOS/Windows systems use a carriage return and line feed. Oberon Systems use only a carriage return. If we're going to the trouble of re-creating our \"WriteString\" and \"WriteLn\" procedures it also makes sense to handle the different line ending options.  In this case I've chosen to use an INTEGER variable global to the module called \"eolType\". I have a small set of constants to indicate which line ending is needed. In \"WriteLn\" I use that value as a guide to which line ending to use with the rider writing to the file.\n\nThe reason I chose this approach is because I want my writing procedures to use the same procedure signatures as the \"Files\" module. In a future post I will explore type conversion and a revised implementation of my \"Fmt\" module focusing on working with plain text files.\n\nAside from our file setup and picking an appropriate end of line marker the shape of the two programs look very similar.\n\nReferences and resources\n------------------------\n\nYou can see a definition of the [Files](https://miasap.se/obnc/obncdoc/basic/Files.def.html \"My example module definition is based on the on Karl created in OBNC\") at Karl Landström's documentation for his compiler along with the definitions for [In](https://miasap.se/obnc/obncdoc/basic/In.def.html) and [Out](https://miasap.se/obnc/obncdoc/basic/Out.def.html).\n\n\nNext & Previous\n---------------\n\n- Next [Portable Conversions (Integers)](../../11/26/Portable-Conversions-Integers.html)\n- Prev [Combining Oberon-07 with C using Obc-3](../../06/14/Combining-Oberon-07-with-C-using-Obc-3.html)\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2021, R. S. Doiel",
        "date": "2021-11-12",
        "keywords": [
          "Oberon",
          "Files",
          "plain text"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 20,
        "title": "Revisiting Files"
      },
      "url": "posts/2021/11/22/Revisiting-Files.json"
    },
    {
      "content": "\nSetting up FreeDOS 1.3rc4 with Qemu\n-----------------------------------\n\nBy R. S. Doiel, 2021-11-27\n\nIn this article I'm going explore setting up FreeDOS with Qemu\non my venerable Dell 4319 running Raspberry Pi Desktop OS (Debian\nGNU/Linux).  First step is to download FreeDOS \"Live CD\" in the\n1.3 RC4 release. See http://freedos.org/download/ for that.\n\nInstalling Qemu\n---------------\n\nI needed to install Qemu in my laptop. It runs the Raspberry Pi\nDesktop OS (i.e. Debian with Raspberry Pi UI). I choose to install\nthe \"qemu-system\" package since I will likely use qemu for other\nthings besides FreeDOS. The qemu-system package contains all the\nvarious systems I might want to emulate in other projects as well\nas several qemu utilities that are handy.  Here's the full sequence\nof `apt` commands I ran (NOTE: these included making sure my laptop\nwas up to date before I installed qemu-system).\n\n~~~\nsudo apt update\nsudo apt upgrade\nsudo apt install qemu-system\n~~~\n\nNow that I had the software available it was time to figure out\nhow to actually knit things together and run FreeDOS.\n\n\nObtaining FreeDOS 1.3rc4\n------------------------\n\nBefore I get started I create a folder in my home directory\nfor running everything. You can name it what you want\nbut I called mine `FreeDOS_13` and changed into that folder\nfor the work in this article.\n\n~~~\nmkdir FreeDOS_13\ncd FreeDOS_13\n~~~\n\nI initially tried the CD images but ran into odd problems with\nqemu (possibly due to my lack of experience with qemu).\nAfter looking at that various options the USB Full release\nseemed like a good choice. It comes both as an image you can\n\"burn\" to your USB drive both also as a \"vmdk\" file used with\nemulators.\n\n~~~\ncurl -L -O https://www.ibiblio.org/pub/micro/pc-stuff/freedos/files/distributions/1.3/previews/1.3-rc4/FD13-FullUSB.zip\nunzip FD13-FullUSB.zip\n~~~\n\nAt this point you should see the FreeDOS \"vmdk\" file, and \"img\" file and readme files if you list the directory out. I'm going to use the \"vmdk\" file to install FreeDOS on my virtual harddrive freedos.img.\n\n~~~\nls -l \n~~~\n\nPrepping my virtual machine\n---------------------------\n\nA virtual machine is not just a CPU and some random\naccess memory. A machine can include storage devices. For\nthe retro \"DOS\" experience you might looking virtual devices\nfor a \"harddrive\", \"floppy drive\" and \"CD-ROM drive\". \nQemu provides a tool called `qemu-img` for creating \nthese types of virtual devices.\n\nThe basic command is `qemu-img` using the \"create\" option with\nsome parameters.  The parameter are filename and size (see\n`man qemu-img` for gory details). I am calling my virtual\nharddrive \"freedos.img\".  With `qemu-img` the size can be\nspecified with a suffix like \"K\" for kilobytes,  \"M\" for\nmegabytes and \"G\" for gigabytes. DOS is a minimal requirements\na small (by today's standards) 750 megabyte harddrive seems\nappropriate.\n\n~~~\nqemu-img create freedos.img 750M\n~~~\n\nFor my purposes I need a harddrive so I stopped there. You\ncan always create other drives and then restart your virtual\nmachine with the appropriate options.\n\nBring up my FreeDOS box\n-----------------------\n\nNow I was ready to boot from installation media and install\nFreeDOS 1.3rc4 on my virtual harddrive.  For that I\nuse a \"qemu\" command for the system I want to emulate.\nI picked `qemu-system-i386` (see can see\nthe gory details of that command using `man qemu-system-i386`).\nTo install FreeDOS I'm going to boot from the vmdk file \nprovided for the purpose of installation. I then use the FreeDOS\ninstaller to make my freedos.img file bootable with all the\nDOS software I want to play with.\n\n~~~\nqemu-system-i386 \\\n   -m 8 \\\n   -boot menu=on,strict=on \\\n   -hda freedos.img \\\n   -hdb FD13FULL.vmdk\n~~~\n\nAt this point you should see the machine start to boot, press Esc\nwhen prompted and select the second hard drive to boot from (that's\nour vmdk drive).  The drive is then treated like the CD-ROM, follow\nthe programs instructions for installation. You will need to reboot\nseveral times during the process. Until your full installation is\ncomplete you'll need to select the second harddrive as the boot drive\nand continue the installation.\n\nThe first time I successfully installed FreeDOS 1.3rc4 I just installed\nthe plain dos. When I re-did the process I install everything. It\nfills up my 750M virtual harddrive but rc4 includes development tools\nlike a C compiler.  That I think made it worth it.\n\nHere's a Bash script you can use to build your FreeDOS machine.\n\n~~~\n#!/bin/bash\n\nif [ ! -f freedos.img ]; then\n  echo \"Creating fresh Harddisk as drive C:\"\n  qemu-img create freedos.img 750M\nfi\necho \"Booting machine using FD13FULL.vmdk for installation\"\nqemu-system-i386 \\\n    -m 8 \\\n    -boot menu=on,strict=on \\\n    -hda freedos.img \\\n    -hdb FD13FULL.vmdk\n~~~\n\nAnd here is one for running it.\n\n~~~\n#!/bin/bash\n\necho \"Booting machine using freedos.img as drive C:\"\nqemu-system-i386 \\\n    -m 8 \\\n    -boot menu=on,strict=on \\\n    -hda freedos.img\n~~~\n\nNext step, explore FreeDOS and see what I can build.\n\nPutting everything together\n---------------------------\n\nBelow is a [script](run-freedos-1.3rc4.bash) I developed automating either building or running your FreeDOS setup.\n\n~~~\n#!/bin/bash\n\nif [ ! -f FD13FULL.vmdk ]; then\n    if [ ! -f FD13-FullUSB.zip ]; then\n      echo \"Missing FD13FULL.vmdk, downloading FD13-FullUSB.zip\"\n      curl -L -O https://www.ibiblio.org/pub/micro/pc-stuff/freedos/files/distributions/1.3/previews/1.3-rc4/FD13-FullUSB.zip\n    fi\n    echo \"Unzipping FD13-FullUSB.zip\"\n    unzip FD13-FullUSB.zip\nfi\n\nif [ ! -f freedos.img ]; then\n  echo \"Creating fresh Harddisk as drive C:\"\n  qemu-img create freedos.img 750M\n  echo \"Booting machine using FD13FULL.vmdk as drive C:\"\n  echo \"Installing FreeDOS on drive D:\"\n  qemu-system-i386 \\\n      -name FreeDOS \\\n      -machine pc \\\n      -m 32 \\\n      -boot order=c \\\n      -hda FD13FULL.vmdk \\\n      -hdb freedos.img \\\n      -parallel none \\\n      -vga cirrus \\\n      -display gtk\nelse\n  echo \"Booting machine using freedos.img on drive C:\"\n  qemu-system-i386 \\\n      -name FreeDOS \\\n      -machine pc \\\n      -m 32 \\\n      -boot menu=on,strict=on \\\n      -hda freedos.img \\\n      -parallel none \\\n      -vga cirrus \\\n      -display gtk\nfi\n~~~\n\n\nReference material\n------------------\n\nMy inspiration for this was the description of manual install in\nthe FreeDOS book section of the website, [Manual Install](https://www.freedos.org/books/get-started/june14-manual-install.html).\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2021, R. S. Doiel",
        "date": "2021-11-27",
        "keywords": [
          "FreeDOS",
          "Qemu"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Setting up FreeDOS 1.3rc4 with Qemu"
      },
      "url": "posts/2021/11/27/FreeDOS-1.3rc4-with-Qemu.json"
    },
    {
      "content": "\nPortable conversions (Integers)\n===============================\n\nBy R. S. Doiel, 2021-11-26\n\nAn area in working with Oberon-07 on a POSIX machine that has proven problematic is type conversion. In particular converting to and from INTEGER or REAL and ASCII.  None of the three compilers I am exploring provide a common way of handling this. I've explored relying on C libraries but that approach has it's own set of problems.  I've become convinced a better approach is a pure Oberon-07 library that handles type conversion with a minimum of assumptions about the implementation details of the Oberon compiler or hardware. I'm calling my conversion module \"Types\". The name is short and descriptive and seems an appropriate name for a module consisting of type conversion tests and transformations.  My initial implementation will focusing on converting integers to and from ASCII.\n\nINTEGER to ASCII and back again\n-------------------------------\n\nI don't want to rely on the representation of the INTEGER value in the compiler or at the machine level. That has lead me to think in terms of an INTEGER as a signed whole number. \n\nThe simplest case of converting to/from ASCII is the digits from zero to nine (inclusive). Going from an INTEGER to an ASCII CHAR is just looking up the offset of the character representing the \"digit\". Like wise going from ASCII CHAR to a INTEGER is a matter of mapping in the reverse direction.  Let's call these procedures `DigitToChar` and  `CharToDigit*`.\n\nSince INTEGER can be larger than zero through nine and CHAR can hold non-digits I'm going to add two additional procedures for validating inputs -- `IsIntDigit` and `IsCharDigit`. Both return TRUE if valid, FALSE if not.\n\nFor numbers larger than one digit I can use decimal right shift to extract the ones column value or a left shift to reverse the process.  Let's called these `IntShiftRight` and `IntShiftLeft`.  For shift right it'd be good to capture the ones column being lost. For shift left it would be good to be able to shift in a desired digit. That way you could shift/unshift to retrieve to extract and put back values.\n\nA draft definition for \"Types\" should look something like this.\n\n~~~\nDEFINITION Types;\n\n(* Check if an integer is a single digit, i.e. from 0 through 9 returns\n   TRUE, otherwise FALSE *)\nPROCEDURE IsIntDigit(x : INTEGER) : BOOLEAN;\n\n(* Check if a CHAR is \"0\" through \"9\" and return TRUE, otherwise FALSE *)\nPROCEDURE IsCharDigit(ch : CHAR) : BOOLEAN;\n\n(* Convert digit 0 through 9 into an ASCII CHAR \"0\" through \"9\",\n   ok is TRUE if conversion successful, FALSE otherwise *)\nPROCEDURE DigitToChar(x : INTEGER; VAR ch : CHAR; VAR ok : BOOLEAN);\n\n(* Convert a CHAR \"0\" through \"9\" into a digit 0 through 9, ok\n   is TRUE is conversion successful, FALSE otherwise *)\nPROCEDURE CharToDigit(ch : CHAR; VAR x : INTEGER; VAR ok : BOOLEAN);\n\n(* Shift an integer to the right (i.e. x * 0.1) set \"r\" to the\n   value shifted out (ones column lost) and return the shifted value.\n   E.g.  x becomes 12, r becomes 3.\n\n       x := IntShiftRight(123, r);\n   \n *)\nPROCEDURE IntShiftRight(x : INTEGER; VAR r : INTEGER) : INTEGER;\n\n(* Shift an integer to the left (i.e. x * 10) adding the value y\n   after the shift.\n\n   E.g. x before 123\n\n       x := IntShiftRight(12, 3);\n\n *)\nPROCEDURE IntShiftLeft(x, y : INTEGER) : INTEGER;\n\n(* INTEGER to ASCII *)\nPROCEDURE Itoa(src : INTEGER; VAR value : ARRAY OF CHAR; VAR ok : BOOLEAN);\n\n(* ASCII to INTEGER *)\nPROCEDURE Atoi(src : ARRAY OF CHAR; VAR value : INTEGER; VAR ok : BOOLEAN);\n\nEND Types.\n~~~\n\n\nNOTE: Oberon-07 provides us the ORD and CHR built as part of the\nlanguage.  These are for working with the encoding and decoding\nvalues as integers. This is not the same thing as the meaning\nof \"0\" versus the value of 0.  Getting to and from the encoding\nto the meaning of the presentation can be done with some simple\narithmetic.\n\nPutting it all together\n-----------------------\n\n~~~\n(* DigitToChar converts an INTEGER less than to a character. E.g.\n   0 should return \"0\", 3 returns \"3\", 0 returns \"9\" *)\nPROCEDURE DigitToChar*(i : INTEGER) : CHAR;\nBEGIN\n  RETURN (CHR(ORD(\"0\") + i))\nEND DigitToChar;\n\n(* CharToDigit converts a single \"Digit\" character to an INTEGER value.\n   E.g. \"0\" returns 0, \"3\" returns 3, \"9\" returns 9. *)\nPROCEDURE CharToDigit(ch : CHAR) : INTEGER;\nBEGIN\n  RETURN (ORD(ch) - ORD(\"0\"))\nEND CharToDigit;\n~~~\n\nThis implementation is naive. It assumes the ranges of the input values\nwas already checked. In practice this is going to encourage bugs.\n\nIn a language like Go or Python you can return multiple values (in\nPython you can return a tuple). In Oberon-07 I could use a\nRECORD type to do that but that feels a little too baroque. Oberon-07\nlike Oberon-2, Oberon, Modula and Pascal does support \"VAR\" parameters. \nWith a slight modification to our procedure signatures I can support\neasy assertions about the conversion. Let's create two functional\nprocedures `IsIntDigit()` and `IsCharDigit()` then update our\n`DigitToChar()` and `CharToDigit()` with an a  \"VAR ok : BOOLEAN\"\nparameter.\n\n~~~\n(* IsIntDigit returns TRUE is the integer value is zero through nine *)\nPROCEDURE IsIntDigit(i : INTEGER) : BOOLEAN;\nBEGIN \n  RETURN ((i >= 0) & (i <= 9))\nEND IsIntDigit;\n\n(* IsCharDigit returns TRUE if character is zero through nine. *)\nPROCEDURE IsCharDigit(ch : CHAR) : BOOLEAN;\nBEGIN\n  RETURN ((ch >= \"0\") & (ch <= \"9\"))\nEND IsCharDigit;\n\n(* DigitToChar converts an INTEGER less than to a character. E.g.\n   0 should return \"0\", 3 returns \"3\", 0 returns \"9\" *)\nPROCEDURE DigitToChar*(i : INTEGER; VAR ok : BOOLEAN) : CHAR;\nBEGIN\n  ok := IsIntDigit(i);\n  RETURN (CHR(ORD(\"0\") + i))\nEND DigitToChar;\n\n(* CharToDigit converts a single \"Digit\" character to an INTEGER value.\n   E.g. \"0\" returns 0, \"3\" returns 3, \"9\" returns 9. *)\nPROCEDURE CharToDigit(ch : CHAR; VAR ok : BOOLEAN) : INTEGER;\nBEGIN\n  ok := IsCharDigit(ch);\n  RETURN (ORD(ch) - ORD(\"0\"))\nEND CharToDigit;\n~~~\n\nWhat about values are greater nine? Here we can take advantage\nof our integer shift procedures.  `IntShiftRight` will move the\nINTEGER value right reducing it's magnitude (i.e. x * 0.1). It\nalso captures the ones column lost in the shift.  Repeatedly calling\n`IntShiftRight` will let us peal off the ones columns until the\nvalue \"x\" is zero. `IntShiftLeft` shifts the integer to the\nleft meaning it raises it a magnitude (i.e. x * 10). `IntShiftLeft`\nalso rakes a value to shift in on the right side of the number.\nIn this way we can shift in a zero and get `x * 10` or shift in\nanother digit and get `(x * 10) + y`. This means you can use\n`IntShiftRight` and recover an `IntShiftLeft`.\n\n~~~\n\n(* IntShiftRight converts the input integer to a real, multiplies by 0.1\n   and converts by to an integer. The value in the ones column is record\n   in the VAR parameter r.  E.g. IntShiftRight(123) return 12, r is set to 3. *)\nPROCEDURE IntShiftRight*(x : INTEGER; VAR r : INTEGER) : INTEGER;\n  VAR i : INTEGER; isNeg : BOOLEAN;\nBEGIN\n  isNeg := (x < 0);\n  i := FLOOR(FLT(ABS(x)) * 0.1);\n  r := ABS(x) - (i * 10);\n  IF isNeg THEN\n    i := i * (-1);\n  END;\n  RETURN i\nEND IntShiftRight;\n\n(* IntShiftLeft multiples input value by 10 and adds y. E.g. IntShiftLeft(123, 4) return 1234 *)\nPROCEDURE IntShiftLeft*(x, y : INTEGER) : INTEGER;\n  VAR i : INTEGER; isNeg : BOOLEAN;\nBEGIN\n  isNeg := (x < 0);\n  i := (ABS(x) * 10) + y;\n  IF isNeg THEN\n    i := i * (-1);\n  END;\n  RETURN i\nEND IntShiftLeft;\n\n~~~\n\nI have what I need for implementing `Itoa` (integer to ASCII).\n\n\n~~~\n\n(* Itoa converts an INTEGER to an ASCII string setting ok BOOLEAN to\n   TRUE if value ARRAY OF CHAR holds the full integer, FALSE if\n   value was too small to hold the integer value.  *)\nPROCEDURE Itoa*(x : INTEGER; VAR value : ARRAY OF CHAR; ok : BOOLEAN);\n  VAR i, j, k, l, minL : INTEGER; tmp : ARRAY BUFSIZE OF CHAR; isNeg : BOOLEAN;\nBEGIN\n  i := 0; j := 0; k := 0; l := LEN(value); isNeg := (x < 0);\n  IF isNeg THEN\n    (* minimum string length for value is 3, negative sign, digit and 0X *)\n    minL := 3;\n  ELSE \n    (* minimum string length for value is 2, one digit and 0X *)\n    minL := 2; \n  END;\n  ok := (l >= minL) & (LEN(value) >= LEN(tmp));\n  IF ok THEN\n    IF IsIntDigit(ABS(x)) THEN\n      IF isNeg THEN\n         value[i] := \"-\"; INC(i);\n      END;\n      value[i] := DigitToChar(ABS(x), ok); INC(i); value[i] := 0X;\n    ELSE\n      x := ABS(x); (* We need to work with the absolute value of x *)\n      i := 0; tmp[i] := 0X;\n      WHILE (x >= 10) & ok DO\n        (* extract the ones columns *)\n        x := IntShiftRight(x, k); (* a holds the shifted value, \n                                     \"k\" holds the ones column \n                                     value shifted out. *)\n        (* write append k to our temp array holding values in\n           reverse number magnitude *)\n        tmp[i] := DigitToChar(k, ok); INC(i); tmp[i] := 0X;\n      END;\n      (* We now can convert the remaining \"ones\" column. *)\n      tmp[i] := DigitToChar(x, ok); INC(i); tmp[i] := 0X;\n      IF ok THEN\n        (* now reverse the order of tmp string append each\n           character to value *)\n        i := 0; j := Strings.Length(tmp) - 2;\n        IF isNeg THEN\n          value[i] := \"-\"; INC(i);\n        END;\n        j := Strings.Length(tmp) - 1;\n        WHILE (j > -1) DO\n          value[i]:= tmp[j]; \n          INC(i); DEC(j);\n          value[i] := 0X;\n        END;\n        value[i] := 0X;\n      END;\n    END; \n  ELSE\n    ok := FALSE;\n  END;\nEND Itoa;\n\n~~~\n\nIntegers in Oberon are signed. So I've chosen to capture the sign in the `isNeg` variable. This lets me work with the absolute value for the actual conversion.  One failing in this implementation is I don't detect an overflow.  Also notice that I am accumulating the individual column values in reverse order (lowest magnitude first).  That is what I need a temporary buffer. I can then copy the values in reverse order into the VAR ARRAY OF CHAR. Finally I also maintain the ok BOOLEAN to track if anything went wrong.\n\nWhen moving from an ASCII representation I can simplified the code by having a local (to the module) procedure for generating magnitudes.\n\nGoing the other way I can simplify my `Atoi` if I have an local to the module \"magnitude\" procedure.\n\n~~~\n\n(* magnitude takes x and multiplies it be 10^y, If y is positive zeros\n   are appended to the right side (i.e. multiplied by 10). If y is\n   negative then the result is shifted left (i.e.. multiplied by\n   0.1 via IntShiftRight().).  The digit(s) shift to the fractional\n   side of the decimal are ignored. *)\nPROCEDURE magnitude(x, y : INTEGER) : INTEGER;\n  VAR z, w : INTEGER;\nBEGIN\n  z := 1;\n  IF y >= 0 THEN\n    WHILE y > 0 DO\n      z := IntShiftLeft(z, 0);\n      DEC(y);\n    END;\n  ELSE\n    WHILE y < 0 DO\n      x := IntShiftRight(x, w);\n      INC(y);\n    END;\n  END;\n  RETURN (x * z)\nEND magnitude;\n\n~~~\n\nAnd with that I can put together my `Atoi` (ASCII to integer) procedure.  I'll need to add some sanity checks as well.\n\n~~~\n\n(* Atoi converts an ASCII string to a signed integer value\n   setting the ok BOOLEAN to TRUE on success and FALSE on error. *)\nPROCEDURE Atoi*(source : ARRAY OF CHAR; VAR value : INTEGER; VAR ok : BOOLEAN);\n  VAR i, l, a, m: INTEGER; isNeg : BOOLEAN;\nBEGIN\n  (* \"i\" is the current CHAR position we're analyzing, \"l\" is the\n     length of our string, \"a\" holds the accumulated value,\n     \"m\" holds the current magnitude we're working with *)\n  i := 0; l := Strings.Length(source);\n  a := 0; m := l - 1; isNeg := FALSE; ok := TRUE;\n  (* Validate magnitude and sign behavior *)\n  IF (l > 0) & (source[0] = \"-\") THEN\n    INC(i); DEC(m);\n    isNeg := TRUE;\n  ELSIF (l > 0) & (source[0] = \"+\") THEN\n    INC(i); DEC(m);\n  END;\n\n  (* The accumulator should always hold a positive integer, if the\n     sign flips we have overflow, ok should be set to FALSE *)\n  ok := TRUE;\n  WHILE (i < l) & ok DO\n    a := a + magnitude(CharToDigit(source[i], ok), m);\n    IF a < 0 THEN\n      ok := FALSE; (* we have an overflow condition *)\n    END;\n    DEC(m);\n    INC(i);\n  END;\n  IF ok THEN\n    IF (i = l) THEN\n      IF isNeg THEN\n        value := a * (-1);\n      ELSE\n        value := a;\n      END;\n    END;\n  END;\nEND Atoi;\n\n~~~\n\nHere's an example using the procedures.\n\nConverting an integer 1234 to an string \"1234\".\n\n~~~\n\n   x := 1234; s := \"\"; ok := FALSE;\n   Types.Itoa(x, s, ok);\n   IF ok THEN \n     Out.String(s); Out.String(\" = \");\n     Out.Int(x,1);Out.Ln;\n   ELSE\n     Out.String(\"Something went wrong\");Out.Ln;\n   END;\n\n~~~\n\nConverting a string \"56789\" to integer 56789.\n\n~~~\n\n   x := 0; src := \"56789\"; ok := FALSE;\n   Types.Atoi(src, x, ok);\n   IF ok THEN \n     Out.Int(x,1); Out.String(\" = \"); Out.String(s); \n     Out.Ln;\n   ELSE\n     Out.String(\"Something went wrong\");Out.Ln;\n   END;\n\n~~~\n\n\nReferences and resources\n------------------------\n\nImplementations for modules for this article are linked here [Types](./Types.Mod), [TypesTest](./TypesTest.Mod) and [Tests](./Tests.Mod). \n\nExpanded versions of the `Types` module will be available as part of Artemis Project -- [github.com/rsdoiel/Artemis](https://github.com/rsdoiel/Artemis).\n\nPrevious\n--------\n\n- [Revisiting Files](../../11/22/Revisiting-Files.html)\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2021, R. S. Doiel",
        "date": "2021-11-26",
        "keywords": [
          "Oberon",
          "Modules",
          "Types",
          "conversion"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 21,
        "title": "Portable Conversions (Integers)"
      },
      "url": "posts/2021/11/26/Portable-Conversions-Integers.json"
    }
  ]
}