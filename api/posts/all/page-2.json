{
  "page": 2,
  "total_pages": 6,
  "has_more": true,
  "next_page": "posts/all/page-3.json",
  "values": [
    {
      "content": "\n# First Personal Search Engine Prototype\n\nBy R. S. Doiel, 2023-08-10\n\nI've implemented a first prototype of my personal search engine which\nI will abbreviate as \"pse\" from here on out. I implemented it using \nthree [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) scripts\nrelying on [sqlite3](https://sqlite.org), [wget](https://en.wikipedia.org/wiki/Wget) and [PageFind](https://pagefind.app) to do the heavy lifting.\n\nBoth Firefox and newsboat store useful information in sqlite databases.  Firefox's `moz_places.sqlite` holds both all the URLs visited as well as those that are associated with bookmarks (i.e. the SQLite database `moz_bookmarks.sqlite`).  I had about 2000 bookmarks, less than I thought with many being stale from link rot. Stale page URLs really slow down the harvest process because of the need for wget to wait on various timeouts (e.g. DNS, server response, download times).  The \"history\" URLs would make an interesting collection to spider but you'd probably want to have an exclude list (e.g. there's no point in saving queries to search engines, web mail, shopping sites). Exploring that will wait for another prototype.\n\nThe `cache.db` associated with Newsboat provided a rich resource of content and much fewer stale links (not surprising because I maintain that URL list more much activity then reviewing my bookmarks).  Between the two I had 16,000 pages. I used SQLite 3 to query the url values from the various DB into sorting for unique URLs into a single text file one URL per line.\n\nThe next thing after creating a list of pages I wanted to search was to download them into a directory using wget.  Wget has many options, I choose to enable timestamping, create a protocol directory and then a domain and path directory for each item. This has the advantage of being able to transform the Path into a URL later.\n\nOnce the content was harvested I then used PageFind to index the all the harvested content. Since I started using PageFind originally the tool has gained an option called `--serve` which provides a localhost web service on port 1414.  All I needed to do was add an index.html file to the directory where I harvested the content and saved the PageFind indexes. Then I used PageFind to again to provide a localhost based personal search engine.\n\nWhile the total number of pages was small (16k pages) I did find interesting results just trying out random words. This makes the prototype look promising.\n\n## Current prototype components\n\nI have simple Bash script that gets the URLs from both Firefox bookmarks and Newsboat's cache then generates a single text file of unique URLs I've named \"pages.txt\".\n\nI then use the \"pages.txt\" file to harvest content with wget into a tree structure like \n\n- htdocs\n    - http (all the http based URLs I harvest go in here)\n    - https (all the https based URLs I harvest go in here)\n    - pagefind (this holds the PageFind indexes and JavaScript to implement the search UI)\n    - index.html (this holds the webpage for the search UI using the libraries in `pagefind`)\n\nSince I'm only downloaded the HTML the 16k pages does not take up significant disk space yet.\n\n## Prototype Implementation\n\nHere's the bash scripts I use to get the URLs, harvest content and launch my localhost search engine based on PageFind.\n\nGet the URLs I want to be searchable. I use to environment variables\nfor finding the various SQLite 3 databases (i.e. PSE_MOZ_PLACES, PSE_NEWSBOAT).\n\n~~~\n#!/bin/bash\n\nif [ \"$PSE_MOZ_PLACES\" = \"\" ]; then\n    printf \"the PSE_MOZ_PLACES environment variable is not set.\"\n    exit 1\nfi\nif [ \"$PSE_NEWSBOAT\" = \"\" ]; then\n    printf \"the PSE_NEWSBOAT environment variable is not set.\"\n    exit 1\nfi\n\nsqlite3 \"$PSE_MOZ_PLACES\" \\\n    'SELECT moz_places.url AS url FROM moz_bookmarks JOIN moz_places ON moz_bookmarks.fk = moz_places.id WHERE moz_bookmarks.type = 1 AND moz_bookmarks.fk IS NOT NULL' \\\n    >moz_places.txt\nsqlite3 \"$PSE_NEWSBOAT\" 'SELECT url FROM rss_item' >newsboat.txt\ncat moz_places.txt newsboat.txt |\n    grep -E '^(http|https):' |\n    grep -v '://127.0.' |\n    grep -v '://192.' |\n    grep -v 'view-source:' |\n    sort -u >pages.txt\n~~~\n\nThe next step is to have the pages. I use wget for that.\n\n~~~\n#!/bin/bash\n#\nif [ ! -f \"pages.txt\" ]; then\n    echo \"missing pages.txt, skipping harvest\"\n    exit 1\nfi\necho \"Output is logged to pages.log\"\nwget --input-file pages.txt \\\n    --timestamping \\\n    --append-output pages.log \\\n    --directory-prefix htdocs \\\n    --max-redirect=5 \\\n    --force-directories \\\n    --protocol-directories \\\n    --convert-links \\\n    --no-cache --no-cookies\n~~~\n\nFinally I have a bash script that generates the index.html page, an Open Search Description XML file, indexes the harvested sites and launches PageFind in server mode.\n\n~~~\n#!/bin/bash\nmkdir -p htdocs\n\ncat <<OPENSEARCH_XML >htdocs/pse.osdx\n<OpenSearchDescription xmlns=\"http://a9.com/-/spec/opensearch/1.1/\"\n                       xmlns:moz=\"http://www.mozilla.org/2006/browser/search/\">\n  <ShortName>PSE</ShortName>\n  <Description>A Personal Search Engine implemented via wget and PageFind</Description>\n  <InputEncoding>UTF-8</InputEncoding>\n  <Url rel=\"self\" type=\"text/html\" method=\"get\" template=\"http://localhost:1414/index.html?q={searchTerms}\" />\n  <moz:SearchForm>http://localhost:1414/index.html</moz:SearchForm>\n</OpenSearchDescription>\nOPENSEARCH_XML\n\ncat <<HTML >htdocs/index.html\n<html>\n<head>\n<link\n  rel=\"search\"\n  type=\"application/opensearchdescription+xml\"\n  title=\"A Personal Search Engine\"\n  href=\"http://localhost:1414/pse.osdx\" />\n<link href=\"/pagefind/pagefind-ui.css\" rel=\"stylesheet\">\n</head>\n<body>\n<h1>A personal search engine</h1>\n<div id=\"search\"></div>\n<script src=\"/pagefind/pagefind-ui.js\" type=\"text/javascript\"></script>\n<script>\n    window.addEventListener('DOMContentLoaded', function(event) {\n\t\tlet page_url = new URL(window.location.href),\n    \t    query_string = page_url.searchParams.get('q'),\n      \t\tpse = new PagefindUI({ element: \"#search\" });\n\t\tif (query_string !== null) {\n\t\t\tpse.triggerSearch(query_string);\n\t\t}\n    });\n</script>\n</body>\n</html>\nHTML\n\npagefind \\\n--source htdocs \\\n--serve\n~~~\n\nThen I just language my web browser pointing at `http://localhost:1414/index.html`. I can even pass the URL a `?q=...` query string if I want.\n\nFrom a functionality point of view this is very bare bones and I don't think 16K pages is enough to make it compelling (I think I need closer to 100K for that).\n\n## What I learned from the prototype so far\n\nThis prototype suffers from several limitations.\n\n1. Stale links in my pages.txt make the harvest process really really slow, I need to have a way to avoid stale links getting into the pages.txt or have them removed from the pages.txt\n2. PageFind's result display uses the pages I downloaded to my local machine. It would be better if the result link was translated to point at the actual source of the pages, I think this can be done via JavaScript in my index.html when I setup the PageFind search/results element. Needs more exploration\n\n16K pages is a very tiny corpus. I get interesting results from my testing but not good enough to make me use first.  I'm guessing I need a corpus of at least 100K pages to be compelling for first search use.\n\nIt is really nice having a localhost personal search engine. It means that I can keep working with my home network connection is problematic. I like that. Since the website generated for my localhost system is a \"static site\" I could easily replicate that to net and make it available to other machines.\n\nRight now the big time sync is harvesting content to index. I'm not certain yet how much space disk space will be needed for my 100K page target corpus.\n\nSetting up indexing and the search UI were the easiest part of the process.  PageFind is so easy to work with compare to enterprise search applications.\n\n## Things to explore\n\nI can think of several ways to enlarge my search corpus. The first is there are a few websites I use for reference that are small enough to mirror. Wget provides a mirror function. Working from a \"sites.txt\" list I could mirror those sites periodically and have their content available for indexing.\n\nWhen experimenting with the mirror option I notice I wind up with PDF that are linked in the pages being mirrored.  If I used the Unix find command to locate all the PDF I could use another tool to extract there text.  Doing that would enlarge my search beyond plain text and HTML.  I would need to think this through as ultimately I'd want to be able to recover the path to the PDF when those results are displayed.\n\nAnother approach would be to work with my full web browsers' history as\nwell as it's bookmarks. This would significantly expand the corpus. If I did this I could also check the \"head\" of the HTML for references to feeds that could be folded into my feed link harvests. This would have the advantage of capture content from sources I find useful to read but would catch blog posts I might have skipped due to limited reading time.\n\nI use Pocket to read the pages I find interesting in my feed reader.  Pocket has an API and I could get some additional interesting pages from it. Pocket also has various curated lists and they might have interesting pages to harvest and index. I think the trick would be to use those suggests against an exclude list of some sort. E.g. Makes not sense to try to harvest paywall stuff or commercial sites more generally. One of the values I see in pse is that it is a personal search engine not a replacement for commercial search engines generally.\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 2,
        "pubDate": "2023-03-10",
        "series": "Personal Search Engine",
        "title": "First Personal Search Engine Prototype",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/10/first-prototype-pse.json"
    },
    {
      "content": "\n# Prototyping a personal search engine\n\nBy R. S. Doiel, 2023-03-07\n\n> Do we really need a search engine to index the \"whole web\"? Maybe a curated subset is better.\n\nAlex Schreoder's post [A Vision for Search](https://alexschroeder.ch/wiki/2023-03-07_A_vision_for_search) prompted me to write up an idea I call a \"personal search engine\".   I've been thinking about a \"a personal search engine\" for years, maybe a decade.\n\nWith the current state of brokenness in commercial search engines, especially with the implosion of the commercial social media platforms, we have an opportunity to re-think search on a more personal level.\n\nThe tooling around static site generation where a personal search is an extension of your own website suggests a path out of the quagmire of commercial search engines.  Can techniques I use for my own site search, be extended into a personal search engine?\n\n## A Broken Cycle\n\nSearch engines happened pretty early on in the web. If my memory is correct they showed up with the arrival of support for [CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface \"Common Gateway Interface\") in early web server software. Remembering back through the decades I see a pattern.\n\n1. Someone comes up with a clever way to index web content and determine relevancy\n2. They index enough the web to be interesting and attract early adopters\n3. They go mainstream, this compels them to have a business model, usually some form of ad-tech\n4. They index an ever larger portion of the web, the results from the search engine starts to degrade\n5. The business model becomes the primary focus of the company, the indexing gets exploited (e.g. content farms, page hijacking), the search results degrade.\n\nStage four and five can be summed up as the \"bad search engine stage\". When things get bad enough a new search engine comes on the scene and the early adopters jump ship and the cycle repeats. This was well established by the time some graduate students at Stanford invented page rank. I think it is happening now with search integrated ChatGPT.\n\nI think we're at the point in the cycle where there is an opportunity for something new. Maybe even break the loop entirely.\n\n## How do I use search?\n\nMy use of search engines can be described in four broad categories.\n\n1. Look for a specific answer queries\n    - `spelling of <VALUE>`\n    - `meaning of <VALUE>`\n    - `location of <VALUE>`\n    - `convert <UNIT> from <VALUE> to <UNIT>`\n2. Shopping queries\n    - pricing an item\n    - finding an item\n    - finding marketing material on an item\n3. Look for subject information\n    - a topic search\n    - news event\n    - algorithms\n4. Look for information I know exists\n    - technical documentation\n    - an article I read\n    - an article I want to read next\n\nMost of my searches are either subject information or retrieving something I know exists. Both are particularly susceptible to degradation when the business model comes to dominate the search results.\n\nA personal search engine for me would address these four types of searches before I reach for alternatives. In the mean time I'm stuck attempting to mitigate the bad search experience as best I can.\n\n## Mitigating the bad search engine experience\n\n> As commercial search engines degrade I rely on a given website's own search more\n\nI've noticed the follow search behavior practice changes in my own web usage.  For shopping I tend to go to the vendors I trust and use their searches on their websites.  To learn about a place, it's Wikipedia and if I trying to get a sense of going there I'll probably rely on an Open Street Map to avoid the ad-tech in commercial services. I dread using the commercial maps because usage correlates so strongly with the spam I encounter the next time I use an internet connected device.\n\nFor spelling and dictionary I can use Wiktionary. Location information I use Wikipedia and Open Street Maps. Weather info I have links into [NOAA](https://www.weather.gov/) website. Do I really need to use the commercial services?\n\nIt seems obvious that the commercial services for me are at best a fallback experience. They are no longer the \"go to\" place on the web to find stuff. I miss the convenience of using my web browsers URL box as a search box but the noise of the commercial search engines means the convenience is not worth the cost.\n\nWhat I would really like is a search service that integrated **my trusted sources** with a single search box but without the noise of the commercial sites. Is this possible? How much work would it be?\n\nI think a personal (or even small group) search engine is plausible and desirable. I think we can build a prototype with some off the shelf parts.\n\n## Observations\n\n1. I only need to index a tiny subset of the web, I don't want a web crawler that needs to be monitored and managed\n2. The audience of the search engine is me and possibly some friends\n3. There are a huge number of existing standards, protocols and structured data formats and practices I could leverage to mange building a search corpus and for indexing.\n4. Static site generators have moved site search from services (often outsourced to commercial search engines) to browser based solutions (e.g. [PageFind](https://pagefind.app), [LunrJS](https://lunrjs.com))\n5. A localhost site could stage pages for indexing and I could leverage my personal website to expose my indexes to my web devices (e.g. my phone).\n6. Tools like wget can mirror websites and that could also be used to stage content for a personal search engine\n7. There is a growing body of Open Access data, journal articles and books, these could be indexed and made available in a personal search engine with some effort\n\n## Exploring a personal search engine concept\n\nWhen I've brought up the idea of \"a personal search engine\" over the years with colleagues I've been consistently surprise at the opposition I encounter.  There are so many of reasons not to build something, including a personal search engine. That has left me thinking more deeply about the problem, a good thing in my experience.  I've synthesized that resistance into three general questions. Keeping those questions in mind will be helpful in evaluating the costs in time for prototyping a personal search engine and ultimately if the prototype should turn into an open source project.\n\n1. How would a personal search engine know/discover \"new\" content to include?\n2. Search engines are hard to setup and maintain (e.g. Solr, Opensearch), why would I want to spend time doing that?\n3. Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nConstraints can be a good thing to consider as well. Here's some constraints I think will be helpful when considering a prototype implementation.\n\n- I maintain my personal website using a Raspberry Pi 400. The personal search engine needs to respect the limitations of that device.\n- I'd like to be able to access my personal search engine from all my networked devices (e.g. my phone when I am away from home)\n- I have little time to prototype or code anything\n- I need to explain the prototype easily if I want others to help expand on the ideas\n- If it breaks I need to easily fix it\n\nI believe recent evolution of static site generation and site search offer an adjacent technology that can be leverage to demonstrate a personal search engine as a prototype. The prototype of a personal search engine could be an extension of my existing website.\n\n## Addressing challenges\n\n### How can a personal search engine know about new things?\n\nThe first challenge boils down to discovering content you want to index. What I'm describing is a personal search engine. I'm not trying to \"index the whole web\" or even a large part of it. I suspect the corpus I regularly search probably is in the neighborhood of a 100,000 pages or so. Too big for a bookmark list but magnitudes smaller than search engine deployments commonly see in an enterprise setting. I also am NOT suggesting a personal search engine will replace commercial search engines or even compete with them. What I'm thinking of is an added tool, not a replacement.\n\nCurating content is labor intensive. This is why Yahoo evolved from a curated web directory to become a hybrid web directory plus search engine before its final demise.  I don't want to have to change how I currently find content on the web. When I do stumble on something interesting I need a mechanism to easily add it to my personal search engine. Fortunately I think my current web reading habits can function like a [mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk).\n\nMost \"new\" content I find isn't from using a commercial search engine. When I look at my habits I find two avenues for content discovery dominate. I come across something via social media (today that's RSS feeds provided via Mastodon and Yarn Social/Twtxt) or from RSS, Atom and JSON feeds of blogs or websites I follow. Since the social media platforms I track support RSS I typically read all this content via newsboat which is a terminal based feed reader. I still find myself using the web browser's bookmark feature. It's just the bookmarks aren't helpful if they remain only in my web browser.  I also use [Pocket](https://getpocket.com) to read things later. I think all these can serve as a \"link discovery\" mechanism for a personal search engine. It's just a matter of collecting the URLs into a list of content I want to index, staging the content, index it and publish the resulting indexes on my personal website using a browser based search engine to query them.\n\nThis link discovery approach is different from how commercial search engines work.  Commercial engines rely on crawlers that retrieve a web page, analyze the content, find new links in the page then recursively follows those to scan whole domains and websites.  Recursive crawlers aren't automatic. It's easy for them to get trapped in link loops and often they can be a burden to the sites they are crawling (hence robot.txt files suggesting to crawlers what needs to be avoided).  I don't need to index the whole web, usually not even whole websites.  I'm interested in page level content and I can get a list of web pages from by bookmarks and the feeds I follow.\n\n\nA Quick digression:\n\nBlogs, in spite of media hype, haven't \"gone away\".  Presently we're seeing a bit of a renaissance with projects like [Micro.blog](https://micro.blog) and [FeedLand](http://docs.feedland.org/about.opml \"this is a cool project from Dave Winer\"). The \"big services\" like [WordPress](https://wordpress.com), [Medium](https://medium.com), [Substack](https://substack.com) and [Mailchimp](https://mailchimp.com/) provide RSS feeds for their content. RSS/Atom/JSON feed syndication all are alive and well at least for the sites I track and content I read. I suspect this is the case for others.  What is a challenge is knowing how to find the feed URL.  But even that I've notice is becoming increasingly predictable. I suspect given a list of blog sites I could come up with a way of guessing the feed URL in many cases even without an advertised URL in the HTML head or RSS link in the footer.\n\n### Search engines are hard to setup and maintain, how can that be made easier?\n\nI think this can be addressed in several ways. First is splitting the problem of content retrieval, indexing and search UI.  [PageFind](https://pagefind.app) is the current static site search I use on my blog.  It does a really good job at indexing blog content will little configuration. PageFind is clever about the indexes it builds.  When PageFind indexes a site is builds a partitioned index. Each partition is loaded by the web browser only when the current search string suggests it is needed. This means you can index a large number of pages (e.g. 100,000 pages) before it starts to feel sluggish. Indexing is fast and can be done on demand after harvesting the new pages you come across in your feeds. If the PageFind indexes are saved in my static site directory (a Git repository) I can implement the search UI there implementing the personal search engine prototype. The web browser is the search engine and PageFind tool is the indexer. The harvester is built by extracting interesting URLs from the feeds I follow and the current state of my web browsers' bookmarks and potentially from content in Pocket. Note the web browser bookmarks are synchronized across my devices so if I encounter an interesting URL in the physical world I can easily add it my personal search engine too the next time I process the synchronized bookmark file.\n\n### Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nEnterprise Search Engine Software is complicated to setup, very resource intensive and requires upkeep. For me Solr, Elasticsearch, Opensearch falls into the category \"day job\" duty and I do not want that burden for my personal search engine. Fortunately I don't need to run Solr, Elasticsearch or Opensearch. I can build a decent search engine using [PageFind](https://pagefind.app).  PageFind is simple to configured, simple to index with and it's indexes scale superbly for a browser based search engine UI. Hosting is reduced to the existing effort I put into updating my personal blog and automating the link extraction from the feeds I follow and my web browsers' current bookmark file.\n\nI currently use PageFind for web content I mirror to a search directory locally for off line reading. From that experience I know it can handle at least 100,000 pages. I know it will work on my Raspberry Pi 400. I don't see a problem in a prototype personal search engine assuming a corpus in the neighborhood of 100,000 pages.\n\n\n## Sketching the prototype\n\nHere's a sketch of a prototype of \"a personal search engine\" built on PageFind.\n\n1. Generate a list of URLs pointing at pages I want to index (this can be done by mining my bookmarks and feed reader content).\n2. Harvest and stage the pages on my local file system, maintaining a way to associated their actual URL with the staged copy\n3. Index with PageFind and save the resulting indexes my local copy of my personal website\n4. Have a page on my personal website use these indexes to implement a search and results page\n\nThe code that I would need to be implemented is mostly around extracting URL from my browser's bookmark file and my the feeds managed in my feed reader. Since newsboat is open source and it stores it cached feeds in a SQLite3 database in principle I could use the tables in that database to generate a list of content to harvest for indexing. I could write a script that combines the content from my bookmarks file and newsboat database rendering a flat list to harvest, stage and then index with PageFind. A prototype could be done in Bash or Python without too much of an effort.\n\nOne challenge remains after harvesting and staging is solved. It would be nice to use my personal search engine as my default search engine. After all I am already curating the content. I think this can be done by supporting the [Open Search Description](https://developer.mozilla.org/en-US/docs/Web/OpenSearch) to make my personal search engine a first class citizen in my browser URL bar. Similarly I could turn the personal search engine page into a PWA so I can have it on my phone's desktop along the other apps I commonly use.\n\nObservations that maybe helpful for a successful prototype\n\n1. I don't need to crawl the whole web just the pages that interest me\n2. I don't need to create or monitor a recursive web crawler\n3. I avoid junk because I'm curating the sources through my existing web reading practices\n4. I am targeting a small search corpus, approximately 100,000 pages or so\n5. I am only indexing HTML, Pagefind can limit the elements it indexes\n\nA prototype of a personal search engine seems possible. The challenge will be finding the time to implement it.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 1,
        "pubDate": "2023-03-07",
        "series": "Personal Search Engine",
        "title": "Prototyping a personal search engine",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/07/prototyping-a-personal-search-engine.json"
    },
    {
      "content": "\n# SQL query to CSV, a missing datatool\n\nBy R. S. Doiel, 2023-01-13\n\nUpdate: 2023-03-13\n\nAt work we maintain allot of metadata related academic and research publications in SQL databases. We use SQL to query the database and export what we need in tab delimited files. Often the exported data includes a column containing publication or article titles.  Titles in library metadata can be a bit messy. They contain a wide set of UTF-8 characters include math symbols and various types of quotation marks. The exported tab delimited data usually needs clean up before you can import it successfully into a spreadsheet.\n\nIn the worst cases we debug what the problem is then write a Python script to handle the tweak to fix things.  This results in allot of extra work and slows down the turn around for getting reports out quickly. This is particularly true of data stored in MySQL 8 (though we also use SQLite 3 and Postgres).\n\nThis got me thinking about how to get a clean export (tab or CSV) from our SQL databases today.  It would be nice if you provided a command line tool with a data source string (e.g. in a config file or the environment), a SQL query and the tool would use that to render a CSV or tab delimited file to standard out or a output file. It would work something like this.\n\n```\n    sql2csv -o eprint_status_report.csv -config=$HOME/.my.cnf \\\n\t    'SELECT eprintid, title, eprint_status FROM eprint' \n```\n\nThe `sql2csv` would take the results of the query and write to the CSV file.\n\nThe nice thing about this approach is that I could support the three relational databases we use -- i.e. MySQL 8, Postgres and SQLite3 with one common tool so my Bash scripts that run the reports would be very simple rather than specialized to one database system or the other.\n\nI hope to experiment with this approach in the next release of [datatools](https://github.com/caltechlibrary/datatools), an open source project maintained at work.\n\n## update\n\nJon Woodring pointed out to me today that both SQLite3 and PostgreSQL clients can output to CSV without need of an external tool. Wish MySQL client did that! Instead MySQL client supports tab delimited output. I'm still concidering sql2csv due to the ammount work I do with MySQL database but I'm not sure if it will make it into to the datatools project or now since I suspect our MySQL usage will decline overtime as more projects are built with PostgreSQL and SQLite3.\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "sql",
          "csv",
          "tab delimited"
        ],
        "pubDate": "2023-01-03",
        "title": "SQL query to CSV, a missing datatool",
        "updateDate": "2023-03-13"
      },
      "url": "posts/2023/01/03/sql-to-csv-a-missing-datatool.json"
    },
    {
      "content": "\n# Go and MySQL timestamps\n\nBy R. S. Doiel, 2022-12-12\n\nThe Go [sql](https://pkg.go.dev/database/sql) package provides a nice abstraction for working with SQL databases. The underlying drivers and DBMS can present some quirks that are SQL dialect and driver specific such as the [MySQL driver](github.com/go-sql-driver/mysql).  Sometimes that is not a big deal. [MySQL](https://dev.mysql.com) can maintain a creation timestamp as well as a modified timestamp easily via the SQL schema definition for the field. Unfortunately if you need to work with the MySQL timestamp at a Go level (e.g. display the timestamp in a useful way) the int64 provided via the driver isn't compatible with the `int64` used in Go's `time.Time`. To work around this limitation I've found it necessary to convert the MySQL timestamp to a formatted string using [DATE_FORMAT](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_date-format \"DATE_FORMAT is a MySQL date/time function returning a string value\") and from the Go side convert the formatted string into a `time.Time` using `time.Parse()`. Below is some Golang pseudo code showing this approach.\n\n```\n// Format used by MySQL strings representing date/times\nconst MySQLTimestamp = \"2006-01-02 15:04:05\"\n\n// GetRecordUpdate takes a configuration with a db attribute previously\n// opened and an id string returning a record populated with id and updated values where updated is an attribute of type time.Time. We use MySQL's\n// `DATE_FORMAT()` function to convert the timestamp into a string and\n// Go's `time.Parse()` to convert the string into a `time.Time` value.\nfunc GetRecordUpdate(cfg, id string) {\n\tstmt := `SELECT id, DATE_FORMAT(updated, \"%Y-%m-%d %H:%i:%s\") FROM some_tabl WHERE id = ?`\n\trow, err := cfg.db.Query(stmt, id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer row.Close()\n\trecord := new(Record)\n\tif row.Next() {\n\t\tvar updated string\n\t\tif err := row.Scan(&record.ID, &updated); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trecord.Updated, err = time.Parse(MySQLTimestamp, updated)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\terr = row.Err()\n\treturn record, err\n}\n```\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "golang",
          "sql",
          "timestamps"
        ],
        "pubDate": "2022-12-12",
        "title": "Go and MySQL timestamps"
      },
      "url": "posts/2022/12/12/Go-and-MySQL-Timestamps.json"
    },
    {
      "content": "\n# Progress and time remaining\n\nBy R. S. Doiel, 2022-11-05\n\nI often find myself logging output when I'm developing tools.  This is typically the case where I am iterating over data and transforming it. Overtime I've come to realize I really want a few specific pieces of information for non-error logging (e.g. `-verbose` which monitors progress as well as errors).\n\n- percentage completed\n- estimated time allocated (i.e. time remaining)\n\nTo do that I need three pieces of information.\n\n1. the count of the current iteration(e.g. `i`)\n2. the total number of iterations required (e.g. `tot`)\n3. The time just before I started iterating(e.g. `t0`)\n\nThe values for `i` and `tot` let me compute the percent completed. The percent completed is trivial `(i/tot) * 100.0`. Note on the first pass (i.e. `i == 0`) you can skip the percentage calculation.\n\n\n```golang\nimport (\n\t\"time\"\n\t\"fmt\"\n)\n\n// Show progress with amount of time running\nfunc progress(t0 time.Time, i int, tot int) string {\n    if i == 0 {\n        return \"\"\n    }\n\tpercent := (float64(i) / float64(tot)) * 100.0\n\tt1 := time.Now()\n\t// NOTE: Truncating the duration to seconds\n\treturn fmt.Sprintf(\"%.2f%% %v\", percent, t1.Sub(t0).Truncate(time.Second))\n}\n```\n\nHere's how you might use it.\n\n```golang\n\ttot := len(ids)\n\tt0 := time.Now()\n\tfor i, id := range ids {\n\t\t// ... processing stuff here ... and display progress every 1000 records\n\t\tif (i % 1000) == 0 {\n\t\t\tlog.Printf(\"%s records processed\", progress(t0, i, tot))\n\t\t}\n\t}\n```\n\nAn improvement on this is to include an time remaining. I need to calculated the estimated time allocation (i.e. ETA). I know `t0` so I can estimate that with this formula `estimated time allocation = (((current running time since t0)/ the number of items processed) * total number of items)`[^1]. ETA adjusted for time running gives us time remaining[^2]. The first pass of the function progress has a trivial optimization since we don't have enough delta t0 to compute an estimate. Calls after that are computed using our formula.\n\n[^1]: In code `(rt/i)*tot` is estimated time allocation\n\n[^2]: Estimated Time Remaining, in code `((rt/i)*tot) - rt`\n\n```golang\nfunc progress(t0 time.Time, i int, tot int) string {\n\tif i == 0 {\n\t\treturn \"0.00 ETR Unknown\"\n\t}\n\t// percent completed\n\tpercent := (float64(i) / float64(tot)) * 100.0\n\t// running time\n    rt := time.Now().Sub(t0)\n    // estimated time allocation - running time = time remaining\n    eta := time.Duration((float64(rt)/float64(i)*float64(tot)) - float64(rt))\n    return fmt.Sprintf(\"%.2f%% ETR %v\", percent, eta.Truncate(time.Second))\n}\n```\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "programming",
          "golang",
          "log info"
        ],
        "pubDate": "2022-12-05",
        "title": "Progress and time remaining"
      },
      "url": "posts/2022/12/05/progress-and-time-remaining.json"
    },
    {
      "content": "\n# Pandoc, Pagefind and Make\n\nRecently I've refresh my approach to website generation using three programs.\n\n- [Pandoc](https://pandoc.org)\n- [Pagefind](https://pagefind.app) for providing a full text search of documentation\n- [GNU Make](https://www.gnu.org/software/make/)\n    - [website.mak](website.mak) Makefile\n\nPandoc does the heavy lifting. It renders all the HTML pages, CITATION.cff (from the projects [codemeta.json](codemeta.github.io \"codemeta.json is a metadata documentation schema for documenting software projects\")) and rendering an about.md file (also from the project's codemeta.json). This is done with three Pandoc templates. Pandoc can also be used to rendering man pages following a simple page recipe.\n\nI've recently adopted Pagefind for indexing the HTML for the project's website and providing the full text search UI suitable for a static website. The Pagefind indexes can be combined with your group or organization's static website providing a rich cross project search (exercise left for another post).\n\nFinally I orchestrate the site construction with GNU Make. I do this with a simple dedicated Makefile called [website.mak](#website.mak).\n\n\n## website.mak\n\nThe website.mak file is relatively simple.\n\n```makefile\n#\n# Makefile for running pandoc on all Markdown docs ending in .md\n#\nPROJECT = PROJECT_NAME_GOES_HERE\n\nMD_PAGES = $(shell ls -1 *.md) about.md\n\nHTML_PAGES = $(shell ls -1 *.md | sed -E 's/.md/.html/g') about.md\n\nbuild: $(HTML_PAGES) $(MD_PAGES) pagefind\n\nabout.md: .FORCE\n        cat codemeta.json | sed -E 's/\"@context\"/\"at__context\"/g;s/\"@type\"/\"at__type\"/g;s/\"@id\"/\"at__id\"/g' >_codemeta.json\n        if [ -f $(PANDOC) ]; then echo \"\" | pandoc --metadata title=\"About $(PROJECT)\" --metadata-file=_codemeta.json --template codemeta-md.tmpl >about.md; fi\n        if [ -f _codemeta.json ]; then rm _codemeta.json; fi\n\n$(HTML_PAGES): $(MD_PAGES) .FORCE\n\tpandoc -s --to html5 $(basename $@).md -o $(basename $@).html \\\n\t\t--metadata title=\"$(PROJECT) - $@\" \\\n\t    --lua-filter=links-to-html.lua \\\n\t    --template=page.tmpl\n\tgit add $(basename $@).html\n\npagefind: .FORCE\n\tpagefind --verbose --exclude-selectors=\"nav,header,footer\" --bundle-dir ./pagefind --source .\n\tgit add pagefind\n\nclean:\n\t@if [ -f index.html ]; then rm *.html; fi\n\t@if [ -f README.html ]; then rm *.html; fi\n\n.FORCE:\n```\n\nOnly the \"PROJECT\" value needs to be set. Typically this is just the name of the repository's base directory.\n\n## Pandoc, filters and templates\n\nWhen write my Markdown documents I link to Markdown files instead of the HTML versions. This serves two purposes. First GitHub can use this linking directory and second if you decide to repurposed the website as a Gopher or Gemini resource\nyou don't linking to the Markdown file makes more sense.  To convert the \".md\" names to \".html\" when I render the HTML I use a simple Lua filter called [links-to-html.lua](https://stackoverflow.com/questions/40993488/convert-markdown-links-to-html-with-pandoc#49396058 \"see the stackoverflow answer that shows this technique\").\n\n```lua\n# links-to-html.lua\nfunction Link(el)\n  el.target = string.gsub(el.target, \"%.md\", \".html\")\n  return el\nend\n```\n\nThe \"page.tmpl\" file provides a nice wrapper to the Markdown rendered as HTML by Pandoc. It includes the site navigation and project copyright information in the wrapping HTML. It is based on the default Pandoc page template with some added markup for navigation and copyright info in the footer. I also update the link to the CSS to conform with our general site branding requirements. You can generate a basic template using Pandoc.\n\n```shell\npandoc --print-default-template=html5\n```\n\nI also use Pandoc to generate an \"about.md\" file describing the project and author info.  The content of the about.md is taken directly from the project's codemeta.json file after I've renamed the \"@\" JSON-LD fields (those cause problems for Pandoc). You can see the preparation of a temporary \"_codemeta.json\" using `cat` and `sed` to rename the fields. This is I use a Pandoc template to render the Markdown from.\n\n```pandoc\n---\ntitle: $name$\n---\n\nAbout this software\n===================\n\n$name$ $version$\n----------------\n\n$if(author)$\n### Authors\n\n$for(author)$\n- $it.givenName$ $it.familyName$\n$endfor$\n$endif$\n\n$if(description)$\n$description$\n$endif$\n\n\n$if(license)$- License: $license$$endif$\n0$if(codeRepository)$- GitHub: $codeRepository$$endif$\n$if(issueTracker)$- Issues: $issueTracker$$endif$\n\n\n$if(programmingLanguage)$\n### Programming languages\n\n$for(programmingLanguage)$\n- $programmingLanguage$\n$endfor$\n$endif$\n\n$if(operatingSystem)$\n### Operating Systems\n\n$for(operatingSystem)$\n- $operatingSystem$\n$endfor$\n$endif$\n\n$if(softwareRequirements)$\n### Software Requiremets\n\n$for(softwareRequirements)$\n- $softwareRequirements$\n$endfor$\n$endif$\n\n$if(relatedLink)$\n### Related Links\n\n$for(relatedLink)$\n- [$it$]($it$)\n$endfor$\n$endif$\n```\n\nThis same technique can be repurposed to render a CITATION.cff if needed.\n\n## Pagefind\n\nPagefind provides three levels of functionality. First it will generate indexes for a full text search of your\nproject's HTML pages. It also builds the necessary search UI for your static site. I include the search UI via a\nMarkdown document that embeds the HTML markup described at [Pagefind.app](https://pagefind.app/docs/)'s Getting started\npage.  When I invoke Pagefind I use the `--bundle-dir` option to be \"pagefind\" rather than \"_pagefind\".  The reason is GitHub Pages ignores the \"_pagefind\" (probably ignores all directories with \"_\" prefix).\n\nIf you need a quick static web server while you're writing and developing your documentation website Pagefind can\nprovide that using the `--serve` option. Assuming you're in your project's directory then something like this should do the trick.\n\n```shell\n    pagefind --source . --bundle-dir=pagefind --serve\n```\n",
      "data": {
        "keywords": [
          "Pandoc",
          "Pagefind",
          "make",
          "static site"
        ],
        "pubDate": "2022-11-30",
        "title": "Pandoc, Pagefind and Make"
      },
      "url": "posts/2022/11/28/pandoc-pagefind-and-make.json"
    },
    {
      "content": "\n# Initial Impression of Pagefind\n\nBy R. S. Doiel, 2022-11-21\n\nI'm interested in site search that does not require using server side services (e.g. Solr/Elasticsearch/Opensearch). I've used [LunrJS](https://lunrjs.com) on my person blog site for several years.  The challenge with LunrJS is indexes become large and that limits how much your can index and still have a quick loading page. [Pagefind](https://pagefind.app) addresses the large index problem. The search page only downloads the portion of the indexes it needs. The index and search functionality are compiled down to WASM files. This does raise challenges if you're targeting older web browsers.\n\nPagefind is a [rust](https://www.rust-lang.org/) application build using `cargo` and `rustc`. Unlike the documentation on the [Pagefind](https://pagefind.app) website which suggests installing via `npm` and `npx` I recommend installing it from sources using the latest release of cargo/rustic.  For me I found getting the latest cargo/rustc is easiest using [rustup](https://rustup.rs/). Pagefind will not compile using older versions of cargo/rustc (e.g. the example currently available from Mac Ports for M1 Macs).\n\nHere's the steps I took to bring Pagefind up on my M1 Mac.\n\n1. Install cargo/rust using rustup\n2. Make sure `$HOME/.cargo/bin` is in my PATH\n3. Clone the Pagefind Git repository\n4. Change to the repository directory\n5. Build and install pagefind\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\ngit clone git@github.com git@github.com:CloudCannon/pagefind.git src/github.com/CloudCannon/pagefind\ncd src/github.com/CloudCannon/pagefind\ncargo install pagefind --features extended\n```\n\nNext steps were\n\n1. Switch to my local copy of my website\n2. Build my site in the usual page\n3. Update my `search.html` page to use pagefind\n4. Index my site using pagefind\n5. Test my a local web server\n\nTo get the HTML/JavaScript needed to embed pagefind in your search page see [Getting Started](https://pagefind.app/docs/). The HTML/JavaScript fragment is at the top of the page. After updating `search.html` I ran the pagefind command[^1].\n\n```\npagefind --verbose --bundle-dir ./pagefind --source .\n```\n\nThe indexing is wicked fast and it gives you nice details. I verified everything worked as expected using `pttk ws` static site web server. I then published my website. You can see the results at <http://rsdoiel.sdf.org/search.html> and <https://rsdoiel.github.io/search.html>\n\n[^1]: I specified the bundle directory because GitHub pages had a problem with the default `_pagefind`.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-21",
        "keywords": [
          "site search",
          "pagefind",
          "rust",
          "cargo",
          "rustup",
          "M1",
          "macOS"
        ],
        "pubDate": "2022-11-21",
        "title": "Initial Impressions of Pagefind"
      },
      "url": "posts/2022/11/21/initial-impressions-pagefind.json"
    },
    {
      "content": "\nBrowser based site search\n=========================\n\nBy R. S. Doiel, 2022-11-18\n\nI recently read Brewster Kahle’s 2015 post about his vision for a [distributed web](https://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/). Many of his ideas have carried over into [DWeb](https://wiki.mozilla.org/Dweb), [Indie Web](https://indieweb.org/), [Small Web](https://benhoyt.com/writings/the-small-web-is-beautiful/), [Small Internet](https://cafebedouin.org/2021/07/28/the-small-internet/) and the like. A point he touches on is site search running in the web browser.\n\nI've use this approach in my own website relying on [LunrJS](https://lunrjs.com) by Oliver Nightingale. It is a common approach for small sites built using Markdown and [Pandoc](https://pandoc.org).  In the Brewster article he mentions [js-search](https://github.com/cebe/js-search), an implementation I was not familiar with. Like LunrJS the query engine runs in the browser via JavaScript but unlike LunrJS the indexes are built using PHP rather than JavaScript. The last couple of years I've used [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) to generating indexes for my own website site while using LunrJS for the browser side query engine. Today I check to see what the [Hugo](https://gohugo.io/tools/search/) community is using and found [Pagefind](https://github.com/cloudcannon/pagefind). Pagefind looks impressive. There was a presentation on at [Hugo Conference 2022](https://hugoconf.io/). It takes building a Lucene-like index several steps further. I appears to handle much larger indexes without requiring the full indexes to be downloaded into the browser.  It seems like a good candidate for prototyping personal search engine.\n\nHow long have been has browser side search been around? I do not remember when I started using. I explored seven projects on GitHub that implemented browser side site search. This is an arbitrary selection projects but even then I had no idea that this approach dates back a over decade!\n\n| Project | Indexer | query engine | earliest commit[^1] | recent commit[^2] |\n|---------|---------|--------------|:-------------------:|:-----------------:|\n| [LunrJS](https://github.com/olivernn/lunr.js) | JavaScript | JavaScript | 2011 | 2020 |\n| [Fuse.io](https://github.com/krisk/Fuse) | JavaScript/Typescript | JavaScript/Typescript | 2012 | 2022 |\n| [search-index](https://github.com/fergiemcdowall/search-index) | JavaScript | JavaScript | 2013 | 2016 |\n| [js-search](https://github.com/cebe/js-search) (cebe) | PHP | JavaScript | 2014 | 2022 |\n| [js-search](https://github.com/bvaughn/js-search) (bvaughn)| JavaScript | JavaScript | 2015 | 2022 |\n| [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) | Python | Python or JavaScript | 2018 | 2022 |\n| [Pagefind](https://github.com/cloudcannon/pagefind) | Rust | WASM and JavaScript | 2022 | 2022 |\n\n[^1]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n[^2]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-18",
        "keywords": [
          "search",
          "web browser",
          "dweb",
          "static site",
          "lunrjs",
          "pagefind"
        ],
        "pubDate": "2022-11-18",
        "title": "Browser based site search"
      },
      "url": "posts/2022/11/18/browser-side-site-search.json"
    },
    {
      "content": "\nTwitter's pending implosion\n===========================\n\nBy R. S. Doiel, 2022-11-11\n\nIt looks like Twitter continues to implode as layoffs and resignations continue. If bankers, investors and lenders call in the loans [bankruptcy appears to be possible](https://www.reuters.com/technology/twitter-information-security-chief-kissner-decides-leave-2022-11-10/). So what's next?\n\n\nThe problem\n-----------\n\nTwitter has been troubled for some time. The advertising model corrodes content. Twitter is effectively a massive RSS-like distribution system. It has stagnated as the APIs became more restrictive. The Advertising Business Model via [Ad-tech](https://pluralistic.net/tag/adtech/ \"per Cory Doctorow 'ad-fraud'\") encourages decay regardless of system.  Non-Twitter examples include commercial search engines (e.g. Google, Bing et el). Their usefulness usefulness declines over time. I believe this due to the increase in \"noise\" in the signal. The \"noise\" is driven be business models. That usually boils down to content who's function is to attract your attention so it can be sold for money. A corollary is [fear based journalism](https://medium.com/@oliviacadby/fear-mongering-journalisms-downfall-aac1f4f5756d). That has even caught the attention of a [Pope](https://www.9news.com.au/world/fear-based-journalism-is-terrorism-pope/4860b502-5dbb-4eef-abcf-57582445fc2c). Not fun.\n\nI suspect business models don't encourage great content. Business models are generally designed to turn a profit. They tend to get refined and tuned to that purpose. The evolution of Twitter and Google's search engine would make good case studies in that regard.\n\n\nA small hope\n------------\n\nI don't know what is next but I know what I find interesting. I've looked at Mastodon a number of times. It's not going away but the W3C activity pub spec is horribly complex. Complexity slows adoption. It reminds me of SGML. Conceptually interesting but in practice was too heavy. It did form inspiration for HTML though, and that has proven successful. What gives me hope is that Mastodon has survived. I think casting a wide net is interesting. The wider net is something I've heard called the \"small web\".\n\nThe small web\n-------------\n\nFor a number of years there has been a slowly growing  \"small web\" movement. I think it is relatively new term, I didn't find it in [Wikipedia](https://en.wikipedia.org/w/index.php?search=small+web&ns0=1 \"today is 2022-11-11\") when I looked today. As I see it the \"small web\" has been driven by a number of things. It is not a homogeneous movement but rather a collection of various efforts and communities.  I think it likely to continue to evolve. At times evolve rapidly. Perhaps it will coalesce at some point.  Here's what appears to me to be the common motivations as of 2022-11-11.\n\n- desire for simplicity\n- desire for authenticity\n- lower resource footprint\n- text as a primary but not exclusive medium\n- hyperlinks encouraged\n- a space where you're not a product\n- desire for decentralization (so you're not a product)\n- a desire to have room to grow (because you're not a product)\n\nThe \"small web\" is a term I've seen pop up in Gopherspace, among people who promote [Gemini](https://gemini.circumlunar.space/), [Micro blogging](https://micro.blog \"as an example of micro blogging\") and in the [Public Access Unix](https://sdf.org) communities.\"small web\" as a term does not return useful results in the commercial search engines I've checked. Elements seem to be part of [DWeb](https://getdweb.net/) which Mozilla is [championing](https://wiki.mozilla.org/Dweb). Curiously in spite of the hype and marketing I don't see \"small web\" in [web 3.0](https://www.forbes.com/advisor/investing/cryptocurrency/what-is-web-3-0/). I think blockchain has proven environmentally dangerous and tends to compile down to various forms of [grift](https://pluralistic.net/2022/05/27/voluntary-carbon-market/).\n\n\nSmall web\n---------\n\nWhat does \"small web\" mean to me?  I think it means\n\n- simple protocols that are flexible and friendly to tool creation\n- built on existing network protocols with a proven track record (e.g. IPv4, IPv6)\n- decentralized by design as was the early Internet\n- low barrier to participation\n    - e.g. a text editor, static site providing a URL to a twtxt file\n- text centric (at least for the moment)\n- integrated with the larger Internet, i.e. supports hyper links\n- friendly to distributed personal search engines (e.g. LunrJS running over curated set of JSONfeeds or twtxt urls)\n- \"feed\" oriented discovery based on simple formats (e.g. [RSS 2.0](https://cyber.harvard.edu/rss/rss.html), [JSONfeed](https://www.jsonfeed.org/), [twtxt](https://twtxt.readthedocs.io/en/latest/), [OPML](https://en.wikipedia.org/wiki/OPML), even [Gophermaps](https://en.wikipedia.org/wiki/Gopher_(protocol) \"see Source code of a menu title\"))\n- sustainable and preservation friendly\n    - example characteristics\n        - clone-able (e.g. as easy as cloning a Git Repo)\n        - push button update to Internet Archive's way back machine\n        - human and machine readable metadata\n\nI think the \"small web\" already exists. Examples include readable personal websites hosted as \"static pages\" via GitHub and S3 buckets are a good examples of prior art in a \"small web\".  Gopherspace is a good example of the \"small web\". I think the various [tilde communities](https://tilde.club) hosted on [Public Access Unix](https://en.wikipedia.org/wiki/SDF_Public_Access_Unix_System) are examples. Even the venerable \"bloggosphere\" of [Wordpress](https://wordpress.com) and the newer [Micro.blog](https://micro.blog/) is evidence that the \"small web\" already is hear. [Dave Winer](https://scripting.com)'s [Feedland](http://feedland.org/) is a good example of innovation in the \"small web\" happen today.  [Yarn.social](https://yarn.social) built on twtxt file format is very promising. I would argue right now the \"small web\" is the internet that already exists outside the walled gardens of Google, Meta/Facebook, Twitter, TikTok, Pinterest, Slack, Trello, Discord, etc.\n\nI think it is significant that the \"small web\" existed before the Pandemic. It continued to thrive during it. It is likely to evolve beyond it. The pending shift has already happening as it is already populated by \"early adopters\" and appears to be growing into larger community participation.  For the \"main stream\" it is waiting to be \"discovered\" or perhaps \"re-discovered\" depending on your point of view.\n\nHow do you participate?\n-----------------------\n\nYou may already be participating in the \"small web\".  Do you blog? Do your read feeds? Do you use a non-soloed social media platform like Mastodon? Do you use Gopher? The \"small web\" is defined by choice and is characterized by simplicity. It is a general term. You're the navigator not an algorithm tuned to tune someone a profit. If you are not sure where to start you can join a communities like [sdf.org](https://sdf.org) and get started there. You can explore [Gopherspace](https://floodgap.com) via a WWW proxy. You can create a static website and host a [twtxt](https://twtxt.readthedocs.io/en/latest/) file on GitHub or a [Yarn Pod](https://yarn.social). You can create a site via [Micro.blog](https://micro.blog) or [Feedland](http://feedland.org). You can blog. You can read RSS feeds or read twtxt feed with [twtxt](https://twtxt.readthedocs.io/en/latest/user/intro.html), [twet](https://github.com/quite/twet) or [yarn.social](https://yarn.social). You participate by stepping outside the walled gardens and seeing the larger \"Internet\".\n\nI think the important thing is to realize the alternatives are already here, you don't need to wait for invention, invitation or permission. You can move beyond the silos today. You don't need to have your attention captured then bought and sold. It's not so much a matter of \"giving up\" a silo but rather stepping outside one and breathing some fresh air.\n\nThings to watch\n---------------\n\n- [Feedland](https://feedland.org)\n- [yarn.social](https://yarn.social) and [twtxt](https://twtxt.readthedocs.io/en/latest/)\n- [Micro.blog](https://micro.blog/)\n- [Mastodon](https://joinmastodon.org/)\n- [Gopherspace](http://gopher.floodgap.com/gopher/gw?a=gopher%3A%2F%2Fgopher.floodgap.com%2F1%2Fworld), see [Gopherspace in 2020](https://cheapskatesguide.org/articles/gopherspace.html) as a nice orientation to see the internet through lynx and text\n- Even [Project Gemini](https://gemini.circumlunar.space/)\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "keywords": [
          "small web",
          "twtxt",
          "micro blogging",
          "social networks"
        ],
        "pubDate": "2022-11-11",
        "title": "Twitter's pending implosion"
      },
      "url": "posts/2022/11/11/Twitter-implosion.json"
    },
    {
      "content": "\nCompiling Pandoc from source\n============================\n\nBy R. S. Doiel, 2022-11-07\n\nI started playing around with Pandoc's __pandoc-server__ last Friday. I want to play with the latest version of Pandoc.  When I gave it a try this weekend I found that my Raspberry Pi 400's SD card was too small. This lead me to giving the build process a try on my Ubuntu desktop. These are my notes about how I going about building from scratch.  I am not a Haskell programmer and don't know the tool chain or language. Take everything that follows with a good dose of salt but this is what I did to get everything up and running. I am following the compile from source instructions in Pandoc's [INSTALL.md](https://github.com/jgm/pandoc/blob/master/INSTALL.md)\n\nI'm running this first on an Intel Ubuntu box because I have the disk space available there. If it works then I'll try it directly on my Raspberry Pi 400 with an upgrade SD card and running the 64bit version of Raspberry Pi OS.\n\nI did not have Haskell or Cabal installed when I started this process.\n\nSteps\n-----\n\n1. Install __stack__ (it will install GHC)\n2. Clone the GitHub repo for [Pandoc](https://github.com/jgm/pandoc)\n3. Setup __stack__ for Pandoc\n4. Build and test with __stack__\n5. Install __stack__ install\n6. Make a symbolic link from __pandoc__ to __pandoc-server__\n\n```\nsudo apt update\nsudo apt search \"haskell-stack\"\nsudo apt install \"haskell-stack\"\nstack upgrade\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup \nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nThis step takes a long time and on the Raspberry Pi it'll take allot longer.\n\nThe final installation of Pandoc was in my `$HOME/.local/bin` directory. Assuming this is early in your path this can allow you to experiment with a different version of Pandoc from the one installed on your system. \n\nI also wanted to try the latest of __pandoc-server__.  This was not automatically installed and is not mentioned in the INSTALL.md file explicitly. But looking at the discussion of running __pandoc-server__ in CGI mode got me thinking. I then checked the installation on my Ubuntu box for the packaged version of pandoc-server and saw that is was a symbolic link.  Adding a `ln` command to my build instruction solved the problem.\n\nI decided to try compiling Pandoc on my M1 mac.  First I needed to get __stack__ installed. I use Mac Ports but it wasn't in the list of available packages.  Fortunately the Haskell Stack website provides a shell script for installation on Unixes. I wanted to install __stack__ in my home `bin` directory not `/usr/bin/slack`. So after reviewing the downloaded install script I found the `-d` option for changing where it installs to. It indicated I need to additional work with __xcode__.\n\n```\ncurl -sSL https://get.haskellstack.org/ > stack-install.sh\nmore stack-install.sh\nsh stack-install.sh -d $HOME/bin\n```\n\nThe __stack__ installation resulted in a message in this form.\n\n```\nStack has been installed to: $HOME/bin/stack\n\nNOTE: You may need to run 'xcode-select --install' and/or\n      'open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg'\n      to set up the Xcode command-line tools, which Stack uses.\n\nWARNING: '$HOME/.local/bin' is not on your PATH.\n    Stack will place the binaries it builds in '$HOME/.local/bin' so\n    for best results, please add it to the beginning of PATH in your profile.\n```\n\nI already had xcode setup for compiling Go so those addition step was not needed.  I only needed to add `$HOME/.local/bin` to my search path.\n\nI then followed the steps I used on my Ubuntu Intel box.\n\n```\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup\nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nNow when I have a chance to update my Raspberry Pi 400 to a suitable sized SD Card (or external drive) I'll be ready to compile a current version of Pandoc from source.\n\nAdditional notes\n----------------\n\n[stack](https://docs.haskellstack.org/en/stable/) is a Haskell build tool. It setups up an Haskell environment per project. If a project requires a specific version of the Haskell compiler it'll be installed and made accessible for the project. In this way it's a bit like having a specific environment for Python. The stack website indicates that it targets cross platform development in Haskell which is nice.  Other features of stack remind me of Go \"go\" command in that it can build things or Rust's \"cargo\" command. Like __cargo__ it can update itself which is nice. That is what I did after installing the Debian package version used by Ubuntu. Configuration of a \"stack\" project uses YAML files. Stack uses __cabal__, Haskell's older build tool but subsumes __cabal-install__ for setting up __cabal__ and __ghc__. It appears from my reading that __stack__ addresses some of the short falls __cabal__ originally had and specifically focusing on reproducible compiles. This is important in sharing code as well as if you want to integrate automated compilation and testing. It maintains a project with \"cabal files\" so there is the ability to work with older non-stack code if I read the documentation correctly. Both __cabal__ and __stack__ seem to be evolving in parallel taking different approaches but influencing one another. Both systems use \"cabal files\" for describing projects and dependencies as of 2022. The short version of [Why Stack](https://docs.haskellstack.org/en/stable/#why-stack) can be found the __stack__ website.\n\n[Hackage](https://hackage.haskell.org/) is a central repository of Haskell packages. \n\n[Stackage](https://www.stackage.org/) is a curated subset of Hackage packages. It appears to be the preferred place for __stack__ to pull from.\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-07",
        "keywords": [
          "pandoc",
          "pandoc-server",
          "pandoc-citeproc",
          "haskell-stack",
          "cabal",
          "ghc"
        ],
        "pubDate": "2022-11-07",
        "title": "Compiling Pandoc from source"
      },
      "url": "posts/2022/11/07/compiling-pandoc-from-source.json"
    },
    {
      "content": "\nFeeds, formats, and plain text\n==============================\n\nBy R. S. Doiel, 2022-11-01\n\nThere has been a proliferation of feed formats. My personal preferred format is RSS 2.0. It's stable and proven the test of type. Atom feeds always felt a little like, \"not invented here so we're inventing it again\", type of thing. The claim was they could support read/write but so can RSS 2.0 specially with the namespace possibilities. The innovative work [Dave Winer](https://scripting.com) has done in the past and is doing today with [Feedland](https://feedland.org) is remarkably impressive.\n\nIn my experience the format of the feed is less critical than the how to author the metadata.  Over the last several years I've moved to static hosting as my preferred way of hosting a website. My writing is typically in Markdown or Fountain formats and frontmatter like used in RMarkdown has proven very convenient. The \"blogit\" command that started out from an idea in [mkpage](https://github.com/caltechlibrary/mkpage \"Make Page, a Pandoc preprocessor and tool set\") has been implemented in [pttk](https://github.com/rsdoiel/pttk \"Plain Text Toolkit\"). So for me metadata authoring makes sense in the front matter. That has the advantage that Pandoc can leverage the information in its templates (that is what I use to render HTML, man pages and the occasional PDF). It also is a food source for data to include in a feed.\n\nI've recently become aware of a really simple text format called [twtxt](https://twtxt.readthedocs.io/en/latest/). This simple format is meant for micro blogging but is also useful as a feed source and format. Especially in terms of rendering content for Gopherspace which I've re-engaged in recently. [Yarn.social](https://yarn.social) has built an entire ecosystem around it. Very impressive. The format is so simple it can be done with a pipe and the \"echo\" command in the shell.  It looks promising in terms for personal search ingest as well.\n\nOne of the formats that Dave Winer supports in Feedland and is used in the micro blogging community he has connected with is [jsonfeeds](https://www.jsonfeed.org/). It is lightweight and to me feels allot like RSS 2.0 without the XML-isms that go along with it.  I'm playing with the idea that in pttk it'll be the standard feed format and that from it I can then render our traditional feed friends of RSS 2.0 and Atom.\n\nI've looked at the ActivityPub from the Mastodon community but like [James Mill](https://prologic.github.io/prologic/ \"aka prologic\") I find it too complex. What is needed is something simple, really simple.  That's why I've been looking closely at Gopherspace again. The Gophermap can function as a bookmark file, a \"home page\" a list of feeds. A little archaic but practical in its simplicity. The only challenges I've run into has been figuring out that expectations of the Gopher server software. Currently I've settled on [gophernicus](https://gophernicus.org) as that is was it supported at [sdf.org](https://sdf.org) where I have a gopher \"hole\".\n\nAs pttk grows and I explore where I can take simple text processing I'm not targeting Gopherspace, twtxt and static websites. I've looked at [Gemini](https://gemini.circumlunar.space/docs/specification.gmi) but haven't grokked the point yet.  Their choice of yet another markup for content seems problematic at best. For me gopher solves the problems that would make me look at Gemini and I can use most any structured text I want. The text just needs to be readable easily by humans. The Gophermap provides can be enhanced menus much like \"index.html\" pages have become (a trunk that branches and eventually leads to a leaf). \n\n[OPML](http://home.opml.org/) remains a really nice outline data format.  It's something I'd like to eventually integrate with pttk. It can be easily represented as JSON. Just need to figure what problem I am trying to solve by using it.  Share a list of feeds is the classic case but looking at twtxt as well as the [newsboat](https://newsboat.org/) URL list makes me think it is more than I need. We'll see.  It is certainly reasonable to generate from a simpler source. If I ever write a personal search engine (something I've been thinking about to nearly a decade) it'd be a good way to share curated indexes sources as well as sources to crawl.  I just need to think that through more.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "plain text",
          "small internet",
          "rss",
          "jsonfeed",
          "gopher"
        ],
        "pubDate": "2022-11-01",
        "title": "feeds, formats and plain text"
      },
      "url": "posts/2022/11/01/Feeds-formats-and-plain-text.json"
    },
    {
      "content": "\nInstalling Cargo/Rest on Raspberry Pi 400\n=========================================\n\nOn my Raspberry Pi 400 I'm running the 64bit Raspberry Pi OS.\nThe version of Cargo and Rustc are not recent enough to install\n[ncgopher](https://github.com/jansc/ncgopher). What worked for\nme was to first install cargo via the instructions in the [The Cargo Book](https://doc.rust-lang.org/cargo/getting-started/installation.html). \n\n~~~shell\ncurl https://sh.rustup.rs -sSf | sh\n~~~\n\nAn important note is if you previously installed a version of Cargo/Rust\nvia the debian package system you should uninstall it before running the\ninstructions above from the Cargo Book.\n\nWith this version I was able to install __ncgopher__ using the simple\nrecipe of \n\n~~~shell\ncargo install ncgopher\n~~~\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "keywords": [
          "64bit",
          "Rapsberry Pi OS",
          "Cargo",
          "rustc"
        ],
        "pubDate": "2022-10-31",
        "title": "Installing Cargo/Rust on Raspberry Pi 400"
      },
      "url": "posts/2022/11/01/installing-cargo-rust-r400.json"
    },
    {
      "content": "\n# 7:30 AM, Oberon Language: A minimum SYSTEM module\n\nPost: Tuesday, October 18, 2022, 7:30 AM\n\nIt occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.\n\n",
      "data": {
        "keywords": [
          "A minimum SYSTEM module"
        ],
        "no": 1,
        "pubDate": "2022-10-18",
        "series": "Oberon Language",
        "title": "7:30 AM, Oberon Language: A minimum SYSTEM module"
      },
      "url": "posts/2022/10/18/Wishlist-Oberon-in-2023-2022-10-18_070730.json"
    },
    {
      "content": "\nWish list for Oberon in 2023\n===========================\n\nNext year will be ten years since Prof. Wirth and Paul Reed released [Project Oberon 2013](https://www.projectoberon.com).  It took me most of that decade to stumble on the project and community.  I am left wondering if Prof. Wirth and Paul Reed sat down today what would they design? I think only minor changes are needed and those mostly around assumptions.\n\nOberon-07 changing assumptions\n------------------------------\n\nThe language of Oberon-07 doesn't need to change. I do think the assumptions of the compiler are worth revisiting.  A CHAR should not be assumed to be an eight bit byte.  A CHAR should represent a character or symbol in a language. Many if not most of the Oberon community speaks language other than American English and that which is trivially represented in seven or eight bit ASCII.  While changing the representation assumption in Oberon-07 does increase complexity I feel restrict the character presentation of a CHAR to eight bits puts us on the side of \"too simple\" in the equation of \"Simpler but not to simple\".\n\nI am concerned about the assumption of an INTEGER as 32 bits. Increasingly I've seen single board computer implementations that are 64 bits.  Today feels allot like when I started in computing where personal computers were shifting from eight or sixteen bits to thirty two.  I suspect increasingly we will find that eight, sixteen and thirty two bit computers are relegated to the realm of specialized computers. While supporting these other widths will remain important I think shifting assumptions to sixty four bit machines makes sense now. Is a 32 bit machine \"too simple\" in our equation of \"simpler but not too simple\"?\n\n\n\nOberon as Operating System\n--------------------------\n\nThe operating system I still find liberating in 2023 as when I first was able to use it.  The challenge in 2023 though is the three button mouse. I think the supporting the historic mouse remains important but that the viewers should also support navigation via the keyboard and easily support touch systems that lack a mouse.  Being backward compatibly while adopting an enhance UI would make things more complex bit if care is taken I think that it can be done while keep the equation balanced as \"simpler, but not too simple\".\n\nTransforming my assumptions in 2023\n-----------------------------------\n\nI think the Artemis Project should presume that the representation of CHAR and INTEGER may change and probably should change. The portable modules should support compiling Oberon-07 programs on non-Oberon 2014 Systems without change.  I am skeptical that I can create a module system that provides a base line with the historic Oberon implementations. I think the Oakwood modules are just too limited. I think the assumption is I need implementations for Project Oberon 2013 modules as the base line perhaps enhanced with a few additional modules to supporting networking, UTF-8, JSON, and XML. The goal I think is that using Artemis on a non-Oberon System should facilitate bootstrapping an Oberon System 2023 should one come to exist.\n\nErrata\n------\n\n7:00 - 7:30; Oberon Language; A minimum SYSTEM module; It occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-10-16",
        "keywords": [
          "Oberon",
          "Oberon-07",
          "Oberon System",
          "Artemis"
        ],
        "pubDate": "2022-10-16",
        "title": "Wish list for Oberon in 2023"
      },
      "url": "posts/2022/10/16/Wishlist-Oberon-in-2023.json"
    },
    {
      "content": "\n# 7:30 AM, Gopher: Setup\n\nPost: Monday, October 10, 2022, 7:30 AM\n\nAccount verified, Yippee!\n\n",
      "data": {
        "keywords": [
          "Setup"
        ],
        "no": 1,
        "pubDate": "2022-10-10",
        "series": "Gopher",
        "title": "7:30 AM, Gopher: Setup"
      },
      "url": "posts/2022/10/10/getting-things-setup-2022-10-10_070730.json"
    },
    {
      "content": "\n\nGetting things setup\n====================\n\nBy R. S. Doiel, 2022-10-09\n\nI'm digging my [gopherhole on sdf.org](gopher://sdf.org:70/0/users/rsdoiel)\nas I wait for my validation to go through.  The plan is to migrate content\nfrom rsdoiel.github.io to here and host it in a Gopher context.  It's\ninteresting learning my way around sdf.org. Reminds me of my student days\nwhen I first had access to a Unix system.  Each Unix has it own flavors and\neven for the same Unix type/version each system has it's own particular\nvariation. Unix lends itself to customization and that why one system can\n\"feel\" or \"look\" different than the next.\n\nI'm trying to remember how to use Pico (vi isn't available yet).\nDiscovering how far \"mkgopher\" can go (very far it turns out).\n\nI'm looking forward to validation so I can have access to Git and\n\"move in\" to this gopherspace in a more sustainable way.\n\nThings to read and do\n---------------------\n\n- wait to be validated\n- learn [gitia](https://git.sdf.org) and setup up a mirror my personal projects and blog\n- read up on [gophernicus](https://www.gophernicus.org/) (the gopher server used by sdf.org)\n- [twenex project](https://www.twenex.org/), sounds interesting,\n  I remember accessing a TOPS-20 system at Whitesands in New Mexico\n  once upon a time.\n- figure out to access comp.lang.oberon if it is available on sdf.org\n- figure out, after validation, if I can compile OBNC for working on\n  Artemis and Oberon-07 code projects\n\nUpdates\n-------\n\n7:30 - 7:30; Gopher; Setup; Account verified, Yippee!\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-10-09",
        "keywords": [
          "gopher",
          "public unix"
        ],
        "pubDate": "2022-10-09",
        "title": "Getting things setup"
      },
      "url": "posts/2022/10/09/getting-things-setup.json"
    },
    {
      "content": "\nThinking about Gopher\n=====================\n\nBy R. S. Doiel, 2022-09-28\n\nLast weekend I visited the [Gophersphere](gopher://gopher.floodgap.com \"Floodgap is a good starting point for Gopher\") for the first time in a very long time. I'm happy to report it is still alive an active. It remains fast, lower resource consuming. This resulted in finding a Gopher protocol package in Go and adding light weight Gopher server to [pttk](https://rsdoiel.github.io/pttk) my current vehicle for experimenting with plain text writing tools.\n\nI've been thinking allot this past half week about where to explore in Gopher. The biggest issue I ran into turned out to be easily solve. Gopher protocol is traditionally served over port 70 but if you're running a \\*nix if you are just experimenting on localhost it is easier to use port 7000 (analogous to port 80 becoming 8000 or 8080 in the websphere). But some Gopher clients will only serve port 70. Two clients work very well at 7000 and they are Lynx (the trusty old console web browser) and one written in Rust called [ncgopher](https://github.com/jansc/ncgopher). The latter I find I use most of the time. It also supports Gemini sites though I am less interested in Gemini at the movement.  Gopher has a really nice sweet spot of straight forward implementation for both client and server. It would be a good exercise for moving from beginner to intermediate programming classes as you would be introducing network programming, a little parsing and the client server application models. It's a really good use case of looking back (Gopher is venerable in Internet age) and looking forward (a radical simplification of distributing readable material and related files).\n\nConstraints and creativity\n--------------------------\n\nThe simplicity and limitations of Gopher are inspiring. The limitations are particularly important as they are good constraints that help focus where to innovate. Gopher is a protocol ripe for software innovation precisely because of it's constraints.\n\nGophermaps is a good example. The Go package [git.mills.io/prologic/go-gopjer](https://git.mills.io/prologic/go-gopher) supports easily building servers that have Gophermaps the way of structuring the Gopher menus (aka selectors in Gopher parlance). A Gophermaps is a plain text file where you have lines that start with a Gopher supported document type (see [Gopher protocol](https://en.wikipedia.org/wiki/Gopher_(protocol) for details) a label followed by a tab character, a relative path followed by a tab character, a hostname followed by a tab character and a port number.  Very simple to parse.  The problem is Gopher clients expect all the fields for them to interpret them as a linked resource (e.g. a text file, binary file, image, or another Gopher selector). When I first encountered Gopher at USC so many years ago (pre-Mosaic, pre-Netscape) Gophermaps selectors are trivial to setup and you could build a service that supported ftp and Gopher in the same directory structure. All the \"development\" of a gopher site was done directly on the server in the directories where the files would live. Putting in all values seemed natural. Today I don't develop on a \"production server\" if I can avoid it. My writing is done on a small pool of machines at home, each with its own name. Explicitly writing a hostname and port with the eye to publishing to a public site then becomes a game of running `sed` to correct hostname and ports across the updated Gophermap files.\n\n> Gopher selectors form \"links\" to navigate through a Gopher site or through the Gophersphere depending on what they point at\n\nWithout changing the protocol you could modify the go-gopher package's function for presenting a Gophermap where the hostname port is assumed to the gopher server name and port if it was missing. Another approach would be to translate a place holder value. This would facilitate keeping your Gopher site under version control (e.g. Git and GitHub) while allowing you to easily deploy a version of the site in a public setting or in your development setting.  The constraint of the Gophermap definition as needed by the protocol doesn't mean it forces a cumbersome choice on your writing process.\n\nSimilarly the spaces versus tabs (invisible by default in many editors) because a non-issue by adopting editors that support [editorconfig](https://editorconfig.org) or even making the server slightly more complex in correctly identifying when to convert spaces to tabs expanding them out to a Gopher selectors.\n\nClient sites there are also many opportunities.  [Little Gopher Client](http://runtimeterror.com/tools/gopher/) pulls out the selectors its finds into a nice tree (like a bookmark tree) in a left panel and puts the text in the main window.  ncgopher let's you easily bookmark things and has a very clean, easy on the eyes reading experience in the console. In principle you the client could look at the retrieved selector and choose to display different file types based on the file extension as well as the selector type retrieved. This would let you include a richer experience in the Gophersphere for light weight markup like Commonmark files while still running nicely on Gopher protocol. Lots of room to innovate because the protocol is simple, limited and stable after all these years.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "draft": true,
        "pubDate": "2022-09-28",
        "title": "Thinking about Gopher"
      },
      "url": "posts/2022/09/28/thinking-about-gopher.json"
    },
    {
      "content": "\nRust development notes\n======================\n\nby R. S. Doiel, 2022-09-27\n\nI recently wanted to try [ncgopher](https://github.com/jansc/ncgopher) which is a [rust](https://rust-lang.org) based application. I was working on a an M1 Mac mini. I use [Mac Ports](https://www.macports.org) for my userland applications and installed [cargo](https://doc.rust-lang.org/cargo/) to pickup the rust compiler and build tool\n\n```shell\nsudo port install cargo\n```\n\nAll went well until I tried to build ncgopher and got an error as follows\n\n```\ncargo build --release\n    Updating crates.io index\nerror: Unable to update registry `crates-io`\n\nCaused by:\n  failed to fetch `https://github.com/rust-lang/crates.io-index`\n\nCaused by:\n  failed to authenticate when downloading repository: git@github.com:rust-lang/crates.io-index\n\n  * attempted ssh-agent authentication, but no usernames succeeded: `git`\n\n  if the git CLI succeeds then `net.git-fetch-with-cli` may help here\n  https://doc.rust-lang.org/cargo/reference/config.html#netgit-fetch-with-cli\n\nCaused by:\n  no authentication available\nmake: *** [build] Error 101\n```\n\nThis seemed odd as I could run `git clone git@github.com:rust-lang/crates.io-index` successfully. Re-reading the error message a dim light went on. I checked the cargo docs and the value `net.git-fetch-with-cli` defaults to false. That meant that cargo was using its own embedded git. OK, that makes sense but how do I fix it. I had no problem using cargo installed via ports on an Intel iMac so what gives? When cargo got installed on the M1 there was now `.cargo/config.toml` file. If you create this and set the value of `git-fetch-with-cli` to true then the problem resolves itself.\n\nIt was good that the error message provided a lead. It's also good that cargo has nice documentation. My experience though still left the taste of [COIK](https://www.urbandictionary.com/define.php?term=coik). Not sure how to improve the situation. It's not really a cargo bug (unless config.taml should be always created), it's not a rust bug and I don't even think it is a ports packaging bug.  If I was a new developer just getting familiar with git I don't think I would have known how to solve my problem even with the documentation provided. Git is something that has always struggled with COIK. While I like it it does make things challenging.\n\nIf I wind up playing with rust more then I'll add somemore notes here in the future.\n\nMy `$HOME/.cargo/config.toml` file looks like to have cargo use the git cli instead of the built in rust library.\n\n```\n[net]\ngit-fetch-with-cli = true\n```\n\n\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "rust",
          "cargo",
          "M1",
          "macOS",
          "ports"
        ],
        "pubDate": "2022-09-27",
        "title": "Rust development notes"
      },
      "url": "posts/2022/09/27/rust-development-notes.json"
    },
    {
      "content": "\n# 7:30 AM, Golang: pttk\n\nPost: Monday, September 26, 2022, 7:30 AM\n\nrenamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n\n",
      "data": {
        "keywords": [
          "pttk"
        ],
        "no": 1,
        "pubDate": "2022-09-26",
        "series": "Golang",
        "title": "7:30 AM, Golang: pttk"
      },
      "url": "posts/2022/09/26/golang-development-2022-09-26_070730.json"
    },
    {
      "content": "\nPostgreSQL dump and restore\n===========================\n\nThis is a quick note on easily dumping and restoring a specific database\nin Postgres 14.5.  This example has PostgreSQL running on localhost and\n[psql](https://www.postgresql.org/docs/current/app-psql.html) and\n[pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) are both available.\nOur database administrator username is \"admin\", the database to dump is called \"collections\". The SQL dump\nfile will be named \"collections-dump-2022-09-19.sql\".\n\n```shell\n\tpg_dump --username=admin --column-inserts \\\n\t    collections >collections-dump-2022-09-19.sql\n```\n\nFor the restore process I follow these steps\n\n1. Using `psql` create an empty database to restore into\n2. Using `psql` replay (import) the dump file in the new database to restoring the data\n\nThe database we want to restore our content into is called \"collections_snapshot\"\n\n```shell\n\tpsql -U dbadmin\n\t\\c postgres\n\tDROP DATABASE IF EXISTS collections_snapshot;\n\tCREATE DATABASE collections_snapshot;\n\t\\c collections_snapshots\n\t\\i ./collections-dump-2022-09-19.sql\n\t\\q\n```\n\nOr if you want to stay at the OS shell level\n\n```shell\n\tdropdb collections_snapshot\n\tcreatedb collections_snapshot\n\tpsql -U dbadmin --dbname=collections_snapshot -f ./collections-dump-2022-09-19.sql\n```\n\n\nNOTE: During this restore process `psql` will display some output. This is normal. The two\ntypes of lines output are shown below.\n\n```sql\n\tINSERT 0 1\n\tALTER TABLE\n```\n\nIf you want to stop the input on error you can use the `--set` option to set the error behavior\nto abort the reload if an error is encountered.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-09-19",
        "draft": true,
        "keywords": [
          "PostgreSQL"
        ],
        "pubDate": "2022-09-19",
        "title": "PostgreSQL dump and restore"
      },
      "url": "posts/2022/09/19/PostgreSQL-Dump-and-Restore.json"
    },
    {
      "content": "\n# 12:30 PM, SQL: Postgres\n\nPost: Monday, September 19, 2022, 12:30 PM\n\nSetting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 1,
        "pubDate": "2022-09-19",
        "series": "SQL",
        "title": "12:30 PM, SQL: Postgres"
      },
      "url": "posts/2022/09/19/rosette-notes-2022-09-19_121230.json"
    },
    {
      "content": "\nOrdering Front Matter\n=====================\n\nBy R. S. Doiel, 2022-08-30\n\nA colleague of mine ran into an interesting Pandoc behavior. He was combining a JSON metadata document and a converted word document and wanted the YAML front matter to have a specific order of fields (makes it easier for us humans to quickly scan it and see what the document was about).\n\nThe order he wanted in the front matter was\n\n- title\n- interviewer\n- interviewee\n- abstract\n\nThis was for a collection of oral histories. When my friend use Pandoc's `--metadata-json` to read the JSON metadata it rendered the YAML fine except the attributes were listed in alphabetical order.\n\nWe found a solution by getting Pandoc to treat the output not as Markdown plain text so that we could template the desired order of attributes.\n\nHere's the steps we used.\n\n1. create an empty file called \"empty.txt\" (this is just so you pandoc doesn't try to read standard input and processes\nyou metadata.json file with the template supplied)\n2. Create a template with the order you want (see below)\n3. Use pandoc to process your \".txt\" file and your JSON metadata file using the template (it makes it tread it as plain text even though we're going to treat it as markdown later)\n4. Append the content of the word file and run pandoc over your combined file as you would normally to generate your HTML\n\n\nThis is the contents of our [metadata.json](metadata.json) file.\n\n```json\n    {\n        \"title\": \"Interview with Mojo Sam\", \n        \"interviewee\": \"Mojo Sam\", \n        \"interviewer\": \"Tom Lopez\",\n        \"abstract\": \"Interview in three sessions over sevaral decases, 1970 - 20020. The interview was conducted next door to reality via a portal in Old Montreal\"\n    }\n```\n\n[frontmatter.tmpl](frontmatter.tmpl) is the template we used to render ordered front matter.\n\n```\n    ---\n    $if(title)$title: \"$title$\"$endif$\n    $if(interviewee)$interviewee: \"$interviewee$\"$endif$\n    $if(interviewer)$interviewer: \"$interviewer$\"$endif$\n    $if(abstract)$abstract: \"$abstract$\"$endif$\n    ---\n```\n\nHere's the commands we used to generate a \"doc.txt\" file with the \nfront matter in the desired order. Not \"empty.txt\" is just an empty\nfile so Pandoc will not read from standard input and just work with the\nJSON metadata and our template.\n\n```\ntouch empty.txt\npandoc --metadata-file=metadata.json --template=frontmatter.tmpl empty.txt\n```\n\nThe output of the pandoc command looks like this.\n\n```\n    ---\n    title: \"Interview with Mojo Sam\"\n    interviewee: \"Mojo Sam\"\n    interviewer: \"Tom Lopez\"\n    abstract: \"Interview in three sessions over sevaral decases, 1970 -\n    20020. The interview was conducted next door to reality via a portal in\n    Old Montreal\"\n    ---\n```\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "pandoc",
          "front matter"
        ],
        "pubDate": "2022-08-30",
        "title": "Ordering front matter"
      },
      "url": "posts/2022/08/30/Ordering-Frontmatter.json"
    },
    {
      "content": "\nPostgres 14 on Ubuntu 22.04 LTS\n===============================\n\nby R. S. Doiel, 2022-08-26\n\nThis is just a quick set of notes for working with Postgres 14 on an Ubuntu 22.04 LTS machine.  The goal is to setup Postgres 14 and have it available for personal work under a user account (e.g. jane.doe). \n\nAssumptions\n\n- include `jane.doe` is in the sudo group\n- `jane.doe` is the one logged in and installing Postgres for machine wide use\n- `jane.doe` will want to work with her own database by default\n\nSteps\n\n1. Install Postgres\n2. Confirm installation\n3. Add `jane.doe` user providing access\n\nBelow is the commands I typed to run to complete the three steps.\n\n~~~shell\nsudo apt install postgresql postgresql-contrib\nsudo -u createuser --interactive\njane.doe\ny\n~~~\n\nWhat we've accomplished is installing Postgres, we've create a user in Postgres DB environment called \"jane.doe\" and given \"jane.doe\" superuser permissions, i.e. the permissions to manage Postgres databases.\n\nAt this point we have a `jane.doe` Postgres admin user. This means we can run the `psql` shell from the Jane Doe account to do any database manager tasks. To confirm I want to list the databases available\n\n~~~shell\npsql \nSELECT datname FROM pg_database;\n\\quit\n~~~\n\nNOTE: This post is a distilation of what I learned from reading Digital Ocean's [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel",
        "byline": "R. S. Doiel",
        "daft": true,
        "number": 4,
        "pubDate": "2022-08-26",
        "series": "SQL Reflections",
        "title": "Postgres 14 on Ubuntu 22.04 LTS"
      },
      "url": "posts/2022/08/26/postgres-14-on-ubuntu-22.04-LTS.json"
    },
    {
      "content": "\n# 10:30 AM, SQL: Postgres\n\nPost: Friday, August 26, 2022, 10:30 AM\n\nIf you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 2,
        "pubDate": "2022-08-26",
        "series": "SQL",
        "title": "10:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/26/rosette-notes-2022-08-26_101030.json"
    }
  ]
}