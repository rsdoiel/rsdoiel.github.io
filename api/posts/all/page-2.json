{
  "page": 2,
  "total_pages": 5,
  "has_more": true,
  "next_page": "posts/all/page-3.json",
  "values": [
    {
      "content": "\n# Initial Impression of Pagefind\n\nBy R. S. Doiel, 2022-11-21\n\nI'm interested in site search that does not require using server side services (e.g. Solr/Elasticsearch/Opensearch). I've used [LunrJS](https://lunrjs.com) on my person blog site for several years.  The challenge with LunrJS is indexes become large and that limits how much your can index and still have a quick loading page. [Pagefind](https://pagefind.app) addresses the large index problem. The search page only downloads the portion of the indexes it needs. The index and search functionality are compiled down to WASM files. This does raise challenges if you're targeting older web browsers.\n\nPagefind is a [rust](https://www.rust-lang.org/) application build using `cargo` and `rustc`. Unlike the documentation on the [Pagefind](https://pagefind.app) website which suggests installing via `npm` and `npx` I recommend installing it from sources using the latest release of cargo/rustic.  For me I found getting the latest cargo/rustc is easiest using [rustup](https://rustup.rs/). Pagefind will not compile using older versions of cargo/rustc (e.g. the example currently available from Mac Ports for M1 Macs).\n\nHere's the steps I took to bring Pagefind up on my M1 Mac.\n\n1. Install cargo/rust using rustup\n2. Make sure `$HOME/.cargo/bin` is in my PATH\n3. Clone the Pagefind Git repository\n4. Change to the repository directory\n5. Build and install pagefind\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\ngit clone git@github.com git@github.com:CloudCannon/pagefind.git src/github.com/CloudCannon/pagefind\ncd src/github.com/CloudCannon/pagefind\ncargo install pagefind --features extended\n```\n\nNext steps were\n\n1. Switch to my local copy of my website\n2. Build my site in the usual page\n3. Update my `search.html` page to use pagefind\n4. Index my site using pagefind\n5. Test my a local web server\n\nTo get the HTML/JavaScript needed to embed pagefind in your search page see [Getting Started](https://pagefind.app/docs/). The HTML/JavaScript fragment is at the top of the page. After updating `search.html` I ran the pagefind command[^1].\n\n```\npagefind --verbose --bundle-dir ./pagefind --source .\n```\n\nThe indexing is wicked fast and it gives you nice details. I verified everything worked as expected using `pttk ws` static site web server. I then published my website. You can see the results at <http://rsdoiel.sdf.org/search.html> and <https://rsdoiel.github.io/search.html>\n\n[^1]: I specified the bundle directory because GitHub pages had a problem with the default `_pagefind`.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-21",
        "keywords": [
          "site search",
          "pagefind",
          "rust",
          "cargo",
          "rustup",
          "M1",
          "macOS"
        ],
        "pubDate": "2022-11-21",
        "title": "Initial Impressions of Pagefind"
      },
      "url": "posts/2022/11/21/initial-impressions-pagefind.json"
    },
    {
      "content": "\nBrowser based site search\n=========================\n\nBy R. S. Doiel, 2022-11-18\n\nI recently read Brewster Kahleâ€™s 2015 post about his vision for a [distributed web](https://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/). Many of his ideas have carried over into [DWeb](https://wiki.mozilla.org/Dweb), [Indie Web](https://indieweb.org/), [Small Web](https://benhoyt.com/writings/the-small-web-is-beautiful/), [Small Internet](https://cafebedouin.org/2021/07/28/the-small-internet/) and the like. A point he touches on is site search running in the web browser.\n\nI've use this approach in my own website relying on [LunrJS](https://lunrjs.com) by Oliver Nightingale. It is a common approach for small sites built using Markdown and [Pandoc](https://pandoc.org).  In the Brewster article he mentions [js-search](https://github.com/cebe/js-search), an implementation I was not familiar with. Like LunrJS the query engine runs in the browser via JavaScript but unlike LunrJS the indexes are built using PHP rather than JavaScript. The last couple of years I've used [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) to generating indexes for my own website site while using LunrJS for the browser side query engine. Today I check to see what the [Hugo](https://gohugo.io/tools/search/) community is using and found [Pagefind](https://github.com/cloudcannon/pagefind). Pagefind looks impressive. There was a presentation on at [Hugo Conference 2022](https://hugoconf.io/). It takes building a Lucene-like index several steps further. I appears to handle much larger indexes without requiring the full indexes to be downloaded into the browser.  It seems like a good candidate for prototyping personal search engine.\n\nHow long have been has browser side search been around? I do not remember when I started using. I explored seven projects on GitHub that implemented browser side site search. This is an arbitrary selection projects but even then I had no idea that this approach dates back a over decade!\n\n| Project | Indexer | query engine | earliest commit[^1] | recent commit[^2] |\n|---------|---------|--------------|:-------------------:|:-----------------:|\n| [LunrJS](https://github.com/olivernn/lunr.js) | JavaScript | JavaScript | 2011 | 2020 |\n| [Fuse.io](https://github.com/krisk/Fuse) | JavaScript/Typescript | JavaScript/Typescript | 2012 | 2022 |\n| [search-index](https://github.com/fergiemcdowall/search-index) | JavaScript | JavaScript | 2013 | 2016 |\n| [js-search](https://github.com/cebe/js-search) (cebe) | PHP | JavaScript | 2014 | 2022 |\n| [js-search](https://github.com/bvaughn/js-search) (bvaughn)| JavaScript | JavaScript | 2015 | 2022 |\n| [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) | Python | Python or JavaScript | 2018 | 2022 |\n| [Pagefind](https://github.com/cloudcannon/pagefind) | Rust | WASM and JavaScript | 2022 | 2022 |\n\n[^1]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n[^2]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-18",
        "keywords": [
          "search",
          "web browser",
          "dweb",
          "static site",
          "lunrjs",
          "pagefind"
        ],
        "pubDate": "2022-11-18",
        "title": "Browser based site search"
      },
      "url": "posts/2022/11/18/browser-side-site-search.json"
    },
    {
      "content": "\nTwitter's pending implosion\n===========================\n\nBy R. S. Doiel, 2022-11-11\n\nIt looks like Twitter continues to implode as layoffs and resignations continue. If bankers, investors and lenders call in the loans [bankruptcy appears to be possible](https://www.reuters.com/technology/twitter-information-security-chief-kissner-decides-leave-2022-11-10/). So what's next?\n\n\nThe problem\n-----------\n\nTwitter has been troubled for some time. The advertising model corrodes content. Twitter is effectively a massive RSS-like distribution system. It has stagnated as the APIs became more restrictive. The Advertising Business Model via [Ad-tech](https://pluralistic.net/tag/adtech/ \"per Cory Doctorow 'ad-fraud'\") encourages decay regardless of system.  Non-Twitter examples include commercial search engines (e.g. Google, Bing et el). Their usefulness usefulness declines over time. I believe this due to the increase in \"noise\" in the signal. The \"noise\" is driven be business models. That usually boils down to content who's function is to attract your attention so it can be sold for money. A corollary is [fear based journalism](https://medium.com/@oliviacadby/fear-mongering-journalisms-downfall-aac1f4f5756d). That has even caught the attention of a [Pope](https://www.9news.com.au/world/fear-based-journalism-is-terrorism-pope/4860b502-5dbb-4eef-abcf-57582445fc2c). Not fun.\n\nI suspect business models don't encourage great content. Business models are generally designed to turn a profit. They tend to get refined and tuned to that purpose. The evolution of Twitter and Google's search engine would make good case studies in that regard.\n\n\nA small hope\n------------\n\nI don't know what is next but I know what I find interesting. I've looked at Mastodon a number of times. It's not going away but the W3C activity pub spec is horribly complex. Complexity slows adoption. It reminds me of SGML. Conceptually interesting but in practice was too heavy. It did form inspiration for HTML though, and that has proven successful. What gives me hope is that Mastodon has survived. I think casting a wide net is interesting. The wider net is something I've heard called the \"small web\".\n\nThe small web\n-------------\n\nFor a number of years there has been a slowly growing  \"small web\" movement. I think it is relatively new term, I didn't find it in [Wikipedia](https://en.wikipedia.org/w/index.php?search=small+web&ns0=1 \"today is 2022-11-11\") when I looked today. As I see it the \"small web\" has been driven by a number of things. It is not a homogeneous movement but rather a collection of various efforts and communities.  I think it likely to continue to evolve. At times evolve rapidly. Perhaps it will coalesce at some point.  Here's what appears to me to be the common motivations as of 2022-11-11.\n\n- desire for simplicity\n- desire for authenticity\n- lower resource footprint\n- text as a primary but not exclusive medium\n- hyperlinks encouraged\n- a space where you're not a product\n- desire for decentralization (so you're not a product)\n- a desire to have room to grow (because you're not a product)\n\nThe \"small web\" is a term I've seen pop up in Gopherspace, among people who promote [Gemini](https://gemini.circumlunar.space/), [Micro blogging](https://micro.blog \"as an example of micro blogging\") and in the [Public Access Unix](https://sdf.org) communities.\"small web\" as a term does not return useful results in the commercial search engines I've checked. Elements seem to be part of [DWeb](https://getdweb.net/) which Mozilla is [championing](https://wiki.mozilla.org/Dweb). Curiously in spite of the hype and marketing I don't see \"small web\" in [web 3.0](https://www.forbes.com/advisor/investing/cryptocurrency/what-is-web-3-0/). I think blockchain has proven environmentally dangerous and tends to compile down to various forms of [grift](https://pluralistic.net/2022/05/27/voluntary-carbon-market/).\n\n\nSmall web\n---------\n\nWhat does \"small web\" mean to me?  I think it means\n\n- simple protocols that are flexible and friendly to tool creation\n- built on existing network protocols with a proven track record (e.g. IPv4, IPv6)\n- decentralized by design as was the early Internet\n- low barrier to participation\n    - e.g. a text editor, static site providing a URL to a twtxt file\n- text centric (at least for the moment)\n- integrated with the larger Internet, i.e. supports hyper links\n- friendly to distributed personal search engines (e.g. LunrJS running over curated set of JSONfeeds or twtxt urls)\n- \"feed\" oriented discovery based on simple formats (e.g. [RSS 2.0](https://cyber.harvard.edu/rss/rss.html), [JSONfeed](https://www.jsonfeed.org/), [twtxt](https://twtxt.readthedocs.io/en/latest/), [OPML](https://en.wikipedia.org/wiki/OPML), even [Gophermaps](https://en.wikipedia.org/wiki/Gopher_(protocol) \"see Source code of a menu title\"))\n- sustainable and preservation friendly\n    - example characteristics\n        - clone-able (e.g. as easy as cloning a Git Repo)\n        - push button update to Internet Archive's way back machine\n        - human and machine readable metadata\n\nI think the \"small web\" already exists. Examples include readable personal websites hosted as \"static pages\" via GitHub and S3 buckets are a good examples of prior art in a \"small web\".  Gopherspace is a good example of the \"small web\". I think the various [tilde communities](https://tilde.club) hosted on [Public Access Unix](https://en.wikipedia.org/wiki/SDF_Public_Access_Unix_System) are examples. Even the venerable \"bloggosphere\" of [Wordpress](https://wordpress.com) and the newer [Micro.blog](https://micro.blog/) is evidence that the \"small web\" already is hear. [Dave Winer](https://scripting.com)'s [Feedland](http://feedland.org/) is a good example of innovation in the \"small web\" happen today.  [Yarn.social](https://yarn.social) built on twtxt file format is very promising. I would argue right now the \"small web\" is the internet that already exists outside the walled gardens of Google, Meta/Facebook, Twitter, TikTok, Pinterest, Slack, Trello, Discord, etc.\n\nI think it is significant that the \"small web\" existed before the Pandemic. It continued to thrive during it. It is likely to evolve beyond it. The pending shift has already happening as it is already populated by \"early adopters\" and appears to be growing into larger community participation.  For the \"main stream\" it is waiting to be \"discovered\" or perhaps \"re-discovered\" depending on your point of view.\n\nHow do you participate?\n-----------------------\n\nYou may already be participating in the \"small web\".  Do you blog? Do your read feeds? Do you use a non-soloed social media platform like Mastodon? Do you use Gopher? The \"small web\" is defined by choice and is characterized by simplicity. It is a general term. You're the navigator not an algorithm tuned to tune someone a profit. If you are not sure where to start you can join a communities like [sdf.org](https://sdf.org) and get started there. You can explore [Gopherspace](https://floodgap.com) via a WWW proxy. You can create a static website and host a [twtxt](https://twtxt.readthedocs.io/en/latest/) file on GitHub or a [Yarn Pod](https://yarn.social). You can create a site via [Micro.blog](https://micro.blog) or [Feedland](http://feedland.org). You can blog. You can read RSS feeds or read twtxt feed with [twtxt](https://twtxt.readthedocs.io/en/latest/user/intro.html), [twet](https://github.com/quite/twet) or [yarn.social](https://yarn.social). You participate by stepping outside the walled gardens and seeing the larger \"Internet\".\n\nI think the important thing is to realize the alternatives are already here, you don't need to wait for invention, invitation or permission. You can move beyond the silos today. You don't need to have your attention captured then bought and sold. It's not so much a matter of \"giving up\" a silo but rather stepping outside one and breathing some fresh air.\n\nThings to watch\n---------------\n\n- [Feedland](https://feedland.org)\n- [yarn.social](https://yarn.social) and [twtxt](https://twtxt.readthedocs.io/en/latest/)\n- [Micro.blog](https://micro.blog/)\n- [Mastodon](https://joinmastodon.org/)\n- [Gopherspace](http://gopher.floodgap.com/gopher/gw?a=gopher%3A%2F%2Fgopher.floodgap.com%2F1%2Fworld), see [Gopherspace in 2020](https://cheapskatesguide.org/articles/gopherspace.html) as a nice orientation to see the internet through lynx and text\n- Even [Project Gemini](https://gemini.circumlunar.space/)\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "keywords": [
          "small web",
          "twtxt",
          "micro blogging",
          "social networks"
        ],
        "pubDate": "2022-11-11",
        "title": "Twitter's pending implosion"
      },
      "url": "posts/2022/11/11/Twitter-implosion.json"
    },
    {
      "content": "\nCompiling Pandoc from source\n============================\n\nBy R. S. Doiel, 2022-11-07\n\nI started playing around with Pandoc's __pandoc-server__ last Friday. I want to play with the latest version of Pandoc.  When I gave it a try this weekend I found that my Raspberry Pi 400's SD card was too small. This lead me to giving the build process a try on my Ubuntu desktop. These are my notes about how I going about building from scratch.  I am not a Haskell programmer and don't know the tool chain or language. Take everything that follows with a good dose of salt but this is what I did to get everything up and running. I am following the compile from source instructions in Pandoc's [INSTALL.md](https://github.com/jgm/pandoc/blob/master/INSTALL.md)\n\nI'm running this first on an Intel Ubuntu box because I have the disk space available there. If it works then I'll try it directly on my Raspberry Pi 400 with an upgrade SD card and running the 64bit version of Raspberry Pi OS.\n\nI did not have Haskell or Cabal installed when I started this process.\n\nSteps\n-----\n\n1. Install __stack__ (it will install GHC)\n2. Clone the GitHub repo for [Pandoc](https://github.com/jgm/pandoc)\n3. Setup __stack__ for Pandoc\n4. Build and test with __stack__\n5. Install __stack__ install\n6. Make a symbolic link from __pandoc__ to __pandoc-server__\n\n```\nsudo apt update\nsudo apt search \"haskell-stack\"\nsudo apt install \"haskell-stack\"\nstack upgrade\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup \nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nThis step takes a long time and on the Raspberry Pi it'll take allot longer.\n\nThe final installation of Pandoc was in my `$HOME/.local/bin` directory. Assuming this is early in your path this can allow you to experiment with a different version of Pandoc from the one installed on your system. \n\nI also wanted to try the latest of __pandoc-server__.  This was not automatically installed and is not mentioned in the INSTALL.md file explicitly. But looking at the discussion of running __pandoc-server__ in CGI mode got me thinking. I then checked the installation on my Ubuntu box for the packaged version of pandoc-server and saw that is was a symbolic link.  Adding a `ln` command to my build instruction solved the problem.\n\nI decided to try compiling Pandoc on my M1 mac.  First I needed to get __stack__ installed. I use Mac Ports but it wasn't in the list of available packages.  Fortunately the Haskell Stack website provides a shell script for installation on Unixes. I wanted to install __stack__ in my home `bin` directory not `/usr/bin/slack`. So after reviewing the downloaded install script I found the `-d` option for changing where it installs to. It indicated I need to additional work with __xcode__.\n\n```\ncurl -sSL https://get.haskellstack.org/ > stack-install.sh\nmore stack-install.sh\nsh stack-install.sh -d $HOME/bin\n```\n\nThe __stack__ installation resulted in a message in this form.\n\n```\nStack has been installed to: $HOME/bin/stack\n\nNOTE: You may need to run 'xcode-select --install' and/or\n      'open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg'\n      to set up the Xcode command-line tools, which Stack uses.\n\nWARNING: '$HOME/.local/bin' is not on your PATH.\n    Stack will place the binaries it builds in '$HOME/.local/bin' so\n    for best results, please add it to the beginning of PATH in your profile.\n```\n\nI already had xcode setup for compiling Go so those addition step was not needed.  I only needed to add `$HOME/.local/bin` to my search path.\n\nI then followed the steps I used on my Ubuntu Intel box.\n\n```\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup\nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nNow when I have a chance to update my Raspberry Pi 400 to a suitable sized SD Card (or external drive) I'll be ready to compile a current version of Pandoc from source.\n\nAdditional notes\n----------------\n\n[stack](https://docs.haskellstack.org/en/stable/) is a Haskell build tool. It setups up an Haskell environment per project. If a project requires a specific version of the Haskell compiler it'll be installed and made accessible for the project. In this way it's a bit like having a specific environment for Python. The stack website indicates that it targets cross platform development in Haskell which is nice.  Other features of stack remind me of Go \"go\" command in that it can build things or Rust's \"cargo\" command. Like __cargo__ it can update itself which is nice. That is what I did after installing the Debian package version used by Ubuntu. Configuration of a \"stack\" project uses YAML files. Stack uses __cabal__, Haskell's older build tool but subsumes __cabal-install__ for setting up __cabal__ and __ghc__. It appears from my reading that __stack__ addresses some of the short falls __cabal__ originally had and specifically focusing on reproducible compiles. This is important in sharing code as well as if you want to integrate automated compilation and testing. It maintains a project with \"cabal files\" so there is the ability to work with older non-stack code if I read the documentation correctly. Both __cabal__ and __stack__ seem to be evolving in parallel taking different approaches but influencing one another. Both systems use \"cabal files\" for describing projects and dependencies as of 2022. The short version of [Why Stack](https://docs.haskellstack.org/en/stable/#why-stack) can be found the __stack__ website.\n\n[Hackage](https://hackage.haskell.org/) is a central repository of Haskell packages. \n\n[Stackage](https://www.stackage.org/) is a curated subset of Hackage packages. It appears to be the preferred place for __stack__ to pull from.\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-07",
        "keywords": [
          "pandoc",
          "pandoc-server",
          "pandoc-citeproc",
          "haskell-stack",
          "cabal",
          "ghc"
        ],
        "pubDate": "2022-11-07",
        "title": "Compiling Pandoc from source"
      },
      "url": "posts/2022/11/07/compiling-pandoc-from-source.json"
    },
    {
      "content": "\nFeeds, formats, and plain text\n==============================\n\nBy R. S. Doiel, 2022-11-01\n\nThere has been a proliferation of feed formats. My personal preferred format is RSS 2.0. It's stable and proven the test of type. Atom feeds always felt a little like, \"not invented here so we're inventing it again\", type of thing. The claim was they could support read/write but so can RSS 2.0 specially with the namespace possibilities. The innovative work [Dave Winer](https://scripting.com) has done in the past and is doing today with [Feedland](https://feedland.org) is remarkably impressive.\n\nIn my experience the format of the feed is less critical than the how to author the metadata.  Over the last several years I've moved to static hosting as my preferred way of hosting a website. My writing is typically in Markdown or Fountain formats and frontmatter like used in RMarkdown has proven very convenient. The \"blogit\" command that started out from an idea in [mkpage](https://github.com/caltechlibrary/mkpage \"Make Page, a Pandoc preprocessor and tool set\") has been implemented in [pttk](https://github.com/rsdoiel/pttk \"Plain Text Toolkit\"). So for me metadata authoring makes sense in the front matter. That has the advantage that Pandoc can leverage the information in its templates (that is what I use to render HTML, man pages and the occasional PDF). It also is a food source for data to include in a feed.\n\nI've recently become aware of a really simple text format called [twtxt](https://twtxt.readthedocs.io/en/latest/). This simple format is meant for micro blogging but is also useful as a feed source and format. Especially in terms of rendering content for Gopherspace which I've re-engaged in recently. [Yarn.social](https://yarn.social) has built an entire ecosystem around it. Very impressive. The format is so simple it can be done with a pipe and the \"echo\" command in the shell.  It looks promising in terms for personal search ingest as well.\n\nOne of the formats that Dave Winer supports in Feedland and is used in the micro blogging community he has connected with is [jsonfeeds](https://www.jsonfeed.org/). It is lightweight and to me feels allot like RSS 2.0 without the XML-isms that go along with it.  I'm playing with the idea that in pttk it'll be the standard feed format and that from it I can then render our traditional feed friends of RSS 2.0 and Atom.\n\nI've looked at the ActivityPub from the Mastodon community but like [James Mill](https://prologic.github.io/prologic/ \"aka prologic\") I find it too complex. What is needed is something simple, really simple.  That's why I've been looking closely at Gopherspace again. The Gophermap can function as a bookmark file, a \"home page\" a list of feeds. A little archaic but practical in its simplicity. The only challenges I've run into has been figuring out that expectations of the Gopher server software. Currently I've settled on [gophernicus](https://gophernicus.org) as that is was it supported at [sdf.org](https://sdf.org) where I have a gopher \"hole\".\n\nAs pttk grows and I explore where I can take simple text processing I'm not targeting Gopherspace, twtxt and static websites. I've looked at [Gemini](https://gemini.circumlunar.space/docs/specification.gmi) but haven't grokked the point yet.  Their choice of yet another markup for content seems problematic at best. For me gopher solves the problems that would make me look at Gemini and I can use most any structured text I want. The text just needs to be readable easily by humans. The Gophermap provides can be enhanced menus much like \"index.html\" pages have become (a trunk that branches and eventually leads to a leaf). \n\n[OPML](http://home.opml.org/) remains a really nice outline data format.  It's something I'd like to eventually integrate with pttk. It can be easily represented as JSON. Just need to figure what problem I am trying to solve by using it.  Share a list of feeds is the classic case but looking at twtxt as well as the [newsboat](https://newsboat.org/) URL list makes me think it is more than I need. We'll see.  It is certainly reasonable to generate from a simpler source. If I ever write a personal search engine (something I've been thinking about to nearly a decade) it'd be a good way to share curated indexes sources as well as sources to crawl.  I just need to think that through more.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "plain text",
          "small internet",
          "rss",
          "jsonfeed",
          "gopher"
        ],
        "pubDate": "2022-11-01",
        "title": "feeds, formats and plain text"
      },
      "url": "posts/2022/11/01/Feeds-formats-and-plain-text.json"
    },
    {
      "content": "\nInstalling Cargo/Rest on Raspberry Pi 400\n=========================================\n\nOn my Raspberry Pi 400 I'm running the 64bit Raspberry Pi OS.\nThe version of Cargo and Rustc are not recent enough to install\n[ncgopher](https://github.com/jansc/ncgopher). What worked for\nme was to first install cargo via the instructions in the [The Cargo Book](https://doc.rust-lang.org/cargo/getting-started/installation.html). \n\n~~~shell\ncurl https://sh.rustup.rs -sSf | sh\n~~~\n\nAn important note is if you previously installed a version of Cargo/Rust\nvia the debian package system you should uninstall it before running the\ninstructions above from the Cargo Book.\n\nWith this version I was able to install __ncgopher__ using the simple\nrecipe of \n\n~~~shell\ncargo install ncgopher\n~~~\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "keywords": [
          "64bit",
          "Rapsberry Pi OS",
          "Cargo",
          "rustc"
        ],
        "pubDate": "2022-10-31",
        "title": "Installing Cargo/Rust on Raspberry Pi 400"
      },
      "url": "posts/2022/11/01/installing-cargo-rust-r400.json"
    },
    {
      "content": "\n# 7:30 AM, Oberon Language: A minimum SYSTEM module\n\nPost: Tuesday, October 18, 2022, 7:30 AM\n\nIt occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.\n\n",
      "data": {
        "keywords": [
          "A minimum SYSTEM module"
        ],
        "no": 1,
        "pubDate": "2022-10-18",
        "series": "Oberon Language",
        "title": "7:30 AM, Oberon Language: A minimum SYSTEM module"
      },
      "url": "posts/2022/10/18/Wishlist-Oberon-in-2023-2022-10-18_070730.json"
    },
    {
      "content": "\nWish list for Oberon in 2023\n===========================\n\nNext year will be ten years since Prof. Wirth and Paul Reed released [Project Oberon 2013](https://www.projectoberon.com).  It took me most of that decade to stumble on the project and community.  I am left wondering if Prof. Wirth and Paul Reed sat down today what would they design? I think only minor changes are needed and those mostly around assumptions.\n\nOberon-07 changing assumptions\n------------------------------\n\nThe language of Oberon-07 doesn't need to change. I do think the assumptions of the compiler are worth revisiting.  A CHAR should not be assumed to be an eight bit byte.  A CHAR should represent a character or symbol in a language. Many if not most of the Oberon community speaks language other than American English and that which is trivially represented in seven or eight bit ASCII.  While changing the representation assumption in Oberon-07 does increase complexity I feel restrict the character presentation of a CHAR to eight bits puts us on the side of \"too simple\" in the equation of \"Simpler but not to simple\".\n\nI am concerned about the assumption of an INTEGER as 32 bits. Increasingly I've seen single board computer implementations that are 64 bits.  Today feels allot like when I started in computing where personal computers were shifting from eight or sixteen bits to thirty two.  I suspect increasingly we will find that eight, sixteen and thirty two bit computers are relegated to the realm of specialized computers. While supporting these other widths will remain important I think shifting assumptions to sixty four bit machines makes sense now. Is a 32 bit machine \"too simple\" in our equation of \"simpler but not too simple\"?\n\n\n\nOberon as Operating System\n--------------------------\n\nThe operating system I still find liberating in 2023 as when I first was able to use it.  The challenge in 2023 though is the three button mouse. I think the supporting the historic mouse remains important but that the viewers should also support navigation via the keyboard and easily support touch systems that lack a mouse.  Being backward compatibly while adopting an enhance UI would make things more complex bit if care is taken I think that it can be done while keep the equation balanced as \"simpler, but not too simple\".\n\nTransforming my assumptions in 2023\n-----------------------------------\n\nI think the Artemis Project should presume that the representation of CHAR and INTEGER may change and probably should change. The portable modules should support compiling Oberon-07 programs on non-Oberon 2014 Systems without change.  I am skeptical that I can create a module system that provides a base line with the historic Oberon implementations. I think the Oakwood modules are just too limited. I think the assumption is I need implementations for Project Oberon 2013 modules as the base line perhaps enhanced with a few additional modules to supporting networking, UTF-8, JSON, and XML. The goal I think is that using Artemis on a non-Oberon System should facilitate bootstrapping an Oberon System 2023 should one come to exist.\n\nErrata\n------\n\n7:00 - 7:30; Oberon Language; A minimum SYSTEM module; It occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-10-16",
        "keywords": [
          "Oberon",
          "Oberon-07",
          "Oberon System",
          "Artemis"
        ],
        "pubDate": "2022-10-16",
        "title": "Wish list for Oberon in 2023"
      },
      "url": "posts/2022/10/16/Wishlist-Oberon-in-2023.json"
    },
    {
      "content": "\n# 7:30 AM, Gopher: Setup\n\nPost: Monday, October 10, 2022, 7:30 AM\n\nAccount verified, Yippee!\n\n",
      "data": {
        "keywords": [
          "Setup"
        ],
        "no": 1,
        "pubDate": "2022-10-10",
        "series": "Gopher",
        "title": "7:30 AM, Gopher: Setup"
      },
      "url": "posts/2022/10/10/getting-things-setup-2022-10-10_070730.json"
    },
    {
      "content": "\n\nGetting things setup\n====================\n\nBy R. S. Doiel, 2022-10-09\n\nI'm digging my [gopherhole on sdf.org](gopher://sdf.org:70/0/users/rsdoiel)\nas I wait for my validation to go through.  The plan is to migrate content\nfrom rsdoiel.github.io to here and host it in a Gopher context.  It's\ninteresting learning my way around sdf.org. Reminds me of my student days\nwhen I first had access to a Unix system.  Each Unix has it own flavors and\neven for the same Unix type/version each system has it's own particular\nvariation. Unix lends itself to customization and that why one system can\n\"feel\" or \"look\" different than the next.\n\nI'm trying to remember how to use Pico (vi isn't available yet).\nDiscovering how far \"mkgopher\" can go (very far it turns out).\n\nI'm looking forward to validation so I can have access to Git and\n\"move in\" to this gopherspace in a more sustainable way.\n\nThings to read and do\n---------------------\n\n- wait to be validated\n- learn [gitia](https://git.sdf.org) and setup up a mirror my personal projects and blog\n- read up on [gophernicus](https://www.gophernicus.org/) (the gopher server used by sdf.org)\n- [twenex project](https://www.twenex.org/), sounds interesting,\n  I remember accessing a TOPS-20 system at Whitesands in New Mexico\n  once upon a time.\n- figure out to access comp.lang.oberon if it is available on sdf.org\n- figure out, after validation, if I can compile OBNC for working on\n  Artemis and Oberon-07 code projects\n\nUpdates\n-------\n\n7:30 - 7:30; Gopher; Setup; Account verified, Yippee!\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-10-09",
        "keywords": [
          "gopher",
          "public unix"
        ],
        "pubDate": "2022-10-09",
        "title": "Getting things setup"
      },
      "url": "posts/2022/10/09/getting-things-setup.json"
    },
    {
      "content": "\nThinking about Gopher\n=====================\n\nBy R. S. Doiel, 2022-09-28\n\nLast weekend I visited the [Gophersphere](gopher://gopher.floodgap.com \"Floodgap is a good starting point for Gopher\") for the first time in a very long time. I'm happy to report it is still alive an active. It remains fast, lower resource consuming. This resulted in finding a Gopher protocol package in Go and adding light weight Gopher server to [pttk](https://rsdoiel.github.io/pttk) my current vehicle for experimenting with plain text writing tools.\n\nI've been thinking allot this past half week about where to explore in Gopher. The biggest issue I ran into turned out to be easily solve. Gopher protocol is traditionally served over port 70 but if you're running a \\*nix if you are just experimenting on localhost it is easier to use port 7000 (analogous to port 80 becoming 8000 or 8080 in the websphere). But some Gopher clients will only serve port 70. Two clients work very well at 7000 and they are Lynx (the trusty old console web browser) and one written in Rust called [ncgopher](https://github.com/jansc/ncgopher). The latter I find I use most of the time. It also supports Gemini sites though I am less interested in Gemini at the movement.  Gopher has a really nice sweet spot of straight forward implementation for both client and server. It would be a good exercise for moving from beginner to intermediate programming classes as you would be introducing network programming, a little parsing and the client server application models. It's a really good use case of looking back (Gopher is venerable in Internet age) and looking forward (a radical simplification of distributing readable material and related files).\n\nConstraints and creativity\n--------------------------\n\nThe simplicity and limitations of Gopher are inspiring. The limitations are particularly important as they are good constraints that help focus where to innovate. Gopher is a protocol ripe for software innovation precisely because of it's constraints.\n\nGophermaps is a good example. The Go package [git.mills.io/prologic/go-gopjer](https://git.mills.io/prologic/go-gopher) supports easily building servers that have Gophermaps the way of structuring the Gopher menus (aka selectors in Gopher parlance). A Gophermaps is a plain text file where you have lines that start with a Gopher supported document type (see [Gopher protocol](https://en.wikipedia.org/wiki/Gopher_(protocol) for details) a label followed by a tab character, a relative path followed by a tab character, a hostname followed by a tab character and a port number.  Very simple to parse.  The problem is Gopher clients expect all the fields for them to interpret them as a linked resource (e.g. a text file, binary file, image, or another Gopher selector). When I first encountered Gopher at USC so many years ago (pre-Mosaic, pre-Netscape) Gophermaps selectors are trivial to setup and you could build a service that supported ftp and Gopher in the same directory structure. All the \"development\" of a gopher site was done directly on the server in the directories where the files would live. Putting in all values seemed natural. Today I don't develop on a \"production server\" if I can avoid it. My writing is done on a small pool of machines at home, each with its own name. Explicitly writing a hostname and port with the eye to publishing to a public site then becomes a game of running `sed` to correct hostname and ports across the updated Gophermap files.\n\n> Gopher selectors form \"links\" to navigate through a Gopher site or through the Gophersphere depending on what they point at\n\nWithout changing the protocol you could modify the go-gopher package's function for presenting a Gophermap where the hostname port is assumed to the gopher server name and port if it was missing. Another approach would be to translate a place holder value. This would facilitate keeping your Gopher site under version control (e.g. Git and GitHub) while allowing you to easily deploy a version of the site in a public setting or in your development setting.  The constraint of the Gophermap definition as needed by the protocol doesn't mean it forces a cumbersome choice on your writing process.\n\nSimilarly the spaces versus tabs (invisible by default in many editors) because a non-issue by adopting editors that support [editorconfig](https://editorconfig.org) or even making the server slightly more complex in correctly identifying when to convert spaces to tabs expanding them out to a Gopher selectors.\n\nClient sites there are also many opportunities.  [Little Gopher Client](http://runtimeterror.com/tools/gopher/) pulls out the selectors its finds into a nice tree (like a bookmark tree) in a left panel and puts the text in the main window.  ncgopher let's you easily bookmark things and has a very clean, easy on the eyes reading experience in the console. In principle you the client could look at the retrieved selector and choose to display different file types based on the file extension as well as the selector type retrieved. This would let you include a richer experience in the Gophersphere for light weight markup like Commonmark files while still running nicely on Gopher protocol. Lots of room to innovate because the protocol is simple, limited and stable after all these years.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "draft": true,
        "pubDate": "2022-09-28",
        "title": "Thinking about Gopher"
      },
      "url": "posts/2022/09/28/thinking-about-gopher.json"
    },
    {
      "content": "\nRust development notes\n======================\n\nby R. S. Doiel, 2022-09-27\n\nI recently wanted to try [ncgopher](https://github.com/jansc/ncgopher) which is a [rust](https://rust-lang.org) based application. I was working on a an M1 Mac mini. I use [Mac Ports](https://www.macports.org) for my userland applications and installed [cargo](https://doc.rust-lang.org/cargo/) to pickup the rust compiler and build tool\n\n```shell\nsudo port install cargo\n```\n\nAll went well until I tried to build ncgopher and got an error as follows\n\n```\ncargo build --release\n    Updating crates.io index\nerror: Unable to update registry `crates-io`\n\nCaused by:\n  failed to fetch `https://github.com/rust-lang/crates.io-index`\n\nCaused by:\n  failed to authenticate when downloading repository: git@github.com:rust-lang/crates.io-index\n\n  * attempted ssh-agent authentication, but no usernames succeeded: `git`\n\n  if the git CLI succeeds then `net.git-fetch-with-cli` may help here\n  https://doc.rust-lang.org/cargo/reference/config.html#netgit-fetch-with-cli\n\nCaused by:\n  no authentication available\nmake: *** [build] Error 101\n```\n\nThis seemed odd as I could run `git clone git@github.com:rust-lang/crates.io-index` successfully. Re-reading the error message a dim light went on. I checked the cargo docs and the value `net.git-fetch-with-cli` defaults to false. That meant that cargo was using its own embedded git. OK, that makes sense but how do I fix it. I had no problem using cargo installed via ports on an Intel iMac so what gives? When cargo got installed on the M1 there was now `.cargo/config.toml` file. If you create this and set the value of `git-fetch-with-cli` to true then the problem resolves itself.\n\nIt was good that the error message provided a lead. It's also good that cargo has nice documentation. My experience though still left the taste of [COIK](https://www.urbandictionary.com/define.php?term=coik). Not sure how to improve the situation. It's not really a cargo bug (unless config.taml should be always created), it's not a rust bug and I don't even think it is a ports packaging bug.  If I was a new developer just getting familiar with git I don't think I would have known how to solve my problem even with the documentation provided. Git is something that has always struggled with COIK. While I like it it does make things challenging.\n\nIf I wind up playing with rust more then I'll add somemore notes here in the future.\n\nMy `$HOME/.cargo/config.toml` file looks like to have cargo use the git cli instead of the built in rust library.\n\n```\n[net]\ngit-fetch-with-cli = true\n```\n\n\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "rust",
          "cargo",
          "M1",
          "macOS",
          "ports"
        ],
        "pubDate": "2022-09-27",
        "title": "Rust development notes"
      },
      "url": "posts/2022/09/27/rust-development-notes.json"
    },
    {
      "content": "\n# 7:30 AM, Golang: pttk\n\nPost: Monday, September 26, 2022, 7:30 AM\n\nrenamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n\n",
      "data": {
        "keywords": [
          "pttk"
        ],
        "no": 1,
        "pubDate": "2022-09-26",
        "series": "Golang",
        "title": "7:30 AM, Golang: pttk"
      },
      "url": "posts/2022/09/26/golang-development-2022-09-26_070730.json"
    },
    {
      "content": "\nPostgreSQL dump and restore\n===========================\n\nThis is a quick note on easily dumping and restoring a specific database\nin Postgres 14.5.  This example has PostgreSQL running on localhost and\n[psql](https://www.postgresql.org/docs/current/app-psql.html) and\n[pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) are both available.\nOur database administrator username is \"admin\", the database to dump is called \"collections\". The SQL dump\nfile will be named \"collections-dump-2022-09-19.sql\".\n\n```shell\n\tpg_dump --username=admin --column-inserts \\\n\t    collections >collections-dump-2022-09-19.sql\n```\n\nFor the restore process I follow these steps\n\n1. Using `psql` create an empty database to restore into\n2. Using `psql` replay (import) the dump file in the new database to restoring the data\n\nThe database we want to restore our content into is called \"collections_snapshot\"\n\n```shell\n\tpsql -U dbadmin\n\t\\c postgres\n\tDROP DATABASE IF EXISTS collections_snapshot;\n\tCREATE DATABASE collections_snapshot;\n\t\\c collections_snapshots\n\t\\i ./collections-dump-2022-09-19.sql\n\t\\q\n```\n\nOr if you want to stay at the OS shell level\n\n```shell\n\tdropdb collections_snapshot\n\tcreatedb collections_snapshot\n\tpsql -U dbadmin --dbname=collections_snapshot -f ./collections-dump-2022-09-19.sql\n```\n\n\nNOTE: During this restore process `psql` will display some output. This is normal. The two\ntypes of lines output are shown below.\n\n```sql\n\tINSERT 0 1\n\tALTER TABLE\n```\n\nIf you want to stop the input on error you can use the `--set` option to set the error behavior\nto abort the reload if an error is encountered.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-09-19",
        "draft": true,
        "keywords": [
          "PostgreSQL"
        ],
        "pubDate": "2022-09-19",
        "title": "PostgreSQL dump and restore"
      },
      "url": "posts/2022/09/19/PostgreSQL-Dump-and-Restore.json"
    },
    {
      "content": "\n# 12:30 PM, SQL: Postgres\n\nPost: Monday, September 19, 2022, 12:30 PM\n\nSetting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 1,
        "pubDate": "2022-09-19",
        "series": "SQL",
        "title": "12:30 PM, SQL: Postgres"
      },
      "url": "posts/2022/09/19/rosette-notes-2022-09-19_121230.json"
    },
    {
      "content": "\nOrdering Front Matter\n=====================\n\nBy R. S. Doiel, 2022-08-30\n\nA colleague of mine ran into an interesting Pandoc behavior. He was combining a JSON metadata document and a converted word document and wanted the YAML front matter to have a specific order of fields (makes it easier for us humans to quickly scan it and see what the document was about).\n\nThe order he wanted in the front matter was\n\n- title\n- interviewer\n- interviewee\n- abstract\n\nThis was for a collection of oral histories. When my friend use Pandoc's `--metadata-json` to read the JSON metadata it rendered the YAML fine except the attributes were listed in alphabetical order.\n\nWe found a solution by getting Pandoc to treat the output not as Markdown plain text so that we could template the desired order of attributes.\n\nHere's the steps we used.\n\n1. create an empty file called \"empty.txt\" (this is just so you pandoc doesn't try to read standard input and processes\nyou metadata.json file with the template supplied)\n2. Create a template with the order you want (see below)\n3. Use pandoc to process your \".txt\" file and your JSON metadata file using the template (it makes it tread it as plain text even though we're going to treat it as markdown later)\n4. Append the content of the word file and run pandoc over your combined file as you would normally to generate your HTML\n\n\nThis is the contents of our [metadata.json](metadata.json) file.\n\n```json\n    {\n        \"title\": \"Interview with Mojo Sam\", \n        \"interviewee\": \"Mojo Sam\", \n        \"interviewer\": \"Tom Lopez\",\n        \"abstract\": \"Interview in three sessions over sevaral decases, 1970 - 20020. The interview was conducted next door to reality via a portal in Old Montreal\"\n    }\n```\n\n[frontmatter.tmpl](frontmatter.tmpl) is the template we used to render ordered front matter.\n\n```\n    ---\n    $if(title)$title: \"$title$\"$endif$\n    $if(interviewee)$interviewee: \"$interviewee$\"$endif$\n    $if(interviewer)$interviewer: \"$interviewer$\"$endif$\n    $if(abstract)$abstract: \"$abstract$\"$endif$\n    ---\n```\n\nHere's the commands we used to generate a \"doc.txt\" file with the \nfront matter in the desired order. Not \"empty.txt\" is just an empty\nfile so Pandoc will not read from standard input and just work with the\nJSON metadata and our template.\n\n```\ntouch empty.txt\npandoc --metadata-file=metadata.json --template=frontmatter.tmpl empty.txt\n```\n\nThe output of the pandoc command looks like this.\n\n```\n    ---\n    title: \"Interview with Mojo Sam\"\n    interviewee: \"Mojo Sam\"\n    interviewer: \"Tom Lopez\"\n    abstract: \"Interview in three sessions over sevaral decases, 1970 -\n    20020. The interview was conducted next door to reality via a portal in\n    Old Montreal\"\n    ---\n```\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "pandoc",
          "front matter"
        ],
        "pubDate": "2022-08-30",
        "title": "Ordering front matter"
      },
      "url": "posts/2022/08/30/Ordering-Frontmatter.json"
    },
    {
      "content": "\nPostgres 14 on Ubuntu 22.04 LTS\n===============================\n\nby R. S. Doiel, 2022-08-26\n\nThis is just a quick set of notes for working with Postgres 14 on an Ubuntu 22.04 LTS machine.  The goal is to setup Postgres 14 and have it available for personal work under a user account (e.g. jane.doe). \n\nAssumptions\n\n- include `jane.doe` is in the sudo group\n- `jane.doe` is the one logged in and installing Postgres for machine wide use\n- `jane.doe` will want to work with her own database by default\n\nSteps\n\n1. Install Postgres\n2. Confirm installation\n3. Add `jane.doe` user providing access\n\nBelow is the commands I typed to run to complete the three steps.\n\n~~~shell\nsudo apt install postgresql postgresql-contrib\nsudo -u createuser --interactive\njane.doe\ny\n~~~\n\nWhat we've accomplished is installing Postgres, we've create a user in Postgres DB environment called \"jane.doe\" and given \"jane.doe\" superuser permissions, i.e. the permissions to manage Postgres databases.\n\nAt this point we have a `jane.doe` Postgres admin user. This means we can run the `psql` shell from the Jane Doe account to do any database manager tasks. To confirm I want to list the databases available\n\n~~~shell\npsql \nSELECT datname FROM pg_database;\n\\quit\n~~~\n\nNOTE: This post is a distilation of what I learned from reading Digital Ocean's [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel",
        "byline": "R. S. Doiel",
        "daft": true,
        "number": 4,
        "pubDate": "2022-08-26",
        "series": "SQL Reflections",
        "title": "Postgres 14 on Ubuntu 22.04 LTS"
      },
      "url": "posts/2022/08/26/postgres-14-on-ubuntu-22.04-LTS.json"
    },
    {
      "content": "\n# 10:30 AM, SQL: Postgres\n\nPost: Friday, August 26, 2022, 10:30 AM\n\nIf you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 2,
        "pubDate": "2022-08-26",
        "series": "SQL",
        "title": "10:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/26/rosette-notes-2022-08-26_101030.json"
    },
    {
      "content": "\n# 12:00 PM, SQL: Postgres\n\nPost: Wednesday, August 24, 2022, 12:00 PM\n\nI miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%'\n`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 3,
        "pubDate": "2022-08-24",
        "series": "SQL",
        "title": "12:00 PM, SQL: Postgres"
      },
      "url": "posts/2022/08/24/rosette-notes-2022-08-24_121200.json"
    },
    {
      "content": "\nA Quick intro to PL/pgSQL\n========================\n\nPL/pgSQL is a procedure language extended from SQL. It adds flow control and local state for procedures, functions and triggers. Procedures, functions and triggers are also the compilation unit. Visually PL/pgSQL looks similar to the MySQL or ORACLE counter parts. It reminds me of a mashup of ALGO and SQL. Like the unit of compilation, the unit of execution is also procedure, function or trigger. \n\nThe Postgres documentation defines and explains the [PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql.html) and how it works.  This document is just a quick orientation with specific examples to provide context.\n\nHello World\n-----------\n\nHere is a \"helloworld\" procedure definition.\n\n```sql\n    CREATE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n       RAISE NOTICE 'Hello WORLD!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nLet's take a look this line by line.\n\n1. CREATE PROCEDURE defines the procedure and the starting and ending delimiter for the procedure (e.g. `AS $$` the procedure's text ends when `$$` is encountered an second time.\n2. DECLARE is the block where you would declare the variables used in the procedure, we have none in this example\n3. The BEGIN starts the actual procedure instructions\n4. The `RAISE NOTICE` line is how you can display output to the console when the procedure is run\n5. The END completes the procedure definition\n6. the `$$ LANGUAGE plpgsql;` concludes the text defining the procedure and tells the database engine that procedure is written in PL/pgSQL.\n\nWe can run the procedure using the \"CALL\" query.\n\n```sql\n    CALL helloworld()\n```\n\nNOTE: If you want to change the procedure you can \"DROP\" it first otherwise you'll get an error that it already exists.\n\n```sql\n    DROP PROCEDURE helloworld;\n```\n\nImproving my workflow\n---------------------\n\nSQL procedures are generally stored in the RDBMs in database environment. You can think of them as records in the system's database. Procedures and functions are created and can be dropped. While they can be manually typed in the database's shell it is easier to maintain them in plain text files outside the RDBM environment.  \n\n1. Write the procedure in a text file.\n2. Load the text file (e.g. FILENAME) into Postgres \n   a. outside the Postgres shell use `psql -f FILENAME` \n   b. inside the Postgres shell used `\\i FILENAME`\n3. Call the procedure to test it\n\nTo turn these steps into a look I use a \"CREATE OR REPLACE\" statement and be able to reload the updated procedure easier see [43.12. Tips for Developing in PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql-development-tips.html).  Note in the revised example the \"-- \" lines are comments.\n\nOur revised [helloworld](helloworld.plpgsql).\n\n```sql\n    --\n    -- Create (or replace) the new \"helloworld\" procedure.\n    -- NOTE: this can be run with \"CALL\"\n    --\n    CREATE OR REPLACE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n        RAISE NOTICE 'Hello World!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\n\nHi There\n--------\n\n[hithere](hithere.plpgsql) is similar to our helloworld example except it is a function that takes a parameter of the person's name. The function returns a \"VARCHAR\", so this should work as part of a select statement.\n\n```sql\n    --\n    -- This is a \"Hi There\" function. The function takes\n    -- a single parameter and forms a greeting.\n    --\n    CREATE OR REPLACE FUNCTION hithere(name varchar) RETURNS varchar AS $$\n    DECLARE\n      greeting varchar;\n    BEGIN\n        IF name = '' THEN\n            greeting := 'Hi there!';\n        ELSE\n            greeting := 'Hello ' || name || '!';\n        END IF;\n        RETURN greeting;\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nGiving it a try.\n\n```shell\n    SELECT hithere('Mojo Sam');\n```\n\nFurther reading\n---------------\n\n- [Conditionals](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONDITIONALS)\n- [Loops](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONTROL-STRUCTURES-LOOPS)\n- [Calling a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-CALLING-PROCEDURE)\n- [Early return from a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-RETURNING-PROCEDURE)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-24",
        "keywords": [
          "postgres",
          "sql",
          "psql",
          "plsql",
          "plpgsql"
        ],
        "number": 3,
        "pubDate": "2022-08-24",
        "series": "SQL Reflections",
        "title": "A Quick into to PL/pgSQL"
      },
      "url": "posts/2022/08/24/plpgsql-quick-intro.json"
    },
    {
      "content": "\n# 11:30 AM, SQL: Postgres\n\nPost: Monday, August 22, 2022, 11:30 AM\n\nThree things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 4,
        "pubDate": "2022-08-22",
        "series": "SQL",
        "title": "11:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/22/rosette-notes-2022-08-22_111130.json"
    },
    {
      "content": "\nRosette Notes\n=============\n\nBy R. S. Doiel, 2022-08-19\n\n> A dance around two relational databases, piecing together similarities as with the tiny mosaic tiles of a guitar's rosette\n\nWhat follows are my preliminary notes learning Postgres 12 and 14.\n\nPostgres & MySQL\n----------------\n\nThis is a short comparison of some administrative commands I commonly use. The first column describes the task followed by the SQL to execute for Postgres 14.5 and then MySQL 8. The presumption is you're using `psql` to access Postgres and `mysql` to  access MySQL. Values between `<` and `>` should be replaced with an appropriate value.\n\n| Task                    | Postgres 14.5                     | MySQL 8           |\n|-------------------------|------------------------------------|-------------------|\n| show all databases      | `SELECT datname FROM pg_database;` | `SHOW DATABASES;` |\n| select a database       | `\\c <dbname>`                      | `USE <dbname>`    |\n| show tables in database | `\\dt`                              | `SHOW TABLES;`    |\n| show columns in table   | `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '<table_name>';` | `SHOW COLUMNS IN <table_name>` |\n\nReflections\n-----------\n\nThe Postgres shell, `psql`, provides the functionality of showing a list of tables via a short cut while MySQL choose to add the `SHOW TABLES` query. For me `SHOW ...` feels like SQL where as `\\d` or `\\dt` takes me out of SQL space. On the other hand given Postgres metadata structure the shortcut is appreciated and I often query for table names as I forget them. `\\dt` quickly becomes second nature and is shorter to type than `SHOW TABLES`. \n\nConnecting to a database with `\\c` in `psql` is like calling an \"open\" in programming language. The \"connection\" in `psql` is open until explicitly closed or the shell is terminated.  Like `USE ...` in the MySQL shell it make working with multiple database easy.  The difference are apparent when you execute a `DROP DATABASE ...` command. In `psql` you need to `CLOSE` the database first or the `DROP` will fail.  The MySQL shell will happily let you drop the current database you are currently using.\n\nThe challenge I've experienced learning `psql` after knowing MySQL is my lack of familiarity with the metadata Postgres maintains about databases and structures.  On the other hand everything I've learned about standards base SQL applies to managing Postgres once remember the database/table I need to work with.  A steeper learning curve from MySQL's `SHOW` but it also means writing external programs for managing Postgres databases and tables is far easier because everything is visible because that is how you manage Postgres. MySQL's `SHOW` is very convenient but at the cost of hiding some of its internal structures.\n\nBoth MySQL and Postgres support writing programs in SQL. They also support stored procedures, views and triggers. They've converged in the degree in which they have both implemented SQL language standards.  The differences are mostly in approach to managing databases.  There are some differences, necessitated by implementation choices, in the `CREATE DATABASE`, `CREATE TABLE` or `ALTER` statements but you can often use the basic form described in ANSI SQL and get the results you need. When doing performance tuning the dialect differences are more important.\n\nDump & Restore\n--------------\n\nBoth Postgres and MySQL provide command line programs for dumping a database. MySQL provides a single program where as Postgres splits it in two. Check the man pages (or website docs) for details in their options. Both sets of programs are highly configurable allowing you to dump just schema, just data or both with different expectations.\n\n| Postgres 14.5      | MySQL 8                         |\n|--------------------|---------------------------------|\n| `pg_dumpall`       | `mysqldump --all-databases`     |\n| `pg_dump <dbname>` | `mysqldump --database <dbname>` |\n\nThe `pg_dumpall` tool is designed to restore an entire database instance. It includes account and ownership information. `pg_dump` just focuses on the database itself. If you are taking a snapshot production data to use in a test `pg_dump` output is easier to work with. It captures the specific database with out entangling things like the `template1` database or database user accounts and ownership.\n\nYou can restore a database dump in both Postgres and MySQL. The tooling is a little different.\n\n| Postgres 14.5                   | MySQL 8                                      |\n|---------------------------------|----------------------------------------------|\n| `dropdb <dbname>`               | `mysql -execute 'DROP DATABASE <dbname>;'`   |\n| `createdb <dbname>`             | `mysql -execute 'CREATE DATABASE <dbname>;'` |\n| `psql -f <dump_filename>`       |`mysql <dbname> < <dump_filename>`            |\n\nNOTE: These instructions work for a database dumped with `pg_dump` for the Postgres example. In principle it is the same way you can restore from `pg_dumpall` but if you Postgres instance already exists then you're going to run into various problems, e.g. errors about `template1` db.\n\nLessons learned along the way\n-----------------------------\n\n2022-08-22\n\n8:00 - 11:30; SQL; Postgres; Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n\n2022-08-24\n\n11:00 - 12:00; SQL; Postgres; I miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%';`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n2022-08-26\n\n9:30 - 10:30; SQL; Postgres; If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n2022-09-19\n\n10:30 - 12:30; SQL; Postgres; Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-19",
        "keywords": [
          "postgres",
          "mysql",
          "sql",
          "psql"
        ],
        "number": 2,
        "pubDate": "2022-08-19",
        "series": "SQL Reflections",
        "title": "Rosette Notes: Postgres and MySQL",
        "updated": "2022-09-19"
      },
      "url": "posts/2022/08/19/rosette-notes.json"
    },
    {
      "content": "\nPttk and STN\n============\n\nBy R. S. Doiel, started 2022-08-15\n(updated: 2022-09-26, pdtk was renamed pttk)\n\nThis log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the \"blogit\" action that will translates the simple timesheet notation (aka STN) to a short blog post.  My \"short post\" interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.\n\n2022-08-15\n\n16:45 - 17:45; Golang; ptdk, stngo; Thinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n2022-08-14\n\n14:00 - 17:00; Golang; pdtk, stngo; Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n2022-08-12\n\n16:00 - 16:30; Golang; stngo; A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n2022-09-26\n\n6:30 - 7:30; Golang; pttk; renamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2022-08-15",
        "pubDate": "2022-08-15",
        "title": "PTTK and STN",
        "updated": "2022-09-26"
      },
      "url": "posts/2022/08/15/golang-development.json"
    },
    {
      "content": "\n# 5:45 PM, Golang: ptdk,  stngo\n\nPost: Monday, August 15, 2022, 5:45 PM\n\nThinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n",
      "data": {
        "keywords": [
          "ptdk",
          "stngo"
        ],
        "no": 4,
        "pubDate": "2022-08-15",
        "series": "Golang",
        "title": "5:45 PM, Golang: ptdk,  stngo"
      },
      "url": "posts/2022/08/15/golang-development-2022-08-15_170545.json"
    }
  ]
}