{
  "page": 1,
  "total_pages": 6,
  "has_more": true,
  "next_page": "posts/all/page-2.json",
  "values": [
    {
      "content": "\n# Build your own static web server with Deno\n\nOne of things I have in my web toolbox is a static site web server. It only runs on localhost. It amazes me how often I wind up using it. PHP and Python can launch one easily from the command line but I have always found they were lacking. What I want is a simple web server that runs only on localhost. It can serve content from a specified directory and should handle common content types appropriately (e.g. JavaScript files are served as \"application/javascript\" not as \"text/plain\"). I should be able choose the port the server runs on. I should be able to specify a document root for the content I want to expose. It should default to a sensible location like the \"htdocs\" directory in my current working directory.\n\nWhen I started working with the web (when people used the NCSA web server), web servers were considered complex and hard to implement. I remember most network systems were presumed complex. Today most programming languages have some sort of library, module or package that makes implementing a web server trivial. This is true for JavaScript running under a JavaScript run time engine.\n\nDeno is a JavaScript and TypeScript runtime. I prefer Deno over other JavaScript runtimes like NodeJS. Deno runs sandboxed. This is similar to how the web browser treats JavaScript. Deno's standard library aligns with web browser implementation too. Deno has a good set of standard modules. Many modules can also be used browser side. \n\nI'll be using some Deno standard JavaScript modules in this post. The standard module \"@std/http/file-server\" provides most of what you need to implement a static content server. Two other modules will round things out in how I want my web server to behave. They are \"@std/fs/exists\" and \"@std/yaml/parse\".\n\nLet's build a simple but useful static web server and add it to our web toolbox.\n\nBefore I build my static web server I need some web content. I'm going to need an HTML file and a JavaScript file. This will provide content to test. The web content should be created in a directory called \"htdocs\". On macOS, Linux and Windows the command I run from in the terminal application to create the \"htdocs\" directory is `mkdir htdocs`. Using your my editor, I created the HTML file called \"helloworld.html\" inside the \"htdocs\" directory.\n\n~~~html\n<!DOCTYPE html>\n<head>\n  <script type=\"module\" src=\"helloworld.js\"></script>\n</head>\n<html lang=\"en-US\">\n  <body>Hello World</body>\n</html>\n~~~\n\nI created a \"helloworld.js\" inside the \"htdocs\" directory too.\n\n~~~JavaScript\nconst body = document.querySelector(\"body\");\nconst elem = document.createElement(\"div\");\nelem.innerText = 'Hello World 2!';\nbody.append(elem);\n~~~\n\nThis provides content to test my prototypes. Using these files I can make sure the prototype properly serves out a web page, handles a file listing and properly services the JavaScript.\n\nYour directory tree should look something like this.\n\n~~~shell\ntree htdocs\nhtdocs/\n├── helloworld.html\n└── helloworld.js\n\n1 directory, 2 files\n~~~\n\n## First prototype\n\nUsing a text editor, I create a file called `webserver_v1.js`. I need to do several things in JavaScript to build our static web server.\n\n1. import a function called `serveDir` from the \"@std/http/file-server\" module\n2. I need to set two constants, a port number and a root document path\n3. It is helpful to display the setting for the port and document root when the server starts up\n4. I can using Deno's built in `serve` method to handle inbound requests and then dispatch them to `serveDir`\n\nLet's start with the import, `\"@std/http/file-server`.  Notice that it starts with and \"@\". This indicates to the JavaScript runtime that the full URL to the module is defined by an import map. When you build a Deno project you can generate a file called `deno.json`. It can include an import map. The `deno add` command provides a really easy way to manage this mapping. As of Deno 2 the standard modules are available from [jsr.io](https://jsr.io), a reliable JavaScript registry. This includes our standard module `@std/http/file-server`. I can \"add\" it  to my project using the following command.\n\n~~~shell\ndeno add jsr:@std/http/file-server\n~~~\n\nIf the \"deno.json\" file does not exist this command will create it. If it does exist Deno will update it to reflect the new module. I can look inside the \"deno.json\" file after running this command and see my import map.\n\n~~~json\n{\n  \"imports\": {\n    \"@std/http\": \"jsr:@std/http@^1.0.18\"\n  }\n}\n~~~\n\nThe Deno runtime knows how to contact jsr.io and use it to retrieve the module requested.  By default it picks the current stable version. In my case that is v1.0.18. Deno updates happen pretty steadily through out the year. When I try this a month from now it'll probably be a different version number.\n\nNow that Deno is setup, I need to write my first prototype static web server.\n\n~~~JavaScript\n/**\n * webserver_v1.js - A simple static file server for serving files from the \"htdocs\" directory.\n */\nimport { serveDir } from \"@std/http/file-server\";\n\nconst port = 8000;\nconst rootPath = \"htdocs\"; // Serve files from the \"htdocs\" directory\n\nconsole.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);\n\n// Start a simple server\nDeno.serve({\n  port,\n}, async (req) => {\n  try {\n    // Serve files from the specified directory\n    return await serveDir(req, {\n      fsRoot: rootPath,\n      urlRoot: \"\",\n      showDirListing: true,\n      showDotfiles: false, // Exclude files starting with a period\n    });\n  } catch (err) {\n    console.error(err);\n    // Return a 404 response if something goes wrong\n    return new Response(\"404: Not Found\", { status: 404 });\n  }\n});\n~~~\n \nThe `Deno.serve` manages the inbound request and the async anonymous function handles the mapping to the file server module function called `serveDir`. A try catch wraps the `serveDir` function. If that function fails a 404 response is created and returned. Pretty simple.\n\nLet's see if the code we typed in works. Deno provides three helpful commands for working with your program code \n\n1. check \n2. lint\n3. fmt\n\nCheck reads the JavaScript (or TypeScript) file and makes sure it makes sense from the compilation point of view.  The lint command goes a step further. It checks to see if best practices have been followed. Lint is completely optional but check needs to pass before Deno will attempt to run or compile the program. The `fmt` command will format your source code in a standard way. I'm going to use check and lint.\n\n~~~shell\ndeno check webserver_v1.js\ndeno lint webserver_v1.js\n~~~\n\nAll went well. In both cases I see a line indicating it checked the file. If I had made errors check and lint would have complained and included lines describing errors.\n\nDeno can run our JavaScript and TypeScript files. To test my program I try the following. \n\n~~~shell\ndeno run webserver_v1.js\n~~~\n\nWhen I tried this I saw the following message. \n\n~~~shell\nServer running on http://localhost:8000/, serving htdocs\n┏ ⚠️  Deno requests net access to \"0.0.0.0:8000\".\n┠─ Requested by `Deno.listen()` API.\n┠─ To see a stack trace for this prompt, set the DENO_TRACE_PERMISSIONS environmental variable.\n┠─ Learn more at: https://docs.deno.com/go/--allow-net\n┠─ Run again with --allow-net to bypass this prompt.\n┗ Allow? [y/n/A] (y = yes, allow; n = no, deny; A = allow all net permissions) > \n~~~\n\nI type \"y\" and press enter. New lines appear.\n\n~~~shell\nServer running on http://localhost:8000/, serving htdocs\n✅ Granted net access to \"0.0.0.0:8000\".\nListening on http://0.0.0.0:8000/ (http://localhost:8000/)\n~~~\n\nI point my web browser to \"http://localhost:8000/\". Do I see anything? No. In my terminal window I see another prompt about permissions.\n\n~~~shell\n┏ ⚠️  Deno requests read access to \"htdocs\".\n┠─ Requested by `Deno.stat()` API.\n┠─ To see a stack trace for this prompt, set the DENO_TRACE_PERMISSIONS environmental variable.\n┠─ Learn more at: https://docs.deno.com/go/--allow-read\n┠─ Run again with --allow-read to bypass this prompt.\n┗ Allow? [y/n/A] (y = yes, allow; n = no, deny; A = allow all read permissions)\n~~~\n\nAgain answer \"y\". I then see something this in my terminal window.\n\n~~~shell\n[2025-06-30 16:27:31] [GET] / 200\nNo such file or directory (os error 2): stat '/Users/rsdoiel/Sandbox/Writing/Books/A_Simple_Web/htdocs/favicon.ico'\n[2025-06-30 16:27:31] [GET] /favicon.ico 404\n~~~\n\nI reload my web browser page, what do I see? A list of files. I know that file directory listing works.\nOne of the files is \"helloworld.html\".  I click on it. I my simple web page with the words \"Hello World\" and \"Hello World 2\". Yippee, I've created a static web server.\n\nYou might be wondering how I shutdown the web server. In the terminal window I press control and the letter c, aka \"Ctrl-C\". This will shuts down the web server. I can confirm it is shutdown in the web browser by reloading the page. I see an connection error page now.\n\nI don't want to answer questions about permissions each time I run my prototype. I can specify the permissions I want to grant on the command line.  I know from my test that my program needs \"net\" and \"read\" permissions. I can grant this using the following command.\n\n~~~shell\ndeno run --allow-net --allow-read webserver_v1.js\n~~~\n\nBetter yet I can compile our JavaScript program into an executable file. An executable is handy because I can run it without Deno being installed on a different computer as long as it runs the same operating system and has the same CPU type. Compiling to an executable makes this prototype similar to our tools in my web tool box. It let's me treat it just like my terminal application, text editor and web browser.\n\n~~~shell\ndeno compile --allow-net --allow-read webserver_v1.js\n~~~\n\nThis results in a file being created called \"webserver_v1\" (or on Windows, \"webserver_v1.exe\"). This file can be run from this directory or moved to another directory where I store other programs (e.g. `$HOME/bin` or `$HOME\\bin` on Windows).\n\n## Improving on v1\n\nWhile webserver_v1.js is helpful it could be more friendly. What if I want to use a different port number? What if I want to server out content my current directory or maybe I want to service content on a different mounted drive? I can do that by adding support for command line arguments.\n\n~~~JavaScript\n/**\n * webserver_v2.js - A simple static file server with configurable port and root directory.\n */\nimport { serveDir } from \"@std/http/file-server\";\n\nconst defaultPort = 8000;\nconst defaultRoot = \"htdocs\";\n\n// Parse command-line arguments\nconst args = Deno.args;\nlet rootPath = defaultRoot;\nlet port = defaultPort;\n\n// Check the command arguments and set the port and rootPath appropriately\nif (args.length > 0) {\n  // Check if the first argument is a port number\n  const portArg = parseInt(args[0], 10);\n  if (!isNaN(portArg)) {\n    port = portArg;\n  } else {\n    // If not a port number, assume it's the root path\n    rootPath = args[0];\n  }\n\n  // Check if the second argument is a root path\n  if (args.length > 1) {\n    rootPath = args[1];\n  }\n}\n\nconsole.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);\n\n// Start a simple server\nDeno.serve({\n  port,\n}, async (req) => {\n  try {\n    // Serve files from the specified directory\n    return await serveDir(req, {\n      fsRoot: rootPath,\n      urlRoot: \"\",\n      showDirListing: true,\n      showDotfiles: false, // Exclude files starting with a period\n    });\n  } catch (err) {\n    console.error(err);\n    // Return a 404 response if something goes wrong\n    return new Response(\"404: Not Found\", { status: 404 });\n  }\n});\n~~~\n\nI can compile that using the following deno compile command\n\n~~~shell\ndeno compile --allow-net --allow-read webserver_v2.js\n~~~\n\nWe can run the new webserver using the following command.\n\n~~~shell\n./webserver_v2 8001 .\n~~~\n\nPoint the web browser at <http://localhost:8001>. What do I see the directory? Yep, I see the files in my root directory of my project including the \"htdocs\" directory I created. Can I find and display \"helloworld.html\"? Yep and it works as in the first prototype. I shutdown the web server and then start it again using just the executable name.\n\nmacOS and Linux\n\n~~~shell\n./webserver_v2\n~~~\n\non Windows\n\n~~~shell\n.\\webserver_v2\n~~~\n\nWhat do you see? Can you find \"helloworld.html\"? Stop the web server. I copy \"helloworld.html\" to \"index.html\". After copying I restart the web server again.\n\nOn macOS and Linux\n\n~~~shell\ncp htdocs/helloworld.html htdocs/index.html\n./webserver_v2\n~~~\n\nOn Windows\n\n~~~pwsh\ncopy htdocs\\helloworld.html htdocs\\index.html\n.\\webserver_v2\n~~~\n\nI point the web browser at <http://localhost:8000>, what do I see? I don't see the file directory any more, I see the contents of  I copied into the \"index.html\" file, \"Hello World\" and \"Hello World 2\".\n\nCan this be improved?  It'd be nice to web able to just type \"webserver_v2\" and have the program using a default port and htdocs directory of my choice. That can be supported by using a configuration file. YAML is an easy to read and easy to type notation. It even supports comments which is nice in configuration files. YAML expresses the same types of data structures as JSON (JavaScript Object Notation). Below an example of a configuration file. I type it in and save it using the filename \"webserver.yaml\".\n\n~~~yaml\n# Set root path for web content to the current directory.\nhtdocs: .\n# Set the port number to listen on to 8002\nport: 8002\n~~~\n\nFrom the point of the view of my prototype it'll need to check if the \"webserver.yaml\" file exists before attempting to read it. Deno has a module for that. It'll also need to read the YAML, parse it and get an object that exposes my preferred settings. Deno has a standard model for working with YAML too. The modules I'm interested in are `@std/fs/exists` and `@std/yaml`. I'll need to \"add\" them to my deno project.\n\n~~~shell\ndeno add jsr:@std/fs/exists\ndeno add jsr:@std/yaml\n~~~\n\nTime for an improved version of the static web server. This prototype should be called, \"webserver_v3.js\".\n\n~~~JavaScript\n/**\n * webserver_v3.js - A simple static file server with configurable port and root directory via YAML.\n */\nimport { serveDir } from \"@std/http/file-server\";\nimport { parse } from \"@std/yaml/parse\";\nimport { exists } from \"@std/fs/exists\";\n\nconst defaultPort = 8000;\nconst defaultRoot = \"htdocs\";\n\n// Function to read and parse YAML configuration file\nasync function readConfigFile(filePath) {\n  try {\n    const fileContent = await Deno.readTextFile(filePath);\n    return parse(fileContent);\n  } catch (err) {\n    console.error(\"Error reading or parsing YAML file:\", err);\n    return null;\n  }\n}\n\n// Parse command-line arguments\nconst args = Deno.args;\nlet rootPath = defaultRoot;\nlet port = defaultPort;\n\nif (args.length > 0) {\n  // Check if the first argument is a port number\n  const portArg = parseInt(args[0], 10);\n  if (!isNaN(portArg)) {\n    port = portArg;\n  } else {\n    // If not a port number, assume it's the root path\n    rootPath = args[0];\n  }\n\n  // Check if the second argument is a root path\n  if (args.length > 1) {\n    rootPath = args[1];\n  }\n} else {\n  // Check for YAML configuration file\n  const configFilePath = \"webserver.yaml\";\n  if (await exists(configFilePath)) {\n    const config = await readConfigFile(configFilePath);\n    if (config) {\n      rootPath = config.htdocs || defaultRoot;\n      port = config.port || defaultPort;\n    }\n  }\n}\n\nconsole.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);\n\n// Start a simple server\nDeno.serve({\n  port,\n}, async (req) => {\n  try {\n    // Serve files from the specified directory\n    return await serveDir(req, {\n      fsRoot: rootPath,\n      urlRoot: \"\",\n      showDirListing: true,\n      showDotfiles: false, // Exclude files starting with a period\n    });\n  } catch (err) {\n    console.error(err);\n    // Return a 404 response if something goes wrong\n    return new Response(\"404: Not Found\", { status: 404 });\n  }\n});\n~~~\n\nLike before I compile it with the my desired permissions.\n\n~~~shell\ndeno compile --allow-net --allow-read webserver_v3.js\n./webserver_v3\n~~~\n\nI point the web browser at <http://localhost:8002>. What do I see? I see the contents of the index.html file. Can I display \"helloworld.html\" too? Yep. I remove the \"index.html\" file, then use my browser back button to go to the initial URL, yep I see a file directory listing again. Looks like this prototype works.\n\nI think I have a useful localhost static content web server. It's time to rename my working prototype, compile and install it so it is available in my toolbox.\n\n1. Copy `webserver_v3.js` to `webserver.js` \n2. Use `deno compile` to create an executable\n3. Create a \"$HOME/bin\" directory if necessary\n4. Move the executable to a location in the executable PATH with, example \"$HOME/bin\"\n5. Try running the program\n\nNOTE: If you are following along and have to create \"$HOME/bin\" then you may need to added to your environment's PATH.\n\nOn macOS and Linux\n\n~~~shell\ncp webserver_v3.js webserver.js\ndeno compile --allow-net --allow-read webserver.js\nmkdir -p $HOME/bin\nmv ./webserver $HOME/bin\nwebserver\n~~~\n\nOn Windows\n\n~~~shell\ncopy webserver_v3.js webserver.js\ndeno install --global --allow-net --allow-read webserver.js\nNew-Item -ItemType Directory -Path \"$HOME\\bin\" -Force\nmove webserver.exe $HOME\\bin\\\nwebserver\n~~~\n\nThere you have it. I have a new convenient static web server for serving static content on localhost.\n\n",
      "data": {
        "abstract": "This post discusses static web server implementation using Deno.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2025, R. S. Doiel",
        "dateCreated": "2025-06-30",
        "dateModified": "2025-07-01",
        "keywords": [
          "web service",
          "static web site",
          "JavaScript"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 8,
        "pubDate": "2025-06-30",
        "series": "Deno and TypeScript",
        "title": "Build a Static Web Server with Deno"
      },
      "url": "posts/2025/06/30/Build_a_Static_Web_Server.json"
    },
    {
      "content": "\n# Rethinking REST\n\n## How to embrace the read write abstraction using SQL databases\n\nBy R. S. Doiel, 2025-06-07\n\n[Roy Fielding](https://en.wikipedia.org/wiki/Roy_Fielding)'s 2000 dissertation describing [REST](https://en.wikipedia.org/wiki/REST) is a brilliant work. It revolutionized web services. I've spent a good chunk of my career implementing back end systems using a REST approach. REST's superpower is the mapping of HTTP methods to core database operations of create (POST), read (GET), update (PUT), and delete (DELETE). The has simplified machine to machine communication. That is a good thing.\n\nREST has a browser problem. A quarter century after Fielding presented REST, the web browser still requires JavaScript to talk directly to a REST service. The core problem is the REST methods are not defined in the semantics of HTML. They are only available in HTTP protocol layer. JavaScript plays the role of solving the mapping of actions to REST methods. I can program over that impedance server side, browser side or both. The penalty is increased complexity. I think this complexity unnecessarily.\n\n**What abstraction aligns with the grain of both web service and web browser?**\n\nSir Tim invented HTTP and HTML on a NeXT cube. The NeXT cube was a Unix system like the systems used by the Physicists at CERN where Sir Tim was employed. From Unix you can trace the concept of \"everything is a file\". File interaction can be boiled down to reads and writes. A second influence was the practice of using plain text to encode data. These characteristics influenced Sir Tim's choices when he invented HTTP and HTML. These characteristics inform the grain of the modern web.\n\nWhat are the challenges of building on a read write abstraction rather than the database abstraction of create, read, update and delete? Do we toss out the database completely? That would be a too high of a cost.  Databases solve some important problems. This includes managing concurrent access, data protections and versatile query support. Database are the right choice in most cases for web applications. **So how do I get to a read write (RW) abstraction? The database wants create, read, update and delete (CRUD)?**\n\nThe short answer is we already do it. It's just messy. Typically we do this server side. It can be implemented browser side using JavaScript. Sometimes both places. We may layer that step as a micro service or embedded in some monolithic monstrosity. It's there someplace. It doesn't need to be a mess.\n\nLet's consider that for a moment. Server side the web service receives a request containing web form data. The service decodes the web form, hopefully validates the contents, then figures out if it is a \"create\" or \"update\" in the database system before attempting either an `insert` or `update` operation. The database schema usually reflects the form data. If the form has repeating fields then you might have more than one table and need to maintain relationships between the tables. This can quickly become complex.\n\nServer side this complexity was answered via object relational models (ORM).  Browser side we've seen similar approaches to the ORM in the development of frameworks that \"bind\" data in to an object model that can be sent to a back end system (often a REST API). The problem with both the server side ORM and browser side data binding frameworks is they tend to add allot of complexity. Ultimately they wind up dictating the approach you take to solve problems. Over time the frameworks become more complex too as they try to be a generalize solution to complex schema implementations. This accrues another source of complexity. The price of either becomes loss of flexibility, loss of performances and often deep levels of knowledge about the framework or ORM.  The longer lived your application is the more likely that this will not end well. I believe we can avoid this by taking stock of where database systems and the web have evolved since 2000.\n\n\n**What am I proposing?**\n\nLet's look at the deepest layer in our stack, the relational database. Several changes have happen on the database side that I think can help us build web application aligned with the read write abstraction core to our web browsers.  The first is a concept called upsert. Upsert is the idea of combining the behavior of `insert` and `update` into one operation. The upsert gives us our write operation.\n\nWhat about the mapping of a web form's data?  The second change in relational database world is the wide adoption of JSON column support. We can treat web form contents as a JSON expression. Modern SQL can query the JSON columns along with the other supported data types.\n\nA third changed was the arrival of SQLite in 2000. SQLite is SQL engine that does not require a separate database management system. Since 2000 SQLite has grown in usage. It now is used more commonly than Microsoft SQL server, Oracle, MySQL or PostgreSQL. The old requirement of using a stand alone database management system as part of the web stack has now turned into an option.\n\nSQLite3 provides support for both JSON columns and upsert. The upsert concept is implemented as an `on conflict` clause in your `insert` statement.  SQLite3 also support SQL triggers. Using the JSON column, upsert and triggers the SQLite3 database can handle the mapping of data as well as mapping our read write (RW) operations to the database CRUD operations. Better yet SQLite3 is an embedded SQL engine so you don't have to run a database management system at all. \n\nUse of JSON columns can radically simplify your JSON schema for many use cases. The model I am suggesting can be used to implement simple content management systems, metadata managers and form processor systems. Here's a table design suitable to many simple web applications.\n\n~~~SQL\nCREATE TABLE IF NOT EXISTS data (\n   id TEXT NOT NULL PRIMARY KEY,\n   src JSON DEFAULT NULL,\n   updated DATETIME DEFAULT CURRENT_TIMESTAMP,\n   version INT DEFAULT 0\n);\n~~~\n\nThe `id` holds a unique identifier like a file path does in a file system. The `src` column holds our JSON source. The `updated` column records the ISO-8601 timestamp of when your object is updated.  You might be wondering about `version` column and a missing `created` column. SQL can be used to automate data versioning and reduce create and update into a write operation. This is done by adding a second table. The scheme change in the second table from the first is how the primary key is defined.\n\n~~~SQL\nCREATE TABLE IF NOT EXISTS data_history (\n   id TEXT NOT NULL,\n   src JSON DEFAULT NULL,\n   updated DATETIME DEFAULT CURRENT_TIMESTAMP,\n   version INT DEFAULT 0,\n   PRIMARY KEY (id, version)\n);\n~~~\n\nThe SQL engine (SQLite3) does the actual version management using an SQL trigger. The \"on conflict\" of an insert triggers an \"update\" operation. The \"update\" action then triggers the `write_data` action before it completes.\n\nHere is how our upsert is implemented.\n\n~~~SQL\nINSERT INTO data (id, src) values (?, ?) \nON CONFLICT (id) DO\n  UPDATE SET src = excluded.src\n  WHERE excluded.id = id;\n~~~\n\nThe `write_data` trigger is responsible for two things. Inserts a new row into the `data_history` table using the the current row's values. Next it updates the `data` table's `version` number and `updated` timestamp automatically.\n\n~~~SQL\nCREATE TRIGGER write_data BEFORE UPDATE OF src ON data\nBEGIN\n  -- Now insert a new version into data_history.\n  INSERT INTO data_history (id,src, updated, version)\n    SELECT id, src, updated, version FROM data WHERE id =id; \n  -- Handle updating the updated timestamp and version number\n  UPDATE data SET updated = datetime(), version = version + 1\n    WHERE old.id = new.id;\nEND; \n~~~\n\n\nSo when I insert a new object there is no conflict so a simple insert is performed on the `data` table.  The row's version and `upgrade` columns get populated by the schema defaults. The next time the row is update it triggers the `write_data` operation where the row is recorded (copied) to the `data_history` table before being updated to reflect the changed values.\n\nHow do you find out when a record was created without a column called created?\n\nIn the follow SQL we perform a left join with the `data_history` table. We filter the history table for a row with the same id but a version number of 0. If a row is found then the value of `data_history.updated` will not be null. A `ifnull` function can be used to pick that value otherwise we use the `data.updated` value from `data` table. Here is how that SQL would look.\n\n~~~SQL\nSELECT data.id as id, \n  data.src as src,\n  data.updated as updated,\n  ifnull(data_history.updated, data.updated) as created,\n  data.version\nFROM data LEFT JOIN data_history ON\n  ((data.id = data_history.id) and (data_history.version = 0))\nWHERE data.id = ?;\n~~~\n\nThe complexity of mapping CRUD to RW is now completely contained in the SQL engine. While I have use SQLite3 for this specific example in practice these features are available in most modern relational database management systems. It's matter of knowing the specifics of the dialect.\n\nIsn't this a whole lot of SQL to write? Perhaps. By leveraging JSON columns the needs to elaborate on this SQL are minimal. Effectively these four statements can function like an SQL component. I think the investment is small. It solves a large class of web application storage needs.  You could even use a template to automatically generate them. Once written your can re-use them as needed.\n\n**Why did I focus on SQLite3?**\n\nBecause reducing the layers in our web stack reduces complexity. With SQLite3 we don't need database management system running. It's one less thing to manage, monitor and defend. In a cloud environment it can mean renting one less service.\n\n**What layers remain? What are their responsibilities?**\n\nIn 1999 web applications had a data management component, a user management component and an authentication and authorization component. The point of the application was the data management component. You were required to implement the others to keep the data safe while it was on the Internet.\n\nToday authentication and authorization can be handled by single sign-on systems. In the academic and research settings you typically see combinations like Apache2 + Shibboleth or NginX + Shibboleth. On the commercial Internet you see systems like OpenID and OAuth2. For a decade or more the systems I've designed and implemented take advantaged of single sign-on.  My application doesn't have to have a user management component or an authentication and authorization component at all.\n\nI do need a layer that validates the inputs and returns the resources requested. I usually implement this as a \"localhost\" web service that relies on the \"front end\" web service for authentication and authorization. If my layer uses SQLite3 for data storage then the \"stack\" is just a \"front end\" web server providing authentication and authorization and a \"back end\" persistence layer providing validation, storage and retrieval.\n\nAn advantage of this simple stack is I can develop, test and improve the localhost web service and know it'll plug into the front end when I am ready for a production deployment.  The front end deals in requests and responses, the back end deals in requests and responses. Meanwhile I have all the advantages of a SQL database on the \"back end\".\n\nAre there times I might need more layers?  Sure.  If I was managing millions of objects I would not store them in a single SQLite database.I'd use a database management system like PostgreSQL.  If I need a rich full text search engine I might use Solr or Open Search for that. If I am storing large objects then I might have a middle ware that can speak S3 protocol to store or retrieve those objects. My point is those are no longer a requirement. Extra layers or parallel services are now only options. They are available if and only if I need them.\n\nExample.  If I want to basic full text search, SQL databases have index types that support this.  SQLite3 is included there.\nBy leveraging SQL triggers I can extract data from my stored JSON column and populate full text search columns or even other tables as needed.I can get allot of the advantages of a full text search before I reach for an external system like Solr.\n\nSo here are my take way items for you.\n\n1. The web and databases continue to evolve.\n2. Take advantage of the improvements to simplify your code and your implementations\n3. Evaluate if you really need that heavy stack when you build your next application\n4. Use the simplest of abstractions that solve the problem required\n5. Consider a simple data interaction model like read write before you reach for REST\n\nEnjoy.\n",
      "data": {
        "abstract": "I am re-thinking my reliance on REST's implementation of the CRUD abstraction in favor of the simpler\nread write file abstraction in my web application. This can be accomplished in SQL easily. This post\ncovers an example of doing this in SQLite3 while also implementing JSON object versioning.\n\nCoverted are implenting the write abstraction using an upsert operation based on `insert` and SQLite3's\n`on conflict` clause. The object versioning is implemented using a simple trigger on the JSON column.\nThe trigger maintains the version number and updated timestamp.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2025, R. S. Doiel",
        "dateCreated": "2025-05-31",
        "dateModified": "2025-06-09",
        "keywords": [
          "web service",
          "web applications",
          "web browsers",
          "REST",
          "read write web",
          "SQL",
          "SQLite3"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 7,
        "pubDate": "2025-06-08",
        "series": "SQL Reflections",
        "title": "Rethinking REST"
      },
      "url": "posts/2025/06/07/Rethinking-REST.json"
    },
    {
      "content": "\n# PowerShell and Edit on Windows, macOS and Linux\n\nBy R. S. Doiel, 2025-06-05\n\nOne of the challenges of multi platform support is the variance in tools. Unix and related operating systems are pretty unified these days. The differences are minor today as opposed to twenty years ago. If you need to support Windows too it's a whole different story. You can jump to Linux Subsystem for Windows but that is really like using a container inside Windows and doesn't solve the problem when you need to work across the whole system. \n\nWindows' shell experience is varied. Originally it was command com, essentially a enhanced CP/M shell. Much later as Windows moved beyond then replaced MS-DOS they invented PowerShell. Initially a Windows only system. Fast forward today things have change. PowerShell runs across Windows, macOS and Linux. It is even licensed under an MIT style license.\n\n- <https://github.com/PowerShell/PowerShell>\n\nPowerShell is intended as a system scripting language and as such is focused on the types of things you need too to manage a system. It has vague overtones of Java, .NET and F#. If you are familiar with those it probably feels familiar for me it wasn't familiar. Picking up PowerShell has boiled down to my thinking I can do X in Bash then doing a search to find out the equivalent in PowerShell 7 or above.  There are something examples out there that are Windows specific because there isn't a matching service under that other operating systems but if you focus on PowerShell itself rather than Windows particular feature it is very useful. It also means while you're picking up how Windows might approach something you can re-purpose that knowledge on the other operating systems. That's really handy for admin type tasks.\n\nOne of the things I've been playing with is creating a set of scripts that have a common name but deal with the specifics of the target OS. That way when I need to run a generalized task I can deploy the OS specific version to the platform but then start thinking about managing the heterogeneous environments in a unified way. E.g. scripts like \"require-reboot.ps1\", \"safe-to-reboot.ps1\", \"disk-is-used-by.ps1\".\n\nOnce you start getting serious about learning a system admin script language you also learn you need to vet the quality of your the scripts you are writing. On Unix I use a program called [shellcheck](https://www.shellcheck.net/) and [shfmt](https://github.com/patrickvane/shfmt) to format my scripts. How do you do that for Powershell?\n\nRecently I discovered recently is PSScriptAnalyzer. Like shellcheck it will perform static analysis on your script and let you know of lurking issues to be aware of. The beauty of it is I can evaluate a script in PowerShell on macOS and know that I've caught issues that would have pinched me if I ran it Linux or Windows.  That's kinds of sweet.\n\nYou need to [install PSScriptAnalyzer](https://learn.microsoft.com/en-us/powershell/utility-modules/psscriptanalyzer/overview?view=ps-modules) but it is easy to do under PowerShell.\n\n~~~pwsh\nInstall-Module -Name PSScriptAnalyzer -Force\n~~~\n\nor\n\n~~~pwsh\nInstall-PSResource -Name PSScriptAnalyzer -Reinstall\n~~~\n\nIf I want [run the analyzer](https://learn.microsoft.com/en-us/powershell/utility-modules/psscriptanalyzer/using-scriptanalyzer?view=ps-modules&source=recommendations) on a script called `installer.ps1` I'd run lit like\n\n~~~psh\nInvoke-ScriptAnalyzer -Path ./installer.ps1 -Settings PSGallery -Recurse\n~~~\n\nFormatting PowerShell scripts I am currently testing out [PowerShell-Beautifier](https://github.com/DTW-DanWard/PowerShell-Beautifier). It's a \"cmdlet\" and an easy install into PSGallery following the instructions in the GitHub repo.\n\nHere's an example of formatting the previous example so it uses tabs instead of spaces.\n\n~~~pwsh\nEdit-DTWBeautifyScript ./installer.ps1 -IndentType Tabs\n~~~\n\n# Now that I got a shell running across systems, what about an editor?\n\nMS-DOS acquired an editor called \"edit\" at some point in time (version 4 or 5?).  I remember it was a simple full screen non model editor. Recently Microsoft has created a similar editor that runs in a terminal on Windows and Linux. Like PowerShell it arrived as an Open Source Project. You don't mind some rough edges I have compiled it successfully on macOS. A few features are not implemented but it can open and save files.  It builds with the latest Rust (e.g. run `rustup update` if you haven't in a while) and generates an installable executable using Cargo. While I'm not likely to switch editors it's nice to have one that is really cross platform in doesn't require odd Unix adapter libraries to be installed to compile it. It's just a Rust program so like Go the build process is pretty consistent when you compile on Windows, Linux or macOS.\n\n- <https://github.com/Microsoft/edit>\n\nSo you have an administrative shell environment and a common editor. If you happen to have to document admin chores across systems at least now you can document one admin language and editor and know it'll run if installed.\n",
      "data": {
        "author": "R. S. Doiel",
        "dateCreated": "2025-06-05",
        "keywords": [
          "Windows",
          "macOS",
          "Linux"
        ],
        "pubDate": "2025-06-05",
        "title": "PowerShell and Edit for macOS, Linux and Windows"
      },
      "url": "posts/2025/06/05/PowerShell_and_Edit.json"
    },
    {
      "content": "\n# A quick note on types in Deno+TypeScript\n\nUnderstanding the plumbing of a program that is built with Deno in TypeScript can be challenging if you can't identify the type of variables or constants.  TypeScript inherits the JavaScript function, `typeof`. This works nicely for simple types like `string`, `boolean`, `number` but is  less useful when compared to a class or interface name of a data structure.\n\nThere are three approaches I've found helpful in my exploration of type metadata when working with Deno+TypeScript. (NOTE: in the following\nthe value `VARIABLE_OR_CONSTANT` would be replaced with the object you are querying for type metadata)\n\n`typeof`\n: This is good for simple types but when a type is an object you get `[object object]` response.\n\n`Object.protototype.toString.call(VARIABLE_OR_CONSTANT)`\n: This is what is behind the `typeof` function but can be nice to know. It returns the string representation of the `VARIABLE_OR_CONSTANT` you pass to it.\n\n`VARIABLE_OR_CONSTANT.constructor.name`\n: This will give you the name derived from the object's constructor, effectively the class name. It doesn't tell you if the the `VARIABLE_OR_CONSTANT` is an interface. If you construct an object as an object literal then the name returned will be `Object`.\n\nHere's the three types in action.\n\n~~~TypeScript\n  let fp = await Deno.open('README.md');\n  console.log(typeof(fp));\n  console.log(Object.prototype.toString.call(fp);\n  console.log(fp.constructor.name);\n  await fp.close()\n  \n  let t = { \"one\": 1 };\n  console.log(typeof(t));\n  console.log(Object.prototype.toString.call(t);\n  console.log(t.constructor.name);\n~~~\n\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-05-25",
        "keywords": [
          "TypeScript",
          "Deno"
        ],
        "pubDate": "2025-05-25",
        "series": "Deno+TypeScript",
        "title": "A quick note on types in Deno+TypeScript"
      },
      "url": "posts/2025/05/25/a_quick_notes_on_types.json"
    },
    {
      "content": "\n# New Life for Fielded Searches\n\nBy R. S. Doiel, 2025-04-10\n\n[Simon Willison](https://simonwillison.net/2025/Apr/9/an-llm-query-understanding-service/) posted an article yesterday that caught my eye. He was pointing out how even small language models can be used to breakdown a search query into fielded tokens.  Some of the earliest search engine, way before Google's one box, search engines were built on SQL databases. Full text search was tricky. Eventually full text search become a distinct service, e.g. [Solr](https://solr.apache.org). The full text search engine enabled the simple search to become the expected way to handle search. Later search engines like Google's use log analysis to improve this experience further. When you use a common search string like \"spelling of anaconda\", \"meaning of euphemism\", \"time in Hawaii\" these results are retrieved from a cache. The ones that are location/time sensitive can be handled by simple services that either populate the cache or return a result then populate the cache with a short expatriation time. Life in search land was good.  Then large language models hit the big time and the \"AI\" hyperbole cranked up to 11.\n\nThere has been flirtation with replacing full text engines or more venerable SQL databases with large language models.  There is a catch. Well many catches but let me focus on just one. The commercial large language models are a few years out of date. When you use a traditional search engine you expect the results to reflect recent changes. Take shopping a price from two years ago isn't has useful as today's price given the tariff fiasco. Assembling large language models takes allot of time, compute resources and energy. Updating them with today's changes isn't a quick process even if you can afford the computer and energy costs. So what do? One approach as been to allow the results of a large language model to have agency. Agency is to use a traditional search engine to get results.  They're serious challenges with this approach. These include performance, impact on other web services and of course security.\n\nWhat if the language model is used in the initial query evaluation stage?  This is what Simon's article is about. He points out that even a smaller language model can be used to successfully take a query string and break it down into a fielded JSON object. Let's call that a search object. The search object then can be run against a traditional full text search engine or a SQL engine. These of course can be provided locally on your own servers.  [Ollama](https://ollama.app) provides an easy JSON API on localhost that can be used as part of your query parsing stack. This may be leveling especially if your organization has collection specific content to search (e.g. a library, archive or museum).\n\nConstructing an language model enable stack could look something like this.\n\n1. front end web service (accepts the queries)\n2. Ollama is used to turn the raw query string into a JSON search object\n3. The JSON search object is then sent to your full text search engine or SQL databases to gather results\n4. results are formatted and returned to the browser.\n\nThe key point in Simon Willison's post is that you can use a smaller language model. This means you don't need more hardware to add the Ollama integration. You can shoe horn it into your existing infrastructure. \n\nThis pipeline is a simple to construct.  This trick part is finding the right model and evaluating the results and deciding when the LLM translation to a JSON search object is good enough. Worst case is the original query string can still be passed off to your full text engine. So far so good. A slightly more complex search stack with hopefully improved usefulness.\n\n## a few steps down the rabbit hole\n\nWhere I think things become interesting is when you consider where search processing can happen. In the old days the whole stack had to be on the server. Today that's isn't true.  The LLM piece might still be best running server side but the full text search engine can be provided along with your statically hosted website. You can even integrate with a statically hosted JSON API. In light of that let's revisit my sequence.\n\n1. Front end web service response with a search page to the browser\n2. Browser evaluates the search page, gets the query string\n3. The browser then sends it to the Ollama web service that is returns a JSON search object (fielded object)\n4. The browser applies the object to our static JSON API calculating some results, it also runs query string through the static site search getting results\n5. results are merged and displayed in the browser\n\nSo you might be wonder about the static site search (browser side search) and JSON API I mentioned. For static site search I've found [PageFind](https://pagefind.app) to be really handy. For the JSON API I'd go with [FlatLake](https://flatlake.app). The two eliminate much of what used to be required from dynamic websites like those provided by Drupal and WordPress. A key observation here is that your query string only leaves the browser once in order to get back the language model result. This is a step toward a more private search but it doesn't pass the goal of avoiding log traces of searches.\n\nI first encounter browser side search solution with Oliver Nightingale's [Lunrjs](https://lunrjs.com). I switch to PageFind because Cloud Cannon had the clever idea to partition the indexes and leverage WASM for delivering the search. PageFind has meant providing full text search for more than 10K objects.\n\n**Could a PageFind approach work for migrating the language model service browser side?**\n\nIf the answer were yes, then would be a huge win for privacy. It would benefit libraries, archives and museums by allowing them to host rich search experiences while also taking advantage of the low cost and defensibilty of static site deployments.\n\n",
      "data": {
        "abstract": "A day dreaming in response to a Simon Willison post on using language models\nto convert queries into fielded searches. In this post I extrapolate how this\ncould result in a more private search experience and allow for an enhanced\nsearch experience for static websites.\n",
        "author": "R. S. Doiel",
        "dateCreated": "2025-04-10",
        "dateModified": "2025-04-10",
        "keywords": [
          "search",
          "LLM",
          "Ollama",
          "PageFind",
          "FlatLake"
        ],
        "number": 3,
        "pubDate": "2025-04-10",
        "series": "Personal Search Engine",
        "title": "New Life for Fielded Search"
      },
      "url": "posts/2025/04/10/New_Life_for_Fielded_Search.json"
    },
    {
      "content": "\n# LLM first impressions a few weeks in\n\nBy R.S. Doiel, 2025-03-30\n\nWriting code with an LLM is a far cry from what the hype cycle promises. It requires care. An attention to detail. It imposes significant compute resources. The compute resources requires a significant amount of electricity consumption. It doesn't bring speed of usage. Even cloud hosted LLM are slow beyond the first few iterations. If you want to find a happy medium between describing what you want and how you want it done, you need to commit to a non trivial amount of effort. Depending on your level of experience it may be faster to limit code generation to specific parts of a project. It maybe faster to code simple projects yourself.\n\nWhen I compare working with an LLMs like Gemma, Llama, Phi, Mistral, Chat-GPT to traditional [RAD](https://en.wikipedia.org/wiki/Rapid_application_development \"Rapid Application Development\") tools the shiny of the current \"AI\" hype is diminished. RAD tools often are easier to operate. They have been around so long we have forgotten about them. They use significantly less compute resources. RAD tools are venerable in 2025.\n\nThe computational overhead as well as the complexity of running an integrated LLM environment that supports [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation \"Retrieval Augmented Generation\"), [agency](https://en.wikipedia.org/wiki/Software_agent \"software agent explained\") and [tools](https://www.forbes.com/councils/forbestechcouncil/2025/03/27/your-essential-primer-on-large-language-model-agent-tools/ \"A Forbes article on tool use with large language models\") is much more than a simple code generator. A case in point. The best front end I've tried so for in my LLM experiments is [Msty.app](https://mysty.app). It takes a desperate set of services and presents a near turn-key solution. You don't even need to install [Ollama](https://ollama.com) as it ships with it. It checks all the boxes, RAG, agency and tools. But this simplicity only masks the complexity. Msty is a closed source solution. It is only offered on the x86 platform. The x86 computers are known for their energy consumption. So that's a downer. There are more energy efficient ARM and RISC-V solutions out there.  A Raspberry Pi 500 can run Ollama and should also be able to run Msty.app. But since Msty.app is closed source, I can't download and compile an ARM version myself. I can't compile a version for my Windows ARM machine either. That machine has much more RAM than the Pi. It even has more RAM than the x86 Windows box I run Msty on. Unfortunately I'm out of luck, the best I can do is running Msty.app under emulation. Not ideal. Scaling up with Msty.app means using a remote hosted LLM solution. How much energy am I saving when I run remote? When things go South the complexity costs in diagnosing problems with distributed systems is more than running things on a single box. I understand their business model.  Providing a Freemium version is a classic loss leader. People who like Msty will pay to upgrade to paid plans. From a big picture perspective it makes less sense to me.\n\n- How do we get more bang for the energy consumed?\n- Does the LLM enhanced coding environment push development forward?\n- Are LLM yet an appropriate tool in the toolbox of our profession?\n- Why am I continuing to explore using LLM for code development?\n\nA few weeks in I am looking for reasons to continue exploring LLM for code generation. I have a much better understanding of how it could fit into my approach to software development. I see reason to be hopeful but I also continue to have serious reservations.\n\nMy hands and wrists aren't the same as when I started in my career. I am hoping that an LLM can reduce my typing and reduce the risk of [RSI](https://en.wikipedia.org/wiki/Repetitive_strain_injury \"repetitive strain injury\"). I am also hoping that I can reduce the need for close reading and therefore eye strain. I'm currently experimenting with taking advantage of the slowness of the LLM eval cycle. Accessibility features in the OS will read back parts of the screen for me. Are these features worth burning down forests, floods and other climate driven issues? No they are not. Am I convinced that these LLM benefits will pan out? No. I'm hoping this aspect of LLM usage gets studied. I'm generally interested in tools that are a good ergonomic benefit for developers over the long haul. We still need a livable planet regardless.\n\nWithout using an LLM my normal development approach is to write out a summary of what I want to do in a project. I clarify the modules I want and the operations I need to perform. I code tests before coding the modules.  For many people this approach appears upside down. I've found this inverted approach yields better documentation. Writing the documentation clarifies my software architecture and how I will organize the required code. Writing tests first means I don't write more code than needed. The result is software I can look at a year or more later and still understand.\n\nThe first step in my current approach is well suited to adopting an LLM for code generation.  The LLM I've tried are decent at writing minimal test code. A good front end to Ollama or other LLM runner can automate some of the development cycle. The process is not fast. Once you get beyond the trivial the process is slow. It is slow like the compilers I used in my youth. It works OK for the most part. I can speed things up a little with faster hardware. That cost is increased electricity consumption. \n\nAn intriguing analog to developing with an LLM environment is [Literate programming](https://en.wikipedia.org/wiki/Literate_programming) advocated by [Knuth](https://en.wikipedia.org/wiki/Donald_Knuth). The LLM I've tried have used a chat interface. Even if the chat conversation is read back there is allot of duplicate text to listen to. Current chat responses are usually presented in Markdown with embedded code blocks. This is feels like a simplified version of Knuth's approach using TeX with embedded code blocks. It feels familiar to me. The Chat UI model presents many of the challenges I remember trying out literate programming. For some people a literate approach resulted in better code. I'm not sure it was true for everyone. For some an LLM might encourage more thinking as dialog can be useful that way. It might be easier to be more objectively when vetting because the ego isn't challenged in the generated code. It is hard to say with certainty.\n\nI have found one significant advantage of the LLM generated code. It is a byproduct of large training sets. The generated code tends to look average. It tends to avoid obfuscation. It mostly is reasonably readable. If the computing resources were significantly lower I'd feel more comfortable with this approach over the long run. The current state needs radical simplification. The LLM development environment is overly complex. If it is running on my hardware why does it need to be implemented as a web service at all? I think allot of the complexity is intentional. It benefits consolidation of tooling in the cloud. The cloud where we rent the tools of our trade. LLM development environment's complexity and energy consumption weight heavily on me as I explore this approach. It is too early to tell if it should be a regular tool in my toolbox. It is too early to know if it is useful for sustainable software practices.\n\n\n\n\n",
      "data": {
        "abstract": "A first take of LLM use for coding projects.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-03-30",
        "dateCreated": "2025-03-30",
        "dateModified": "2025-03-30",
        "pubDate": "2025-03-30",
        "title": "LLM first impressions a few weeks in"
      },
      "url": "posts/2025/03/30/LLM_first_impressions_a_few_weeks_in.json"
    },
    {
      "content": "\n# Building Web Components using Large Language Models\n\nBy R. S. Doiel, 2025-03-13\n\nIn March I started playing around with large language models to create a couple web components. I settled on a paid subscription for Mistral's Le Chat. You can see the results of my work project at <https://github.com/caltechlibrary/CL-web-compoments>. The release at or before \"v0.0.4\" were generated using Mistral's Le Chat.\n\nThe current state of LLM offered online do not replace a human programmer. I'm sure that is surprising to those who thing of this as a solved problem. I found that is was my experience as a developer that I could detect the problems in the generated code. It was also required in coming up with the right prompts to get the code I wanted. \n\nThe key to success of using an LLM for code generation is domain knowledge. You need domain knowledge of the problem you're solving by creating new software and domain knowledge of the machine or run time that the software will target.\n\nThe ticket to working with an LLM using a chat interface is your domain knowledge. That's how you know what to ask. That starting point is useful when using the LLM explore **widely documented** topics and approaches. The LLM is not good at inferring novel solutions but rather at using what it has been trained on.  That has been the key with the current crop of LLM I tried.\n\nI think using an LLM to generate code alongside a human client has potential. The human client brings subject knowledge. The human software developer brings more domain knowledge. Between the two they can guide the LLM in generating useful code. Its a little like training a literalist four year old that has the ability to type. I think this three way collaboration has possibilities. The LLM may prove helpful in bridging the human client and software developer divide.\n\n## my experiment proceeded\n\nWorking with a chat interface and LLM is a non-trivial iterative process. It can be frustratingly slow and pedantic. I often felt I took two steps forward then a step backward. I am unsure if I arrived at the desire code faster using the LLM. I am unsure the result was better quality software.\n\nI spent two weeks of working with a Mistral's Le Chat LLM on two web components. I am happy with the results at this stage of development. I am not certain the web components will continue to be developed with an LLM. My experience left me with questions about how LLM generated code will help or hurt sustainable software.\n\n## Generating web components\n\nMy experiment focused on two web components I needed for a work project. The first successfully completed component was called `AToZUL` in \"[a_to_z_ul.js](https://raw.githubusercontent.com/caltechlibrary/CL-web-components/refs/heads/main/a_to_z_ul.js)”. This web component is intended to wrap a simple UL list of links. It turns the wrapped UL into an A to Z list. Taking the wrapping approach of a native HTML element was my idea not the LLM's. I asked the LLM to implement a fallback but each LLM I tried this with inevitably relied on JavaScript. This fails when JavaScript is unavailable in the browser[^1]. How can the LLM do better than rehashing of the training data?\n\n[^1]: By providing a non JavaScript implementation I can interact with the web page using terminal based browser or using simple web scraping libraries.\n\nThe second web component successfully generated is called `CSVTextarea` in the \"[csvtextarea.js](https://raw.githubusercontent.com/caltechlibrary/CL-web-components/refs/heads/main/csvtextarea.js)\". This web component wraps a TEXTAREA that has CSV data in it's inner text. The web component presents a simple table for data entry. It features auto complete for columns using DATALIST elements. The `CSVTextarea` emits \"focused\" and \"changed\" events when cells receive focus or change. Additional methods are implemented to push data into the component or retrieving data from the component. Both web components include optional attributes \"css-href\" to include CSS that overrides the default that ships with the component.\n\nAt the start of March a spent a couple days working with the free versions of several LLM. I found Chat-GPT to be useless. I found Claude and Mistral to be promising. In all cases I found the \"free\" versions to be crippled for generating web components. I settled on a paid subscription to Mistral's Le Chat. The code generation was nearly as good as Claude but less expensive per month.\n\nOut side of work hours I tested code generation for several personal projects as well as code porting.  I was unhappy with the results for porting code from Go to Deno 2.x and TypeScript. The LLM are not trained on recent data. They seem to be about two years out. This includes the paid ones. As the result the best I could do was generate code for Deno 1.x. \n\nTypeScript and Go have some strong parallels. I'd previously hand ported code. I compared my code with the LLM results and was surprised at the deficits in the generated code. I think this boils down to the training sets used. \n\nOf all the programming tasks I tried the best results for code generation were targeting JavaScript running in a web browser. This isn't surprising as the LLM likely trained on web data.\n\n## My development setup for the experiments\n\nI initially tried VSCode and CoPilot. For me it us annoying and highly unproductive[^2]. My reaction certainly is a reflection on my background. I prefer minimal IDE. I am happy with an editor that is limited to syntax highlighting, auto indent and line numbering. When I use VSCode I turn most features off. Your mileage may very with your preferences.\n\n[^2]: It was annoying enough that I initially dismissed using an LLM. My colleague, Tommy, however encouraged me to give it a more serious try. If you lived through the \"editor wars\" of the early ARPAnet get ready for them to reignite. We're going to see allot of organizations claiming superior dev setups for integrating LLM into IDE.\n\nI ran my tests using a terminal app, a text editor, a localhost web server and Firefox. The log output of the web server was available in one terminal window and another for my text editor. One browser tab was open to Mistral's Le Chat. The other tabs open to the HTML pages I used for testing the results of the generated code. It required a fair amount of cut and paste which is tedious. This is far from an ideal setup.\n\nI looked at [Ollama](https://ollama.com) to see about running the LLM from the command line.  Long run this seems like a better approach for me. Unfortunately to use Mistral.ai's trained models I would need to purchase a more expensive subscription. The price point is roughly the same as Anthropic's Claude, a closed sourced option. For now I am sticking with cut and paste.\n\nUsing Ollama there is the possibility of using Mistral's open source models and training them further on data I curate. At some point I'll give it a try. I remain concerned about the electric bill. If collectively we're going to run these systems they will need clean alternative sources of electricity. Otherwise we are in for an environmental impact like we saw with BitCoin and Block-Chains. I think this is a major problem in the LLM space. I don't think we can ignore it even when the \"AI\" hype cycle is hand waving it away.\n\nI liked using Ollama to test free models to understand their differences. I recommend this as an option if you are working from macOS or Windows. The quality of generated code varies considerably between models. It is also true that the speed and processing requirements varies considerably between models. I am sure this is why hardware vendors think they will be able to sell hardware with \"AI Chips\" built in. I'm skeptical.\n\nI think small language models targeting specific domains could really improve the use case for language models generating code. They could shine for specific tasks or programming languages. They might be reasonable to run on embedded platforms too. I think small language models are an overlooked area in the current \"AI\" hype cycle.\n\n## What I took away from this experiment\n\nI think a good front end developer[^3] could find an LLM useful as an \"assistant\". I think a novice developer will easily be lead astray. As a community of practitioners we should guard against the bias, \"computer must be correct\". This is not new. It happened when \"expert systems\" were the rage before the first AI winter. It'll be an easy trap to fall into for the public.\n\n[^3]: I am not a front end developer. I have spent most of my career writing back end systems and middleware.\n\nI have a great deal of concern about compromised training data. There is so much mischief possible. It has already been established that \"poisoning\" the LLM via training data and prompts will result in generating dubious code[^4]. I don't see much attention paid to the security and provenance of training data, let alone good tools to vet the generated code. This is a security bomb waiting to explode.\n\n[^4]: See [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1). Think of the decades waste on SQL injections then multiply that by magnitudes as people come to trust the results of LLM to build really complex things. \n\nToday's software systems are really complex. Reproducibility has become a requirement in mitigating the problem. This is why you've seen a rise in virtual environments. The LLM itself doesn't improve this situation. I've used the same text prompts with the same LLM but different chat session and gotten significantly different results. The LLM as presently implemented exacerbate the problems of reproducibility. \n\nTo an certain extent we can strengthen our efforts around quality assurance. The trouble I've found is this too is a domain where LLMs are being applied. If the quality assurance LLM is tainted we don't get the assurance we need. There also is the very human problem of typos in our prompts. That's a very deep rabbit hole by itself.\n\n## Lessons learned\n\nI got the best results by composing a long specification as an initial state prompt to kick off code generation. I still needed to fix bugs using short prompts interactively. With the CSVTextarea I threw away five versions before I got to something useful. Each chat session lasted a couple hours. They were spread out over many days.\n\nThere was a clear point when additional prompts don't improve the results of generated code. I found three cases where I lead the LLM down a rabbit hole.\n\n1. the terms I used weren't what the LLM was trained on so it couldn't respond the way I wanted\n2. the visible results, e.g. the web component failing to render, doesn't indicate the underlying problem, this leads to prompt churn\n3. their was a subtle assumption in the generated code I didn't pick up and correct early on\n\nA beneficial result of using an LLM to generate code is that it encourages reading the source. Reading code is generally not taught enough. You're going to be reading allot of code using the current crop of commercially available LLM. That is a good thing in my book.\n\nTo solve the rabbit hole problem I adopted the following practices. Keep all my prompts in a Markdown document, along with notes to myself. When I felt the LLM had gone down a rabbit whole have the LLM generate a \"comprehensive detailed specification\" of the code generated. Compare the LLM specification against my own prompts helped simplify things when starting over.\n\nBe willing to throw away the LLM results and start over. This is important in exploratory programming but also when you're using LLM generated code to solve a known problem. If you can tolerate the process of writing and refining the prompts in your native language the LLM will happily attempt to generate code for them. I'd love to get some English and Philosophy graduates using LLM for code generation. It'd be interesting to see how their skills may out perform those of traditionally educated software engineering graduates. I think the humanities fields that could benefit from a quantitative approach may find LLM to generate code to do analysis really compelling.\n\nWhile I like the results I got for this specific tests I remain on the fence about **general usefulness** of LLM in the area of code generation. I suspect it'll take time before the shared knowledge and practice in using them emerges. There is also the problem of energy consumption.  This feels like the whole \"proof of work\" problem consuming massive amounts of electricity in the block chain tech. That alone was enough to turn me off of block chain for most of its proposed applications. Hopefully alternatives will be developed to avoid that outcome with large language models.\n\n",
      "data": {
        "abstract": "Quick discussion of my recent experience bootstrapping the CL-web-components project\n",
        "author": "Doiel, R. S.",
        "byline": "R. S. Doiel",
        "dateCreated": "2025-03-13",
        "keywords": [
          "LLM",
          "Web Components",
          "JavaScript"
        ],
        "number": 2,
        "pubDate": "2025-03-13",
        "series": "Code Generation",
        "title": "Building Web Components using Large Language Models"
      },
      "url": "posts/2025/03/13/Building_Web_Component_using_an_LLM.json"
    },
    {
      "content": "\n# Moving beyond git template repositories with CodeMeta\n\nBy R. S. Doiel, 2025-01-31 (updated: 2025-02-03)\n\nA nice feature of GitHub is the ease in starting a new repository with a complete set of documents[^1]. This feature creates a problem. The \"template\" documents require editing. Then they require more editing to keep them from being stale. If you're serious about keeping documentation up to date then the copy edit work must be continuous. Copy edit work is tedious. Is there a path beyond the git repository templates that avoid stale [software artifacts](https://en.wikipedia.org/wiki/Artifact_(software_development))?\n\n[^1]: The feature is built around using another Git repository as a template. GitHub has a nice UI for this, essentially it is forking the \"template repository\" when you create your new repository.\n\nExisting development tools suggestion a solution. For decades software developers have had tools to extract documentation from the comments in source code.  [Knuth](https://en.wikipedia.org/wiki/Donald_Knuth) pioneered this with his [literate programming](https://en.wikipedia.org/wiki/Literate_programming) approach. A simplified take on this is [Javadoc](https://en.wikipedia.org/wiki/Javadoc) used in the Java programming community. Today most open source programming languages have similar tools available. In 2025 it is trivial to generate source code documentation directly from the comments in your source code. Can an approach like this be used for more of our software artifacts?\n\nI believe leveraging [CodeMeta](https://codemeta.github.io \"See the section titled, Motivation\"), developed by the research programming community, gives us a path forward for general software development.\n\n## How is CodeMeta relevant? \n\nThe CodeMeta data is meant to be machine readable and human manageable. That's a developer sweet spot. The CodeMeta object let's you track metadata like name, description, version, authorship and contributors. It includes where the project source repository is hosted and the license used. Over time the list of metadata attributes in the CodeMeta object has grown. Today we have a long list of a project's metadata that can be tracked and acted upon.\n\nWhen I started including CodeMeta files in my projects at Caltech Library I did so at software release. It was a task to do at the end of the process like the task of reviewing and updating the cloned documentation files. Treating the CodeMeta as an after thought created a burden. It was a disincentive to adopting CodeMeta. \n\nI think we make the CodeMeta object relevant to developers by treating it as primary source code. Editing it should be the first rather than last thing updated before a release. **An accurate CodeMeta file is actionable.** Ignored this and you ignore CodeMeta's super power.\n\nCan CodeMeta be used to help lower the documentation burden? Yes.\n\nIn 2025 there is enough information in a complete CodeMeta object to create structured documents. This includes documents like a README and INSTALL. Keeping track of the content as metadata can lower the effort in managing stale documentation. It may be enough to eliminate the need for git template repositories. At a minimum it can shrink what is needed from template repositories.\n\nHere's the steps I used to take setting up a project.\n\n1. Write the README\n2. Document how the software will work\n3. Write tests to confirm the software works\n4. Write the software\n5. Create/update my software artifacts to reflect the current state such as the codemeta.json file.\n\nRepeat as needed. The last step was always tedious. The longer a project is around the more artifacts need to be managed. It was easy to want to short cut that last step.\n\nHere's what I have been experimenting with.\n\n1. Create or update the CodeMeta file\n2. Document or update how the software should work\n3. Create or update tests to confirm the software works as documented\n4. Write or update the software to pass the tests\n5. **Generate** additional software artifacts from the CodeMeta document.\n\nStep five is automated. In practice step five can be integrated with your standard build processes. Us humans focus on steps one through four. Life just got a little easier for the busy developer.\n\nSeveral questions are suggested by this proposal.\n\n1. How do I make it easy to create and update the CodeMeta file?\n2. How do I generate the software artifacts?\n3. What artifacts can be generated in a reliable way?\n\nThe CodeMeta object is stored as a JSON document in your repository. That means you can readily build tooling around it.\n\nToday you can use the CodeMeta [generator](https://codemeta.github.io/codemeta-generator/) to bootstrap the creation of a CodeMeta file.  If you're willing to do some cut and paste work the generator can even be used to maintain your CodeMeta file.  \n\nWhat about generating our artifacts?\n\nFor the last few years I've relied on Pandoc templates, see [codemeta-pandoc-examples](https://github.com/caltechlibrary/codemeta-pandoc-examples). I use Pandoc and these templates to generate files like CITATION.cff, about.md, installer scripts and version files for the Python, Go and TypeScript programming languages. The trouble with this approach is Pandoc tends to be well knowing in the data science community but not in the general developer community. The Pandoc template language is obscure. This has lead me to believe new tools are needed beyond Pandoc and templates.\n\n***\n\n## CMTools\n\nI recently started prototyping two programs for working with [CodeMeta](https://codemeta.github.io) objects. The prototype code is available at [github.com/caltechlibrary/CMTools](https://github.com/caltechlibrary/CMTools). The two programs are for editing (`cme`) and for transforming (`cmt`) the CodeMeta object. I am currently testing the prototypes in selected Caltech Library projects.\n\n### What challenges have the prototypes raise?\n\nThe CodeMeta object is sprawling. `cme` was started as command line alternative to the generator.  It was initially designed with an interactive, prompt and response, user interface. It would iterate over all the top level attributes prompting for changes. This can be tedious. I quickly added support to shorten the process by including a list of specific attributes to edit. \n\n~~~shell\ncme codmeta.json                       # <-- iterate over all\ncme codemeta.json version dateModified # <-- just listed attributes\n~~~\n\nThe prompt and response approach works well for simple attribute types like name, description and version. The more complex attributes like author or contributor were challenging. To avoid the need to increase the types of prompts or be forced into a menu system I'm experimenting with using YAML to display the current value and accept YAML as the user response. YAML is much easier to type and copy edit than JSON. It is easy to transform into JSON. The downside is you need to know the structure and attribute names ahead of time. That gives `cme` a training cost.\n\nMulti line values are tricky to work with if you rely on standard input. To address this I added a feature to allow the use the editor of your choice.  If you are on macOS or Linux the default editor is nano. On Windows it is notepad.exe.  You can pick a different editor by setting the EDITOR environment variable.  In the example below I've chosen the [Micro Editor](https://micro-editor.github.io) for setting values. It didn't solve the problem of knowing the YAML attributes in advance but does make it easier to copy edit the YAML. Micro Editor is Open Source and available for macOS, Linux and Windows. Support for other editors could be added. Further prototyping and development work is needed to support alternatives to editing YAML.\n\nOn macOS and Linux\n\n~~~shell\nexport EDITOR=micro\ncme codemeta.json author -e\n~~~\n\nor on Windows\n\n~~~ps1\nSet-Variable EDITOR micro\ncme codemeta.json author -e\n~~~\n\nIn the 0.0.9 release of the `cme` prototype I added the ability to set the attribute values from the command line. This has helped in automated environments. CodeMeta attribute name is used as the key. An equal sign, \"=\", is the delimiter. What follows the equal sign is treated as the value. This works well for simple fields, e.g. version, dateModified.\n\n~~~shell\ncme codemeta.json version=\"0.0.2\" dateModified=\"2025-01-30\"\n~~~\n\nComplex attribute editing using this approach is very challenging.\n\n### What can CMTools generate?\n\nThe `cmt` prototype has limited abilities. It can render about.md and CITATION.cff files. It can generate \"version\" source code files for Python (version.py), Go (version.go), TypeScript (version.ts) and JavaScript (version.js). I am actively working on porting the remaining Pandoc templates from codemeta-pandoc-examples to `cmt`. README and INSTALL will be added after the template port is complete.\n\n~~~shell\ncmt codemeta.json about.md CITATION.cff version.py \\\\\n  README.md INSTALL.md installer.sh installer.ps1\n~~~\n\n## What's next?\n\nCMTools is at an early stage of development (January 2025). The project is focused finding the balance of editing and generating. Improvements will flow base on our usage.\n\nThe [v0.0.14 release](https://github.com/caltechlibrary/CMTools/releases/tag/v0.0.14) includes the basic features discussed in this post for both `cme` and `cmt`. RSD 2025-02-03\n",
      "data": {
        "abstract": "An exploration of using CodeMeta objects to generate of software artifacts as an alternative to Git template repositories.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "createDate": "2025-01-30",
        "keywords": [
          "software development",
          "programming",
          "CodeMeta"
        ],
        "pubDate": "2025-01-31",
        "series": "Code Generation",
        "series_no": 1,
        "title": "Moving beyond git template repositories with CodeMeta",
        "updateDate": "2025-02-03"
      },
      "url": "posts/2025/01/31/moving_beyond_git_templates.json"
    },
    {
      "content": "\n# Raspberry Pi 4 & 400 Power Supply Issues\n\nBy R. S. Doiel, 2024-11-20\n\nI have a Raspberry Pi 4 and Raspberry Pi 400. The former builds my personal website and my [Antenna](https://rsdoiel.github.io/antenna). I use the latter as a casual portable development box.\n\nOver the last year or two I noticed I would get voltage warnings popping up. Typical this was prevalent when I was compiling Go, Pandoc or Deno. The power supply I was using was the Official Raspberry Pi 4 power supply that came with the Raspberry Pi Desktop Kit and Raspberry Pi 400 Computer Kit.  At first I thought the power supplies were going bad or had become damaged.  I tried replacing the power supply with one of the extras I had picked up (power supplies can and do fail). Still had problems with low voltage.\n\nI did some digging but found nothing useful than the old recommendations of making sure the power supplies were appropriately and to spec. I did read somewhere in my searching that the official power supplies didn't have allot of headroom. This got me thinking.  The Raspberry Pi OS has been updated many times since I got these devices. Maybe there are more power demands happening than I realized.  I've also moved away from using the internal micro SD cards to using an external thumb drive to boot, that would place an additional power requirement on the system.\n\nI looked at [PiShop.us](https://www.pishop.us) which is linked from the Raspberry Pi website as an official retailer.  They had the official supplies that came with my kits but also had one that had a higher wattage rating. I ordered [Raspberry Pi 27W USB-C Power Supply White US](https://www.pishop.us/product/raspberry-pi-27w-usb-c-power-supply-white-us/). Connected it up to my Raspberry Pi 400 via the powered monitor. Booted from the thumb drive and then tried rebuilding Go from Go 1.19 that ships with Raspberry Pi OS compiling Go 1.21.9 and 1.23.3 from source. No voltage errors.  Looks like I just out grew the stock power supply.  If this remains working without problems I will order another one and use it with my old Pi Drive enclosure and my Pi 4. Keeping my fingers crossed.\n\n\n\n\n",
      "data": {
        "abstract": "Quick notes on some low voltage issues I ran into with my Raspberry Pi 4 and 400 using the stock power supply with thumb drives.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-11-20",
        "createDate": "2024-11-20",
        "keywords": [
          "Raspberry Pi",
          "Power Supply",
          "Voltage errors"
        ],
        "pubDate": "2024-11-20",
        "title": "Raspberry Pi 4 & 400 Power Supply Issues"
      },
      "url": "posts/2024/11/20/power-supply-issues.json"
    },
    {
      "content": "\n# Two Rust command line tools for web work\n\nBy R. S. Doiel, 2024-11-06\n\nI've noticed that I'm using more tools written in the Rust language. I'd like to highlight two that have impressed me. Both are from [Cloud Cannon](https://cloudcannon.com). They make static website development interesting without unnecessarily increasing complexity.\n\nHistorically static site development meant limited interactivity browser side. Then JavaScript arrived. That enabled the possibility of an interactive static site. Two areas remained challenging. First was search. An effective search engine used to required running specialized software like [Solr](https://solr.apache.org) or using a SAAS[^1] search solution.  When the renaissance in static sites happened these options were seen as problematic.\n\n[^1]: SAAS, Software as a Service. This is how the web provides application software and application functionality. SAAS are convenient but often have significant drawbacks in terms of privacy and content ownership. SAAS dates back to pre-Google era. Early search engines like Alta Vista provided search as a service. Today many sites use Google, Bing or DuckDuckGo to provide search as a service.\n\nStatically hosted search arrived when [Oliver Nightingale](https://github.com/olivernn) created [LunrJS](https://lunrjs.com/). LunrJS provides a Solr like search experience run from within your web browser. You needed to write some JavaScript to generate indexes and of course JavaScript had to be available in the browser to run the search engine. In spite of having to write some JavaScript to perform indexing it was easier to setup and manage than Solr. LunrJS added benefit of avoiding running a server. Things were good but there were limitations.  If you wanted to index more than ten thousand or so objects the indexes grew to be quite large. This made search painfully slow. That's because your web browser needed to download the whole search index before search could run in the browser.  There were variations on Oliver's approach but none really seemed to solve the problem completely. Still for small websites LunrJS was very good.\n\nFast forward and a little over a decade ago Cloud Cannon emerged as a company trying to make static site content management easier.  One of the things they created was a tool called [PageFind](https://pagefind.app). Like LunrJS PageFind provides a search engine experience for static websites that runs in the browser.  But it includes a few important improvements. First you don't need to write a program to build your indexes. PageFind comes with a Rust based tool called, unsurprisingly, \"pagefind\". It builds the indexes for you with minimal configuration. The second difference from LunrJS is PageFind builds collection of partial indexes that can be loaded on demand. This means you can index sites with many more than 10K objects and still use a browser side search. That's huge. I've used it on sites with as many as hundred thousand objects. That's a magnitude difference content that can be searched! A very clever feature of PageFind is that you can combined indexes from multiple sites.  This means the search with my blog can also cover my GitHub project sites that use PageFind for their web documentation. Very helpful.\n\nPageFind does have limitations. It only indexes HTML documents. Unlike Solr it's not going to easily serve as a search engine for your collection of PDFs. At least without some effort on your part. Like LunrJS it also requires JavaScript to be available in the web browser. So if you're using RISC OS and a browser like NetSurf or Dillo, you're out of luck. Still it is a viable solution when you don't want to run a server and you don't want to rely on a SAAS solution like Google or Bing.\n\n> Wait! There's more from Cloud Cannon!\n\nIf you start providing JavaScript widgets for your website content you'll probably miss having a database backed JSON API. You can create one as part of your site rendering process but it is a big chore. Cloud Cannon, in their quest for a static site content management system, provides an Open Source solution for this too. It's called [FlatLake](https://flatlake.app). Like PageFind it is a command line tool. Instead of analyzing HTML documents FlatLake works on Markdown documents. More specifically Markdown documents with YAML front matter[^2]. It uses that to render a read only JSON API from the metadata in your documents front matter.  You define which attributes you want to expose in your API in a YAML file. FlatLake creates the necessary directory structure and JSON documents to reflect that. Want to create a tag cloud? Your JSON API can provided the data for that. You want to provide alternate views into your website such as indexes or article series views?  Again you now have a JSON API that aggregates your metadata to render that. Beyond some small amount of configuration FlatLake does the heavy lifting of generating and managing the JSON API for your site. It's consistent and predictable. Start a new site and add FlatLake and you get a familiar JSON API out of the box.\n\n[^2]: Front matter is a block of text at the top of your Markdown document usually expressed in YAML. Site building tools like [Pandoc](https://pandoc.org/chunkedhtml-demo/8.10-metadata-blocks.html#extension-yaml_metadata_block) can use the YAML block to populate and control templating. Similarly R-Markdown provides a similar functionality. FlatLake takes advantage of that. \n\n\n## PageFind and FlatLake in action\n\nMy personal website is indexed with PageFind.  The indexes are located at <https://rsdoiel.github.io/pagefind>. The search page is at <https://rsdoiel.github.io/search.html>. I index my content with the following command (which will run on macOS, Windows or Linux).\n\n~~~shell\npagefind --verbose --exclude-selectors=\"nav,header,footer\" --output-path ./pagefind --site .\n~~~\n\nThat command index all the HTML content excluding nav, header and footers.  The JavaScript in my [search.html](https://rsdoiel.github.io/search.html) page looks like this.\n\n~~~JavaScript\nlet pse = new PagefindUI({\n    element: \"#search\",\n    showSubResults: true,\n    highlightParam: \"highlight\",\n    mergeIndex: [{\n        bundlePath: \"https://rsdoiel.github.io/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/shorthand/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/pttk/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/skimmer/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/scripttools/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/fountain/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/osf/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/fdx/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/stngo/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/opml/pagefind\"\n    }]\n})\nwindow.addEventListener('DOMContentLoaded', (event) => {\n    let page_url = new URL(window.location.href),\n        query_string = page_url.searchParams.get('q');\n    if (query_string !== null) {\n        console.log('Query string: ' + query_string);\n        pse.triggerSearch(query_string);\n    }\n});\n~~~\n\nThis supports searching content in some of my GitHub project sites as well as my blog.\n\nOne of the things I am working on is updating how I render my website.  I have a home grown tool called [pttk](https://rsdoiel.github.io/pttk \"Plain Text Toolkit\") that includes a \"blogit\" feature. Blog it takes cares care of adding Markdown documents to my blog and generates a a JSON document that contains metadata from the Markdown documents.  That later feature is no longer needed with the arrival of FlatLake. FlatLake also has the advantage that I can define other metadata collections to include in my JSON API. The next incarnation of pttk will be simpler as the JSON API will be provided by FlatLake.\n\nFlatLake analyzes the Markdown documents in my website and build out a JSON API as folders and JSON documents. If I do this as the first step in rendering my site the rendering process can take advantage of that. It replace my \"blog.json\" file. It can even replace the directory traversal I previously needed to use with building the site. That's because I can take advantage of the exposed metadata in a highly consistent way. You can explore the JSON API being generated at <https://rsdoiel.github.io/api>. I haven't yet landed on my final API organization but when I do I'll be able to trim the code for producing my website significantly. Here's the outline I expect to follow.\n\n1. Run FlatLake on the Markdown content and update the JSON API content\n2. Read the JSON API and render my site\n3. Run PageFind and update my site indexes\n4. Deploy via GitHub Pages with `git` or `gh`\n\n## Installing PageFind and FlatLake\n\nBoth PageFind and FlatLake are written in Rust. If you have Rust installed on your machine then Cargo will do all your lifting.  When I have a new machine I install [Rust](https://www.rust-lang.org/) with [Rustup](https://rustup.rs). On an running Machine I run `rustup upgrade` to get the latest Rust.  I then install (or updated) PageFind and FlatLake with Cargo.\n\n~~~shell\nrustup upgrade\ncargo install pagefind\ncargo install flatlake\n~~~\n\nI've run PageFind on macOS, Windows, Linux. On ARM 64 and Intel CPUs. I even run it on Raspberry Pi!. Rust supports all these platforms so where Rust runs PageFind and FlatLake can follow.\n\nPageFind solves my search needs.  FlatLake is simplifying the tooling I use to generate my website. My plain text toolkit (pttk) needs to do much less. It feels close to the grail of static website management system built from simple precise tools. Git hands version control. Pandoc renders the Markdown to HTML. PageFind provides search and FlatLake provides the next generation JSON API. A Makefile or Deno task can knit things together into a publication system.\n",
      "data": {
        "abstract": "A quick review of a PageFind and FlatLake by Cloud Cannon. A brief description of how I use them.\n",
        "byline": "R. S. Doiel",
        "created": "2024-11-05",
        "keywords": [
          "web",
          "rust",
          "search",
          "JSON API",
          "Cloud Cannon"
        ],
        "pubDate": "2024-11-06",
        "title": "Rust tools for Web Work",
        "updated": "2024-11-06"
      },
      "url": "posts/2024/11/06/rust-tools-for-web-work.json"
    },
    {
      "content": "\n# When Deno+TypeScript, when Go?\n\nBy R. S. Doiel, 2024-11-06\n\nLast month I gave a [presentation](https://caltechlibrary.github.io/cold/presentations/presentation1.html) on a project written in [Deno](https://deno.com)+[TypeScript](https://typescriptlang.org). The project included code that needed to be shared server and browser side.  At the conclusion of my talk the question came up, \"When would I choose Go versus Deno+TypeScript for a project?\"[^1]. The answer I came up with at the time was I would choose Deno+TypeScript when I needed to share code between server and browser. That was the question that had lead me to Deno in the first place. Deno+TypeScript shared many of the advantages of Go[^2]. I like writing in Go. The weak point in my Go based projects I thought was limited to porting Go code to JavaScript when I needed it run browser side. I went home and slept well that night. Since then I have been reflecting a little more on the question. \n\n[^1]: I have presented allot of Go based projects to the SoCal Code4Lib group\n\n[^2]: Deno lets to cross compile TypeScript to binary executables. This, like Go, makes it trivial to develop and deploy. Deno provides tooling that seems inspired by the Go command.\n\nLearning Go isn't difficult and the learning resources are pretty good. I find Go well suited to the library and archive software domain. I believe typed languages are good for working with structured metadata. Go compiles to a binary and is trivial to deploy. Go has really good tooling making it easier to write better quality code.\n\nMy colleagues and I all know Python. Python is our language of collaboration. I'm the only one that knows Go on our team of four. The leap from Python to Go isn't huge but it is a leap. I made that leap because I needed the features that Go provided at the time[^3]. Since that time the uptake in libraries and archives of coding in Go has been minimal[^4].\n\n[^3]: This was over a decade ago, back when Go was very much a child of Robert Griesemer, Rob Pike and Ken Thompson.\n\n[^4]: After a decade I know only a half dozen or so Go programmers working in the library and archive domain.\n\nMy colleagues and I know JavaScript. Most of the people I've met through Code4Lib know JavaScript. TypeScript is a superset of JavaScript and Deno can compile it[^5]. The path from JavaScript to TypeScript is less of a leap and more of a stretch. Valid JavaScript is valid TypeScript after all.\n\n[^5]: Deno can also compile TypeScript/JavaScript making your project as easy to deploy as a Go project. TypeScript is a typed programming language so offers similar benefits when working with structured metadata. Deno has tooling inspired by the Go command's tooling.\n\nLearning Deno command is easy and not a big ask. It is certainly allot easier learning Deno than Git. Git knowledge has grown over the decade that I've known the Code4Lib community. I suspect Deno will be easier to adopt. When I take software sustainability into consideration I suspect those projects I write in Deno+TypeScript may out live the ones I've written in Go.\n\nIn hindsight I don't think my answer about sharing code between browser and server is the whole criteria. Libraries and archives tend to have a small team to zero software development staff. While Go is a very popular language in 2024 few writing software for libraries and archives know it. Go adoption in our community hasn't materialized.  In the meantime most of the people I've met via Code4Lib know JavaScript. The Go developers I know of in our community also know JavaScript.\n\nTypeScript is a small stretch to pickup if you know JavaScript. TypeScript has good, free, online documentation[^6]. TypeScript is by definition a superset of JavaScript. If your project requires participation by other developers and you choose Deno+TypeScript over Go you have a wider pool of possible helping hands. Deno+TypeScript gives most of the benefits that Go offers today. I think this is compelling for software sustainability. \n\n[^6]: See [www.typescriptlang.org](https://www.typescriptlang.org/docs/) for TypeScript and [MDN](https://developer.mozilla.org/en-US/docs/Web/JavaScript) for JavaScript.\n\nWhen would I pick Deno+TypeScript over Go? I now have three criteria questions I ask myself.\n\n- Do I need to share code between browser and server?\n  - If yes, maybe this is a Deno+TypeScript project.\n- Do I need to largest pool of programmers available to lend a hand?\n  - If yes, maybe this is a Deno+TypeScript project.\n- Do I want to require Go knowledge to participate in the project?\n  - If yes then it might be a Go project.\n\nIf my answers are \"yes\", \"yes\", \"no\" then the project should proceed with Deno+TypeScript. If I answer the last one is \"no\" then it shouldn't be a Go project.\n",
      "data": {
        "abstract": "Brief discussion of when I choose Deno+TypeScript versus Go for work projects.\n",
        "byline": "R. S. Doiel, 2024-11-06",
        "createDate": "2024-11-06",
        "keywords": [
          "Go",
          "TypeScript",
          "Programming Languages"
        ],
        "pubDate": "2024-11-06",
        "title": "When Deno+TypeScript, when Go?"
      },
      "url": "posts/2024/12/06/when_deno_when_go.json"
    },
    {
      "content": "\n# Limit and offset for row pruning\n\nBy R. S. Doiel, 2024-10-31\n\nI recently needed to prune data that tracked report requests and their processing status. The SQLite3 database table is called\n\"reports\" and has four columns.\n\n- `id` (uuid)\n- `created` (request date stamp)\n- `updated` (status updated date stamp)\n- `src` (a JSON column with the status details)\n\nThe problem is the generated report can be requested as needed. I wanted to maintain the request data for the most recent one. The \"src\" column has the report name and status. That is easily checked using the JSON notation supported by SQLite3 (v3.47.0). It's easy to get the most recent completed row with a simple SELECT statement using both an ORDER clause and LIMIT clause.\n\n~~~sql\nselect id\n  from reports\n  where src->>'report_name' = 'myreport' and\n        src->>'status' = 'completed'\n  order by updated desc\n  limit 1\n~~~\n\nThis gives me the key to the most recent record.  How do I get a list of he rows I want to prune?  The answer is to use the LIMIT cause with an OFFSET\nmodifier. The OFFSET let's us skip a certain number of rows before applying the limit.  In this case I want to skip one row and show the rest. This database table doesn't get that big so I can use a limit like one thousand. Here's what that looks like.\n\n~~~sql\nselect id\n  from reports\n  where src->>'report_name' = 'myreport' and\n        src->>'status' = 'completed'\n  order by updated desc\n  limit 1000 offset 1\n~~~\n\nNow that I have my list of ids I can combine it with a DELETE statement which has a WHERE clause. The WHERE clause will use the IN operator to iterate over the list of ids from my select statement.\n\nPutting it all together it looks like this.\n\n~~~sql\ndelete from reports\n  where id in (\n    select id\n      from reports\n      where src->>'report_name' = 'myreport' and\n            src->>'status' = 'completed'\n      order by updated desc\n      limit 1000 offset 1\n)\n~~~\n\nThe nice thing is I can run this regularly. It will never delete the most recent row because the offset value is one.\n\n",
      "data": {
        "abstract": "Noted are how to combine a select statement with limit and offset clauses with a delete statement to prune rows.",
        "byline": "R. S. Doiel",
        "created": "2024-10-31",
        "keywords": [
          "sql",
          "SQLite3"
        ],
        "pubDate": "2024-10-31",
        "title": "Limit and offset for row pruning"
      },
      "url": "posts/2024/10/31/limit_and_offset_for_row_pruning.json"
    },
    {
      "content": "\n# SQLite3 json_patch is a jewel\n\nBy R. S. Doiel, 2024-10-31\n\nIf you’re working with an SQLite3 database table and have JSON or columns you need to merge with other columns then the `json_path` function comes in really handy.\nI have a SQLite3 database table with four columns.\n\n- _key (string)\n- src (json)\n- created (datestamp)\n- updated (datestamp)\n\nOccasionally I want to return the `_key`, `created` and `updated` columns as part of the JSON held in the `src` column.  In SQLite3 it is almost trivial.\n\n~~~sql\nselect \n  json_patch(json_object('key', _key, 'updated', updated, 'created', created), src) as object\n  from data;\n~~~\n\n",
      "data": {
        "abstract": "Quick note about json_path function in SQLite3",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "created": "2024-10-31",
        "keywords": [
          "sql",
          "SQLite3"
        ],
        "pubDate": "2024-10-31",
        "title": "SQLite3 json_patch is a jewel"
      },
      "url": "posts/2024/10/31/sqlite3_json_patch.json"
    },
    {
      "content": "\n# Quick tour of Deno 2.0.2\n\nBy R. S. Doiel\n\nI've been working with TypeScript this year using Deno. Deno has reached version 2.0. It has proven to be a nice platform for projects. Deno includes thoughtful tooling, good language support, ECMAScript module support and a [good standard library](https://jsr.io/@std).  As a TypeScript and JavaScript platform I find it much more stable and compelling than NodeJS. Deno has the advantage of being able to cross compile TypeScript to an executable which makes deployment of web services as easy for me as it is with Go.\n\n## Easy install with Cargo\n\nDeno is written in Rust. I like installing Deno via Rust's Cargo. You can installed Rust via [Rustup](https://rustup.rs). When I install Deno on a new machine I first check to make sure my Rust is the latest then I use Cargo to install Deno.\n\n~~~shell\nrustup update\ncargo install deno\n~~~\n\n## Easy Deno upgrade\n\nDeno is in active development. You'll want to run the latest releases.  That's easy using Deno. It has a self upgrade option.\n\n~~~shell\ndeno upgrade\n~~~\n\n## Exploring TypeScript\n\nWhen I started using Deno this year I wasn't familiar with TypeScript. Unlike NodeJS Deno can run TypeScript natively. Why write in TypeScript? TypeScript is a superset of JavaScript. That means if you know JavaScript you know most of TypeScript already. Where TypeScript differs is in the support for type safety and other modern language features. Writing TypeScript for Deno is a joy because it supports the web standard ECMAScript Models. That means the code I develop to run server side can be easily targetted to work in modern browsers too. TypeScript began life as a transpiled language targeting JavaScript. With Deno's emit module I can easily transpile my TypeScript to JavaScript. No more messying about with NodeJS and npm.\n\n## Exploring Deno\n\nAs a learning platform I find Deno very refreshing. Deno provides a [REPL](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop). That means you can easily try out TypeScript interactively. Deno is smart about when it runs \"programs\" versus running as a REPL. This is an improvement over NodeJS.\n\nDeno, like your web browser, runs TypeScript and JavaScript in a sand boxed environment. The REPL gives you full access to your machine but running programs via Deno requires you to give explicit permissions to resources like reading from your file system, accessing your environment, accessing the network or importing models from remote systems. This might sound tedious but Deno makes it easy in practice.\n\nDeno projects use a `deno.json` file for initialization. Creating the file is as easy as typing `deno init` in your project directory. Here's an example of setting up a `happy_deno` project.\n\n~~~shell\nmkdir happy_deno\ncd happy_deno\ndeno init\n~~~\n\nIf you list your directory you will see a `deno.json` file (Windows Powershell also supports \"ls\" to list directories).\n\n~~~shell\nls \n~~~\n\nThe init action created the following files.\n\n`deno.json`\n: The project configuration for Deno. It includes default tasks and module imports.\n\n`main.ts`\n: This is the \"main\" program for your project. It's where you'll add your TypeScript code.\n\n`main_test.ts`\n: This is a test program so you can test the code you've written in your \"main\" module.\n\nThe task action by itself will list currently defined tasks, e.g. `deno task` (the \"dev\" task\nwas defined by the init action).\n\n~~~shell\nAvailable tasks:\n- dev\n    deno run --watch main.ts\n~~~\n\nLooking at the `deno.json` file directly we see.\n\n~~~json\n{\n  \"tasks\": {\n    \"dev\": \"deno run --watch main.ts\"\n  }\n}\n~~~\n\nWhat does that do? The \"dev\" task will start deno using the \"run\" action passing it the \"watch\" option when running the file \"main.ts\". What does mean? The \"watch\" option will notice of the \"main.ts\" file changes on disk. It it changes it will re-run the \"main.ts\" program.  Save a change to \"main.ts\" in your editor deno and automagically it runs \"main.ts\" again. The really helps when you are write web services, the service automatically restarts.\n\nHere's an example of the output of running the \"dev\" task with the command `deno task dev`.\n\n~~~\nTask dev deno run --watch main.ts\nWatcher Process started.\nAdd 2 + 3 = 5\nWatcher Process finished. Restarting on file change...\n~~~\n\nUsing your editor, add a \"hello world\" log message to the code in \"main.ts\" so it looks like this.\n\n~~~typescript\nexport function add(a: number, b: number): number {\n  return a + b;\n}\n\n// Learn more at https://docs.deno.com/runtime/manual/examples/module_metadata#concepts\nif (import.meta.main) {\n  console.log(\"Add 2 + 3 =\", add(2, 3));\n  console.log(\"Hello World!\");\n}\n~~~\n\nSave your program and look what happens.\n\n~~~\nWatcher File change detected! Restarting!\nAdd 2 + 3 = 5\nHello World!\nWatcher Process finished. Restarting on file change...\n~~~\n\nAdding additional tasks is just a matter of editing the `deno.json` file and adding them to the `tasks` attributes.\n\nSee [deno task](https://docs.deno.com/runtime/reference/cli/task_runner/) documentation for details.\n\n### Modules in Deno\n\nTypeScript and JavaScript support \"modules\". Specifically Deno supports [ES](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules) modules. The nice thing about this is ES modules can be used with the same import export syntax in your web browser supports. Deno supports local modules and remote modules accessed via URL just like your browser. At work I have our project documentation hosted on GitHub. I can write a TypeScript modules there too. I can then import them into a another project just by using the URL.\n\nWhy is the significant? I don't need to rely on an external system like [npm](https://npmjs.com) for module repositories. All I need is a simple static website. Modules in the Deno community often use <https://jsr.io/> as a common module registery. This includes Deno's standard library modules.  Let's add the standard \"fs\" and \"path\" module to our happy deno project. Use Deno's \"add\" action.\n\n~~~shell\ndeno add jsr:@std/fs\ndeno add jsr:@std/path\n~~~\n\nIf you look at the `deno.json` now it should look something like this.\n\n~~~json\n{\n  \"tasks\": {\n    \"dev\": \"deno run --watch main.ts\"\n  },\n  \"imports\": {\n    \"@std/assert\": \"jsr:@std/assert@1\",\n    \"@std/fs\": \"jsr:@std/fs@^1.0.4\",\n    \"@std/path\": \"jsr:@std/path@^1.0.6\"\n  }\n}\n~~~\n\nTo quit my deno dev task I can press the control key and the \"c\" key (a.k.a. Ctrl-C) in my terminal window. \n\nI mentioned Deno runs programs in a sand box. That is because Deno tries to be secure by default. You must explicitly allow Deno to reach outside the sand box. One resource outside the sand box is the file system. If you use our remote modules we need to give Deno permission to do that too. See [security and permissions](https://docs.deno.com/runtime/fundamentals/security/) on Deno's documentation website for more details.\n\nTo allow reading files on the local file system with the \"dev\" task I would modify the \"dev\" command to look like.\n\n~~~\n    \"dev\": \"deno run --allow-read --watch main.ts\"\n~~~\n\nYou can include multiple permissions by adding the related \"allow\" option (E.g. `--allow-import`, `--allow-env`, `--allow-net`). It is important to realize that importing a moddel doesn't give you permission, you need to explicitly allow Deno to do that. When you compile a program the permissions you allow will also be allowed in the compiled version.\n\n### An exercise for the reader\n\nCreate a TypeScript file called [show_deno_json.ts](show_deno_json.ts). Read in and display the contents of the \"deno.json\" file in the same directory.\n\nHere's so links to documentation that may be helpful in finishing the exercise.\n\n- [reading files](https://docs.deno.com/examples/reading-files/)\n\nAdditional reading.\n\n- [fundamentals](https://docs.deno.com/runtime/fundamentals/)\n- [file system access](https://docs.deno.com/runtime/fundamentals/security/#file-system-access)\n- [standard modules](https://docs.deno.com/runtime/fundamentals/standard_library/)\n- [modules](https://docs.deno.com/runtime/fundamentals/modules/)\n- [deno.json](https://docs.deno.com/runtime/fundamentals/configuration/)\n- [security](https://docs.deno.com/runtime/fundamentals/security/)\n\n## Compiling TypeScript to executable code\n\nOne of the really nice things about Deno + TypeScript is that your development experience can be interactive like interpretive languages (e.g. Python, Lisp) and as convenient to deploy as a [Go](https://golang.org) executable. You can compile our \"main.ts\" file with the following command.\n\n~~~\ndeno compile --allow-read main.ts\n~~~\n\nListing my directory in our project I see the following.\n\n~~~shell\ndeno.json    deno.lock    happy_deno   main.ts      main_test.ts\n~~~\n\n~~~\n./happy_deno\n~~~\n\nNOTE: On a Windows the compiled program is named `happy_deno.exe`, to execute it I would type `.\\happy_deno.exe` in your Powershell session.\n\nBy default Deno uses the project directory for the executable name. You can explicitly set the executable name with a [command line option](https://docs.deno.com/runtime/getting_started/command_line_interface/). You can also use command line options with the compile action to [cross compile](https://en.wikipedia.org/wiki/Cross_compiler) your executable similar to how it is done with Go.\n\nWhy compile your program?  Well it runs slightly fast but more importantly you can now copy the executable to another machine and run it even if Deno isn't installed. This means you no longer have the version dependency problems I typically experience with deploying code from Python and NodeJS projects.  Like Go the Deno compiler is a cross compiler. That means I can compile versions for macOS, Windows and Linux on one machine then copy the platform specific executable to the machines where they are needed. Deno's compiler provides similar advantages to Go.\n\n## TypeScript to JavaScript with Deno\n\nJavaScript is a first class language in modern web browsers but TypeScript is not.  When TypeScript was invented it was positioned as a [transpiled](https://en.wikipedia.org/wiki/Source-to-source_compiler) language. Deno is a first class TypeScript environment but how do I get my TypeScript transpiled to JavaScript?  Deno provides an [emit](https://jsr.io/@deno/emit) module for that. With a five lines of TypeScript I can write a bundler to convert my TypeScript to JavaScript. I can even add running that as a task to my `deno.json` file. Here's an example of \"main_to_js.ts\".\n\n~~~typescript\nimport { bundle } from \"jsr:@deno/emit\";\nconst url = new URL(\"./main.ts\", import.meta.url);\nconst result = await bundle(url);\nconst { code } = result;\nconsole.log(code);\n~~~\n\nThe command I use to run `main_to_js.ts` is\n\n~~~shell\ndeno run --allow-read --allow-env main_to_js.ts\n~~~\n\nMy `deno.json` file will look like this with a \"transpile\" task.\n\n~~~json\n{\n  \"tasks\": {\n    \"dev\": \"deno run --allow-read --watch main.ts\",\n    \"transpile\": \"deno run --allow-read --allow-env main_to_js.ts\"\n  },\n  \"imports\": {\n    \"@std/assert\": \"jsr:@std/assert@1\",\n    \"@std/fs\": \"jsr:@std/fs@^1.0.4\",\n    \"@std/path\": \"jsr:@std/path@^1.0.6\"\n  }\n}\n~~~\n\nNow when I want to see the `main.ts` in JavaScript I can do `deno task transpile`.\n\n## Contrasting Deno + TypeScript with Go and Python\n\nFor me working in Go has been a pleasure in large part because of its tooling. The \"go\" command comes with module management, code formatter, linting, testing and cross compiler right out of the box. I like a garbage collected language. I like type safety. I like the ease which you can work with structured data. I've enjoyed programming with the excellent Go standard library while having the option to include third party modules if needed.\n\nDeno with TypeScript gives me most of what I like about Go out of the box. The `deno` command includes a task runner, module manager, testing, linting (aka check), cross compiler and formatter out of the box. TypeScript interfaces provide a similar experience to working with `struct` types in Go. Unlike Go you can work with Deno interactively similar to using the REPL in Python, Lisp or your favorite SQL client. I like the ES module experience of Deno better than Go's module experience.\n\nWhat makes Deno + TypeScript compelling over writing web services over Python is Deno's cross compiler.  Like Go I can compile executables for macOS, Windows and Linux on one box and target x86_64 and ARM 64 CPUs.No more need to manage virtual environments and no more sorting out things when virtual environments inevitably get crossed up. Copying an executable to the production machines is so much easier.  Many deployments boil down to an `scp` and restarting the services on the report machines. Example `scp myservice apps.example.edu:/serivces/bin/; ssh apps.example.edu \"systemctl restart myservice\"`.  It also means curl installs are trivial. All you need is an SH or Powershell script that can download a zip file, unpack it and copy it into the search path of the host system. Again the single self contained executable is a huge simplifier.\n\nOne feature I miss in Deno + TypeScript is the DSL in Go content strings embedded in struct type definitions. This makes it trivial to write converts for XML, JSON and YAML.  Allot of code in libraries and archives involves structured metadata and that feature ensures the structures definition are consistent between formats. I think adding to/from methods will become a chore at some point.\n\nIf you are working in Data Science domain I think Python still has the compelling code ecosystem. It works, it mature and there is lots of documentation and community out there. While you can run Deno from a [Jupyter notebook](https://docs.deno.com/runtime/reference/cli/jupyter/) I think it'll take a while for TypeScript/JavaScript to reach parity with Python for this application domain.\n\nSwitching from Go to Deno/TypeScript has been largely a matter of getting familiar with Deno, the standard library and remembering JavaScript while adding the TypeScript's type annotations. I've also had to learn TypeScript's approach to type conversions though that feels similar to Go. If I need the same functional code server side and browser side I think the Deno + TypeScript story can be compelling.\n\nPython, Rust, Go and Deno + TypeScript all support creating and running WASM modules.  Of those languages Rust has the best story and most complete experience. Deno runs a close second. Largely because it is written in Rust so what you learn about WASM in rust carries over nicely. The Python story is better than Go at this time. This is largely a result of how garbage collection is integrated into Go.  If I write a Go WASM module there is a penalty paid when you move between the Go runtime space and the hosts WASM runtime space. This will improve over time but it isn't something I've felt comfortable using in my day to day Go work (October 2024, Go v1.23.2).\n\nDeno makes TypeScript is a serious application language. I suspect more work projects to be implemented in TypeScript where shared server and browser code is needed. I has be useful exploring Deno and TypeScript.\n\n",
      "data": {
        "abstract": "A quick tour of Deno 2 and the features I enjoy. Deno includes thoughtful tooling, good language support,\nECMAScript module support and a good standard library. Deno has the advantage of being able to cross compile\nTypeScript to an executable which makes deployment of web services as easy for me as it is with Go.\n",
        "byline": "R. S. Doiell, 2024-10-18",
        "created": "2024-10-18",
        "keywords": [
          "development",
          "languages"
        ],
        "pubDate": "2024-10-18",
        "title": "Quick tour of Deno 2.0.2",
        "updated": "2024-10-21"
      },
      "url": "posts/2024/10/18/a-quick-tour-of-deno-2.json"
    },
    {
      "content": "\n# Web GUI and Deno\n\nBy R. S. Doiel, 2024-07-08\n\nI've been looking at various approaches to implement graphical interfaces for both Deno and other languages.  I had been looking primarily at webview bindings but then stumbled on [webui](https://webui.me). Both could be a viable way to implement a local first human user interface.\n\nHere's an example of the webview implementation of hello world.\n\n~~~typescript\nimport { Webview } from \"@webview/webview\";\n\nconst html = `\n<html>\n  <head></head>\n  <body>\n    <h1>Hello from deno v${Deno.version.deno}</h1>\n    <script>console.log(\"Hi There!\");</script>\n  </body>\n</html>\n`;\n\nconst webview = new Webview();\n\nwebview.navigate(`data:text/html,${encodeURIComponent(html)}`);\nwebview.run();\n~~~\n\nNow here is a functionally equivalent version implemented using webui.\n\n~~~typescript\nimport { WebUI } from \"https://deno.land/x/webui/mod.ts\";\n\nconst myWindow = new WebUI();\n\nmyWindow.show(`\n<html>\n  <head><script src=\"webui.js\"></script></head>\n  <body>\n    <h1>Hello from deno v${Deno.version.deno}</h1>\n    <script>console.log(\"Hi There!\");</script>\n    </body>\n</html>`);\n\nawait WebUI.wait();\n~~~\n\nLet's call these [thing1.ts](thing1.ts) and [thing2.ts](thing2.ts).  To run thing1 I need a little prep since I've used an `@` import. The command I need to map the `webview/webview` module is the `deno add` command.\n\n~~~shell\ndeno add @webview/webview\n~~~\n\nHere's how I check and run thing1.\n\n~~~shell\ndeno check thing1.ts\ndeno run -Ar --unstable-ffi thing1.ts\n~~~\n\nSince I didn't use an `@` import in the webui version I don't need to \"add\" it to Deno. I check and run thing2 similar to thing1.\n\n~~~shell\ndeno check thing2.ts\ndeno run -Ar --unstable-ffi thing2.ts\n~~~\n\nBoth will launch a window with our hello world message. Conceptually the code is similar but the details differ.  In the case of webview you are binding the interaction from the webview browser implementation. You populate your \"page\" using a data URL call (see `webview.navigate()`. Webview is a minimal web browser. It is similar to but not the same as evergreen web browsers like Firefox, Chrome, or Edge. Depending how var you want to push your CSS, JavaScript and HTML this may or may not be a problem.\n\nWebui uses a lighter weight approach. It focuses on a web socket connection between your running code and the user interface. It leaves the browser implementation to your installed browser (e.g. Chrome, Edge or Firefox). There is a difference in how I need to markup the HTML compared to the webview version. In the webui version I have a script element in the head. It loads \"webui.js\". This script is supplied by webui C level code. It \"dials home\" to connect your program code with the web browser handling the display. Webui at the C library level is functioning as a web socket server.\n\nConceptually I like the webui approach. My program code is a \"service\", webui manages the web socket layer and the web browser runs the UI. Web browsers are complex. In the web UI approach my application's binary isn't implementing one. In the webview approach I'm embedding one. Feels heavy. At a practical level of writing TypeScript it may not make much differences. When I compiled both thing1 and thing2 to binaries thing2 was approximately 1M smaller. Is that difference important? Not really sure.\n\nWhat about using webview or webui from other languages? Webview has been around a while. There are many bindings for the C++ code of webview and other languages.  Webui currently supports Rust, Go, Python, TypeScript/JavaScript (via Deno), Pascal as well as a few exotic ones. TypeScript was easy to use either. I haven't tried either out with Python or Go. I'll leave that for another day.\n",
      "data": {
        "abstract": "My notes on two Web GUI modules available for Deno.\n",
        "created": "2024-07-07",
        "keywords": [
          "Deno",
          "TypeScript",
          "webui",
          "webview"
        ],
        "pubDate": "2024-07-08",
        "title": "Web GUI and Deno"
      },
      "url": "posts/2024/07/08/webgui_and_deno.json"
    },
    {
      "content": "\n# Transpiling with Deno\n\n[Deno](https://deno.land) is a fun environment to work in for learning TypeScript.  As I have become comfortable writing server side TypeScript code I know I want to also be able to use some modules in JavaScript form browser side. The question is then how to you go from TypeScript to JavaScript easily with getting involved with a bunch-o-npm packages?  Turns the solution in deno is to use the [deno_emit](https://github.com/denoland/deno_emit/blob/main/js/README.md) module.  Let's say I have a TypeScript module called `hithere.ts`. I want to make it available as JavaScript so I can run it in a web browser. How do I use the `deno_emit` module to accomplish that?\n\n- Write a short TypeScript program\n  - include the transpiler module provided with emit\n  - use the transpiler to generate the JavaScript code\n  - output the JavaScript code\n\nHere's what `transpile.ts` might look like:\n\n~~~typescript\n/* Get the transpiler module from deno's emit */\nimport { transpile } from \"https://deno.land/x/emit/mod.ts\";\n\n/* Get the python to my CL.ts as a URL */\nconst url = new URL(\"./hithere.ts\", import.meta.url);\n/* Transpile the code returning a result */\nconst result = await transpile(url);\n\n/* Get the resulting code and write it to standard out */\nconst code = result.get(url.href);\nconsole.log(code);\n~~~\n\nHere's the `hithere.ts` module:\n\n~~~typescript\n/**\n * hithere takes a name and returns a string of \"hi there \", a name and \"!\". If the name is null\n * it returns \"Hello World!\".\n *\n * @param {string | null} name\n * @returns {string}\n */\nfunction hithere(name: string | null): string {\n\tif (name === null) {\n\t\treturn \"Hello World!\";\n\t}\n\treturn `hi there ${name}!`;\n}\n~~~\n\nTo compile the module I need to give transpile.ts some permissions.\n\n- --allow-read (so I can read my local module\n- --allow-env (the transpiler needs the environment)\n- --allow-net (the deno emit module is not hosted locally)\n\nThe command line could look like this.\n\n~~~shell\ndeno run --allow-read --allow-env --allow-net \\\n  transpile.ts\n~~~\n\nThe result is JavaScript. It still has my comments in the code but doesn't have the TypeScript specific\nannotations.\n\n~~~javascript\n/**\n * hithere takes a name and returns a string of \"hi there \", a name and \"!\". If the name is null\n * it returns \"Hello World!\".\n *\n * @param {string | null} name\n * @returns {string}\n */ function hithere(name) {\n  if (name === null) {\n    return \"Hello World!\";\n  }\n  return `hi there ${name}!`;\n}\n~~~\n",
      "data": {
        "created": "2024-07-03",
        "keywords": [
          "TypeScript",
          "JavaScript",
          "Deno"
        ],
        "pubDate": "2024-07-03",
        "software": [
          "Deno >= v1.44"
        ],
        "title": "Transpiling with Deno"
      },
      "url": "posts/2024/07/03/transpiling_with_deno.json"
    },
    {
      "content": "\n# Bootstrapping a Text Oriented Web\n\nBy R. S. Doiel, 2024-06-14\n\nFirst order of business is to shorten \"text oriented web\" to TOW.  It's easier to type and say.  I'm considering the bootstrapping process from three vantage points. \n\n1. content author\n2. the server software\n3. client software \n\nThe TOW approach is avoids invention in favor of reuse. HTTP protocol is well specified and proven. [Common Mark](https://commonmark.org) has a specification as does [YAML](https://yaml.org/). TOW documents are UTF-8 encoded. A TOW document is a composite of Common Mark with YAML blocks. TOW documents combined with HTTP provide a simplified hypertext platform. \n\n\n## Authoring TOW documents\n\nTOW seeks to simplify the content author experience. TOW removes most of the complexity of content management systems rendering processes. A TOW document only needs to be place in a directory supported by a TOW server. In that way it is as simple as [Gopher](https://en.wikipedia.org/wiki/Gopher_(protocol)). The content author should only need to know [Markdown](https://en.wikipedia.org/wiki/Markdown), specifically the [Common Markdown](https://commonmark.org/) syntax. If they want to create interactive documents or distribute metadata about their documents they will need to be comfortable creating and managing YAML blocks embedded in their Common Mark document. Use of YAML blocks is already a common practice in the Markdown community.\n\nDescribing content forms using YAML has several advantages. First it is much easier to read than HTML source. YAML blocks are not typically rendered by Markdown processor libraries. I can write a simple preprocessor which tenders the YAML content form as HTML. Since HTML is allowed in Markdown documents these could then be run through a standard Markdown to HTML converter.  In the specific case of Pandoc a filter could be written to perform the pre-processor step. It should be possible to always render a TOW document as an HTML5 document. This is deliberate, it should be possible to use the TOW documents to extrapolate a traditional website.\n\n## Server and client software\n\nTOW piggy backs on the HTTP protocol. A TOW document is a composite of Common Mark with embedded YAML blocks when needed. It differs from the existing WWW content only in its insistence that Common Mark and YAML be first class citizens forming a viable representation of a hypertext document. A TOW document URL looks the same as a WWW URL. The way TOW documents distinguish themselves from ordinary web content is via their content type, \"text/tow\" or \"text/x-tow\".  Content forms are sent to a TOW service using the content type \"application/yaml\" content type instead of the various urlencoded content types used by WWW forms. \n\nTOW browsers eschew client side programming. I have several reasons for specifying this. First the TOW concept is a response to current problems and practices in the WWW. I don't want to contribute to the surveillance economy. It also means that's what the client receives avoids on vector if hijacking that the WWW has battled over the years. Importantly this also keeps the TOW browser model very simple. The TOW browser renders TOW content once per load. TOW is following the path that [Gemini protocol](https://geminiprotocol.net/) and [Gemtext](https://hexdocs.pm/gemtext/Gemtext.html). Unlike Gemini it does not require a new protocol and leverages an existing markup. Like Gemini TOW is not replacing anything but only supplying an alternative.\n\nMy vision for implementing TOW is to use existing HTTP protocol. That means a TOW URL looks just like a WWW URL. How do I distinguish between WWW and TOW?  HTTP protocol supports headers. TOW native interaction should use the content type \"text/tow\" or \"text/x-tow\". Content forms submitted to a TOW native server should submit their content encoded as YAML and use the content type \"text/tow\" or \"text/x-tow\". This lets the server know that the reply should remain in \"text/tow\" or \"text/x-tow\".  A TOW enabled browser can be described as a browser that knows how to render TOW documents and submit YAML responses.\n\n## How to proceed?\n\nA TOW document needs to be render-able as HTML+CSS+JavaScript because that is what is available today to bootstrap TOW. The simplest TOW server just needs to be able to send TOW content to a requester with the correct content type header, e.g. \"text/tow\".  That means a server can be easily built in Go using the standard [net/http](https://gopkg.in/net/html) package. That same package could then be combined with a web server package to adapt it into a TOW server supporting translation to HTML+CSS+JavaScript during the bootstrap period.  If the TOW web server received a request where \"text/tow\" wasn't in the acceptable response list then it would return the TOW document translated to HTML+CSS+JavaScript.\n\nA TOW native browser could be built initially as a [PWA](https://en.wikipedia.org/wiki/Progressive_web_app). It just needs to render TOW native documents as HTML5+CSS+JavaScript and be able to send TOW content forms back as YAML using the \"text/tow\" content type. Other client approaches could be taken, e.g. write plugin for the [Dillo browser](https://dillo-browser.github.io/), build something on [Gecko](https://developer.mozilla.org/en-US/docs/Glossary/Gecko), build something on [WebKit](https://webkit.org/), or use [Electron](https://www.electronjs.org/). A PWA is probably good enough for proof of concept.\n\nA minimal TOW proof of concept would be the web service that can handle the translation of TOW documents to HTML+CSS+JavaScript. A complete proof of concept could be implemented TOW native support via [PWA](https://en.wikipedia.org/wiki/Progressive_web_app). \n\n1. tow2html5\n2. towtruck (built using tow2html5)\n3. towby (initially built as tow2html5 WASM module as PWA)\n\n## Proposed programs\n\ntow2html5\n: This can be implemented in Go as both a package and command line interface. The command line interface could function either in preprocessor mode (just translating the YAML forms into HTML5) or as a full processor using an existing Common Mark package. It could also be compiled to a WASM module to support implementing a TOW browser as PWA.\n\ntowtruck\n: This would be a simple web service that performed tow2html5 translation for tow document requests from non-TOW native browsers. If the accepted content type requested includes TOW native then it'd just hand back the TOW file untranslated. I would implemented this as a simple static HTTP web service running on localhost then use Lighttpd, Apache 2 or NginX for a front end web server. This simplifies the TOW native server.\n\ntowby\n: A [PWA](https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps) based TOW browser proof of concept\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-06-14",
        "created": "2024-06-14",
        "keywords": [
          "text oriented web",
          "tow"
        ],
        "pubDate": "2024-06-14",
        "title": "Bootstrapping a Text Oriented Web"
      },
      "url": "posts/2024/06/14/tow_bootstraping.json"
    },
    {
      "content": "\n# RISC OS 5.30, GCC 4.7 and Hello World\n\nBy R. S. Doiel, 2024-06-08 (updated: 2024-06-16)\n\nPresently am I learning RISC OS 5.30 on my Raspberry Pi Zero W. I want to write some programs and I learned C back in University. I am familiar with C on POSIX systems but not on RISC OS. These are my notes to remind myself how things work differently on RISC OS.\n\nI found two resources helpful. First James Hobson had a YouTUBE series on RISC OS and C programming. From this I learned about allocating more space in the Task Window via the Tasks window display of Next and Free memory. Very handy to know. Watching his presentation it became apparent he was walking through some\none's tutorial. This lead to some more DuckDuckGo searches and that is when I stumbled on Steve Fryatt's [Wimp Programming In C](https://www.stevefryatt.org.uk/risc-os/wimp-prog). James Hobson's series (showing visually) help and the detail of Steve Fryatt's tutorial helped me better understanding how things work on RISC OS.\n\nI think these both probably date from around 2016. Things have been evolving in RISC OS since then. I'm not certain that OSLib today plays the same role it played in 2016. Also in the case of Steve Fryatt's tutorial I'm not certain that the DDE and Norcroft compilers are essential in the say way. Since I am waiting on the arrival of the ePic SD Card I figured I'd get started using the\nGCC and tools available via Packman and see how far I can get.\n\n## Getting oriented\n\nWhat I think I need.\n\n1. Editor\n2. C compiler\n3. Probably some libraries\n\nYou need an editor fortunately RISC OS comes with two, `!Edit` and `!StrongED`. You can use both to create C files since they are general purpose text edits.\n\nYou need a C compiler, GCC 4.7.4 is available via Packman. That is a click,\nand download away so I installed that.\n\nI had some libraries already installed so I skipped installing additional ones since I wasn't sure what was currently required.\n\n## Pick a simple goal\n\nWhen learning a new system I find it helpful to set simple goals. It helps from feeling overwhelmed.\n\nMy initial goal is to understand how I can compile a program and run it in the Task Window of RISC OS. The Task Window is a command line environment for RISC OS much like a DOS Window was for MS Windows or the Terminal is for modern macOS.  My initial program will only use standard in and out. Those come with the standard library that ships with the compiler. Minimal dependencies simplifies things. That goes my a good simple intial goal.\n\n> I want to understand the most minimal requirements to compile a C program and run it in Task Window\n\n## Getting started\n\nThe program below is a simple C variation on the \"Hello World\"  program tought to beginner C programers.  I've added a minimal amount of parameter handlnig to se how that works in the Task Window environment. This program will say \"Hello World!\" but if you include parameters it will say \"Hi\" to those too.\n\nThe code looks like this.\n\n~~~C\n#include <stdio.h>\n\nint main(int argc, char *argv[]) {\n  int i = 0;\n  printf(\"Hello World!\\n\");\n  for (i = 1; i < argc; i++)  { \n       printf(\"Hi %s\\n\", argv[i]);\n  }\n  return 0;\n}\n~~~\n\nIn a POSIX system I would name this \"HelloWorld.c\". On RISC OS the \".\" (dot)\nis a directory delimiter. There seems to be two approaches to translating POSIX paths to RISC OS. Samba mounted resources seem to have a simple substitution translatio. A dot used for file extensions in POSIX becomes a slash. The slash directory delimiter becomes a dot. Looking at it from the POSIX side the translation is flipped. A POSIX path like \"Project/HelloWorld/HelloWorld.c\" becomes \"Project.HelloWorld.HelloWorld/c\" in RISC OS.\n\nIn reading of the RISC OS Open forums I heard discussions about a different approach that is more RISC OS centric. It looks like the convention in RISC OS is to put the C files in a directory called \"c\" and the header files in a directory called \"h\". Taking that approach I should instead setup my directory paths like \"Project.HelloWorld.c\" which in POSIX would be \"Project/HelloWorld/c\". It seems to make sense to follow the RISC OS convensions in this case as I am not planning to port my RISC OS C code to POSIX anytime soon and if I did I could easily write a mappnig program to do that. My path to \"HelloWorld\" C source should look like `$.Projects.C_Programming.c.HelloWorld`.\n\nAfter storting this bit out it is time to see if I can compile a simple program with GCC and run it in a Task Window. This is a summary of my initial efforts.\n\nFirst attempt steps\n\n1. Open Task Window\n2. run `gcc --version`\n\nThis failed. GCC wasn't visible to the task window. Without understanding what I was doing I decided maybe I need to launch `!GCC` in `$.Apps.Development` directory. I then tried `gcc --version` again in the Task Window and this time\nthe error was about not enough memory available. I looked the \"Tasks\" window and saw plenty of memory was free. I did NOT realise you could drag the red bar for \"next\" and increase the memory allocation for the next time you opened a Task Window. I didn't find that out until I did some searching and stumbled on James Hobson's videos after watching the recent WROCC Wakefield Show held in Bedford (2024).\n\n> A clever thing about RISC OS is the graphical elements are not strictly informational. Often they are actionable. Dragging is not limited to icons.\n\nSecond attempt steps\n\n1. Open the Tasks window, drag the memory (red bar) allocation to be more than 16K\n2. Open a new Task Window\n3. Find and Click on `!GCC`\n4. In the task window check the GCC version number\n5. Change the directory in the Task Window to where I saved \"HelloWorld\"\n6. Check the directory with \"cat\"\n7. Try to compile with `gcc HelloWorld -o app`, fails\n8. Check GCC options with `--help`\n9. Try to compiled with `gcc -x c HelloWorld -o app`, works\n\nThis sequence was more successful. I did a \"cat\" on the task window and saw I was not in the right folder where my \"HelloWorld\" was saved.  Fortunately James Hobson video shows any easy way of setting the working directory. I brought the window forward that held \"HelloWorld\". Then I used the middle mouse button (context menu) to \"set directory\". I then switched back to the Task Window and low and behold when I did a \"cat\" I could see my HelloWorld file.\n\nI  tried to compile \"HelloWorld\". In James Hobson video he shows how to do this but I couldn't really see what he typed.  When I tried this I got an error\nabout the file type not being determined.  Doing `gcc --help` listed the options\nand I spotted `-x` can be used to explicitly set the type from the GCC point of view. This is something to remember when using GCC. It's a POSIX program running\non RISC OS which is not a POSIX system.  GCC will expect files to have a POSIX references in some case and not others. There's a bit of trial and error around\nthis for me.\n\nNext I tried using the `-x c` option. I try recompiling and after a few moments\nGCC creates a \"app\" file in the current directory. On initial creation it is a Textfile but then the icon quickly switches to a \"App/ELF\" icon.  Double clicking the App icon displays hex code in the Task Window. Not what I was expected. Back in the Task Window I type the following.\n\n~~~shell\napp Henry Mable\n~~~\n\nAnd I get out put of\n\n~~~shell\nHello World!\nHi Henry\nHi Mable\n~~~\n\nMy program works at the CLI level in a Task Window. My initial goal has been met.\n\n## What I learned\n\n1. Remember that RISC OS is a fully GUI system, things you do in windows can change what happens in the whole environment\n2. Remember that the display elements in the GUI maybe actionable\n3. When I double clicked on `!GCC` what it did is add itself to the search path.\n\nI remember something from the Hobson video about setting that in `!Configure`, `!Boot` and picking the right boot configuration action.  I'll leave that for next time. I should also be able to script this in an Obey file and that might be a better approach.\n\nThere are some things I learned about StrongED that were surprising. StrongED's C mode functions like a \"folding\" editor. I saw a red arrow next to my \"main\" functions. If I click it the function folds up except for the function signature and opening curly bracket. Click it again the the arrow changes direction and the full function is visible again.\n\nThe \"build\" icon in StrongED doesn't invoke GCC at the moment. I think the build icon in the ribbon bar maybe looking for a Makefile. If so I need to install Make from Packman. This can be left for next time.\n\nI'd really like to change the editor colors as my eyes have trouble with white background. This too can be left for another day to figure out.\n\n## Next Questions\n\n1. How do I have the GCC compiled \"app\" so that I can double click in the file window and have it run without manually starting the Task Window and running it from there.  Is this a compiler option or do I need an Obey file?\n2. Which libraries do I need to install while I wait on the DDE from ePic to arrive so that I can write a graphical version of Hello World?\n\n## Updates\n\nI got a chance to read more about [Obey files](https://www.riscosopen.org/wiki/documentation/show/CLI%20Basics) and also clicked through the examples in the `SDSF::RISCOSPi.$.Apps.Development.!GCC` directory (shift double click to open the GCC directory. In that directory is an examples\nfolder which contains a Makefile for compile C programs in various forms.\nFrom there it was an easy stop to see how a simple Obey file could be used\nto create a `!Build` and `!Cleanup` scripts.\nwhere all the GCC setup lives). What follows are the two Obey files in the directory holding the \"c\" folder of HelloWorld.\n\nHere's `!build`\n\n~~~riscos\n| !Build will run GCC on c.HelloWorld to create !HelloWorld\nSet HelloWord$Dir <Obey$Dir>\nWimpSlot -min 16k\ngcc -static -O3 -s -O3 -o !HelloWorld c.HelloWorld\n~~~\n\nand `!Cleanup`\n\n~~~riscos\n| !Cleanup removes the binaries created with !Build\nSet HelloWorld$Dir <Obey$Dir>\nDelete !HelloWorld\n~~~\n\n",
      "data": {
        "abstract": "These are my notes on learning to program a Raspberry Pi Zero W\nunder RISC OS using GCC 4.7 and RISC OS 5.30\n",
        "created": "2024-06-08",
        "keywords": [
          "RISC OS",
          "Raspberry Pi",
          "GCC",
          "C Programming"
        ],
        "pubDate": "2024-06-08",
        "references": [
          "Steve Fryatt's tutorial <https://www.stevefryatt.org.uk/risc-os/wimp-prog>",
          "James Hobson's YouTUBE Video showing a summary of Steve Fryatt's tutorial"
        ],
        "title": "RISC OS 5.30, GCC 4.7 and Hello World"
      },
      "url": "posts/2024/06/08/riscos_gcc_and_hello.json"
    },
    {
      "content": "\n# Exploring RISC OS 5.30 on a Raspberry Pi Zero W\n\nBy R. S. Doiel, 2024-06-04\n\nBack on April, 27, 2024 [RISC OS Open](https://riscosopen.org) [announced](https://www.riscosopen.org/news/articles/2024/04/27/risc-os-5-30-now-available) the release of RISC OS 5.30. This release includes WiFi support for the Raspberry Pi Zero W. This may sound like a small thing. WiFi is taken for granted on many other operating systems.  This is the news I've been waiting for before diving into RISC OS. My Pi Zero W running RISC OS **just works** with my wireless network. That is wonderful.\n\n## RISC OS and Pi Zero W gives you a personal networked computer\n\nHere's my setup.\n\n- First generation Raspberry Pi Zero W (running RISC OS 5.30 with Pinboard2)\n- Raspberry Pi Keyboard and Mouse (the keyboard provides a nice USB hub for the Zero)\n- A powered portable monitory (the monitor provides power to the Pi Zero)\n\nFor additional disk storage I have a old Pi 3B+ with a 3.14G Western Digital hard drive. It is configured as a Samba file server running the bookworm release of Raspberry Pi OS[^1].\n\n[^1]: RISC OS 5.3 only supports SMB with LANMAN1. LANMAN1 is turned off by default for Samba on R-Pi OS bookworm.\n\n## Quick summary\n\nI've been playing around with RISC OS 5.30  on my Pi Zero for a couple weeks now. I've really come to enjoy using it. RISC OS is different from macOS and Windows in its approach to the graphical user interface. Like with the Oberon Operating System you need to accept that difference and shed your assumptions about how things should and do work. I've found the difference invigorating. My Pi Zero with RISC OS has become my \"fun desktop\" for recreational computing.\n\n## Diving in\n\nRISC OS is a small single user operating system. It requires minimal resources. It provides a rich graphical environment. Currently RISC OS 5.30 only runs on one core of an ARM CPU. The Raspberry Pi Zero has only one core so that's a nice fit.\n\nThere is a fair amount of information regarding RISC OS online. There are active user groups and communities too.\nSome of the documentation is quite good. One of the things to keep in mind if you search for \"RISC OS\" on the Internet is that RISC OS has forked. This is a little like the old Unix fork of BSD versus System V. They come from the same origins but have taken different paths and approaches.\n\n>RISC OS's closed fork runs on vintage hardware and emulators only. It does not appear to be actively developed and the version numbers associated include version four and six.\n>\n> The [RISC OS Open](https://www.riscosopen.org) (i.e. RISC OS 5.x) fork is Open Source. It is being actively developed. It is licensed using the Apache 2 Open Source license. It runs on most versions of the Raspberry Pi. It actually runs on many single board computers. See <https://www.riscosopen.org> for details. Today it seems to be a well run Open Source project.\n\nThe reasons for the fork are complicated. From what I've read they are due to how Acorn was broken up when the company ceased to operate as Acorn.  RISC OS as intellectual property wound up in two different companies. They had two different business models and were driven by maximizing profit in the diminished Acorn market. The net resulted was a divided RISC OS community. The division included a level of acrimony. Somehow RISC OS survived this. RISC OS 5 branch even survived a bumpy road to becoming a true Open Source operating system. Today RISC OS 5.30 is licensed under the widely used Apache 2.0 Open Source license. RISC OS 5 even have a small community of commercial software developers writing and updating software for it!\n\nIf, like me, you're starting your RISC OS 5.30 on a Raspberry Pi then you're in luck. The dust seems to have settled. I highly recommend first reading the User Guide found at the bottom of the [common](https://www.riscosopen.org/content/downloads/common) page in the downloads section of the RISC OS Open website. You can also buy a printed version of the User Guide. From there head over and explore RISC OS Open community forums at <https://www.riscosopen.org/forum/>. There is also a more general Acorn enthusiast community at [stardot.co.uk](https://www.stardot.org.uk/forums/). I found this particularly helpful in understanding the historical evolution of RISC OS. Stardot even has a presence of [GitHub](https://github.com/stardot).\n\nWhat follows are a semi-random set of points that I had to wrap my head around in getting oriented on RISC OS 5.30 on the Raspberry Pi Zero W.\n\n## Visibly different\n\nWhen I search for RISC OS on the [Raspberry Pi Forums](https://forums.raspberrypi.com/search.php?keywords=RISC+OS) many of the questions seemed to have more to do with user assumptions about how RISC OS works than about things actually not working. RISC OS is different. It comes from a different era. It was designed with a vastly different set of assumptions than POSIX systems like macOS, Linux, BSD, or Windows. Check your assumptions at the door.\n\n### The mouse and its pointer\n\nRISC OS is a three button mouse oriented system. The left and middle mouse buttons play very specific roles.  The left button is for \"action\" and the middle button provides a \"context menu\". RISC OS does not use \"application menu\" at the top of the screen or top of the window frame like macOS, Windows or Raspberry Pi OS. The \"context menu\" provides that functionality.\n\nWhen you point your mouse at a screen element and press the middle button you get the context menu for that thing.  If you single or double click on an item with the left button you are taking an action.\n\n- The left mouse button takes a action. Sometimes you use a single click and at others a double click\n- The middle mouse button brings up the context menu, on RISC OS this is replaces the need for an application menu\n\n### The Iconbar and Pinboard\n\nWhen you start RISC OS you will see two main visual elements. The desktop is called the \"Pinboard\". This is analogous to desktops on Windows, macOS and Raspberry Pi OS.  It contains a background image and icons.\n\nThe Iconbar is the other big visible element. It is found at the bottom of the screen. The Iconbar predates Window's task bar and may have inspired it.  The Iconbar is responsibly for allowing you to access system resources and the loaded applications and modules.\n\nOn the left side of the Iconbar are icons representing system resources. This includes the SD card holding RISC OS. On the right side of the Iconbar you'll find icons that represent running applications or modules.\n\nRISC OS is a single user operating system and relies on cooperative multitasking. An application or module can be loaded into memory and left in memory for fast reuse.  When you \"launch\" an application it loads into memory. To use the application you can either left double click on the icon in the Iconbar or open a file associated with the application. If you do the later then the application will automatically get added to the Iconbar if it was not already present. When you close an application's window you're not removing the application. To close an application you need to go to the Iconbar and use the context menu (remember that middle mouse button) to \"quit\" it.\n\nOn the left, system resources side, data storage is manipulated via the filer windows. You open a filer window by left double clicking on the resource in the Iconbar. The filer window on RISC OS plays a role similar to the file manager on Windows or a finder window on macOS.\n\n### Iconbar and file resources\n\nThe filer maintains metadata about files. This includes the file type and the last application used to save the file. The file type is explicit with RISC OS and is NOT based on a file extension like with macOS, Windows and Raspberry Pi OS. The file path notation is also significantly different from the POSIX path syntax.\n\nDouble click on a storage resource in the Iconbar opening a filer window. The window will list files, folders and applications. The file system is hierarchical.  An icon will indicate the file type. Some icons are folders, these are sub directories of the current filer window. Most files will have other icons associated based on file type. There are special \"files\" that begin with an exclamation symbol, \"!\". In POSIX this is referred as a \"bang\" symbol but in RISC OS it is called a \"pling\".  Filenames starting with the pling hold applications. Applications are implemented as a directory containing resources. Resources might be application's visual assets, configuration, scripts or executable binaries. Directories as application will be familiar to people running modern versions of macOS or who have used NeXTStep. RISC OS usage predated both those systems. As an end user you just click on the pling name to launch the application. It will then show up in the Iconbar ready for use.\n\nOn the right side of the Iconbar are your running applications. When you first startup RISC OS you'll see two running. First, on the far right is an icon representing the hardware you're running on. If you're on a Raspberry Pi it'll be raspberry icon.  If you're running on a Pine64 Pinebook it'll be a pine cone. Since I'm on a Raspberry Pi Zero I will called it a Raspberry icon. Use your mouse and move the mouse pointer over the Raspberry icon. Left double click on the icon. This starts up a dialog box containing information about system resources. If you single click the middle button on the Raspberry icon you will get a operating system context menu. In the context menu you'll see items for \"Configure\" and \"Task Window\". The Task window provides a command line interface for RISC OS. The configure menu lets you configure how RISC OS runs. This includes things like setting the desktop theme and configuring network services.\n\nPointing your mouse to the left of the Raspberry will place the cursor over an icon that looks like a computer monitor.  Double left clicking on this icon will open a dialog that lets you set the resolution of your monitory. Similarly a single middle click on the icon will open a context menu.  This is a major pattern in interacting with RISC OS on the Iconbar. In fact this is the general pattern of using the mouse through out the system. It's take an action (left) or select an action to take (middle).\n\nThe Now you should see it in the Iconbar towards the right.  Note it is only visible in the Iconbar right now. This is very different than double clicking on macOS Or Windows. If you move your mouse over the icon in the Iconbar and Left double click it'll open your applications main dialog or window (just like what happened when we click on the monitor or Raspberry icons). It's a two step process.\n\nHow do you get more applications on the Iconbar?\n\nUse the filer. The default applications are in the \"Apps\" icon visible on the Iconbar. Left double click on it. It'll open a filer window with an abbreviate list of applications. In that filer window look for an application called \"!Alarm\" (pling alarm). Double left click on that application's icon will cause it to appear in the Iconbar towards the right side. Congratulation you've loaded your first RISC OS application.\n\nYou will find more applications by looking at the contents of the SD Card. To do that double left click on the SD Card icon in the Iconbar. It'll open a filer window. You'll see a folder icon labeled \"Apps\". This is an actual directory on the file system. It is where applications are usually installed on RISC OS. Open that folder by double left clicking on it. You can low launch additional application by left double clicking on their pling names. These like Alarm will show up on the right side of the Iconbar.\n\nHaving things running on the Iconbar is convenient.  If you want to start a new document associated with one of the running application you just left double the icon on the Iconbar.  If you want an application to be processed by something running in the Iconbar you can draft the document from the filer window to the icon on the Iconbar. If you want to save that new document start the application from the Iconbar then use the context menu in the application window or press F3 to get the save dialog box. In the dialog box give your document a name and drag the icon of the file from the dialog box to a filer window where you want to store the document. That last step is important to note. Icons are actionable even when they appear in a dialog box! This is very unlike Windows, macOS or Raspberry Pi OS. The dragging the icon is what links data storage with the application. The next time you want to edit the file you just fine the file and left double click on it to open it. If the application isn't running then it'll be launched and added to the Iconbar automatically.\n\nRemember when you close your application's main window or dialog box it doesn't close the application. It only close the file you were working on. To actually unload the application (and free up additional memory) point your mouse at the icon in the Iconbar and middle click to see the context menu. One of the context menu items will be \"Quit\", this is how you fully shutdown the application.\n\n### Context menus\n\nWhat's all this about context menus? On other operating systems the context menu is called an application menu. It might be at the top of the application window (e.g. Windows) or at the top of the screen (e.g. macOS). On Windows and macOS the words \"context menu\" usually mean a special sub-menu in the application (e.g. like when you use a context menu to copy a link in your web browser). In RISC OS the context menu is the application menu. It is always accessed via the middle mouse button just as we saw in introduction to the Iconbar.\n\nSome applications like the \"!StrongEd\" editor do include a ribbon of icons that will make it easy to do various things. That's just a convenience. Other applications like \"!Edit\" don't provide this convenience. Personally I'm ambivalent to ribbon bars. I think they are most useful where you'd have to navigate down a level or two from the context menu to reach a common action you wanted to perform. There are things I don't like about the ribbon button approach. You loose window space to the ribbon. The second you are increasing the distance of mouse travel to take the action. On the other hand it beats traveling down three levels of nesting in the context menu. It's a trade off.\n\nWhat ever the case remember the middle button as picking an action and the left mouse button as taking the action.\n\n### Window frames\n\nWhen you open an application from the Iconbar it'll become a window (a square on the screen containing a dialog or application content). Looking at the top of of the window frame there is are some buttons and application title area. In the upper left of the window frame you see two buttons. From left to right, the first button you see will look like two squares overlaid. Reminds me of the symbols often used when copying text.  That button is not a copy button. The two overlapping squares indicate that the current window can be sent to the bottom (furthest behind) of the stack of overlapping windows. If you think of a bunch of paper loosely stack and being read it's like taking the top paper and putting it at the back of the stack as you finish reading it. I found this very intuitive and a better experience than how you would take similar actions on macOS and Windows.\n\nNext to the back window button is an button with an \"X\" on it. This button closes the window.\n\nTo the right of the close window button is the title bar. This usually shows the path to the document or data you the application is managing. You'll notice the path doesn't resemble a POSIX path or even a path as you'd find on DOS or CP/M.  I'll circle back to the RISC OS path notation in a bit. The path maybe followed by an asterisk. If the asterisk is present it indicates the data has been modified. If you try to close the window without saving a filer dialog will pop up giving a choice to discard changes, cancel closing or saving the changes. A heads up is saving is approach differently in RISC OS than on macOS or Windows.\n\nTo the right of the title bar you'll see minimize button which looks like a dash and a maximize button that looks like a square. These work similar to what you'd expect on macOS or Windows.\n\nA scroll bar is visible on the right side and bottom. These are nice and wide. Very easy to hit using the mouse unlike modern macOS and Windows scroll bars which are fickle. There is one on the bottom the lower right of the window. The lower right that you can use to stretch or squeeze the window easily.\n\nAlong with the middle mouse button presenting the context menu you'll also notice that RISC OS makes extensive use of dialog boxes to complete actions. If you see an Icon in the dialog it's not there as a branding statement. That icon can be dragged to another window such as the Filer window to save a document. This is a significant difference between RISC OS and other operating systems. The Icon has a purpose beyond identification and branding. Moving the Icon and dropping it usually tells the OS to link two things.\n\n### Some historical context and web browsing\n\nRISC OS has a design philosophy and historical implementation. As I understand it the original RISC OS was written in ARM assembly language with BBC BASIC used to orchestrate the pieces written in assembly. That was how it achieved a responsive fluid system running on minimal resources. Looking back at it historically I kinda of think of BBC BASIC as a scripting language for a visual interface built from assembly language modules. The fast stuff was done in assembly but the high level bits were BBC BASIC. Today C has largely taken over the assembly language duties. It even has taken over some of the UI work too (e.g. Pinboard2 is written in C). The nice thing is BBC BASIC remains integrated and available.\n\n> BBC BASIC isn't the BASIC I remember seeing in college.  BBC BASIC appears more thorough. It includes an inline ARM assembly language support. Check out Bruce Smith's books on ARM Assembly Language on RISC OS if you want to explore that.\n\nEven through RISC OS was originally developed with BBC BASIC and assembler it wasn't developed in a haphazard fashion. A very modular approach was taken. Extending the operating system often means writing a module.  The clarity of the approach as much as the tenacity of the community has enable RISC OS to survive long past the demise of Acorn itself. It has also meant that as things have shifted from assembly to C that the modularity has remained strong aspect of RISC OS design and implementation.\n\nRISC OS in spite of its over 30 years of evolution has a remarkably consistent feel. This is in part a historical accident but also a technical one in how the original system was conceived. That consistency is also one is one of its strengths. Humans tend to prefer consistency when they can get it. It is also the reasons you don't see allot of direct ports of applications from Unix, macOS (even single user system 7) or Windows. When I work on a Linux machine I expect the GUI to be inconsistent. The X Window systems was designed for flexibility and experimentation. The graphical experience only becomes consistent within the specific Linux distribution (e.g. Raspberry Pi OS). Windows also has a history of being pretty lose about it user interface. Windows user seem much happier to accept a Mac ported application then Mac users using a Windows ported one.\n\nWhy do I bring this up? Well like WiFi many people presume availability of evergreen browsers. Unlike WiFi I think I can live without Chrome and friends.  There is a browser called NetSurf that comes with RISC OS 5.30. You can view forums and community website with targeting RISC OS with it. If you're a follower of the Tidleverse you'll find NetSurf sufficient too. It's nice not running JavaScript. If you visit GitHub though it'll look very different. Some of it will not work.  Fortunately there is a new browser on the horizon called Iris. I suspect like the addition of native WiFi support you'll see a bump in usability for RISC OS when it lands.\n\n### Access external storage\n\nRISC OS supports access to external storage services. There are several file server types supported but I found SMB the easiest to get setup. I have a Raspberry Pi 3B+ file server running Samba on Raspberry Pi OS (bookworm). The main configuration change I needed to support talking to RISC OS was to enable LANMAN1 protocol. By default the current Samba shipping with bookworm has LANMAN1 protocol turned off (they are good reasons for this). This is added in the global section of the smb.conf file. I added the following line to turn on the protocol.\n\n~~~shell\nserver min protocol = LANMAN1\n~~~\n\nBecause of the vintage nature of the network calls supported and problems of WiFi being easy to sniff I opted to remove my personal directory from Samba services. I also used a different password to access Samba from my user account (see the smbpasswd manual page for how to do that).\n\nI don't store secret or sensitive stuff on RISC OS nor do I access it from RISC OS. Like running an DOS machine security on RISC OS isn't a feature of the operating system. For casual writing, playing retro games, not a big problem but I would not do my banking on it.\n\n### Connecting to my Raspberry Pi File Server\n\nOnce I had Samba configured to support LANMAN1 I was able to connect to it from RISC OS on the Pi Zero but it was tricky to figure out how at first.  The key was to remember to use the context menu and to use the `!OMNI` application found in the \"Apps\" folder on the Iconbar.\n\nFirst open the Apps folder, then right double click on `!OMNI`.  This will create what looks like a file server on the left side of the Iconbar (where other disk resources are listed).  If you place your mouse pointer over it and middle click the context menu will pop up. Then navigate to the protocol menu, followed by LanMan. Clicking on LanMan should bring up a dialog box that will let you set the connection name, the name of the file server as well as the login credentials for that server.  There is no OK button. When you press enter after entering your password the dialog box knows you're done and OMNI will try to make the connection. You'll see the mouse pointer turn into an hour glass as it negotiates the connection. If the connection is successful you'll see a new window open with the resource's available from that service.\n\nYou can save yourself time by \"saving\" the protocol setup via the context menu. To do that you point to the OMNI icon, pull up the context menu, select protocol then select save protocol.\n\n### RISC OS Paths\n\nThe RISC OS path semantics are NOT the same as POSIX.  The RISC OS file system is hierarchical but does not have a common root like POSIX. Today RISC OS supports several types of file systems and protocols. As a result the path has a vague resemblance to a URI.\n\nThe path starts with the protocol. This is followed by a double colon then the resource name. The resource name is followed by a period (a.k.a. dot) and then the dollar sign. The dot and dollar sign represents the root of the file system resource. The dot (period) is used as a path delimiter. It appears to be a practice to use a slash to indicate a file extension. This is important to remember. A path like `myfile.txt` on RISC OS means there is a folder called \"myfile\" and a file called \"txt\" in side it. On the other hand `myfile/txt` would translate to the POSIX form of `myfile.txt`. Fortunately the SMB client provided by OMNI handles this translation for us.\n\n## first impressions\n\nI find working on RISC OS refreshing. It is proving to be a good writing platform for me. I do have a short wish list. RISC OS seems like a really good platform for exploring the text oriented internet.  I would like to see both a gopher client and server implemented on RISC OS. Similarly I think Gemini protocol makes sense too.  I miss not having Pandoc available as I use that to render my writing to various formats on POSIX systems (e.g. html, pdf, ePub).\n\nWhat might I build in RISC OS?  I'm not sure yet though I have some ideas. I really like RISC OS as a writing platform. The OS itself reminds me of some of the features I like in Scrivener. I wonder if Scrivener's Pinboard got its inspiration from RISC OS?\n\nI've written Fountain, Open Screen Play, Final Draft conversion tools in Go. I'd like to have similar tools available along with a nice editor available on RISC OS. Not sure what the easiest approach to doing that is. I've ordered the [ePic](https://www.riscosopen.org/content/sales/risc-os-epic/epic-overview) SD card and that'll have the [DDE](https://www.riscosopen.org/content/sales/dde) for Raspberry Pi. It might be interesting to port OBNC Oberon-07 compiler to RISC OS and see if I could port my Go code to Oberon-07 in a sensible way.\n",
      "data": {
        "abstract": "In this post I talk about my exploration of using a Raspberry Pi Zero W\nas a desktop computer. This was made possible by the efficiency of \nRISC OS 5.30 which includes native WiFi support for Raspberry Pi computers.\n",
        "byline": "R. S. Doiel, 2024-06-04",
        "keywords": [
          "RISC OS",
          "Raspberry Pi"
        ],
        "pubDate": "2024-06-04",
        "title": "Exploring RISC OS 5.30 on a Raspberry Pi Zero W"
      },
      "url": "posts/2024/06/04/exploring_riscos.json"
    },
    {
      "content": "\n# A quick review of Raspberry Pi Connect\n\nThe Raspberry Pi company has created a nice way to share a Pi Desktop. It is called Raspberry Pi Connect. It is built on the peer-to-peer capability of modern web browsers using [WebRTC](https://en.wikipedia.org/wiki/WebRTC). The connect service requires a Raspberry Pi 4, Raspberry Pi 400 or Raspberry Pi 5 running the [Wayland](https://en.wikipedia.org/wiki/Wayland_(protocol)) display server and Bookworm release of Raspberry Pi OS.\n\nWhen I read the [announcement](https://www.raspberrypi.com/news/raspberry-pi-connect/) I wondered, why create Raspberry Pi Connect? RealVNC has works fine.  RealVNC even has a service to manage your RealVNC setups.\n\nI think the answer has three parts. First it gives us another option for sharing a Pi Desktop. Second it is a chance to make things easier to use. Third if you can share a desktop using WebRTC then you can also provide additional services.\n\nFor me the real motivator is ease of use. In the past when I've used RealVNC between two private networks I've had to setup SSH tunneling. Not unmanageable but certainly not trivial.  I think this is where Raspberry Pi Connect shines. Setting up sharing is a three step process.\n\n1. Start up your Pi desktop, install the software\n2. Create a Raspberry Pi Connect account and register your Pi with the service\n3. On another machine point your web browser at the URL for Raspberry Pi connect and press the connect button\n\nThe next time you want to connect you just turn on your Pi and login. If I have my Pi desktop to auto login then I just turn the Pi on and when it finishes booting it is ready and waiting. On my other machine I point my web browser at the connect website, login and press the connection button.\n\nWhen I change computers I don't have to install VNC viewers. I don't have to worry about setting secure ssh tunnels. I point my web browser at the Raspberry Pi Connect site, login and press the connect button. The \"one less thing to worry about\" can make it feel much less cumbersome.\n\n## How does it work?\n\nThe Raspberry Pi Connect architecture is intriguing. It leverages [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API). WebRTC supports peer to peer real time connection between two web browsers running in separate locations across the internet. Making a WebRTC requires the two sites to use a URL to establish contact. From that location perform some handshaking and see if the peer connection can be establish a directly between the two locations. If the direct connection can't be established then a relay or proxy can be provided as a fallback. \n\nThe Raspberry Pi Connect site provides the common URL to contact. On the Pi desktop side a Wayland based service provides access to the Pi's desktop. On the other side you use a Web Browser to display and interact with the desktop. Ideally the two locations can establish a direct connection. If that is not possible then Raspberry Pi Connect hosts [TURN](https://en.wikipedia.org/wiki/Traversal_Using_Relays_around_NAT) in London as a fallback. A direct connection gives you a responsive shared desktop experience but if you're on the Pacific edge of North America or on a remote Pacific island then traffic being relayed via London can be a painfully slow experience.\n\nThe forum for the Raspberry Pi Connect has a [topic](https://forums.raspberrypi.com/viewtopic.php?t=370591&sid=61d7cdf3c03a7ead49e3da837b0d4f06) discussing the routing algorithm and choices. The short version is exerted below.\n\n> Essentially, when the connection was being established both sides provided their internet addresses (local, and WAN) - and when both sides tested their ability to talk to the other side, they failed. Only after this failure is the TURN server used.\n\n## Questions\n\nCan I replace RealVNC with Raspberry Pi Connect?\n\nIt depends. I still use Raspberry Pi 2, 3 and some Zeros. I'm out of luck using Pi Connect since these devices aren't supported. If you've already installed RealVNC and it's working well for you then sharing via Pi connect is less compelling.\n\nIf I was setting up a new set of Raspberry Pi 4/400 or 5s then I'd probably skip RealVNC and use Pi connect. It's feels much easier and unless the network situation forces you to route traffic through London is reasonably responsive.\n\nIs screen sharing the only thing Raspberry Pi Connect provides?\n\nI expect if Raspberry Pi Connect proves successful we'll see other enhancements. One of the ones mentioned in the forums was SSH services without the hassle of dealing with setting up tunnels. The folks in the Raspberry Pi company, foundation and community are pretty creative. It'll be interesting to see where this leads.\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-05-10",
        "keywords": [
          "raspberry pi"
        ],
        "pubDate": "2024-05-10",
        "title": "A quick review of Raspberry Pi Connect"
      },
      "url": "posts/2024/05/10/quick-review-rpi-connect.json"
    },
    {
      "content": "\n# Building Lagrange on Raspberry Pi OS\n\nThese are my quick notes on building the Lagrange Gemini browser on Raspberry Pi OS. They are based on instructions I found at <gemini://home.gegeweb.org/install_lagrange_linux.gmi>. These are in French and I don't speak or read French. My loss. The author kindly provided the specific command sequence in shell that I could read those. That was all I needed. When I read the site today I had to click through an expired certificate. That's why I think it is a good idea to capture the instructions here for the next time I need them.  I made single change to his instructions. I have cloned the repository from <https://github.com/skyjake/lagrange>.\n\n## Steps to build\n\n1. Install the programs and libraries in Raspberry Pi OS to build Lagrange\n2. Create a directory to hold the repository, then change into it\n3. Clone the repository\n4. Add a \"build\" directory to the repository and change into it\n5. Run \"cmake\" to build the release\n6. Run \"make\" in the build directory and install\n7. Test it out.\n\nWhen you clone the repository you want to clone recursively and get the release branch. Below is a transcript of the commands I typed in my shell to build Lagrange on my Raspberry Pi 4.\n\n~~~shell\nsudo apt install build-essential cmake \\\n     libsdl2-dev libssl-dev libpcre3-dev \\\n     zlib1g-dev libunistring-dev git\nmkdir -p src/github.com/skyjake && cd src/github.com/skyjake \ngit clone --recursive --branch release git@github.com:skyjake/lagrange.git\nmkdir -p lagrange/build && lagrange/build\ncmake ../ -DCMAKE_BUILD_TYPE=Release\nsudo make install\nlagrange &\n~~~\n\nThat's about it. It worked without a hitch. I'd like to thank Gérald Niel who I think created the page on gegeweb.org. I attempted to leave a thank you via the web form but couldn't get past the spam screener since I didn't understand the instructions. C'est la vie.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-05-10",
        "pubDate": "2024-05-10",
        "title": "Building Lagrange on Raspberry Pi OS"
      },
      "url": "posts/2024/05/10/building-lagrange-on-pi-os.json"
    },
    {
      "content": "\n# Getting Started with Miranda\n\nI've been interested in exploring the Miranda programming language. Miranda influenced Haskell. Haskell was used for programs I use almost daily such as [Pandoc](https://pandoc.org) and [shellcheck](https://www.shellcheck.net/). I've given a quick review of [miranda.org.uk](https://miranda.org.uk) to get a sense of the language but to follow along with the [Miranda: The Craft of Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/Miranda_craft/) it is really helpful to have Miranda available on my machine. Today that machine is a Mac Mini, M1 processor, running macOS Sonoma (14.4.x) and the related Xcode C tool chain.  I ran into to minor hiccups in compilation and installation. Both easy to overcome but ones I will surely forget in the future. Thus I write myself another blog post.\n\n## Compilation\n\nFirst down load Miranda source code at <http://miranda.org.uk/downloads>. The version 2.066 is the most recent release I saw linked (2024-04-25), <http://www.cs.kent.ac.uk/people/staff/dat/ccount/click.php?id=11>. The [COPYING](https://www.cs.kent.ac.uk/people/staff/dat/miranda/downloads/COPYING) link shows the terms under which this source release is made available.\n\nNext you need to untar/gzip the tarball you downloaded. Try running `make` to see if it compiles. On my Mac Mini I got a compile error that looks like\n\n~~~shell\nmake\ngcc -w    -c -o data.o data.c\ndata.c:666:43: error: incompatible integer to pointer conversion passing 'word' (aka 'long') to parameter of type 'char *' [-Wint-conversion]\n                     else fprintf(f,\"%c%s\",HERE_X,mkrel(hd[x]));\n                                                        ^~~~~\n1 error generated.\nmake: *** [data.o] Error 1\n~~~\n\nWhile I'm rusty on C I read this as the C compiler being more strict today then it was back in the 1990s. That's a good thing generally.  Next I checked the compiler version. \n\n~~~shell\ngcc --version\nApple clang version 15.0.0 (clang-1500.3.9.4)\nTarget: arm64-apple-darwin23.4.0\nThread model: posix\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\n~~~\n\nI'm using clang and the website mentioned it should compile with clang for other platforms.  I reviewed the data.c file and notice other similar lines that invoked `mkrel(hd[x])` had a `(char *)` cast in front of `hd[x]`. This tells me that being explicit with the compiler might solve my problem. I edited line 666 of data.c to look like\n\n~~~C\n    else fprintf(f,\"%c%s\",HERE_X,mkrel((char *)hd[x]));\n~~~\n\nSave the file and then ran Make again. It compile cleanly. I gave at quick test run of the `mira` command creating an simple function called `addone`\n\n~~~miranda\nmira\n/edit\naddone a = a + 1\n:wq\naddone (addone (addone 3))\n6\n/q\n~~~\n\nMiranda seems to work. The Makefile comes with a an install rule but the install defaults doesn't really work with macOS (it wants to install into `/usr`).\nI'd rather it install into my home directory so I copied the Makefile to `miranda.mak` and change the lines setting `BIN`, `LIB` and `MAN` to the following\nlines.\n\n~~~Makefile\nBIN=$(HOME)/bin\nLIB=$(HOME)/lib#beware no spaces after LIB\nMAN=$(HOME)/man/man1\n~~~\n\nIn my `.profile` I set the `MIRALIB` variable to point at `$HOME/lib/miralib`. I opened a new terminal session and ran `mira` and the interpreter was up and running.\n\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-04-25",
        "keywords": [
          "functional",
          "miranda"
        ],
        "pubDate": "2024-04-25",
        "title": "Getting Started with Miranda"
      },
      "url": "posts/2024/04/25/getting-started.json"
    },
    {
      "content": "\n# A Text oriented web\n\nBy R. S. Doiel, 2024-02-25\n\nThe web is a busy place. There seems to be a gestalt resonant at the moment on the web that can be summarized by two phrases, \"back to basics\" and \"simplification\". It is not the first time I've seen this nor is it likely the last. This blog post describes a thought experiment about a simplification with minimal invention and focus on feature elimination. It's a way to think about the web status quo a little differently. My intention is to explore the implications of a more text centered web experience that could coexist as a subset of today's web.\n\n## The web's \"good stuff\"\n\nI think the following could form the \"good stuff\" in a Crockford[^1] sense of pairing things down to the essential.\n\n- the transport layer should remain HTTP but be limited to a few methods (GET, POST and HEAD) and the common header elements (e.g. length, content-type come to mind)\n- The trio of HTML, CSS and JavaScript is really complex, swap this out for Markdown augmented with YAML (Markdown and YAML already have a synergy in Markdown processors like Pandoc)\n- A Web form is expressed using GHYITS[^2], it is delimited in the Markdown document by the familiar \"`^---$`\" block element, web form content would be encoded as YAML in the body of the POST using the content type \"`application/x-yaml`\".\n- Content would be served using the `text/markdown; charset: utf-8` content type already commonly used to identify Markdown content distinct from `plain/text`\n\nI need a nice name for describing the arrangement of Markdown+YAML over HTTP arrangement. Is the descriptive acronym for \"text oriented web\", i.e. \"tow\", enough? Does it already have a meaning in software or the web? Would the protocol be \"tow://\"? I really need something a bit more clever and catchy if this is going to proceed beyond a thought experiment.\n\n[^1]: Douglas Crockford \"discovered\" JSON, see <https://en.wikipedia.org/wiki/Douglas_Crockford>\n\n[^2]: GHYITS, acronym, GitHub YAML Issue Template Syntax, see <https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/syntax-for-issue-forms>\n\n## Prototyping Options\n\nA proof of concept could be possible using off the self web server and web browser. The missing parts would be setting up the web server to add the `text/markdown; charset: utf-8` header for \"`.md`\" files and to handle processing POST with a content type of `application/x-yaml`. Client side could be implemented in a static web page via JavaScript or WASM module. The JS/WASM could convert the Markdown+YAML into HTML rendering the \"tow\" content. The web form on submit would be intercepted by the JavaScript event handler and reformulated as a POST with a content type of `application/x-yaml`.\n\nBuilding a \"tow\" server and client should be straight forward in Go (probably many languages). The the standard \"http\" package can be used to implement the specialized http server. The `yaml.v3` package to process the YAML POST data. Similar you should be able to create a text client for the command line or even a GUI client via [Fyne](https://fyne.io)\n\n## Exploratory Questions\n\n- What does it mean to have a more text oriented web?\n- What advantages could a text user interface have over a graphical user interface?\n- Can \"tow\" provide enough simple interactivity to support interactive fiction?\n- Could a simple specification be stated clearly in a few pages of text?\n- What possibilities open up when a web browser can send a data structure via YAML to a service?\n- Can we live with a simpler client than a modern evergreen web browser?\n- With a conversation interaction model of \"listener\" and a \"speaker\", does it make sense thinking in terms of client server architecture?\n- How hard is it to support both traditional website and this minimal \"tow\" site using the same corpus?\n- Can this be done sustainably?\n\n## Extrapolations\n\nFrom a thought experiment I can see how to implement this both from a proof of concept level but also from a service and viewer level. I think it even offers an opportunity to function in a peer to peer manner.  If we're focusing primarily on text then the storage requirements can be minimal and the service could even live in a database system like SQLite3 as a form of sandbox of content.  Leveraging HTTP/HTTPS means we don't need any special support for content traveling across the net. With a much smaller foot print you can scratch the itch of a simpler textual experience without the trackers, JavaScript ping backs, etc. It could re-emphasize the conversion versus broadcast metaphor popularized by the walled gardens.  It might provide a more satisifying experience on Mobile since the payloads delivered to the web browser could be much smaller.\n\n## What is needed to demonstrate a standalone \"tow\"?\n\n- A modified HTTP web server (easy to implement in Go and other languages)\n- A viewer/browser, possible to implement via Fyne in Go or as a text application/command line interface in Go\n\n## Why not Gopher or Gemini?\n\nTow is not there to replace anything, not Gopher, Not Gemini, the WWW. It is an exploration of a subset of the WWW protocols with a specific focused on textual interaction. I don't see why a server or browser couldn't support Gopher and Gemini as well as Tow. Given that Markdown can easily be rendered into Gem Text, and Markdown can be treated as plain text I suspect you should be able to support all three text rich systems from the same copy and easily derive a full HTML results if desired too.\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-02-25",
        "createdDate": "2024-02-25",
        "keywords": [
          "web",
          "text"
        ],
        "pubDate": "2024-02-25",
        "title": "A Text Oriented Web"
      },
      "url": "posts/2024/02/25/text_oriented_web.json"
    },
    {
      "content": "\n# Installing pgloader from source\n\nBy R. S. Doiel, 2024-02-01\n\nI'm working on macOS at the moment but I don't use Home Brew so the instructions to install pgloader are problematic for me. Except I know pgloader is a Lisp program and once upon a time I had three different Lisps running on a previous Mac.  So what follows is my modified instructions for bringing pgloader up on my current Mac Mini running macOS Sonoma 14.3 with Xcode already installed.\n\n## Getting your Lisps in order\n\npgloader is written in common list but the instructions at https://pgloader.readthedocs.io/en/latest/install.html specifically mention compiling with [SBCL](https://sbcl.org) which is one of the Lisps I've used in the past. But SBCL isn't (yet) installed on my machine and SBCL is usually compiled using SBCL but can be compiled using other common lists.  Enter [ECL](https://ecl.common-lisp.dev/), aka Embedded Common-Lisp. ECL compiles via a C compiler including the funky setup that macOS has. This means the prep for my machine should look something like\n\n1. Compile then install ECL\n2. Use ECL to compile SBCL\n3. Install SBCL\n4. Now that we have a working SBCL, follow the instructions to compile pgloader and install\n\nNOTE: pgloader requires some specific configuration of SBCL when SBCL is compiled\n\n## Getting ECL up and running\n\nThis recipe is straight forward. \n\n1. Review ECL's current website, find latest releases\n2. Clone the Git repository from GitLab for ECL\n3. Follow the install documentation and compile ECL then install it\n\nHere's the steps I took in the shell (I'm installing ECL, SBCL in my home directory)\n\n```\ncd\ngit clone https://gitlab.com/embeddable-common-lisp/ecl.git \\\n          src/gitlab.com/embeddable-common-lisp/ecl\ncd src/gitlab.com/embeddable-common-lisp/ecl\n./configure --prefix=$HOME\nmake\nmake install\n```\n\n## Getting SBCL up and running\n\nTo get SBCL up and running I grab the sources using Git then compile it with the options recommended by pgloader as well as the options to compile SBCL with another common lisp, i.e. ECL. (note: the `--xc-host='ecl'`)\n\n```\ncd\ngit clone git://git.code.sf.net/p/sbcl/sbcl src/git.code.sf.net/p/sbcl/sbcl\ncd git clone git://git.code.sf.net/p/sbcl/sbcl\nsh make.sh --with-sb-core-compression --with-sb-thread --xc-host='ecl'\ncd ./tests && sh ./run-tests.sh\ncd ..\ncd ./doc/manual && make\ncd ..\nenv INSTALL_ROOT=$HOME sh install.sh\n```\n\nAt this time SBCL should be available to compile pgloader.\n\n## Install Quicklisp\n\nQuicklisp is a package manager for Lisp. It is used by pgloader so also needs to be installed. We have two lisp on our system but since SBCL is the one I need to work for pgloader I install Quicklisp for SBCL.\n\n1. Check the [Quicklisp website](https://www.quicklisp.org/beta/) and see how things are done (it has been a long time since I did some lisp work)\n2. Follow the [instructions](https://www.quicklisp.org/beta/#installation) on the website to install Quicklisp for SBCL\n\nThis leaves me with the specific steps\n\n1. Use curl to download quicklisp.lisp\n2. Use curl to download the signature file\n3. Verify the signature file\n4. If OK, load into SBCL\n5. From the SBCL repl execute the needed commands\n\n```\ncurl -O https://beta.quicklisp.org/quicklisp.lisp\ncurl -O https://beta.quicklisp.org/quicklisp.lisp.asc\ngpg --verify quicklisp.lisp.asc quicklisp.lisp\nsbcl --load quicklisp.lisp\n```\n\nAt this point you're in SBCL repl. You need to issue the follow command\n\n```\n(quicklisp-quickstart:install)\n(quit)\n```\n\n\n## Compiling pgloader\n\nOnce you have SBCL and Quicklisp working you're now ready to look at the rest of the dependencies. Based on the what other Linux systems required I figure I need to have the following available\n\n- SQLite 3, libsqlite shared library (already installed)\n- unzip (already installed)\n- make (already installed)\n- curl (already installed)\n- gawk (already installed)\n- freetds-dev (not installed)\n- libzip-dev (not installed)\n\nTwo libraries aren't installed on my system. I use Mac Ports so doing a quick search both appear to be available.\n\n```\nsudo port search freetds\nsudo port search libzip\nsudo port install freetds libzip\n```\n\n\nOK, now I think I am ready to build pgloader. Here's what I need to do.\n\n1. Clone the git repo for pgloader\n2. Invoke make with the right options\n3. Test installation\n\n```\ncd\ngit git@github.com:dimitri/pgloader.git src/github.com/dimitri/pgloader\ncd src/github.com/dimitri/pgloader\nmake save\n./build/bin/pgloader -h\n```\n\nIf all works well I should see the help/usage text for pgloader. The binary executable\nis located in `./build/bin` so I can copy this into place in `$HOME/bin/` directory.\n\n```\ncp ./build/bin/pgloader $HOME/bin/\n```\n\nHappy Loading.\n, \"PostgreSQL\"\n\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-02-01",
        "keywords": [
          "SQL",
          "Postgres",
          "PostgreSQL",
          "MySQL",
          "pgloader",
          "lisp",
          "macos",
          "ecl",
          "sbcl"
        ],
        "number": 6,
        "pubDate": "2024-02-01",
        "series": "SQL Reflections",
        "title": "Installing pgloader from source"
      },
      "url": "posts/2024/02/01/installing-pgloader-from-source.json"
    }
  ]
}