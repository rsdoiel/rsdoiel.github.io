{
  "page": 3,
  "total_pages": 6,
  "has_more": true,
  "next_page": "posts/all/page-4.json",
  "values": [
    {
      "content": "\nPostgreSQL dump and restore\n===========================\n\nThis is a quick note on easily dumping and restoring a specific database\nin Postgres 14.5.  This example has PostgreSQL running on localhost and\n[psql](https://www.postgresql.org/docs/current/app-psql.html) and\n[pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) are both available.\nOur database administrator username is \"admin\", the database to dump is called \"collections\". The SQL dump\nfile will be named \"collections-dump-2022-09-19.sql\".\n\n```shell\n\tpg_dump --username=admin --column-inserts \\\n\t    collections >collections-dump-2022-09-19.sql\n```\n\nFor the restore process I follow these steps\n\n1. Using `psql` create an empty database to restore into\n2. Using `psql` replay (import) the dump file in the new database to restoring the data\n\nThe database we want to restore our content into is called \"collections_snapshot\"\n\n```shell\n\tpsql -U dbadmin\n\t\\c postgres\n\tDROP DATABASE IF EXISTS collections_snapshot;\n\tCREATE DATABASE collections_snapshot;\n\t\\c collections_snapshots\n\t\\i ./collections-dump-2022-09-19.sql\n\t\\q\n```\n\nOr if you want to stay at the OS shell level\n\n```shell\n\tdropdb collections_snapshot\n\tcreatedb collections_snapshot\n\tpsql -U dbadmin --dbname=collections_snapshot -f ./collections-dump-2022-09-19.sql\n```\n\n\nNOTE: During this restore process `psql` will display some output. This is normal. The two\ntypes of lines output are shown below.\n\n```sql\n\tINSERT 0 1\n\tALTER TABLE\n```\n\nIf you want to stop the input on error you can use the `--set` option to set the error behavior\nto abort the reload if an error is encountered.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-09-19",
        "draft": true,
        "keywords": [
          "PostgreSQL"
        ],
        "pubDate": "2022-09-19",
        "title": "PostgreSQL dump and restore"
      },
      "url": "posts/2022/09/19/PostgreSQL-Dump-and-Restore.json"
    },
    {
      "content": "\n# 12:30 PM, SQL: Postgres\n\nPost: Monday, September 19, 2022, 12:30 PM\n\nSetting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 1,
        "pubDate": "2022-09-19",
        "series": "SQL",
        "title": "12:30 PM, SQL: Postgres"
      },
      "url": "posts/2022/09/19/rosette-notes-2022-09-19_121230.json"
    },
    {
      "content": "\nOrdering Front Matter\n=====================\n\nBy R. S. Doiel, 2022-08-30\n\nA colleague of mine ran into an interesting Pandoc behavior. He was combining a JSON metadata document and a converted word document and wanted the YAML front matter to have a specific order of fields (makes it easier for us humans to quickly scan it and see what the document was about).\n\nThe order he wanted in the front matter was\n\n- title\n- interviewer\n- interviewee\n- abstract\n\nThis was for a collection of oral histories. When my friend use Pandoc's `--metadata-json` to read the JSON metadata it rendered the YAML fine except the attributes were listed in alphabetical order.\n\nWe found a solution by getting Pandoc to treat the output not as Markdown plain text so that we could template the desired order of attributes.\n\nHere's the steps we used.\n\n1. create an empty file called \"empty.txt\" (this is just so you pandoc doesn't try to read standard input and processes\nyou metadata.json file with the template supplied)\n2. Create a template with the order you want (see below)\n3. Use pandoc to process your \".txt\" file and your JSON metadata file using the template (it makes it tread it as plain text even though we're going to treat it as markdown later)\n4. Append the content of the word file and run pandoc over your combined file as you would normally to generate your HTML\n\n\nThis is the contents of our [metadata.json](metadata.json) file.\n\n```json\n    {\n        \"title\": \"Interview with Mojo Sam\", \n        \"interviewee\": \"Mojo Sam\", \n        \"interviewer\": \"Tom Lopez\",\n        \"abstract\": \"Interview in three sessions over sevaral decases, 1970 - 20020. The interview was conducted next door to reality via a portal in Old Montreal\"\n    }\n```\n\n[frontmatter.tmpl](frontmatter.tmpl) is the template we used to render ordered front matter.\n\n```\n    ---\n    $if(title)$title: \"$title$\"$endif$\n    $if(interviewee)$interviewee: \"$interviewee$\"$endif$\n    $if(interviewer)$interviewer: \"$interviewer$\"$endif$\n    $if(abstract)$abstract: \"$abstract$\"$endif$\n    ---\n```\n\nHere's the commands we used to generate a \"doc.txt\" file with the \nfront matter in the desired order. Not \"empty.txt\" is just an empty\nfile so Pandoc will not read from standard input and just work with the\nJSON metadata and our template.\n\n```\ntouch empty.txt\npandoc --metadata-file=metadata.json --template=frontmatter.tmpl empty.txt\n```\n\nThe output of the pandoc command looks like this.\n\n```\n    ---\n    title: \"Interview with Mojo Sam\"\n    interviewee: \"Mojo Sam\"\n    interviewer: \"Tom Lopez\"\n    abstract: \"Interview in three sessions over sevaral decases, 1970 -\n    20020. The interview was conducted next door to reality via a portal in\n    Old Montreal\"\n    ---\n```\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "pandoc",
          "front matter"
        ],
        "pubDate": "2022-08-30",
        "title": "Ordering front matter"
      },
      "url": "posts/2022/08/30/Ordering-Frontmatter.json"
    },
    {
      "content": "\nPostgres 14 on Ubuntu 22.04 LTS\n===============================\n\nby R. S. Doiel, 2022-08-26\n\nThis is just a quick set of notes for working with Postgres 14 on an Ubuntu 22.04 LTS machine.  The goal is to setup Postgres 14 and have it available for personal work under a user account (e.g. jane.doe). \n\nAssumptions\n\n- include `jane.doe` is in the sudo group\n- `jane.doe` is the one logged in and installing Postgres for machine wide use\n- `jane.doe` will want to work with her own database by default\n\nSteps\n\n1. Install Postgres\n2. Confirm installation\n3. Add `jane.doe` user providing access\n\nBelow is the commands I typed to run to complete the three steps.\n\n~~~shell\nsudo apt install postgresql postgresql-contrib\nsudo -u createuser --interactive\njane.doe\ny\n~~~\n\nWhat we've accomplished is installing Postgres, we've create a user in Postgres DB environment called \"jane.doe\" and given \"jane.doe\" superuser permissions, i.e. the permissions to manage Postgres databases.\n\nAt this point we have a `jane.doe` Postgres admin user. This means we can run the `psql` shell from the Jane Doe account to do any database manager tasks. To confirm I want to list the databases available\n\n~~~shell\npsql \nSELECT datname FROM pg_database;\n\\quit\n~~~\n\nNOTE: This post is a distilation of what I learned from reading Digital Ocean's [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel",
        "byline": "R. S. Doiel",
        "daft": true,
        "number": 4,
        "pubDate": "2022-08-26",
        "series": "SQL Reflections",
        "title": "Postgres 14 on Ubuntu 22.04 LTS"
      },
      "url": "posts/2022/08/26/postgres-14-on-ubuntu-22.04-LTS.json"
    },
    {
      "content": "\n# 10:30 AM, SQL: Postgres\n\nPost: Friday, August 26, 2022, 10:30 AM\n\nIf you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 2,
        "pubDate": "2022-08-26",
        "series": "SQL",
        "title": "10:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/26/rosette-notes-2022-08-26_101030.json"
    },
    {
      "content": "\n# 12:00 PM, SQL: Postgres\n\nPost: Wednesday, August 24, 2022, 12:00 PM\n\nI miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%'\n`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 3,
        "pubDate": "2022-08-24",
        "series": "SQL",
        "title": "12:00 PM, SQL: Postgres"
      },
      "url": "posts/2022/08/24/rosette-notes-2022-08-24_121200.json"
    },
    {
      "content": "\nA Quick intro to PL/pgSQL\n========================\n\nPL/pgSQL is a procedure language extended from SQL. It adds flow control and local state for procedures, functions and triggers. Procedures, functions and triggers are also the compilation unit. Visually PL/pgSQL looks similar to the MySQL or ORACLE counter parts. It reminds me of a mashup of ALGO and SQL. Like the unit of compilation, the unit of execution is also procedure, function or trigger. \n\nThe Postgres documentation defines and explains the [PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql.html) and how it works.  This document is just a quick orientation with specific examples to provide context.\n\nHello World\n-----------\n\nHere is a \"helloworld\" procedure definition.\n\n```sql\n    CREATE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n       RAISE NOTICE 'Hello WORLD!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nLet's take a look this line by line.\n\n1. CREATE PROCEDURE defines the procedure and the starting and ending delimiter for the procedure (e.g. `AS $$` the procedure's text ends when `$$` is encountered an second time.\n2. DECLARE is the block where you would declare the variables used in the procedure, we have none in this example\n3. The BEGIN starts the actual procedure instructions\n4. The `RAISE NOTICE` line is how you can display output to the console when the procedure is run\n5. The END completes the procedure definition\n6. the `$$ LANGUAGE plpgsql;` concludes the text defining the procedure and tells the database engine that procedure is written in PL/pgSQL.\n\nWe can run the procedure using the \"CALL\" query.\n\n```sql\n    CALL helloworld()\n```\n\nNOTE: If you want to change the procedure you can \"DROP\" it first otherwise you'll get an error that it already exists.\n\n```sql\n    DROP PROCEDURE helloworld;\n```\n\nImproving my workflow\n---------------------\n\nSQL procedures are generally stored in the RDBMs in database environment. You can think of them as records in the system's database. Procedures and functions are created and can be dropped. While they can be manually typed in the database's shell it is easier to maintain them in plain text files outside the RDBM environment.  \n\n1. Write the procedure in a text file.\n2. Load the text file (e.g. FILENAME) into Postgres \n   a. outside the Postgres shell use `psql -f FILENAME` \n   b. inside the Postgres shell used `\\i FILENAME`\n3. Call the procedure to test it\n\nTo turn these steps into a look I use a \"CREATE OR REPLACE\" statement and be able to reload the updated procedure easier see [43.12. Tips for Developing in PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql-development-tips.html).  Note in the revised example the \"-- \" lines are comments.\n\nOur revised [helloworld](helloworld.plpgsql).\n\n```sql\n    --\n    -- Create (or replace) the new \"helloworld\" procedure.\n    -- NOTE: this can be run with \"CALL\"\n    --\n    CREATE OR REPLACE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n        RAISE NOTICE 'Hello World!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\n\nHi There\n--------\n\n[hithere](hithere.plpgsql) is similar to our helloworld example except it is a function that takes a parameter of the person's name. The function returns a \"VARCHAR\", so this should work as part of a select statement.\n\n```sql\n    --\n    -- This is a \"Hi There\" function. The function takes\n    -- a single parameter and forms a greeting.\n    --\n    CREATE OR REPLACE FUNCTION hithere(name varchar) RETURNS varchar AS $$\n    DECLARE\n      greeting varchar;\n    BEGIN\n        IF name = '' THEN\n            greeting := 'Hi there!';\n        ELSE\n            greeting := 'Hello ' || name || '!';\n        END IF;\n        RETURN greeting;\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nGiving it a try.\n\n```shell\n    SELECT hithere('Mojo Sam');\n```\n\nFurther reading\n---------------\n\n- [Conditionals](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONDITIONALS)\n- [Loops](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONTROL-STRUCTURES-LOOPS)\n- [Calling a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-CALLING-PROCEDURE)\n- [Early return from a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-RETURNING-PROCEDURE)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-24",
        "keywords": [
          "postgres",
          "sql",
          "psql",
          "plsql",
          "plpgsql"
        ],
        "number": 3,
        "pubDate": "2022-08-24",
        "series": "SQL Reflections",
        "title": "A Quick into to PL/pgSQL"
      },
      "url": "posts/2022/08/24/plpgsql-quick-intro.json"
    },
    {
      "content": "\n# 11:30 AM, SQL: Postgres\n\nPost: Monday, August 22, 2022, 11:30 AM\n\nThree things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 4,
        "pubDate": "2022-08-22",
        "series": "SQL",
        "title": "11:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/22/rosette-notes-2022-08-22_111130.json"
    },
    {
      "content": "\nRosette Notes\n=============\n\nBy R. S. Doiel, 2022-08-19\n\n> A dance around two relational databases, piecing together similarities as with the tiny mosaic tiles of a guitar's rosette\n\nWhat follows are my preliminary notes learning Postgres 12 and 14.\n\nPostgres & MySQL\n----------------\n\nThis is a short comparison of some administrative commands I commonly use. The first column describes the task followed by the SQL to execute for Postgres 14.5 and then MySQL 8. The presumption is you're using `psql` to access Postgres and `mysql` to  access MySQL. Values between `<` and `>` should be replaced with an appropriate value.\n\n| Task                    | Postgres 14.5                     | MySQL 8           |\n|-------------------------|------------------------------------|-------------------|\n| show all databases      | `SELECT datname FROM pg_database;` | `SHOW DATABASES;` |\n| select a database       | `\\c <dbname>`                      | `USE <dbname>`    |\n| show tables in database | `\\dt`                              | `SHOW TABLES;`    |\n| show columns in table   | `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '<table_name>';` | `SHOW COLUMNS IN <table_name>` |\n\nReflections\n-----------\n\nThe Postgres shell, `psql`, provides the functionality of showing a list of tables via a short cut while MySQL choose to add the `SHOW TABLES` query. For me `SHOW ...` feels like SQL where as `\\d` or `\\dt` takes me out of SQL space. On the other hand given Postgres metadata structure the shortcut is appreciated and I often query for table names as I forget them. `\\dt` quickly becomes second nature and is shorter to type than `SHOW TABLES`. \n\nConnecting to a database with `\\c` in `psql` is like calling an \"open\" in programming language. The \"connection\" in `psql` is open until explicitly closed or the shell is terminated.  Like `USE ...` in the MySQL shell it make working with multiple database easy.  The difference are apparent when you execute a `DROP DATABASE ...` command. In `psql` you need to `CLOSE` the database first or the `DROP` will fail.  The MySQL shell will happily let you drop the current database you are currently using.\n\nThe challenge I've experienced learning `psql` after knowing MySQL is my lack of familiarity with the metadata Postgres maintains about databases and structures.  On the other hand everything I've learned about standards base SQL applies to managing Postgres once remember the database/table I need to work with.  A steeper learning curve from MySQL's `SHOW` but it also means writing external programs for managing Postgres databases and tables is far easier because everything is visible because that is how you manage Postgres. MySQL's `SHOW` is very convenient but at the cost of hiding some of its internal structures.\n\nBoth MySQL and Postgres support writing programs in SQL. They also support stored procedures, views and triggers. They've converged in the degree in which they have both implemented SQL language standards.  The differences are mostly in approach to managing databases.  There are some differences, necessitated by implementation choices, in the `CREATE DATABASE`, `CREATE TABLE` or `ALTER` statements but you can often use the basic form described in ANSI SQL and get the results you need. When doing performance tuning the dialect differences are more important.\n\nDump & Restore\n--------------\n\nBoth Postgres and MySQL provide command line programs for dumping a database. MySQL provides a single program where as Postgres splits it in two. Check the man pages (or website docs) for details in their options. Both sets of programs are highly configurable allowing you to dump just schema, just data or both with different expectations.\n\n| Postgres 14.5      | MySQL 8                         |\n|--------------------|---------------------------------|\n| `pg_dumpall`       | `mysqldump --all-databases`     |\n| `pg_dump <dbname>` | `mysqldump --database <dbname>` |\n\nThe `pg_dumpall` tool is designed to restore an entire database instance. It includes account and ownership information. `pg_dump` just focuses on the database itself. If you are taking a snapshot production data to use in a test `pg_dump` output is easier to work with. It captures the specific database with out entangling things like the `template1` database or database user accounts and ownership.\n\nYou can restore a database dump in both Postgres and MySQL. The tooling is a little different.\n\n| Postgres 14.5                   | MySQL 8                                      |\n|---------------------------------|----------------------------------------------|\n| `dropdb <dbname>`               | `mysql -execute 'DROP DATABASE <dbname>;'`   |\n| `createdb <dbname>`             | `mysql -execute 'CREATE DATABASE <dbname>;'` |\n| `psql -f <dump_filename>`       |`mysql <dbname> < <dump_filename>`            |\n\nNOTE: These instructions work for a database dumped with `pg_dump` for the Postgres example. In principle it is the same way you can restore from `pg_dumpall` but if you Postgres instance already exists then you're going to run into various problems, e.g. errors about `template1` db.\n\nLessons learned along the way\n-----------------------------\n\n2022-08-22\n\n8:00 - 11:30; SQL; Postgres; Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n\n2022-08-24\n\n11:00 - 12:00; SQL; Postgres; I miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%';`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n2022-08-26\n\n9:30 - 10:30; SQL; Postgres; If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n2022-09-19\n\n10:30 - 12:30; SQL; Postgres; Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-19",
        "keywords": [
          "postgres",
          "mysql",
          "sql",
          "psql"
        ],
        "number": 2,
        "pubDate": "2022-08-19",
        "series": "SQL Reflections",
        "title": "Rosette Notes: Postgres and MySQL",
        "updated": "2022-09-19"
      },
      "url": "posts/2022/08/19/rosette-notes.json"
    },
    {
      "content": "\nPttk and STN\n============\n\nBy R. S. Doiel, started 2022-08-15\n(updated: 2022-09-26, pdtk was renamed pttk)\n\nThis log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the \"blogit\" action that will translates the simple timesheet notation (aka STN) to a short blog post.  My \"short post\" interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.\n\n2022-08-15\n\n16:45 - 17:45; Golang; ptdk, stngo; Thinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n2022-08-14\n\n14:00 - 17:00; Golang; pdtk, stngo; Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n2022-08-12\n\n16:00 - 16:30; Golang; stngo; A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n2022-09-26\n\n6:30 - 7:30; Golang; pttk; renamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2022-08-15",
        "pubDate": "2022-08-15",
        "title": "PTTK and STN",
        "updated": "2022-09-26"
      },
      "url": "posts/2022/08/15/golang-development.json"
    },
    {
      "content": "\n# 5:45 PM, Golang: ptdk,  stngo\n\nPost: Monday, August 15, 2022, 5:45 PM\n\nThinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n",
      "data": {
        "keywords": [
          "ptdk",
          "stngo"
        ],
        "no": 4,
        "pubDate": "2022-08-15",
        "series": "Golang",
        "title": "5:45 PM, Golang: ptdk,  stngo"
      },
      "url": "posts/2022/08/15/golang-development-2022-08-15_170545.json"
    },
    {
      "content": "\n# 5:00 PM, Golang: pdtk,  stngo\n\nPost: Sunday, August 14, 2022, 5:00 PM\n\nToday I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n",
      "data": {
        "keywords": [
          "pdtk",
          "stngo"
        ],
        "no": 3,
        "pubDate": "2022-08-14",
        "series": "Golang",
        "title": "5:00 PM, Golang: pdtk,  stngo"
      },
      "url": "posts/2022/08/14/golang-development-2022-08-14_170500.json"
    },
    {
      "content": "\n# 4:30 PM, Golang: stngo\n\nPost: Friday, August 12, 2022, 4:30 PM\n\nA work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n",
      "data": {
        "keywords": [
          "stngo"
        ],
        "no": 2,
        "pubDate": "2022-08-12",
        "series": "Golang",
        "title": "4:30 PM, Golang: stngo"
      },
      "url": "posts/2022/08/12/golang-development-2022-08-12_160430.json"
    },
    {
      "content": "",
      "data": {},
      "url": "posts/footer.json"
    },
    {
      "content": "\n\nInstalling Golang from Source on RPi-OS for arm64\n==========================================\n\nBy R. S. Doiel, 2022-02-18\n\nThis are my quick notes on installing Golang from source on the Raspberry Pi OS 64 bit.\n\n1. Get a working compiler\n\ta. go to https://go.dev/dl/ and download go1.17.7.linux-arm64.tar.gz\n\tb. untar the tarball in your home directory (it'll unpack to $HOME/go)\n\tc. `cd go/src` and `make.bash`\n2. Move go directory to go1.17\n3. Clone go from GitHub\n4. Compile with the downloaded compiler\n\ta. `cd go/src`\n\tb. `env GOROOT_BOOTSTRAP=$HOME/go1.17 ./make.bash`\n\tc. Make sure `$HOME/go/bin` is in the path\n\td. `go version`\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-02-18",
        "keywords": [
          "raspberry pi",
          "Raspberry Pi OS",
          "arm64"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 1,
        "series": "Raspberry Pi",
        "title": "Installing Golang from source on RPi-OS for arm64"
      },
      "url": "posts/2022/02/18/Installing-Go-from-Source-RPiOS-arm64.json"
    },
    {
      "content": "\nRevealing the Pandoc AST\n========================\n\nI've used Pandoc for a number of years, probably a decade. It's been wonderful\nwatching it grow in capability. When Pandoc started accepting JSON documents as\na support metadata file things really started to click for me. Pandoc became\nmy go to tool for rendering content in my writing and documentation projects.\n\nRecently I've decided I want a little bit more from Pandoc. I've become curious\nabout prototyping some document conversion via Pandoc's filter mechanism. To do\nthat you need to understand the AST, aka abstract syntax tree. \nHow is the AST structure? \n\nIt turns out I just wasn't thinking simply enough (or maybe just not paying\nenough attention while I skimmed Pandoc's documentation). Pandoc's processing\nmodel looks like\n\n```\n\tINPUT --reader--> AST --filter AST --writer--> OUTPUT\n```\n\nI've \"known\" this forever. The missing piece for me was understanding \nthe AST can be an output format.  Use the `--to` option with the value\n\"native\" you get the Haskell representation of the AST. It's that simple.\n\n```\n\tpandoc --from=markdown --to=native \\\n\t   learning-to-write-a-pandoc-filter.md | \\\n\t   head -n 20\n```\n\nOutput\n\n```\n[ Header\n    1\n    ( \"learning-to-write-a-pandoc-filter\" , [] , [] )\n    [ Str \"Learning\"\n    , Space\n    , Str \"to\"\n    , Space\n    , Str \"write\"\n    , Space\n    , Str \"a\"\n    , Space\n    , Str \"Pandoc\"\n    , Space\n    , Str \"filter\"\n    ]\n, Para\n    [ Str \"I\\8217ve\"\n    , Space\n    , Str \"used\"\n    , Space\n```\n\nIf you prefer JSON over Haskell use `--to=json` for similar effect. Here's\nan example piping through [jq](https://stedolan.github.io/jq/).\n\n```\n\tpandoc --from=markdown --to=json \\\n\t   learning-to-write-a-pandoc-filter.md | jq .\n```\n\nWriting filters makes much sense to me now. I can see the AST and see\nhow the documentation describes writing hooks in Lua to process it.\n\n",
      "data": {
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "keywords": [
          "Pandoc",
          "filter"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 4,
        "series": "Pandoc Techniques",
        "title": "Revealing the Pandoc AST"
      },
      "url": "posts/2022/11/17/revealing-pandoc-ast.json"
    },
    {
      "content": "\nArtemis Project Status, 2022\n============================\n\nIt's been a while since I wrote an Oberon-07 post and even longer since I've worked on Artemis. Am I done with Oberon-07 and abandoning Artemis?  No. Life happens and free time to just hasn't been available. I don't know when that will change.\n\nWhat's the path forward?\n------------------------\n\nSince I plan to continue working Artemis I need to find a way forward in much less available time. Time to understand some of my constraints. \n\n1. I work on a variety of machines, OBNC is the only compiler I've consistently been able to use across all my machines\n2. Porting between compilers takes energy and time, and those compilers don't work across all my machines\n3. When I write Oberon-07 code I quickly hit a wall for the things I want to do, this is what original inspired Artemis, so there is still a need for a collection of modules\n4. Oberon/Oberon-07 on Wirth RISC virtual machine is not sufficient for my development needs\n5. A2, while very impressive, isn't working for me either (mostly because I need to work on ARM CPUs)\n\nThese constraints imply Artemis is currently too broadly scoped. I think I need to focus on what works in OBNC for now. Once I have a clear set of modules then I can revisit portability to other compilers.\n\nWhat modules do I think I need? If I look at my person projects I tend to work allot with text, often structured text (e.g. XML, JSON, CSV). I also tend to be working with network services. Occasionally I need to interact with database (e.g. SQLite3, MySQL, Postgres).  Artemis should provide modules to make it easy to write code in Oberon-07 that works in those areas. Some of that I can do by wrapping existing C libraries. Some I can simply write from scratch in Oberon-07 (e.g. a JSON encoder/decoder). That's going to me my focus as my hobby time becomes available and then.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-07-27",
        "keywords": [
          "Oberon",
          "Oberon-07",
          "Artemis"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 22,
        "series": "Mostly Oberon",
        "title": "Artemis Project Status, 2022"
      },
      "url": "posts/2022/07/27/Artemis-Status-Summer-2022.json"
    },
    {
      "content": "\nTurbo Oberon, the dream\n=======================\n\nby R. S. Doiel, 2022-07-30\n\nSometimes I have odd dreams and that was true last night through early this morning. The dream was set in the future. I was already retired. It was a dream about \"Turbo Oberon\".\n\n\"Turbo Oberon\" was an Oberon language. The language compiler was named \"TO\" in my dream. A module's file extension was \".tom\", in honor of Tom Lopez (Meatball Fulton) of ZBS. There were allot of ZBS references in the dream.\n\n\"TO\" was very much a language in the Oberon-07 tradition with minor extensions when it came to bringing in modules. It allowed for a multi path search for module names. You could also express a Module import as a string allowing providing paths to the imported module.\n\nCompilation was similar to Go. Cross compilation was available out of the box by setting a few environment variables. I remember answering questions about the language and its evolution. I remember mentioning in the conversation about how I thought Go felling into the trap of complexity like Rust or C/C++ before it. The turning point for Go was generics. Complexity was the siren song to be resisted in \"Turbo Oberon\". Complexity is seductive to language designers and implementers. I was only an implementer.\n\nEvolution wise \"TO\" was built initially on the Go tool chain. As a result it featured easily cross-compiled binaries and had a rich standard set of Modules like Go but also included portable libraries for implementing graphic user interfaces. \"Turbo Oberon\" evolved as a conversation between Go and the clean simplicity of Oberon-07. Two example applications \"shipped\" with the \"TO\" compiler. They were an Oberon like Operating System (stand alone and hosted) and a Turbo Pascal like IDE. The IDE was called \"toe\" for Turbo Oberon Editor. I don't remember the name of the OS implementation but it might have been \"toos\". I remember \"TO\" caused problems for search engines and catalog systems.\n\nI remember remarking in the dream that programming in \"Turbo Oberon\" was a little like returning to my roots when I first learned to program in Turbo Pascal. Except I could run my programs regardless of the operating system or CPU architecture. \"\"TO\" compiler supported cross compilation for Unix, macOS, Windows on ARM, Intel, RISC-V. The targets were inherited from Go implementation roots.\n\nIn my dream I remember forking Go 1.18 and first replacing the front end of the compiler. I remember it was a challenge understanding the implementation and generate a Go compatible AST. The mapping between Oberon-07 and Go had its challenges. I remember first sticking to a strict Oberon-07 compiler targeting POSIX before enhancing module imports. I remember several failed attempts at getting module imports \"right\". I remember being on the fence about a map data type and going with a Maps module.  I don't remember how introspection worked but saying it was based on an ETH paper for Oberon 2.  I remember the compiler, like Go, eventually became self hosting. It supported a comments based DSL to annotating RECORD types making encoding and decoding convenient, an influence of Go and it's tool chain.\n\nI believe the \"Turbo Oberon Editor\" came first and that was followed by the operating system implementation.\n\nI remember talking about a book that influenced me called, \"Operating Systems through compilers\" but don't know who wrote it. I remember a discussion about the debt owed to Prof. Wirth. I remember that the book showed how once you really understood building the compile you could then build the OS. There was a joke riffing on the old Lisp joke but rephrased, \"all applications evolve not to a Lisp but to an embedded OS\".\n\nIt was a pleasant dream, in the dream I was older and already retired but still writing \"TO\" code and having fun with computers. I remember a closing video shot showing me typing away at what looked like the old Turbo Pascal IDE. As Mojo Sam said in **Somewhere Next Door to Reality**, \"it was a sorta a retro future\".\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2022-07-30",
        "keywords": [
          "Oberon",
          "Wirth",
          "ETH",
          "dreams",
          "compilers",
          "operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Turbo Oberon, the dream"
      },
      "url": "posts/2022/07/30/Turbo-Oberon.json"
    },
    {
      "content": "\n# Working with Structured Data in Deno and TypeScript\n\nOne of the features in Go that I miss in TypeScript is Go's [DSL](https://en.wikipedia.org/wiki/Domain-specific_language \"Domain Specific Language\") for expressing data representations.  Adding JSON, YAML and XML support in Go is simple. Annotating a struct with a string expression. There is no equivalent feature in TypeScript. How do easily support multiple representations in TypeScript?\n\nLet's start with JSON. TypeScript has `JSON.stringify()` and `JSON.parse()`. So getting to JSON representation is trivial, just call the stringify method. Going from text to populated object is done with `JSON.parse`. But there is a catch.\n\nLet's take a simple object I'm defining called \"ObjectN\". The object has a single attribute \"n\". \"n\" holds a number. The initial values is set to zero. What happens when I instantiate my ObjectN then assign it the result from `JSON.parse()`.\n\n~~~TypeScript\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n}\nlet src = `{\"n\": 1}`;\nlet o: ObjectN = new ObjectN();\no = JSON.parse(src);\n// NOTE: This will fail, addThree method isn't available.\nconsole.log(o.addThree());\n~~~\n\nHuston, we have a problem. No \"addThree\" method. That is because JSON doesn't include method representation. What we really want to do is inspect the object returned by `JSON.parse()` and set the values in our ObjectN accordingly. Let's add a method called `fromObject()`.\n(type the following into the Deno REPL).\n\n~~~TypeScript\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n    fromObject(o: {[key: string]: any}): boolean {\n        if (o.n === undefined) {\n            return false;\n        }\n        // Validate that o.n is a number before assigning it.\n        const n = (new Number(o.n)).valueOf();\n        if (isNaN(n)) {\n            return false;\n        }\n        this.n = n;\n        return true;\n    }\n}\nlet src = `{\"n\": 1}`;\nlet o: ObjectN = new ObjectN();\nconsole.log(o.addThree());\no.fromObject(JSON.parse(src));\nconsole.log(o.addThree());\n~~~\n\nNow when we run this code we should see a \"3\" and then a \"4\" output. Wait, `o.fromObject(JSON.parse(src));` looks weird. Why not put `JSON.parse()` inside \"fromObject\"? Why not renamed it \"parse\"?\n\nI want to support many types of data conversion like YAML or XML. I can use my \"fromObject\" method with the result of produced from `JSON.parse()`, `yaml.parse()` and `xml.parse()`. One function works with the result of all three. Try adding this.\n\n~~~TypeScript\nimport * as yaml from 'jsr:@std/yaml';\nimport * as xml from \"jsr:@libs/xml\";\nsrc = `n: 2`;\no.fromObject(yaml.parse(src));\nconsole.log(o.addThree());\nsrc = `<n>3</n>`;\no.fromObject(xml.parse(src));\nconsole.log(o.addThree());\n~~~\n\nThat works!\n\nStill it would be nice to have a \"parse\" method too. How do I do that without winding up with a \"parseJSON()\", \"parseYAML()\" and \"parseXML()\"? What I really want is a \"parseWith\" method which accepts the text and a parse function. TypeScript expects type information about the function being passed. I solve that problem by including a \"ObjectParseType\" definition that works across the three parsing objects -- JSON, yaml and xml.\n\n~~~TypeScript\nimport * as yaml from 'jsr:@std/yaml';\nimport * as xml from \"jsr:@libs/xml\";\n\n// This defines my expectations of the parse function provide by JSON, yaml and xml.\ntype ObjectParseType = (arg1: string, arg2?: any) => {[key: string]: any} | unknown;\n\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n    fromObject(o: {[key: string]: any}) : boolean {\n        if (o.n === undefined) {\n            return false;\n        }\n        // Validate that o.n is a number before assigning it.\n        const n = (new Number(o.n)).valueOf();\n        if (isNaN(n)) {\n            return false;\n        }\n        this.n = n;\n        return true;\n    }\n    parseWith(s: string, fn: ObjectParseType): boolean {\n        return this.fromObject(fn(s) as unknown as {[key: string]: any});\n    }\n}\n\nlet o: ObjectN = new ObjectN();\nconsole.log(`Initial o.addThree() -> ${o.addThree()}`);\nconsole.log(`o.toString() -> ${o.toString()}`);\n\nlet src = `{\"n\": 1}`;\no.parseWith(src, JSON.parse);\nconsole.log(`parse with JSON, o.addThree() -> ${o.addThree()}`);\nconsole.log(`JSON.stringify(o) -> ${JSON.stringify(o)}`);\n\nsrc = `n: 2`;\no.parseWith(src, yaml.parse);\nconsole.log(`parse with yaml, o.addThree() -> ${o.addThree()}`);\nconsole.log(`yaml.stringify(o) -> ${yaml.stringify(o)}`);\n\nsrc = `<?xml version=\"1.0\"?>\n<n>3</n>`;\no.parseWith(src, xml.parse);\nconsole.log(`parse with xml, o.addThree() -> ${o.addThree()}`);\nconsole.log(`xml.stringify(o) -> ${xml.stringify(o)}`);\n~~~\n\nAs long as the parse method returns an object I can now update my ObjectN instance\nfrom the attributes of the object expressed as JSON, YAML, or XML strings. I like this approach because I can add validation and normalization in my \"fromObject\" method and use for any parse method that confirms to how JSON, YAML or XML parse works. The coding cost is the \"ObjectParseType\" type definition and the \"parseWith\" method boiler plate and defining a class specific \"fromObject\". Supporting new representations does require changes to my class definition at all.\n",
      "data": {
        "abstract": "A short discourse on working with structured data in TypeScript and easily\nconverting from JSON, YAML and XML representations.\n",
        "createDate": "2025-02-03",
        "keywords": [
          "Deno",
          "TypeScript",
          "Structured Data"
        ],
        "title": "Working with Structured Data in Deno and TypeScript"
      },
      "url": "posts/2025/02/03/working_with_structured_data.json"
    },
    {
      "content": "\n# Book review, \"Man and the Computer\"\n\nBy R. S. Doiel, 2025-02-10\n\nOpen Library has a wonderful collection of classic Computer related texts. This is a review of one of them.  \"Man and the Computer\" was written by [John G. Kemeny](https://en.wikipedia.org/wiki/John_G._Kemeny) and published in 1972. The book covers the evolution of the Dartmouth Time Sharing System (DTSS) and BASIC[^1].\n\n[^1]: Prof. Kemeny was co-developer of BASIC along with [Thomas Kurtz](https://en.wikipedia.org/wiki/Thomas_E._Kurtz) The author, Kemeny, as a math professor at Dartmouth and eventual became president of the university.\n\nIt is an interesting weekend read. You can read the text at [Open Library](https://openlibrary.org/books/OL5282840M/Man_and_the_computer). The book is short (160 pages or so). It is written for casual reading like a talk given to a small group. \n\nThe first part goes into the innovations that resulted for the undergraduate students who used, developed and extended the systems. The later part of the book covers what the implications of the system had been by the 1970s and what it suggestions for the future through 1990. Given that the book was written before DARPAnet, before Internet and before “Personal Computers” a surprising amount of the  Kemeny's predictions were on target and remaining relevant today. He anticipated Open Access and Cloud Computing and the benefits the were possible. My take away is it is a charming reflection of where computing was at for Dartmouth and its alumni in the start of the 1970s.\n\nStepping back to a bigger picture of computing in the 1960s and 1970s the book does have blind spots.  This is not surprising because the communications were so much more restrictive before the Internet in terms of technical exchange in computing. It is no wonder that there is but one line that mentions the innovations in time-sharing that occurred at RAND with the [Johniac Open Shop System](https://en.wikipedia.org/wiki/JOSS) (aka. JOSS-1, JOSS-2). They system have some parallels with the early DTSS/BASIC incarnations and may have preceded by as much as a few years[^2]. JOSS lead to other systems like CAL, PIL/I, FOCAL and MUMPs. The later, like JOSS, bare similarities to what would be developed at Dartmouth in the form of BASIC.\n\n[^2]: JOSS dates from approximately 1963 and DTSS was released in 1964. Kemeny was published in research memorandum in 1953 and probably talked about his ideas. https://en.wikipedia.org/wiki/JOSS\n\nIt is quite reasonable for this text to the have these blind spots. First it was not intended to be a history of computing (see the Preface of the book) but rather a high level look at what the promise was for people who could work interactively with a computer. I feel it gives insights into the early era where computing access was rapidly increasing and the sense of promise that carried. Well worth the reading time if you're a hobbyist or arm chair computer history buff.\n",
      "data": {
        "abstract": "A book review of a vintage computer publication, \"Man and the Computer\" by\nJohn G. Kemeny, published 1972, ISBN: 0684130092\nRead at the Open Library, <https://openlibrary.org/books/OL5282840M/Man_and_the_computer> \n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-02-10",
        "createDate": "2025-02-10",
        "keywords": [
          "book review",
          "computing",
          "basic",
          "time-sharing",
          "John G. Kemeny"
        ],
        "series": "books",
        "title": "Book review, \"Man and the Computer\""
      },
      "url": "posts/2025/02/10/Man_and_the_Computer.json"
    },
    {
      "content": "\n# Setting up my Raspberry Pi 500, a Portable Workstation\n\nBy R. S. Doiel, 2025-02-14\n\nI'm  writing this on a newly setup Raspberry Pi 500. So far I'm impressed. I was sceptical about moving from my Pi 400 but when it died I was forced to upgrade. It has been worth it.\n\nI've now had a chance to install the various pieces of software I regularly use. Even do some web browsing with it. Image some SD cards for a RISC OS project I'm working on. It feels much quicker than my Pi 400. I think the speed increase is in part the faster CPU but I suspect the program I am running is taking advantage of the 8 Gig RAM instead of the old 4. As I've testing this machine out I noticed I had stopped asking myself the question about launching more than one large program at a time (e.g. my browser is running and being used as I type this in VS Code). When I tried VS Code on my Pi 4B+ and 400 it just felt too sluggish. I'm typing this review in VS Code now on the Pi 500 is keeping up OK.\n\nWhen I purchased the Pi-500 and I also decided to go with the new Raspberry Pi Monitor. Together I have a complete portable workstation. While my previous powered WaveShare monitor was higher resolution the Pi Monitor it  also used allot more power.  With the combination of Pi Monitor and Pi-500 I am now contemplating exploring power consumption so I can find a one battery to power the monitor and Pi-500.\n\nHere's the spec and pricing for this new machine. Some parts I didn't purchase because I already owned them (e.g. the 27W Pi power supply) but I have included the list prices for those who might be interested.\n\nItem                                List Price    Notes                   \n----------------------------------  ------------  -----------------------\nRaspberry Pi 500 unit               $90.00        included  32G SD card  \nRaspberry Pi 27W Power supply       $13.65        used existing one      \nRaspberry Pi Mouse                  $9.25         used existing one      \nMicro HDMI to standard HDMI cable   $5.75         used existing one      \nRaspberry Pi Monitor                $100.00                              \nRaspberry Pi 15W Power supply       $8.00         for monitor            \nRaspberry Pi 128G SD Card           $16.95        upgraded storage       \n\n\n## Software Setup\n\nI created an \"image\" on my 128G SD Card using the Raspberry Pi Imager on another computer.  The imager lets you setup initial user account, WiFi configuration and whether you want SSH services running the first boot. I like using the vanilla 64 bit Raspberry Pi OS distribution. I picked the one for the Raspberry Pi 5 series.\n\nWhen I booted the Pi-500 connected to the Pi Monitor it seemed to cause the screen to cycle through and Pi splash page a few times. Once it completed its first boot it hasn't done that again. I am assuming that was some negotiation between the monitor and the Pi. After a quick click around test I rebooted the machine by doing a full shutdown and power off and the power back up.\n\n### Development and Writing Software\n\nI am planning to use the Pi-500 as a light weight development machine and as a machine for preparing updates for my blog. I installed the additional software below.\n\n- Pandoc 3 via installed via deb package from Pandoc GitHub releases\n- Rust installed via Rustup\n- PageFind installed via Cargo\n- Flatlake installed via Cargo\n- htmlq installed via Cargo\n- ncal installed via Cargo\n- Deno installed via website using CURL\n- Go installed via website's tar ball\n- jq installed via apt\n- SQLite3, libsqlite3-dev installed via apt\n- Hunspell and US English dictionary installed via apt\n\nThe \"build-essential\" package was already installed. I noticed that Git and GNU Make were immediately available with my first boot.\n\nI this setup as a portable  workstation. It feels quick and snappy but is small enough to toss in computer bag. I did test compile a few things. It is possible to peg the CPU but then again it's a little machine after all. I didn't get any warning lights or notifications like I used to on the Pi-400.\n\nWith a connection to my home WiFi network (not a fast connection) it took me about an hour or so to download and install all my extras. This was quicker than the last time I setup a Pi. Some of the time saved was the better hardware and net work performance but much of the time saved was due to the fact that I did not have to compile software from scratch. That was a change from the last time I setup a Pi up.  I guess  Pi and aarch64 processors are common enough that projects are now including it in their regular builds.\n\nIf  I pickup the right capacity battery I suspect I will have a lovely deconstructed Laptop to use as a portable workstation.\n\n",
      "data": {
        "abstract": "Quick notes on configuring a Raspberry Pi 500 as a portable workstation along with a price list.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-02-14",
        "dateCreated": "2025-02-14",
        "keywords": [
          "Raspberry Pi",
          "Workstation",
          "Review"
        ],
        "title": "Setting up my Raspberry Pi 500, a Portable Workstation"
      },
      "url": "posts/2025/02/14/Review_Pi-500_as_portable_workstation.json"
    },
    {
      "content": "\n# Deno 2.1.7, Project Idioms\n\nI've noticed a collection of file and code idioms I've been used in my recent Deno+TypeScript projects at work. I've captured them here as a future reference.\n\n## Project files\n\nMy project generally have the following files, these are derived from the [CodeMeta](https://codemeta.github.io) file using [CMTools](https://caltechlibrary.github.io/CMTools).\n\ncodemeta.json\n: Primary source of project metadata, used to generate various files\n\nCITATION.cff\n: used by GitHub for citations. version, dateModified, datePublished and releaseNotes\n\n\nabout.md\n: A project about page. This is generated based on the codemeta.json file.\n\nREADME.md, README\n: A general read me describing the project and pointing to INSTALL.md, user_manual.md as appropriate\n\nINSTALL.md\n: These are boiler plate description of how to install and compile the software\n\nuser_manual.md\n: This is an index document, a table of contents. It points to other document including Markdown versions of the man page(s).\n\nFor TypeScript projects I also include a following\n\nversion.ts\n: This hold project version information used in the TypeScript co-debase. It is generated from the codemeta.json via CMTools.\n\nhelptext.ts\n: This is where I place a function, `fmtHelp()`, for rendering response to the \"help\" command line option.\n\nI'm currently ambivalent about \"main.ts\" file which is created by `deno init`. My ambivalent is that most of my projects wind up producing more than one program from a shared code base. a single \"main.ts\" doesn't really fit that situation.\n\nThe command line tool will have a TypeScript with it's name. Inside this file I'll have a main function and use the Deno idiom `if (import.meta.main) main();` to invoke it. I don't generally put the command line  TypeScript in my \"mod.ts\" file since it's not going to work in a browser or be useful outside my specific project.\n\nmod.ts\n: I usually re-export modules here that maybe useful outside my project (or in the web browser).\n\ndeps.ts\n: I use this if there are allot of files consistently being imported across the project, otherwise I skip it.\n\n## What I put in Main\n\nI use the main function to define command line options, handle parameters such as data input, output and errors. It usually invokes a primary function modeled in the rest of the project code.\n\nHere is an example Main for a simple \"cat\" like program.\n\n~~~TypeScript\nimport { parseArgs } from \"jsr:@std/cli\";\nimport { licenseText, releaseDate, releaseHash, version } from \"./version.ts\";\nimport { fmtHelp, helpText } from \"./helptext.ts\";\n\nconst appName = \"mycat\";\n\nasync function main() {\n  const app = parseArgs(Deno.args, {\n    alias: {\n      help: \"h\",\n      license: \"l\",\n      version: \"v\",\n    },\n    default: {\n      help: false,\n      version: false,\n      license: false,\n    },\n  });\n  const args = app._;\n\n  if (app.help) {\n    console.log(fmtHelp(helpText, appName, version, releaseDate, releaseHash));\n    Deno.exit(0);\n  }\n  if (app.license) {\n    console.log(licenseText);\n    Deno.exit(0);\n  }\n  if (app.version) {\n    console.log(`${appName} ${version} ${releaseHash}`);\n    Deno.exit(0);\n  }\n\n  let input: Deno.FsFile | any = Deno.stdin;\n\n  // handle case of many file names\n  if (args.length > 1) {\n    for (const arg of args) {\n      input = await Deno.open(`${arg}`);\n      for await (const chunk of input.readable) {\n        const decoder = new TextDecoder();\n        console.log(decoder.decode(chunk));\n      }\n    }\n    Deno.exit(0);\n  }\n  if (args.length > 0) {\n    input = await Deno.open(Deno.args[0]);\n  }\n  for await (const chunk of input.readable) {\n    const decoder = new TextDecoder();\n    console.log(decoder.decode(chunk));\n  }\n}\n\nif (import.meta.main) main();\n~~~\n\n## helptext.ts\n\nThe following is an example of the [helptext.ts](helptext.ts) file for the demo [mycat.ts](mycat.ts).\n\n```TypeScript\nexport function fmtHelp(\n  txt: string,\n  appName: string,\n  version: string,\n  releaseDate: string,\n  releaseHash: string,\n): string {\n  return txt.replaceAll(\"{app_name}\", appName).replaceAll(\"{version}\", version)\n    .replaceAll(\"{release_date}\", releaseDate).replaceAll(\n      \"{release_hash}\",\n      releaseHash,\n    );\n}\n\nexport const helpText =\n  `%{app_name}(1) user manual | version {version} {release_hash}\n% R. S. Doiel\n% {release_date}\n\n# NAME\n\n{app_name}\n\n# SYNOPSIS\n\n{app_name} FILE [FILE ...] [OPTIONS]\n\n# DESCRIPTION\n\n{app_name} implements a \"cat\" like program.\n\n# OPTIONS\n\nOptions come as the last parameter(s) on the command line.\n\n-h, --help\n: display help\n\n-v, --version\n: display version\n\n-l, --license\n: display license\n\n\n# EXAMPLES\n~~~shell\n{app_name} README.md\n{app_name} README.md INSTALL.md\n~~~\n`;\n```\n\n## Generating version.ts\n\nThe [version.ts](version.ts) is generated form two files, [codemeta.json] and [LICENSE] using the CMTools, `cmt` command.\n\n~~~\ncmt codemeta.json veresion.ts\n~~~\n",
      "data": {
        "abstract": "Notes on some of the file and code idioms I'm using with Deno+TypeScript projects.",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "createDate": "2025-01-29",
        "keywords": [
          "Deno",
          "TypeScript",
          "Projects"
        ],
        "title": "Deno 2.1.7, Project Idioms"
      },
      "url": "posts/2025/01/29/project_idioms.json"
    },
    {
      "content": "\n# Deno 2.1.7, Points of Friction\n\nBy R. S. Doiel, 2025-01-26\n\nI have run into a few points of friction in my journey with Deno coming from Go. I miss Go's standard \"io\" and \"bufio\" packages. With the Go code I'm porting TypeScript I'd often need to handle standard input or input from a named file interchangeably. Seems like this should be easy in Deno's TypeScript but there are a few bumps in the road.\n\nHere's the Go idiom I commonly use.\n\n~~~go\nvar err error\ninput := io.Stdin\nif inFilename != \"\" {\n    input, err := os.Open(inFilename)\n    if err !== nil {\n        // ... handle error\n    }\n    defer input.Close();\n}\n// Now I can just pass \"in\" around for processing.\n~~~\n\nConceptually this feels simple though verbose. I can pass around the \"input\" for processing in a way that is agnostic as to file or standard input. This type of Go code works equally on POSIX and Windows.\n\nDeno provide access to [standard input](https://docs.deno.com/api/deno/~/Deno.stdin). Deno supports streamable files. From the docs here's an simple example.\n\n~~~TypeScript\n// If the text \"hello world\" is piped into the script:\nconst buf = new Uint8Array(100);\nconst numberOfBytesRead = await Deno.stdin.read(buf); // 11 bytes\nconst text = new TextDecoder().decode(buf);  // \"hello world\"\n~~~\n\nSetting aside the buffer management code it seems simple and straight forward. It is easy to understand and you could wrap it in a function easily to hide the buffer management part. Yet it doesn't provide the same flexibility as the more verbose Go version. Surely there is an an idiomatic why of doing this in TypeScript already? \n\n## Stability Challenge\n\nDeno currently is a rapidly evolving platform. My first impulse was to reach for packages like `jsr:@std/fs` or `jsr:@sys/fs`. When I search for examples they mostly seem to reference specific versions of \"std/fs\" that are not available via jsr. So what's the \"right\" way to approach this?\n\n## Repl to the rescue.\n\nPoking around in the Deno repl I tried assigning `Deno.stdin` to a local variable. Playing with command line completion I realized it has most of the the methods you would get if you used `Deno.open()` to open a named file.\n\nHere's a little test I ran in the repl after creating a \"hellworld.txt\" text file.\n\n~~~deno\ndeno\nconst stdin = Deno.stdin;\nlet input = Deno.open('helloworld.txt')\nstdin.isTerminal();\ninput.isTerminal();\nstdin.valueOf();\ninput.valueOf();\nDeno.exit(0);\n~~~\n\nThe `valueOf()` reveals their type affiliation. It listed them as `Stdin {}` and `FsFile {}` respectively. I used TypeScript's typing system to let us implement \"mycat.ts\". You can assign multiple types to a variable with a `|` (pipe) symbol in TypeScript. \n\nUsed that result to write a simple cat file implementation.\n\n~~~TypeScript\nasync function catFile() {\n    let input : Stdin | FsFile = Deno.stdin;\n\n    if (Deno.args.length > 0) {\n        input = await Deno.open(Deno.args[0]);\n    }\n\n    const decoder = new TextDecoder();\n\n    // NOTE: the .readable function is available on both types of objects.\n    for await (const chunk of input.readable) {\n        console.log(decoder.decode(chunk));\n    }\n}\n\nif (import.meta.main) catFile();\n~~~\n\nYou can \"run\" this deno to see it in action. Try running it on your \"helloworld.txt\" file.\n\n~~~shell\ndeno run --allow-read mycat.ts helloworld.txt\n~~~\n\nYou can also read from standard input too. Try the command below type in some text then press Ctrl-D or Ctrl-Z if you're on Windows.\n\n~~~shell\ndeno run --allow-read mycat.ts\n~~~\n\nLooks like we have a nice solution. Now I can compile \"mycat.ts\".\n\n## trouble in paradise\n\nWhile you can \"run\" the script you can't compile it. It doesn't pass \"check\". This is the error I get with Deno 2.1.7.\n\n~~~shell\ndeno check mycat.ts\nCheck file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts\nerror: TS2304 [ERROR]: Cannot find name 'Stdin'.\n    let input : Stdin | FsFile = Deno.stdin;\n                ~~~~~\n    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:17\n\nTS2552 [ERROR]: Cannot find name 'FsFile'. Did you mean 'File'?\n    let input : Stdin | FsFile = Deno.stdin;\n                        ~~~~~~\n    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:25\n\n    'File' is declared here.\n    declare var File: {\n                ~~~~\n        at asset:///lib.deno.web.d.ts:622:13\n\nFound 2 errors.\n~~~\n\nIt seems like what works in the repl should also compile but that's isn't the case. I have an open question on Deno's discord help channel and am curious to find the \"correct\" way to handle this problem.\n\n## Update 2025-01-26, 5:00PM\n\nI heard back on Deno Discord channel for help.  With the help of [crowlKat](https://github.com/crowlKats) sorted the problem out.\n\nThe compile and runnable version of [mycat.ts](mycat.ts) looks like this.\n\n~~~typescript\nasync function main() {\n    let input : Deno.FsFile | any = Deno.stdin;\n\n    if (Deno.args.length > 0) {\n        input = await Deno.open(Deno.args[0]);\n    }\n\n    const decoder = new TextDecoder();\n\n    // NOTE: the .readable function is available on both types of objects.\n    for await (const chunk of input.readable) {\n        console.log(decoder.decode(chunk));\n    }\n}\n\nif (import.meta.main) main();\n~~~\n\nThe \"any\" type feels a little ugly but since I am assinging the default value is `Deno.stdin` it covers that case where the `Deno.FsFile` covers the case of a name file.  Where does this leave me? I have a nice clean idiom that does what I want for interacting with standard input or a file stream.  Not necessarily the fast thing on the planet but it works.\n\n\n",
      "data": {
        "abstract": "A short discussion of working with file input in TypeScript+Deno coming from the\nperspective of Go's idiomatic use of io buffers.\n",
        "byline": "R. S. Doiel",
        "createdDate": "2025-01-26",
        "keywords": [
          "deno",
          "text",
          "input"
        ],
        "title": "Deno 2.1.7, Points of Friction"
      },
      "url": "posts/2025/01/26/points_of_friction.json"
    },
    {
      "content": "\n# Two missing features from HTML5, an enhanced form.enctype and a list input type\n\n## Robert's wish list for browsers and web forms handling\n\nBy R. S. Doiel, 2024-02-23\n\nI wish the form element supported a `application/json` encoding type and there was such a thing as a `list-input` element.\n\nI've been thinking about how we can get back to basic HTML documents and move away from JavaScript required to render richer web forms. When web forms arrived on scene in the early 1990s they included a few basic input types. Over the years a few have been added but by and large the data model has remained relatively flat. The exception being the select element with `multiple` attribute set. I believe we are being limited by the original choice of urlencoding web forms and then resort to JavaScript to address it's limitations.\n\nWhat does the encoding of a web form actually look like?  The web generally encodes the form using urlencoding. It presents a stream of key value pairs where the keys are the form's input names and the values are the value of the input element. With a multi-select element the browser simply repeats the key and adds the next value in the selection list to that key.  In Go you can describe this simple data structure as a `map[string][]string`[^1]. Most of the time a key points to a single element array of string but sometimes it can have multiple elements using that key and then the array expands to accommodate. Most of the time we don't think about this as web developers. The library provided with your programming language decodes the form into a more programmer friendly representation. But still I believe this simple urlencoding has held us back. Let me illustrate the problem through a series of simple form examples.\n\n[^1]: In English this could be described as \"a map using a string to point at a list of strings\" with \"string\" being a sequence of letters or characters.\n\nHere's an example of a simple form with a multi select box. It is asking for your choice of ice cream flavors.\n\n~~~html\n<form method=\"POST\">\n  <label for=\"ice-cream-flavors\">Choose your ice cream flavors:</label>\n  <select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple >\n    <option value=\"Chocolate\">Chocolate</option>\n    <option value=\"Coconut\">Cocunut</option>\n    <option value=\"Mint\">Mint</option>\n    <option value=\"Strawberry\">Strawberry</option>\n    <option value=\"Vanilla\">Vanilla</option>\n    <option value=\"Banana\">Banana</option>\n    <option value=\"Peanut\">Peanut</option>\n  </select>\n  <p>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nBy default your web browser will packaged this up and send it using \"application/x-www-form-urlencoded\". If you select \"Coconut\" and \"Strawberry\" then the service receiving your data will get an encoded document that looks like this.\n\n~~~urlencoding\nice-cream-flavors=Coconut&ice-cream-flavors=Strawberry\n~~~\n\nThe ampersands separate the key value pairs. The fact that \"ice-cream-flavors\" name repeats means that the key \"ice-cream-flavors\" will point to an array of values.  In pretty printed JSON representation is a little clearer.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Coconut\", \"Strawberry\" ]\n}\n~~~\n\nSo far so good. Zero need to enhance the spec. It works and has worked for a very long time. Stability is a good thing. Let's elaborate a little further.  I've added a dish choice for the ice cream, \"Sugar Cone\" and \"Waffle Bowl\". That web form looks like.\n\n~~~html\n<form method=\"POST\">\n<label for=\"ice-cream-flavors\">Select the flavor for each scoop of ice cream:</label>\n<select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple>\n  <option value=\"Chocolate\">Chocolate</option>\n  <option value=\"Coconut\">Cocunut</option>\n  <option value=\"Mint\">Mint</option>\n  <option value=\"Strawberry\">Strawberry</option>\n  <option value=\"Vanilla\">Vanilla</option>\n  <option value=\"Banana\">Banana</option>\n  <option value=\"Peanut\">Peanut</option>\n</select>\n<p>\n<fieldset>\n  <legend>Pick your delivery dish</legend>\n  <div>\n    <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n    <label for=\"sugar-cone\">Sugar Cone</label>\n  </div>\n  <div>\n    <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n    <label for=\"waffle-bowl\">Waffle Bowl</label>\n  </div>\n</fieldset>\n<input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nIf we select \"Banana\" and \"Peanut\" flavors served in a \"Waffle Bowl\" the encoded document would reach the web service looking something like this.\n\n~~~urlencoded\nice-cream-flavors=Banana&ice-cream-flavors=Peanut&ice-cream-dish=waffle-cone\n~~~\n\nThat's not too bad. Again this is the state of web form for ages now. In JSON it could be represented as the following.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Banana\", \"Peanut\" ],\n    \"ice-cream-dish\": \"waffle-cone\"\n}\n~~~\n\nThis is great we have a simple web form that can collect a single ice cream order.  But what if we want to actually place several individual ice cream orders as one order? Today we have two choices, multiple web forms that accumulate the orders (circa 2000) or use JavaScript create a web UI that can handle list of form elements. Both have their drawbacks.\n\nIn the case of the old school approach changing web pages just to update an order can be slow and increase uncertainty about your current order. That is why the JavaScript approach has come to be more common. But that JavaScript approach comes at a huge price. It's much more complex, we've seen a dozens of libraries and frameworks that have come and gone trying to manage that complexity in various ways.\n\nIf we supported JSON encoded from submission directly in the web browser I think we'd make a huge step forward. It could decouple the JavaScript requirement. That would avoid much of the cruft that we ship down to the web browser today because we can't manage lists of things without resorting to JavaScript.\n\nLet's pretend there was a new input element type called \"list-input\". A \"list-input\" element can contain any combination of today's basic form elements. Here's my hypothetical `list-input` based from example. In it we're going to select the ice cream flavors and the dish format (cone, bowl) as before but have them accumulate in a list. That form could be expressed in HTML similar to my mock up below.\n\n~~~html\n<form>\n  <label for=\"ice-cream-order\">Place your next order, press submit when you have all of them.</label>\n  <list-input id=\"ice-cream-order\" name=\"ice-cream-order\">\n    <label for=\"ice-cream-flavor\">Select the flavor for each scoop of ice cream:</label>\n    <select id=\"ice-cream-flavor\" name=\"ice-cream-flavor\" multiple>\n      <option value=\"Chocolate\">Chocolate</option>\n      <option value=\"Coconut\">Cocunut</option>\n      <option value=\"Mint\">Mint</option>\n      <option value=\"Strawberry\">Strawberry</option>\n      <option value=\"Vanilla\">Vanilla</option>\n      <option value=\"Banana\">Banana</option>\n      <option value=\"Peanut\">Peanut</option>\n    </select>\n  <p>\n  <fieldset>\n    <legend>Pick your delivery dish</legend>\n    <div>\n      <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n      <label for=\"sugar-cone\">Sugar Cone</label>\n    </div>\n    <div>\n      <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n      <label for=\"waffle-bowl\">Waffle Bowl</label>\n    </div>\n  </fieldset>\n  </list-input>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nWith two additional lines of HTML the input form can now support a list of individual ice cream orders. Assuming only urlencoding is supported then how does that get encoded and sent to the web server? Here is an example set of orders\n\n1. vanilla ice cream with a sugar cone\n2. chocolate with a waffle bowl\n\n~~~urlencoded\nice-cream-flavors=Vanilla&ice-cream-flavors=Chocolate&ice-cream-dish=sugar-cone&ice-cream-dish=waffle-bowl\n~~~\n\nWhich flavor goes with which dish?  That's the problem with urlencoding a list in your web form. We just can't keep the data alignment manageable.  What if the web browser used JSON encoding? \n\n~~~json\n[\n  {\n      \"ice-cream-flavors\": [ \"Vanilla\" ],\n      \"ice-cream-dish\": \"sugar-cone\"\n  },\n  {\n      \"ice-cream-flavors\": [ \"Chocolate\" ],\n      \"ice-cream-dish\": \"waffle-bowl\"\n  }\n~~~\n\nSuddenly the alignment problem goes away. There is precedence for controlling behavior of the web browser submission through the `enctype` attribute. File upload was addressed by adding support for `multipart/form-data`.  In 2024 and for over the last decade it has been common practice in web services to support JSON data submission. I believe it is time that the web browser also supports this directly. This would allow us to decouple the necessity of using JavaScript in browser as we require today. The form elements already map well to a JSON encoding. If JSON encoding was enabled then adding a element like my \"list-input\" would make sense.  Otherwise we remain stuck in a world where hypertext markup language remains very limited and can't live without JavaScript.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "html",
          "web forms",
          "encoding"
        ],
        "title": "Two missing features from HTML5, an enhanced form.enctype and a list input type"
      },
      "url": "posts/2024/02/23/enhanced_form_handling.json"
    }
  ]
}