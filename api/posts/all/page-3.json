{
  "page": 3,
  "total_pages": 5,
  "has_more": true,
  "next_page": "posts/all/page-4.json",
  "values": [
    {
      "content": "\n# 12:00 PM, SQL: Postgres\n\nPost: Wednesday, August 24, 2022, 12:00 PM\n\nI miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%'\n`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 3,
        "pubDate": "2022-08-24",
        "series": "SQL",
        "title": "12:00 PM, SQL: Postgres"
      },
      "url": "posts/2022/08/24/rosette-notes-2022-08-24_121200.json"
    },
    {
      "content": "\nA Quick intro to PL/pgSQL\n========================\n\nPL/pgSQL is a procedure language extended from SQL. It adds flow control and local state for procedures, functions and triggers. Procedures, functions and triggers are also the compilation unit. Visually PL/pgSQL looks similar to the MySQL or ORACLE counter parts. It reminds me of a mashup of ALGO and SQL. Like the unit of compilation, the unit of execution is also procedure, function or trigger. \n\nThe Postgres documentation defines and explains the [PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql.html) and how it works.  This document is just a quick orientation with specific examples to provide context.\n\nHello World\n-----------\n\nHere is a \"helloworld\" procedure definition.\n\n```sql\n    CREATE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n       RAISE NOTICE 'Hello WORLD!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nLet's take a look this line by line.\n\n1. CREATE PROCEDURE defines the procedure and the starting and ending delimiter for the procedure (e.g. `AS $$` the procedure's text ends when `$$` is encountered an second time.\n2. DECLARE is the block where you would declare the variables used in the procedure, we have none in this example\n3. The BEGIN starts the actual procedure instructions\n4. The `RAISE NOTICE` line is how you can display output to the console when the procedure is run\n5. The END completes the procedure definition\n6. the `$$ LANGUAGE plpgsql;` concludes the text defining the procedure and tells the database engine that procedure is written in PL/pgSQL.\n\nWe can run the procedure using the \"CALL\" query.\n\n```sql\n    CALL helloworld()\n```\n\nNOTE: If you want to change the procedure you can \"DROP\" it first otherwise you'll get an error that it already exists.\n\n```sql\n    DROP PROCEDURE helloworld;\n```\n\nImproving my workflow\n---------------------\n\nSQL procedures are generally stored in the RDBMs in database environment. You can think of them as records in the system's database. Procedures and functions are created and can be dropped. While they can be manually typed in the database's shell it is easier to maintain them in plain text files outside the RDBM environment.  \n\n1. Write the procedure in a text file.\n2. Load the text file (e.g. FILENAME) into Postgres \n   a. outside the Postgres shell use `psql -f FILENAME` \n   b. inside the Postgres shell used `\\i FILENAME`\n3. Call the procedure to test it\n\nTo turn these steps into a look I use a \"CREATE OR REPLACE\" statement and be able to reload the updated procedure easier see [43.12. Tips for Developing in PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql-development-tips.html).  Note in the revised example the \"-- \" lines are comments.\n\nOur revised [helloworld](helloworld.plpgsql).\n\n```sql\n    --\n    -- Create (or replace) the new \"helloworld\" procedure.\n    -- NOTE: this can be run with \"CALL\"\n    --\n    CREATE OR REPLACE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n        RAISE NOTICE 'Hello World!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\n\nHi There\n--------\n\n[hithere](hithere.plpgsql) is similar to our helloworld example except it is a function that takes a parameter of the person's name. The function returns a \"VARCHAR\", so this should work as part of a select statement.\n\n```sql\n    --\n    -- This is a \"Hi There\" function. The function takes\n    -- a single parameter and forms a greeting.\n    --\n    CREATE OR REPLACE FUNCTION hithere(name varchar) RETURNS varchar AS $$\n    DECLARE\n      greeting varchar;\n    BEGIN\n        IF name = '' THEN\n            greeting := 'Hi there!';\n        ELSE\n            greeting := 'Hello ' || name || '!';\n        END IF;\n        RETURN greeting;\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nGiving it a try.\n\n```shell\n    SELECT hithere('Mojo Sam');\n```\n\nFurther reading\n---------------\n\n- [Conditionals](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONDITIONALS)\n- [Loops](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONTROL-STRUCTURES-LOOPS)\n- [Calling a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-CALLING-PROCEDURE)\n- [Early return from a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-RETURNING-PROCEDURE)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-24",
        "keywords": [
          "postgres",
          "sql",
          "psql",
          "plsql",
          "plpgsql"
        ],
        "number": 3,
        "pubDate": "2022-08-24",
        "series": "SQL Reflections",
        "title": "A Quick into to PL/pgSQL"
      },
      "url": "posts/2022/08/24/plpgsql-quick-intro.json"
    },
    {
      "content": "\n# 11:30 AM, SQL: Postgres\n\nPost: Monday, August 22, 2022, 11:30 AM\n\nThree things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 4,
        "pubDate": "2022-08-22",
        "series": "SQL",
        "title": "11:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/22/rosette-notes-2022-08-22_111130.json"
    },
    {
      "content": "\nRosette Notes\n=============\n\nBy R. S. Doiel, 2022-08-19\n\n> A dance around two relational databases, piecing together similarities as with the tiny mosaic tiles of a guitar's rosette\n\nWhat follows are my preliminary notes learning Postgres 12 and 14.\n\nPostgres & MySQL\n----------------\n\nThis is a short comparison of some administrative commands I commonly use. The first column describes the task followed by the SQL to execute for Postgres 14.5 and then MySQL 8. The presumption is you're using `psql` to access Postgres and `mysql` to  access MySQL. Values between `<` and `>` should be replaced with an appropriate value.\n\n| Task                    | Postgres 14.5                     | MySQL 8           |\n|-------------------------|------------------------------------|-------------------|\n| show all databases      | `SELECT datname FROM pg_database;` | `SHOW DATABASES;` |\n| select a database       | `\\c <dbname>`                      | `USE <dbname>`    |\n| show tables in database | `\\dt`                              | `SHOW TABLES;`    |\n| show columns in table   | `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '<table_name>';` | `SHOW COLUMNS IN <table_name>` |\n\nReflections\n-----------\n\nThe Postgres shell, `psql`, provides the functionality of showing a list of tables via a short cut while MySQL choose to add the `SHOW TABLES` query. For me `SHOW ...` feels like SQL where as `\\d` or `\\dt` takes me out of SQL space. On the other hand given Postgres metadata structure the shortcut is appreciated and I often query for table names as I forget them. `\\dt` quickly becomes second nature and is shorter to type than `SHOW TABLES`. \n\nConnecting to a database with `\\c` in `psql` is like calling an \"open\" in programming language. The \"connection\" in `psql` is open until explicitly closed or the shell is terminated.  Like `USE ...` in the MySQL shell it make working with multiple database easy.  The difference are apparent when you execute a `DROP DATABASE ...` command. In `psql` you need to `CLOSE` the database first or the `DROP` will fail.  The MySQL shell will happily let you drop the current database you are currently using.\n\nThe challenge I've experienced learning `psql` after knowing MySQL is my lack of familiarity with the metadata Postgres maintains about databases and structures.  On the other hand everything I've learned about standards base SQL applies to managing Postgres once remember the database/table I need to work with.  A steeper learning curve from MySQL's `SHOW` but it also means writing external programs for managing Postgres databases and tables is far easier because everything is visible because that is how you manage Postgres. MySQL's `SHOW` is very convenient but at the cost of hiding some of its internal structures.\n\nBoth MySQL and Postgres support writing programs in SQL. They also support stored procedures, views and triggers. They've converged in the degree in which they have both implemented SQL language standards.  The differences are mostly in approach to managing databases.  There are some differences, necessitated by implementation choices, in the `CREATE DATABASE`, `CREATE TABLE` or `ALTER` statements but you can often use the basic form described in ANSI SQL and get the results you need. When doing performance tuning the dialect differences are more important.\n\nDump & Restore\n--------------\n\nBoth Postgres and MySQL provide command line programs for dumping a database. MySQL provides a single program where as Postgres splits it in two. Check the man pages (or website docs) for details in their options. Both sets of programs are highly configurable allowing you to dump just schema, just data or both with different expectations.\n\n| Postgres 14.5      | MySQL 8                         |\n|--------------------|---------------------------------|\n| `pg_dumpall`       | `mysqldump --all-databases`     |\n| `pg_dump <dbname>` | `mysqldump --database <dbname>` |\n\nThe `pg_dumpall` tool is designed to restore an entire database instance. It includes account and ownership information. `pg_dump` just focuses on the database itself. If you are taking a snapshot production data to use in a test `pg_dump` output is easier to work with. It captures the specific database with out entangling things like the `template1` database or database user accounts and ownership.\n\nYou can restore a database dump in both Postgres and MySQL. The tooling is a little different.\n\n| Postgres 14.5                   | MySQL 8                                      |\n|---------------------------------|----------------------------------------------|\n| `dropdb <dbname>`               | `mysql -execute 'DROP DATABASE <dbname>;'`   |\n| `createdb <dbname>`             | `mysql -execute 'CREATE DATABASE <dbname>;'` |\n| `psql -f <dump_filename>`       |`mysql <dbname> < <dump_filename>`            |\n\nNOTE: These instructions work for a database dumped with `pg_dump` for the Postgres example. In principle it is the same way you can restore from `pg_dumpall` but if you Postgres instance already exists then you're going to run into various problems, e.g. errors about `template1` db.\n\nLessons learned along the way\n-----------------------------\n\n2022-08-22\n\n8:00 - 11:30; SQL; Postgres; Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n\n2022-08-24\n\n11:00 - 12:00; SQL; Postgres; I miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%';`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n2022-08-26\n\n9:30 - 10:30; SQL; Postgres; If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n2022-09-19\n\n10:30 - 12:30; SQL; Postgres; Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-19",
        "keywords": [
          "postgres",
          "mysql",
          "sql",
          "psql"
        ],
        "number": 2,
        "pubDate": "2022-08-19",
        "series": "SQL Reflections",
        "title": "Rosette Notes: Postgres and MySQL",
        "updated": "2022-09-19"
      },
      "url": "posts/2022/08/19/rosette-notes.json"
    },
    {
      "content": "\nPttk and STN\n============\n\nBy R. S. Doiel, started 2022-08-15\n(updated: 2022-09-26, pdtk was renamed pttk)\n\nThis log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the \"blogit\" action that will translates the simple timesheet notation (aka STN) to a short blog post.  My \"short post\" interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.\n\n2022-08-15\n\n16:45 - 17:45; Golang; ptdk, stngo; Thinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n2022-08-14\n\n14:00 - 17:00; Golang; pdtk, stngo; Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n2022-08-12\n\n16:00 - 16:30; Golang; stngo; A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n2022-09-26\n\n6:30 - 7:30; Golang; pttk; renamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2022-08-15",
        "pubDate": "2022-08-15",
        "title": "PTTK and STN",
        "updated": "2022-09-26"
      },
      "url": "posts/2022/08/15/golang-development.json"
    },
    {
      "content": "\n# 5:45 PM, Golang: ptdk,  stngo\n\nPost: Monday, August 15, 2022, 5:45 PM\n\nThinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n",
      "data": {
        "keywords": [
          "ptdk",
          "stngo"
        ],
        "no": 4,
        "pubDate": "2022-08-15",
        "series": "Golang",
        "title": "5:45 PM, Golang: ptdk,  stngo"
      },
      "url": "posts/2022/08/15/golang-development-2022-08-15_170545.json"
    },
    {
      "content": "\n# 5:00 PM, Golang: pdtk,  stngo\n\nPost: Sunday, August 14, 2022, 5:00 PM\n\nToday I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n",
      "data": {
        "keywords": [
          "pdtk",
          "stngo"
        ],
        "no": 3,
        "pubDate": "2022-08-14",
        "series": "Golang",
        "title": "5:00 PM, Golang: pdtk,  stngo"
      },
      "url": "posts/2022/08/14/golang-development-2022-08-14_170500.json"
    },
    {
      "content": "\n# 4:30 PM, Golang: stngo\n\nPost: Friday, August 12, 2022, 4:30 PM\n\nA work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n",
      "data": {
        "keywords": [
          "stngo"
        ],
        "no": 2,
        "pubDate": "2022-08-12",
        "series": "Golang",
        "title": "4:30 PM, Golang: stngo"
      },
      "url": "posts/2022/08/12/golang-development-2022-08-12_160430.json"
    },
    {
      "content": "",
      "data": {},
      "url": "posts/footer.json"
    },
    {
      "content": "\n\nInstalling Golang from Source on RPi-OS for arm64\n==========================================\n\nBy R. S. Doiel, 2022-02-18\n\nThis are my quick notes on installing Golang from source on the Raspberry Pi OS 64 bit.\n\n1. Get a working compiler\n\ta. go to https://go.dev/dl/ and download go1.17.7.linux-arm64.tar.gz\n\tb. untar the tarball in your home directory (it'll unpack to $HOME/go)\n\tc. `cd go/src` and `make.bash`\n2. Move go directory to go1.17\n3. Clone go from GitHub\n4. Compile with the downloaded compiler\n\ta. `cd go/src`\n\tb. `env GOROOT_BOOTSTRAP=$HOME/go1.17 ./make.bash`\n\tc. Make sure `$HOME/go/bin` is in the path\n\td. `go version`\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-02-18",
        "keywords": [
          "raspberry pi",
          "Raspberry Pi OS",
          "arm64"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 1,
        "series": "Raspberry Pi",
        "title": "Installing Golang from source on RPi-OS for arm64"
      },
      "url": "posts/2022/02/18/Installing-Go-from-Source-RPiOS-arm64.json"
    },
    {
      "content": "\nRevealing the Pandoc AST\n========================\n\nI've used Pandoc for a number of years, probably a decade. It's been wonderful\nwatching it grow in capability. When Pandoc started accepting JSON documents as\na support metadata file things really started to click for me. Pandoc became\nmy go to tool for rendering content in my writing and documentation projects.\n\nRecently I've decided I want a little bit more from Pandoc. I've become curious\nabout prototyping some document conversion via Pandoc's filter mechanism. To do\nthat you need to understand the AST, aka abstract syntax tree. \nHow is the AST structure? \n\nIt turns out I just wasn't thinking simply enough (or maybe just not paying\nenough attention while I skimmed Pandoc's documentation). Pandoc's processing\nmodel looks like\n\n```\n\tINPUT --reader--> AST --filter AST --writer--> OUTPUT\n```\n\nI've \"known\" this forever. The missing piece for me was understanding \nthe AST can be an output format.  Use the `--to` option with the value\n\"native\" you get the Haskell representation of the AST. It's that simple.\n\n```\n\tpandoc --from=markdown --to=native \\\n\t   learning-to-write-a-pandoc-filter.md | \\\n\t   head -n 20\n```\n\nOutput\n\n```\n[ Header\n    1\n    ( \"learning-to-write-a-pandoc-filter\" , [] , [] )\n    [ Str \"Learning\"\n    , Space\n    , Str \"to\"\n    , Space\n    , Str \"write\"\n    , Space\n    , Str \"a\"\n    , Space\n    , Str \"Pandoc\"\n    , Space\n    , Str \"filter\"\n    ]\n, Para\n    [ Str \"I\\8217ve\"\n    , Space\n    , Str \"used\"\n    , Space\n```\n\nIf you prefer JSON over Haskell use `--to=json` for similar effect. Here's\nan example piping through [jq](https://stedolan.github.io/jq/).\n\n```\n\tpandoc --from=markdown --to=json \\\n\t   learning-to-write-a-pandoc-filter.md | jq .\n```\n\nWriting filters makes much sense to me now. I can see the AST and see\nhow the documentation describes writing hooks in Lua to process it.\n\n",
      "data": {
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "keywords": [
          "Pandoc",
          "filter"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 4,
        "series": "Pandoc Techniques",
        "title": "Revealing the Pandoc AST"
      },
      "url": "posts/2022/11/17/revealing-pandoc-ast.json"
    },
    {
      "content": "\nArtemis Project Status, 2022\n============================\n\nIt's been a while since I wrote an Oberon-07 post and even longer since I've worked on Artemis. Am I done with Oberon-07 and abandoning Artemis?  No. Life happens and free time to just hasn't been available. I don't know when that will change.\n\nWhat's the path forward?\n------------------------\n\nSince I plan to continue working Artemis I need to find a way forward in much less available time. Time to understand some of my constraints. \n\n1. I work on a variety of machines, OBNC is the only compiler I've consistently been able to use across all my machines\n2. Porting between compilers takes energy and time, and those compilers don't work across all my machines\n3. When I write Oberon-07 code I quickly hit a wall for the things I want to do, this is what original inspired Artemis, so there is still a need for a collection of modules\n4. Oberon/Oberon-07 on Wirth RISC virtual machine is not sufficient for my development needs\n5. A2, while very impressive, isn't working for me either (mostly because I need to work on ARM CPUs)\n\nThese constraints imply Artemis is currently too broadly scoped. I think I need to focus on what works in OBNC for now. Once I have a clear set of modules then I can revisit portability to other compilers.\n\nWhat modules do I think I need? If I look at my person projects I tend to work allot with text, often structured text (e.g. XML, JSON, CSV). I also tend to be working with network services. Occasionally I need to interact with database (e.g. SQLite3, MySQL, Postgres).  Artemis should provide modules to make it easy to write code in Oberon-07 that works in those areas. Some of that I can do by wrapping existing C libraries. Some I can simply write from scratch in Oberon-07 (e.g. a JSON encoder/decoder). That's going to me my focus as my hobby time becomes available and then.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-07-27",
        "keywords": [
          "Oberon",
          "Oberon-07",
          "Artemis"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 22,
        "series": "Mostly Oberon",
        "title": "Artemis Project Status, 2022"
      },
      "url": "posts/2022/07/27/Artemis-Status-Summer-2022.json"
    },
    {
      "content": "\nTurbo Oberon, the dream\n=======================\n\nby R. S. Doiel, 2022-07-30\n\nSometimes I have odd dreams and that was true last night through early this morning. The dream was set in the future. I was already retired. It was a dream about \"Turbo Oberon\".\n\n\"Turbo Oberon\" was an Oberon language. The language compiler was named \"TO\" in my dream. A module's file extension was \".tom\", in honor of Tom Lopez (Meatball Fulton) of ZBS. There were allot of ZBS references in the dream.\n\n\"TO\" was very much a language in the Oberon-07 tradition with minor extensions when it came to bringing in modules. It allowed for a multi path search for module names. You could also express a Module import as a string allowing providing paths to the imported module.\n\nCompilation was similar to Go. Cross compilation was available out of the box by setting a few environment variables. I remember answering questions about the language and its evolution. I remember mentioning in the conversation about how I thought Go felling into the trap of complexity like Rust or C/C++ before it. The turning point for Go was generics. Complexity was the siren song to be resisted in \"Turbo Oberon\". Complexity is seductive to language designers and implementers. I was only an implementer.\n\nEvolution wise \"TO\" was built initially on the Go tool chain. As a result it featured easily cross-compiled binaries and had a rich standard set of Modules like Go but also included portable libraries for implementing graphic user interfaces. \"Turbo Oberon\" evolved as a conversation between Go and the clean simplicity of Oberon-07. Two example applications \"shipped\" with the \"TO\" compiler. They were an Oberon like Operating System (stand alone and hosted) and a Turbo Pascal like IDE. The IDE was called \"toe\" for Turbo Oberon Editor. I don't remember the name of the OS implementation but it might have been \"toos\". I remember \"TO\" caused problems for search engines and catalog systems.\n\nI remember remarking in the dream that programming in \"Turbo Oberon\" was a little like returning to my roots when I first learned to program in Turbo Pascal. Except I could run my programs regardless of the operating system or CPU architecture. \"\"TO\" compiler supported cross compilation for Unix, macOS, Windows on ARM, Intel, RISC-V. The targets were inherited from Go implementation roots.\n\nIn my dream I remember forking Go 1.18 and first replacing the front end of the compiler. I remember it was a challenge understanding the implementation and generate a Go compatible AST. The mapping between Oberon-07 and Go had its challenges. I remember first sticking to a strict Oberon-07 compiler targeting POSIX before enhancing module imports. I remember several failed attempts at getting module imports \"right\". I remember being on the fence about a map data type and going with a Maps module.  I don't remember how introspection worked but saying it was based on an ETH paper for Oberon 2.  I remember the compiler, like Go, eventually became self hosting. It supported a comments based DSL to annotating RECORD types making encoding and decoding convenient, an influence of Go and it's tool chain.\n\nI believe the \"Turbo Oberon Editor\" came first and that was followed by the operating system implementation.\n\nI remember talking about a book that influenced me called, \"Operating Systems through compilers\" but don't know who wrote it. I remember a discussion about the debt owed to Prof. Wirth. I remember that the book showed how once you really understood building the compile you could then build the OS. There was a joke riffing on the old Lisp joke but rephrased, \"all applications evolve not to a Lisp but to an embedded OS\".\n\nIt was a pleasant dream, in the dream I was older and already retired but still writing \"TO\" code and having fun with computers. I remember a closing video shot showing me typing away at what looked like the old Turbo Pascal IDE. As Mojo Sam said in **Somewhere Next Door to Reality**, \"it was a sorta a retro future\".\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2022-07-30",
        "keywords": [
          "Oberon",
          "Wirth",
          "ETH",
          "dreams",
          "compilers",
          "operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Turbo Oberon, the dream"
      },
      "url": "posts/2022/07/30/Turbo-Oberon.json"
    },
    {
      "content": "\n# Working with Structured Data in Deno and TypeScript\n\nOne of the features in Go that I miss in TypeScript is Go's [DSL](https://en.wikipedia.org/wiki/Domain-specific_language \"Domain Specific Language\") for expressing data representations.  Adding JSON, YAML and XML support in Go is simple. Annotating a struct with a string expression. There is no equivalent feature in TypeScript. How do easily support multiple representations in TypeScript?\n\nLet's start with JSON. TypeScript has `JSON.stringify()` and `JSON.parse()`. So getting to JSON representation is trivial, just call the stringify method. Going from text to populated object is done with `JSON.parse`. But there is a catch.\n\nLet's take a simple object I'm defining called \"ObjectN\". The object has a single attribute \"n\". \"n\" holds a number. The initial values is set to zero. What happens when I instantiate my ObjectN then assign it the result from `JSON.parse()`.\n\n~~~TypeScript\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n}\nlet src = `{\"n\": 1}`;\nlet o: ObjectN = new ObjectN();\no = JSON.parse(src);\n// NOTE: This will fail, addThree method isn't available.\nconsole.log(o.addThree());\n~~~\n\nHuston, we have a problem. No \"addThree\" method. That is because JSON doesn't include method representation. What we really want to do is inspect the object returned by `JSON.parse()` and set the values in our ObjectN accordingly. Let's add a method called `fromObject()`.\n(type the following into the Deno REPL).\n\n~~~TypeScript\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n    fromObject(o: {[key: string]: any}): boolean {\n        if (o.n === undefined) {\n            return false;\n        }\n        // Validate that o.n is a number before assigning it.\n        const n = (new Number(o.n)).valueOf();\n        if (isNaN(n)) {\n            return false;\n        }\n        this.n = n;\n        return true;\n    }\n}\nlet src = `{\"n\": 1}`;\nlet o: ObjectN = new ObjectN();\nconsole.log(o.addThree());\no.fromObject(JSON.parse(src));\nconsole.log(o.addThree());\n~~~\n\nNow when we run this code we should see a \"3\" and then a \"4\" output. Wait, `o.fromObject(JSON.parse(src));` looks weird. Why not put `JSON.parse()` inside \"fromObject\"? Why not renamed it \"parse\"?\n\nI want to support many types of data conversion like YAML or XML. I can use my \"fromObject\" method with the result of produced from `JSON.parse()`, `yaml.parse()` and `xml.parse()`. One function works with the result of all three. Try adding this.\n\n~~~TypeScript\nimport * as yaml from 'jsr:@std/yaml';\nimport * as xml from \"jsr:@libs/xml\";\nsrc = `n: 2`;\no.fromObject(yaml.parse(src));\nconsole.log(o.addThree());\nsrc = `<n>3</n>`;\no.fromObject(xml.parse(src));\nconsole.log(o.addThree());\n~~~\n\nThat works!\n\nStill it would be nice to have a \"parse\" method too. How do I do that without winding up with a \"parseJSON()\", \"parseYAML()\" and \"parseXML()\"? What I really want is a \"parseWith\" method which accepts the text and a parse function. TypeScript expects type information about the function being passed. I solve that problem by including a \"ObjectParseType\" definition that works across the three parsing objects -- JSON, yaml and xml.\n\n~~~TypeScript\nimport * as yaml from 'jsr:@std/yaml';\nimport * as xml from \"jsr:@libs/xml\";\n\n// This defines my expectations of the parse function provide by JSON, yaml and xml.\ntype ObjectParseType = (arg1: string, arg2?: any) => {[key: string]: any} | unknown;\n\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n    fromObject(o: {[key: string]: any}) : boolean {\n        if (o.n === undefined) {\n            return false;\n        }\n        // Validate that o.n is a number before assigning it.\n        const n = (new Number(o.n)).valueOf();\n        if (isNaN(n)) {\n            return false;\n        }\n        this.n = n;\n        return true;\n    }\n    parseWith(s: string, fn: ObjectParseType): boolean {\n        return this.fromObject(fn(s) as unknown as {[key: string]: any});\n    }\n}\n\nlet o: ObjectN = new ObjectN();\nconsole.log(`Initial o.addThree() -> ${o.addThree()}`);\nconsole.log(`o.toString() -> ${o.toString()}`);\n\nlet src = `{\"n\": 1}`;\no.parseWith(src, JSON.parse);\nconsole.log(`parse with JSON, o.addThree() -> ${o.addThree()}`);\nconsole.log(`JSON.stringify(o) -> ${JSON.stringify(o)}`);\n\nsrc = `n: 2`;\no.parseWith(src, yaml.parse);\nconsole.log(`parse with yaml, o.addThree() -> ${o.addThree()}`);\nconsole.log(`yaml.stringify(o) -> ${yaml.stringify(o)}`);\n\nsrc = `<?xml version=\"1.0\"?>\n<n>3</n>`;\no.parseWith(src, xml.parse);\nconsole.log(`parse with xml, o.addThree() -> ${o.addThree()}`);\nconsole.log(`xml.stringify(o) -> ${xml.stringify(o)}`);\n~~~\n\nAs long as the parse method returns an object I can now update my ObjectN instance\nfrom the attributes of the object expressed as JSON, YAML, or XML strings. I like this approach because I can add validation and normalization in my \"fromObject\" method and use for any parse method that confirms to how JSON, YAML or XML parse works. The coding cost is the \"ObjectParseType\" type definition and the \"parseWith\" method boiler plate and defining a class specific \"fromObject\". Supporting new representations does require changes to my class definition at all.\n",
      "data": {
        "abstract": "A short discourse on working with structured data in TypeScript and easily\nconverting from JSON, YAML and XML representations.\n",
        "createDate": "2025-02-03",
        "keywords": [
          "Deno",
          "TypeScript",
          "Structured Data"
        ],
        "title": "Working with Structured Data in Deno and TypeScript"
      },
      "url": "posts/2025/02/03/working_with_structured_data.json"
    },
    {
      "content": "\n# Deno 2.1.7, Project Idioms\n\nI've noticed a collection of file and code idioms I've been used in my recent Deno+TypeScript projects at work. I've captured them here as a future reference.\n\n## Project files\n\nMy project generally have the following files, these are derived from the [CodeMeta](https://codemeta.github.io) file using [CMTools](https://caltechlibrary.github.io/CMTools).\n\ncodemeta.json\n: Primary source of project metadata, used to generate various files\n\nCITATION.cff\n: used by GitHub for citations. version, dateModified, datePublished and releaseNotes\n\n\nabout.md\n: A project about page. This is generated based on the codemeta.json file.\n\nREADME.md, README\n: A general read me describing the project and pointing to INSTALL.md, user_manual.md as appropriate\n\nINSTALL.md\n: These are boiler plate description of how to install and compile the software\n\nuser_manual.md\n: This is an index document, a table of contents. It points to other document including Markdown versions of the man page(s).\n\nFor TypeScript projects I also include a following\n\nversion.ts\n: This hold project version information used in the TypeScript co-debase. It is generated from the codemeta.json via CMTools.\n\nhelptext.ts\n: This is where I place a function, `fmtHelp()`, for rendering response to the \"help\" command line option.\n\nI'm currently ambivalent about \"main.ts\" file which is created by `deno init`. My ambivalent is that most of my projects wind up producing more than one program from a shared code base. a single \"main.ts\" doesn't really fit that situation.\n\nThe command line tool will have a TypeScript with it's name. Inside this file I'll have a main function and use the Deno idiom `if (import.meta.main) main();` to invoke it. I don't generally put the command line  TypeScript in my \"mod.ts\" file since it's not going to work in a browser or be useful outside my specific project.\n\nmod.ts\n: I usually re-export modules here that maybe useful outside my project (or in the web browser).\n\ndeps.ts\n: I use this if there are allot of files consistently being imported across the project, otherwise I skip it.\n\n## What I put in Main\n\nI use the main function to define command line options, handle parameters such as data input, output and errors. It usually invokes a primary function modeled in the rest of the project code.\n\nHere is an example Main for a simple \"cat\" like program.\n\n~~~TypeScript\nimport { parseArgs } from \"jsr:@std/cli\";\nimport { licenseText, releaseDate, releaseHash, version } from \"./version.ts\";\nimport { fmtHelp, helpText } from \"./helptext.ts\";\n\nconst appName = \"mycat\";\n\nasync function main() {\n  const app = parseArgs(Deno.args, {\n    alias: {\n      help: \"h\",\n      license: \"l\",\n      version: \"v\",\n    },\n    default: {\n      help: false,\n      version: false,\n      license: false,\n    },\n  });\n  const args = app._;\n\n  if (app.help) {\n    console.log(fmtHelp(helpText, appName, version, releaseDate, releaseHash));\n    Deno.exit(0);\n  }\n  if (app.license) {\n    console.log(licenseText);\n    Deno.exit(0);\n  }\n  if (app.version) {\n    console.log(`${appName} ${version} ${releaseHash}`);\n    Deno.exit(0);\n  }\n\n  let input: Deno.FsFile | any = Deno.stdin;\n\n  // handle case of many file names\n  if (args.length > 1) {\n    for (const arg of args) {\n      input = await Deno.open(`${arg}`);\n      for await (const chunk of input.readable) {\n        const decoder = new TextDecoder();\n        console.log(decoder.decode(chunk));\n      }\n    }\n    Deno.exit(0);\n  }\n  if (args.length > 0) {\n    input = await Deno.open(Deno.args[0]);\n  }\n  for await (const chunk of input.readable) {\n    const decoder = new TextDecoder();\n    console.log(decoder.decode(chunk));\n  }\n}\n\nif (import.meta.main) main();\n~~~\n\n## helptext.ts\n\nThe following is an example of the [helptext.ts](helptext.ts) file for the demo [mycat.ts](mycat.ts).\n\n```TypeScript\nexport function fmtHelp(\n  txt: string,\n  appName: string,\n  version: string,\n  releaseDate: string,\n  releaseHash: string,\n): string {\n  return txt.replaceAll(\"{app_name}\", appName).replaceAll(\"{version}\", version)\n    .replaceAll(\"{release_date}\", releaseDate).replaceAll(\n      \"{release_hash}\",\n      releaseHash,\n    );\n}\n\nexport const helpText =\n  `%{app_name}(1) user manual | version {version} {release_hash}\n% R. S. Doiel\n% {release_date}\n\n# NAME\n\n{app_name}\n\n# SYNOPSIS\n\n{app_name} FILE [FILE ...] [OPTIONS]\n\n# DESCRIPTION\n\n{app_name} implements a \"cat\" like program.\n\n# OPTIONS\n\nOptions come as the last parameter(s) on the command line.\n\n-h, --help\n: display help\n\n-v, --version\n: display version\n\n-l, --license\n: display license\n\n\n# EXAMPLES\n~~~shell\n{app_name} README.md\n{app_name} README.md INSTALL.md\n~~~\n`;\n```\n\n## Generating version.ts\n\nThe [version.ts](version.ts) is generated form two files, [codemeta.json] and [LICENSE] using the CMTools, `cmt` command.\n\n~~~\ncmt codemeta.json veresion.ts\n~~~\n",
      "data": {
        "abstract": "Notes on some of the file and code idioms I'm using with Deno+TypeScript projects.",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "createDate": "2025-01-29",
        "keywords": [
          "Deno",
          "TypeScript",
          "Projects"
        ],
        "title": "Deno 2.1.7, Project Idioms"
      },
      "url": "posts/2025/01/29/project_idioms.json"
    },
    {
      "content": "\n# Deno 2.1.7, Points of Friction\n\nBy R. S. Doiel, 2025-01-26\n\nI have run into a few points of friction in my journey with Deno coming from Go. I miss Go's standard \"io\" and \"bufio\" packages. With the Go code I'm porting TypeScript I'd often need to handle standard input or input from a named file interchangeably. Seems like this should be easy in Deno's TypeScript but there are a few bumps in the road.\n\nHere's the Go idiom I commonly use.\n\n~~~go\nvar err error\ninput := io.Stdin\nif inFilename != \"\" {\n    input, err := os.Open(inFilename)\n    if err !== nil {\n        // ... handle error\n    }\n    defer input.Close();\n}\n// Now I can just pass \"in\" around for processing.\n~~~\n\nConceptually this feels simple though verbose. I can pass around the \"input\" for processing in a way that is agnostic as to file or standard input. This type of Go code works equally on POSIX and Windows.\n\nDeno provide access to [standard input](https://docs.deno.com/api/deno/~/Deno.stdin). Deno supports streamable files. From the docs here's an simple example.\n\n~~~TypeScript\n// If the text \"hello world\" is piped into the script:\nconst buf = new Uint8Array(100);\nconst numberOfBytesRead = await Deno.stdin.read(buf); // 11 bytes\nconst text = new TextDecoder().decode(buf);  // \"hello world\"\n~~~\n\nSetting aside the buffer management code it seems simple and straight forward. It is easy to understand and you could wrap it in a function easily to hide the buffer management part. Yet it doesn't provide the same flexibility as the more verbose Go version. Surely there is an an idiomatic why of doing this in TypeScript already? \n\n## Stability Challenge\n\nDeno currently is a rapidly evolving platform. My first impulse was to reach for packages like `jsr:@std/fs` or `jsr:@sys/fs`. When I search for examples they mostly seem to reference specific versions of \"std/fs\" that are not available via jsr. So what's the \"right\" way to approach this?\n\n## Repl to the rescue.\n\nPoking around in the Deno repl I tried assigning `Deno.stdin` to a local variable. Playing with command line completion I realized it has most of the the methods you would get if you used `Deno.open()` to open a named file.\n\nHere's a little test I ran in the repl after creating a \"hellworld.txt\" text file.\n\n~~~deno\ndeno\nconst stdin = Deno.stdin;\nlet input = Deno.open('helloworld.txt')\nstdin.isTerminal();\ninput.isTerminal();\nstdin.valueOf();\ninput.valueOf();\nDeno.exit(0);\n~~~\n\nThe `valueOf()` reveals their type affiliation. It listed them as `Stdin {}` and `FsFile {}` respectively. I used TypeScript's typing system to let us implement \"mycat.ts\". You can assign multiple types to a variable with a `|` (pipe) symbol in TypeScript. \n\nUsed that result to write a simple cat file implementation.\n\n~~~TypeScript\nasync function catFile() {\n    let input : Stdin | FsFile = Deno.stdin;\n\n    if (Deno.args.length > 0) {\n        input = await Deno.open(Deno.args[0]);\n    }\n\n    const decoder = new TextDecoder();\n\n    // NOTE: the .readable function is available on both types of objects.\n    for await (const chunk of input.readable) {\n        console.log(decoder.decode(chunk));\n    }\n}\n\nif (import.meta.main) catFile();\n~~~\n\nYou can \"run\" this deno to see it in action. Try running it on your \"helloworld.txt\" file.\n\n~~~shell\ndeno run --allow-read mycat.ts helloworld.txt\n~~~\n\nYou can also read from standard input too. Try the command below type in some text then press Ctrl-D or Ctrl-Z if you're on Windows.\n\n~~~shell\ndeno run --allow-read mycat.ts\n~~~\n\nLooks like we have a nice solution. Now I can compile \"mycat.ts\".\n\n## trouble in paradise\n\nWhile you can \"run\" the script you can't compile it. It doesn't pass \"check\". This is the error I get with Deno 2.1.7.\n\n~~~shell\ndeno check mycat.ts\nCheck file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts\nerror: TS2304 [ERROR]: Cannot find name 'Stdin'.\n    let input : Stdin | FsFile = Deno.stdin;\n                ~~~~~\n    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:17\n\nTS2552 [ERROR]: Cannot find name 'FsFile'. Did you mean 'File'?\n    let input : Stdin | FsFile = Deno.stdin;\n                        ~~~~~~\n    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:25\n\n    'File' is declared here.\n    declare var File: {\n                ~~~~\n        at asset:///lib.deno.web.d.ts:622:13\n\nFound 2 errors.\n~~~\n\nIt seems like what works in the repl should also compile but that's isn't the case. I have an open question on Deno's discord help channel and am curious to find the \"correct\" way to handle this problem.\n\n## Update 2025-01-26, 5:00PM\n\nI heard back on Deno Discord channel for help.  With the help of [crowlKat](https://github.com/crowlKats) sorted the problem out.\n\nThe compile and runnable version of [mycat.ts](mycat.ts) looks like this.\n\n~~~typescript\nasync function main() {\n    let input : Deno.FsFile | any = Deno.stdin;\n\n    if (Deno.args.length > 0) {\n        input = await Deno.open(Deno.args[0]);\n    }\n\n    const decoder = new TextDecoder();\n\n    // NOTE: the .readable function is available on both types of objects.\n    for await (const chunk of input.readable) {\n        console.log(decoder.decode(chunk));\n    }\n}\n\nif (import.meta.main) main();\n~~~\n\nThe \"any\" type feels a little ugly but since I am assinging the default value is `Deno.stdin` it covers that case where the `Deno.FsFile` covers the case of a name file.  Where does this leave me? I have a nice clean idiom that does what I want for interacting with standard input or a file stream.  Not necessarily the fast thing on the planet but it works.\n\n\n",
      "data": {
        "abstract": "A short discussion of working with file input in TypeScript+Deno coming from the\nperspective of Go's idiomatic use of io buffers.\n",
        "byline": "R. S. Doiel",
        "createdDate": "2025-01-26",
        "keywords": [
          "deno",
          "text",
          "input"
        ],
        "title": "Deno 2.1.7, Points of Friction"
      },
      "url": "posts/2025/01/26/points_of_friction.json"
    },
    {
      "content": "\n# Two missing features from HTML5, an enhanced form.enctype and a list input type\n\n## Robert's wish list for browsers and web forms handling\n\nBy R. S. Doiel, 2024-02-23\n\nI wish the form element supported a `application/json` encoding type and there was such a thing as a `list-input` element.\n\nI've been thinking about how we can get back to basic HTML documents and move away from JavaScript required to render richer web forms. When web forms arrived on scene in the early 1990s they included a few basic input types. Over the years a few have been added but by and large the data model has remained relatively flat. The exception being the select element with `multiple` attribute set. I believe we are being limited by the original choice of urlencoding web forms and then resort to JavaScript to address it's limitations.\n\nWhat does the encoding of a web form actually look like?  The web generally encodes the form using urlencoding. It presents a stream of key value pairs where the keys are the form's input names and the values are the value of the input element. With a multi-select element the browser simply repeats the key and adds the next value in the selection list to that key.  In Go you can describe this simple data structure as a `map[string][]string`[^1]. Most of the time a key points to a single element array of string but sometimes it can have multiple elements using that key and then the array expands to accommodate. Most of the time we don't think about this as web developers. The library provided with your programming language decodes the form into a more programmer friendly representation. But still I believe this simple urlencoding has held us back. Let me illustrate the problem through a series of simple form examples.\n\n[^1]: In English this could be described as \"a map using a string to point at a list of strings\" with \"string\" being a sequence of letters or characters.\n\nHere's an example of a simple form with a multi select box. It is asking for your choice of ice cream flavors.\n\n~~~html\n<form method=\"POST\">\n  <label for=\"ice-cream-flavors\">Choose your ice cream flavors:</label>\n  <select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple >\n    <option value=\"Chocolate\">Chocolate</option>\n    <option value=\"Coconut\">Cocunut</option>\n    <option value=\"Mint\">Mint</option>\n    <option value=\"Strawberry\">Strawberry</option>\n    <option value=\"Vanilla\">Vanilla</option>\n    <option value=\"Banana\">Banana</option>\n    <option value=\"Peanut\">Peanut</option>\n  </select>\n  <p>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nBy default your web browser will packaged this up and send it using \"application/x-www-form-urlencoded\". If you select \"Coconut\" and \"Strawberry\" then the service receiving your data will get an encoded document that looks like this.\n\n~~~urlencoding\nice-cream-flavors=Coconut&ice-cream-flavors=Strawberry\n~~~\n\nThe ampersands separate the key value pairs. The fact that \"ice-cream-flavors\" name repeats means that the key \"ice-cream-flavors\" will point to an array of values.  In pretty printed JSON representation is a little clearer.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Coconut\", \"Strawberry\" ]\n}\n~~~\n\nSo far so good. Zero need to enhance the spec. It works and has worked for a very long time. Stability is a good thing. Let's elaborate a little further.  I've added a dish choice for the ice cream, \"Sugar Cone\" and \"Waffle Bowl\". That web form looks like.\n\n~~~html\n<form method=\"POST\">\n<label for=\"ice-cream-flavors\">Select the flavor for each scoop of ice cream:</label>\n<select id=\"ice-cream-flavors\" name=\"ice-cream-flavors\" multiple>\n  <option value=\"Chocolate\">Chocolate</option>\n  <option value=\"Coconut\">Cocunut</option>\n  <option value=\"Mint\">Mint</option>\n  <option value=\"Strawberry\">Strawberry</option>\n  <option value=\"Vanilla\">Vanilla</option>\n  <option value=\"Banana\">Banana</option>\n  <option value=\"Peanut\">Peanut</option>\n</select>\n<p>\n<fieldset>\n  <legend>Pick your delivery dish</legend>\n  <div>\n    <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n    <label for=\"sugar-cone\">Sugar Cone</label>\n  </div>\n  <div>\n    <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n    <label for=\"waffle-bowl\">Waffle Bowl</label>\n  </div>\n</fieldset>\n<input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nIf we select \"Banana\" and \"Peanut\" flavors served in a \"Waffle Bowl\" the encoded document would reach the web service looking something like this.\n\n~~~urlencoded\nice-cream-flavors=Banana&ice-cream-flavors=Peanut&ice-cream-dish=waffle-cone\n~~~\n\nThat's not too bad. Again this is the state of web form for ages now. In JSON it could be represented as the following.\n\n~~~json\n{\n    \"ice-cream-flavors\": [ \"Banana\", \"Peanut\" ],\n    \"ice-cream-dish\": \"waffle-cone\"\n}\n~~~\n\nThis is great we have a simple web form that can collect a single ice cream order.  But what if we want to actually place several individual ice cream orders as one order? Today we have two choices, multiple web forms that accumulate the orders (circa 2000) or use JavaScript create a web UI that can handle list of form elements. Both have their drawbacks.\n\nIn the case of the old school approach changing web pages just to update an order can be slow and increase uncertainty about your current order. That is why the JavaScript approach has come to be more common. But that JavaScript approach comes at a huge price. It's much more complex, we've seen a dozens of libraries and frameworks that have come and gone trying to manage that complexity in various ways.\n\nIf we supported JSON encoded from submission directly in the web browser I think we'd make a huge step forward. It could decouple the JavaScript requirement. That would avoid much of the cruft that we ship down to the web browser today because we can't manage lists of things without resorting to JavaScript.\n\nLet's pretend there was a new input element type called \"list-input\". A \"list-input\" element can contain any combination of today's basic form elements. Here's my hypothetical `list-input` based from example. In it we're going to select the ice cream flavors and the dish format (cone, bowl) as before but have them accumulate in a list. That form could be expressed in HTML similar to my mock up below.\n\n~~~html\n<form>\n  <label for=\"ice-cream-order\">Place your next order, press submit when you have all of them.</label>\n  <list-input id=\"ice-cream-order\" name=\"ice-cream-order\">\n    <label for=\"ice-cream-flavor\">Select the flavor for each scoop of ice cream:</label>\n    <select id=\"ice-cream-flavor\" name=\"ice-cream-flavor\" multiple>\n      <option value=\"Chocolate\">Chocolate</option>\n      <option value=\"Coconut\">Cocunut</option>\n      <option value=\"Mint\">Mint</option>\n      <option value=\"Strawberry\">Strawberry</option>\n      <option value=\"Vanilla\">Vanilla</option>\n      <option value=\"Banana\">Banana</option>\n      <option value=\"Peanut\">Peanut</option>\n    </select>\n  <p>\n  <fieldset>\n    <legend>Pick your delivery dish</legend>\n    <div>\n      <input type=\"radio\" id=\"sugar-cone\" name=\"ice-cream-dish\" value=\"sugar-cone\" />\n      <label for=\"sugar-cone\">Sugar Cone</label>\n    </div>\n    <div>\n      <input type=\"radio\" id=\"waffle-bowl\" name=\"ice-cream-dish\" value=\"waffle-bowl\" />\n      <label for=\"waffle-bowl\">Waffle Bowl</label>\n    </div>\n  </fieldset>\n  </list-input>\n  <input type=\"submit\"> <input type=\"reset\">\n</form>\n~~~\n\nWith two additional lines of HTML the input form can now support a list of individual ice cream orders. Assuming only urlencoding is supported then how does that get encoded and sent to the web server? Here is an example set of orders\n\n1. vanilla ice cream with a sugar cone\n2. chocolate with a waffle bowl\n\n~~~urlencoded\nice-cream-flavors=Vanilla&ice-cream-flavors=Chocolate&ice-cream-dish=sugar-cone&ice-cream-dish=waffle-bowl\n~~~\n\nWhich flavor goes with which dish?  That's the problem with urlencoding a list in your web form. We just can't keep the data alignment manageable.  What if the web browser used JSON encoding? \n\n~~~json\n[\n  {\n      \"ice-cream-flavors\": [ \"Vanilla\" ],\n      \"ice-cream-dish\": \"sugar-cone\"\n  },\n  {\n      \"ice-cream-flavors\": [ \"Chocolate\" ],\n      \"ice-cream-dish\": \"waffle-bowl\"\n  }\n~~~\n\nSuddenly the alignment problem goes away. There is precedence for controlling behavior of the web browser submission through the `enctype` attribute. File upload was addressed by adding support for `multipart/form-data`.  In 2024 and for over the last decade it has been common practice in web services to support JSON data submission. I believe it is time that the web browser also supports this directly. This would allow us to decouple the necessity of using JavaScript in browser as we require today. The form elements already map well to a JSON encoding. If JSON encoding was enabled then adding a element like my \"list-input\" would make sense.  Otherwise we remain stuck in a world where hypertext markup language remains very limited and can't live without JavaScript.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "html",
          "web forms",
          "encoding"
        ],
        "title": "Two missing features from HTML5, an enhanced form.enctype and a list input type"
      },
      "url": "posts/2024/02/23/enhanced_form_handling.json"
    },
    {
      "content": "\n# Transpiling & Bundling with Emit\n\nOne of the nice features of Deno is native TypeScript support.  One of the selling strength though is that the same source can run both server side and browser side.  The challenge is that TypeScript does not have native TypeScript support. This is easy to remedy using Deno's [emit](https://jsr.io/@deno/emit) module.\n\nThe emit module supports to important functions, `transpile` and `bundle`. Both will render your TypeScript as JavaScript in a browser friendly manner. The `transpile` function turns a single TypeScript file into an equivalent JavaScript file. Bundle can do that with a TypeScript and all the files it imports so you have a self contained JavaScript file with everything you need.\n\n<!-- The emit module website shows how to write a short TypeScript program to transpile and bundle.  When you combine that with a Deno task it is trivial to automatically make that happen. -->\n\n\nHere's what my `transpile.ts` looks like.\n\n~~~typescript\nimport { transpile } from \"jsr:@deno/emit\";\nimport * as path from \"jsr:@std/path\";\n\n/* Transpile directory_client.ts to JavaScript and render it to \n   htdocs/js/directory_client.js */\nconst js_path = path.join(\"htdocs\", \"js\");\nconst js_name = path.join(js_path, \"directory_client.js\");\nconst url = new URL(\"./directory_client.ts\", import.meta.url);\nconst result = await transpile(url);\nconst code = await result.get(url.href);\n\nawait Deno.mkdir(js_path, { mode: 0o775, recursive: true });\nDeno.writeTextFile(js_name, code);\n~~~\n\nYou can run that with the following long command line.\n\n~~~shell\ndeno run --allow-import --allow-env --allow-read --allow-write --allow-net transpile.ts\n~~~\n\nOf course you can easily turn this into a [Deno task](https://docs.deno.com/runtime/reference/cli/task_runner/).\n\nIf our `directory_client.ts` file contained other modules you can instead use the `bundle` function.  Here's an example of bundling our `directory_client.ts` saving the result as `htdocs/modules/directory_client.js`.\n\n~~~typescript\n/**\n * bundle.ts is an example of \"bundling\" the type script file directory_client.ts\n * into a module and writing it to htdocs/modules.\n */\nimport { bundle } from \"jsr:@deno/emit\";\n\nconst js_path = path.join(\"htdocs\", \"modules\");\nconst js_name = path.join(js_path, \"directory_client.js\");\nconst result = await bundle(\"./directory_client.ts\");\nconst { code } = result;\nawait Deno.mkdir(js_path, { mode: 0o775, recursive: true });\nawait Deno.writeTextFile(js_name, code);\n~~~\n\nYou can run that with the following long command line.\n\n~~~shell\ndeno run --allow-import --allow-env --allow-read --allow-write --allow-net bundle.ts\n~~~\n\nThe bundle will contain the transpiled TypeScript from `directory_client.ts` but also any modules that `directory_client.ts` relied on. If you don't want to include the imported modules then you can set the value of `recursive` to false.\n",
      "data": {
        "abstract": "A brief discussion of using the Deno emit module to transpile and bundle\nTypeScript.\n",
        "byline": "R. S. Doiel, 2024-11-21",
        "keywords": [
          "Deno",
          "TypeScript",
          "transpile",
          "bundle"
        ],
        "title": "Transpiling & Bundling with Emit"
      },
      "url": "posts/2024/11/21/transpiling-and-bundling-with-emit.json"
    },
    {
      "content": "\n# Installing Deno via Cargo and other options\n\nBy R. S. Doiel, 2024-12-13\n\nI've recently needed to install Deno on several Debian flavored Linux boxes.  I wanted to install Deno using the `cargo install --locked deno` command. Notice the `--locked` option, you need that for Deno. This worked for the recent Ubuntu 22.04 LTS release. I needed alternatives for Ubuntu 20.04 LTS and Raspberry Pi OS.\n\n## Using Cargo\n\nPrerequisites:\n\n- Rust (install with [Rustup](https://rustup.rs))\n- CMake\n- Clang, LLVM dev, Clang DEV and the lld (clang) linker\n- SQLite3 and LibSQLite3 dev\n- pkg config\n- libssh dev, libssl dev\n\nThe Debian flavors I work with are recent (Dec. 2024) Ubuntu 22.04 LTS release[^1].\n\nRecently when I was installing Deno 2.1.4 I got errors about building the `flate2` module. I had forgotten to include the `--locked` option in my cargo command. I found this solution in Deno GitHub issue [9524](https://github.com/denoland/deno/issues/9524).\n\n```shell\nsudo apt install -y build-essential cmake clang libclang-dev llvm-dev lld \\\n                    sqlite3 libsqlite3-dev pkg-config libssh-dev libssl-dev\nrustup update\ncargo install deno --locked\n```\n\n## Other options\n\nFor Ubuntu 20.04 LTS and Raspberry Pi OS, use `curl -fsSL https://deno.land/install.sh | sh` to install.\n\nFor Windows on ARM64 use `iwr https://deno.land/install.ps1 -useb | iex`.\n\n `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`\nOn Raspberry Pi OS I added a `nice` before calling `cargo`. Without the \"nice\" it failed after the \"spin\" module.\n\n[^1]: I failed to install Deno this way on Ubuntu 20.04 LTS, just use the cURL + sh script.\n",
      "data": {
        "abstract": "Notes on setting up a Debian flavored Linux boxes, macOS and Windows to install Deno via `cargo install deno`,\n`curl -fsSL https://deno.land/install.sh | sh` or\n`iwr https://deno.land/install.ps1 -useb | iex`\n",
        "byline": "R. S. Doiel, 2024-12-13",
        "createDate": "2024-12-13",
        "keywords": [
          "rust",
          "deno",
          "cargo",
          "Debian",
          "Linux",
          "windows",
          "macOS"
        ],
        "title": "Installing Deno via Cargo and other options"
      },
      "url": "posts/2024/12/13/installing-via-cargo-etc.json"
    },
    {
      "content": "",
      "data": {},
      "url": "posts/nav.json"
    },
    {
      "content": "\n# Postgres Quick Notes, take two\n\nBy R. S. Doiel, 2023-11-17\n\nWhat follows is some quick notes to remind me of the things I do when\nI setup a new instance of PostgreSQL on the various machines I work with.\n\n## Installation approach\n\nIf possible I install Postgres with the system's package manager or follow\nthe directions suggested for installation on the [Postgres website](https://postgres.org).\n\n### macOS and Postgres\n\nFor macOS that's not the route I take if possible is to install via [Postgres App](https://postgresapp.com/).\nThis provides a very nice setup of developing with Postgres on macOS and also allows you to easily\ntest multiple versions of Postgres.  It is not as convenient in the Mac Mini headless configuration\nI also use Postgres on macOS in. In that case I use Mac Ports' package manager to install Postgres.\nUnfortunately just using ports command isn't enough to get running. What follows is my notes on the\nadditional steps I've taken to get things working.\n\nInstall the version of Postgres you want (e.g. PostgreSQL 16) via ports\n\n1. install postgresql16, postgresql16-server, postgres_select\n2. make sure the postgres version is selected using the ports command\n3. make a directory for the default postgres db\n4. make sure the default db directory is owned by the postgres user\n5. run the initialization scripts provided by the posts installer\n6. use the ports command to load the plist\n7. start up the server, make sure the log file is writable\n\nHere's the commands I type in the shell\n\n~~~shell\nsudo port install postgresql16-server postgresql16 postgresql_select\n# Answer y to the prompt\n# After the install completes Ports will suggest the following to complete the process.\nsudo port select postgresql postgresql16\nsudo mkdir -p /opt/local/var/db/postgresql16/defaultdb\nsudo chown postgres:postgres /opt/local/var/db/postgresql16/defaultdb\nsudo -u postgres /bin/sh -c 'cd /opt/local/var/db/postgresql16 && /opt/local/lib/postgresql16/bin/initdb -D /opt/local/var/db/postgresql16/defaultdb'\nsudo port load postgresql16-server\nsudo -u postgres /bin/sh -c '/opt/local/lib/postgresql16/bin/pg_ctl -D /opt/local/var/db/postgresql16/defaultdb -l /opt/local/var/log/postgresql16/postgres.log start'\n~~~\n\n## Database users setup\n\nThis applies to most Postgres installations I do because I am using them to\ndevelop software solutions. In a production setting you'd want a more conservative\nsecurity approach.\n\n1. Make sure you can connect as the postgres user\n2.  For each developer\n    a. Use the Postgres createuser tool to create superuser account(s)\n    b. Use the Postgres createdb tool to create databases for those account(s)\n\nHere's the commands I type in the shell\n\n~~~shell\nsudo -u postgres psql\n~~~\n\nWhen in the psql shell you should be able to use the slash commands like\n\n\\\\l\n: list the databases\n\n\\\\dt\n: list the tables in the database\n\n\\\\d TABLE\\_NAME\n: list the schema for TABLE\\_NAME\n\n\\\\q\n: quit the psql shell\n\nAssuming we have a working Postgres I now create superuser accounts for\ndevelopment and databases that match the username.\n\n~~~shell\nsudo -u postgres createuser --interactive $USER\ncreatedb $USER\n~~~\n\nI should now be able to run the psql shell without specifying the\npostgres username.\n\n~~~shell\npsql\n~~~\n",
      "data": {
        "abstract": "A collection of quick notes for setting and Postgres for development.",
        "byline": "R. S. Doiel, 2023-11-17",
        "keywords": [
          "postgres"
        ],
        "title": "Postgres Quick Notes, take two"
      },
      "url": "posts/2023/11/17/PostgreSQL-Quick-Notes.json"
    },
    {
      "content": "\n# RSS and my web experience\n\nby R. S. Doiel, 2023-12-07\n\nI agree with [Dave Winer](http://scripting.com/2023/12/07/140505.html?title=whyWeWantFeedsInBluesky), Blue Sky should support RSS. Most web systems benefit from supporting RSS. RSS is a base level inter-opt for web software like HTML and JSON can be. While I may not be typical I am an example of a web user who experiences much of the web via RSS. I read blog and site content via RSS. I \"follow\" my friends and colleagues via RSS. This is true when they blog or when they post in a Mastodon community.  I track academic repositories content for [ETH Zurich Research](https://www.rc-blog.ethz.ch/en/feed) and [Caltech](https://feeds.library.caltech.edu/recent/combined.html) via RSS feeds. I check the weather via NOAA's [RSS feed](https://www.weather.gov/rss).  News sites often syndicate still via RSS and then Podcasts, if they are actual Podcasts are distributed via RSS.  All this is to say I think RSS is not dead. It remains easy to render can can be easy to consume.  If a website doesn't provide it it is possible to generate it yourself[1] or find a service to use that does[2]. RSS remains key to how I experience and use the web in 2023.\n\n[1]: Go libraries like [Colly](https://go-colly.org/) and [Gofeeds](https://github.com/mmcdole/gofeed) make it possible to roll your own like the one in skimmer\n\n[2]: https://firesky.tv/ is an example of a service that provides RSS for Bluesky via its raw API, [html2rss](https://html2rss.github.io/) is service that producing RSS feeds for popular sites that don't include them\n\nMy personal approach to feeds is very much tailored to me. It's probably overkill for most people but it works with my vision and cognitive limitations. He's the steps I take in feed reading. They essentially decompose a traditional feed reader and allow for more flexibility for my reading pleasure.\n\n1. Maintain a list of feeds in a simple text file\n2. Harvest those feeds with [skimmer](https://rsdoiel.github.io/skimmer), Skimmer stores the items in an [SQLite3](https://sqlite.org)\n3. I filter the items using SQL and SQLite3 or via an interactive mode provided by Skimmer\n4. Render saved items to Markdown with [skim2md](https://rsdoiel.github.io/skimmer/skim2md.1.html)\n5. Use [Pandoc](https://pandoc.org) to render the Markdown and view Firefox\n\nThe nice thing about this approach is that I can easily script it with Bash or even a Windows bat. I can easily maintain separate lists and separate databases for personal and work related material.  A bonus is the database items can also serve as a corpus for a personal search engine too. If you want to save maintain a public reading list this setup is ideal too. Of course the list of curated items can be transformed into their own RSS feed as well.\n\n[Skimmer](https://rsdoiel.github.io/skimmer/skimmer.1.html) is a deconstructed feed reader. Does that make it post modern feed reader?  Skimmer processes a list of feeds I follow and saves the results in an SQLite 3 database. That database can be used to filter the feeds and flag items as \"saved\". Typically I filter by timestamps. Saved items can be processed with `skim2md` to render a markdown document. `skim2md` has an option to include a \"save to pocket\" button for each item in the output. I use Pandoc to render the page then view that result in Firefox. At my leisure I read the web page and press the \"Save to pocket\" button any item I want to read later. It's a very comfortable experience.\n\nSkimmer lead me to think about a personal news page for myself and family. Skimmer lets me curate separate lists organized around themes. These can then be rendered to individual pages like pages of a newspaper. This has been captured in an experimental project I call [Antenna](https://rsdoiel.github.io/antenna). It even includes a feed search feature thanks to [PageFind](https://pagefind.app)\n",
      "data": {
        "abstract": "RSS is alive and kicking and Bluesky should support it too. Explore my recipe for reading web news.",
        "byline": "R. S. Doiel, 2023-12-07",
        "created": "2023-12-07",
        "keywords": [
          "RSS",
          "Feeds",
          "Social Media",
          "news"
        ],
        "numnber": 1,
        "series": "Simplification and the Web",
        "title": "RSS and my web experience"
      },
      "url": "posts/2023/12/07/rss-and-my-web-experience.json"
    },
    {
      "content": "\n# Find Bluesky RSS Feeds\n\nWith the update to [1.60](https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i) of Bluesky we can now follow people on Bluesky via RSS feeds. This makes things much more convienient for me. \nThe RSS feed is visible via the HTML markup on a person's profile page (which are now public). E.g. My Bluesky profile page is\nat <https://bsky.app/profile/rsdoiel.bsky.social> and if you look at that pages HTML markup you'll see a link element in the head\n\n```html\n <link rel=\"alternate\" type=\"application/rss+xml\" href=\"/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss\">\n```\n\nThat's the RSS feed. So now if you want to follow you can expand the URL to \n\n```\nhttps://bsky.app/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss\n```\n\nAnd use if via your feed reader. This is a sweat feature. It allows me to move my reading from visiting the website\nto getting updates via my feed reader.\n\n\n",
      "data": {
        "byline": "R. S. Doiel, 2023-12-23",
        "keywords": [
          "bluesky",
          "rss"
        ],
        "title": "Finding Bluesky RSS feeds"
      },
      "url": "posts/2023/12/23/finding-blue-sky-rss-feeds.json"
    },
    {
      "content": "\n\n# NodeJS, NPM, Electron\n\nBy R. S. Doiel 2017-10-20\n\nElectron is an app platform leveraging web technologies. Conceptually it is a\nmashup of NodeJS and Chrome browser. [Electron](https://electron.atom.io/) site\nhas a nice starter app. It displays a window with Electron version info and\n'hello world'.\n\nBefore you can get going with _Electron_ you need to have a\nworking _NodeJS_ and _NPM_. I usually compile from source and this\nwas my old recipe (adjusted for v8.7.0).\n\n```shell\n    cd\n    git clone https://github.com/nodejs/node.git\n    cd node\n    git checkout v8.7.0\n    ./configure --prefix=$HOME\n    make && make install\n```\n\nTo install an _Electron Quick Start_ I added the additional steps.\n\n```shell\n    cd\n    git clone https://github.com/electron/electron-quick-start\n    cd electron-quick-start\n    npm install\n    npm start\n```\n\nNotice _Electron_ depends on a working _node_ and _npm_.  When I\ntried this recipe it failed on `npm install` with errors regarding\ninternal missing node modules.\n\nAfter some fiddling I confirmed my node/npm install failed because\nI had install the new version of over a partially installed previous\nversion. This causes the node_modules to be populated with various\nconflicting versions of internal modules.\n\nSorting that out allowed me to test the current version of\n*electron-quick-start* cloned on 2017-10-20 under _NodeJS_ v8.7.0.\n\n## Avoiding Setup Issues in the future\n\nThe *Makefile* for _NodeJS_ includes an 'uninstall' option. Revising\nmy _NodeJS_ install recipe above I now do the following to setup a machine\nto work with _NodeJS_ or _Electron_.\n\n```shell\n    git clone git@github.com:nodejs/node.git\n    cd node\n    ./configure --prefix=$HOME\n    make uninstall\n    make clean\n    make -j 5\n    make install\n```\n\nIf I am on a device with a multi-core CPU (most of the time) you can speed\nup the make process using a `-j CPU_CORE_COUNT_PLUS_ONE` option (e.g. `-j 5`\nfor my 4 core x86 laptop).\n\nOnce _node_ and _npm_ were working normally the instructions in the\n*electron-quick-start* worked flawlessly on my x86.\n\nI have tested the node install recipe change on my Pine64 Pinebook, on \nseveral Raspberry Pi 3s as well as my x86 Ubuntu Linux laptop.\n\nI have not gotten Electron up on my Pine64 Pinebook or Raspberry Pi's yet. \n`npm install` outputs errors suggesting that it is expecting an x86 architecture.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2017, R. S. Doiel",
        "date": "2017-10-20",
        "keywords": [
          "Javascript",
          "NodeJS",
          "Electron"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "NodeJS, NPM, Electron"
      },
      "url": "posts/2017/10/20/node-npm-electron.json"
    }
  ]
}