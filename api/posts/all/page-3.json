{
  "page": 3,
  "total_pages": 6,
  "has_more": true,
  "next_page": "posts/all/page-4.json",
  "values": [
    {
      "content": "\nPostgreSQL dump and restore\n===========================\n\nThis is a quick note on easily dumping and restoring a specific database\nin Postgres 14.5.  This example has PostgreSQL running on localhost and\n[psql](https://www.postgresql.org/docs/current/app-psql.html) and\n[pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) are both available.\nOur database administrator username is \"admin\", the database to dump is called \"collections\". The SQL dump\nfile will be named \"collections-dump-2022-09-19.sql\".\n\n```shell\n\tpg_dump --username=admin --column-inserts \\\n\t    collections >collections-dump-2022-09-19.sql\n```\n\nFor the restore process I follow these steps\n\n1. Using `psql` create an empty database to restore into\n2. Using `psql` replay (import) the dump file in the new database to restoring the data\n\nThe database we want to restore our content into is called \"collections_snapshot\"\n\n```shell\n\tpsql -U dbadmin\n\t\\c postgres\n\tDROP DATABASE IF EXISTS collections_snapshot;\n\tCREATE DATABASE collections_snapshot;\n\t\\c collections_snapshots\n\t\\i ./collections-dump-2022-09-19.sql\n\t\\q\n```\n\nOr if you want to stay at the OS shell level\n\n```shell\n\tdropdb collections_snapshot\n\tcreatedb collections_snapshot\n\tpsql -U dbadmin --dbname=collections_snapshot -f ./collections-dump-2022-09-19.sql\n```\n\n\nNOTE: During this restore process `psql` will display some output. This is normal. The two\ntypes of lines output are shown below.\n\n```sql\n\tINSERT 0 1\n\tALTER TABLE\n```\n\nIf you want to stop the input on error you can use the `--set` option to set the error behavior\nto abort the reload if an error is encountered.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-09-19",
        "draft": true,
        "keywords": [
          "PostgreSQL"
        ],
        "pubDate": "2022-09-19",
        "title": "PostgreSQL dump and restore"
      },
      "url": "posts/2022/09/19/PostgreSQL-Dump-and-Restore.json"
    },
    {
      "content": "\nOrdering Front Matter\n=====================\n\nBy R. S. Doiel, 2022-08-30\n\nA colleague of mine ran into an interesting Pandoc behavior. He was combining a JSON metadata document and a converted word document and wanted the YAML front matter to have a specific order of fields (makes it easier for us humans to quickly scan it and see what the document was about).\n\nThe order he wanted in the front matter was\n\n- title\n- interviewer\n- interviewee\n- abstract\n\nThis was for a collection of oral histories. When my friend use Pandoc's `--metadata-json` to read the JSON metadata it rendered the YAML fine except the attributes were listed in alphabetical order.\n\nWe found a solution by getting Pandoc to treat the output not as Markdown plain text so that we could template the desired order of attributes.\n\nHere's the steps we used.\n\n1. create an empty file called \"empty.txt\" (this is just so you pandoc doesn't try to read standard input and processes\nyou metadata.json file with the template supplied)\n2. Create a template with the order you want (see below)\n3. Use pandoc to process your \".txt\" file and your JSON metadata file using the template (it makes it tread it as plain text even though we're going to treat it as markdown later)\n4. Append the content of the word file and run pandoc over your combined file as you would normally to generate your HTML\n\n\nThis is the contents of our [metadata.json](metadata.json) file.\n\n```json\n    {\n        \"title\": \"Interview with Mojo Sam\", \n        \"interviewee\": \"Mojo Sam\", \n        \"interviewer\": \"Tom Lopez\",\n        \"abstract\": \"Interview in three sessions over sevaral decases, 1970 - 20020. The interview was conducted next door to reality via a portal in Old Montreal\"\n    }\n```\n\n[frontmatter.tmpl](frontmatter.tmpl) is the template we used to render ordered front matter.\n\n```\n    ---\n    $if(title)$title: \"$title$\"$endif$\n    $if(interviewee)$interviewee: \"$interviewee$\"$endif$\n    $if(interviewer)$interviewer: \"$interviewer$\"$endif$\n    $if(abstract)$abstract: \"$abstract$\"$endif$\n    ---\n```\n\nHere's the commands we used to generate a \"doc.txt\" file with the \nfront matter in the desired order. Not \"empty.txt\" is just an empty\nfile so Pandoc will not read from standard input and just work with the\nJSON metadata and our template.\n\n```\ntouch empty.txt\npandoc --metadata-file=metadata.json --template=frontmatter.tmpl empty.txt\n```\n\nThe output of the pandoc command looks like this.\n\n```\n    ---\n    title: \"Interview with Mojo Sam\"\n    interviewee: \"Mojo Sam\"\n    interviewer: \"Tom Lopez\"\n    abstract: \"Interview in three sessions over sevaral decases, 1970 -\n    20020. The interview was conducted next door to reality via a portal in\n    Old Montreal\"\n    ---\n```\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "pandoc",
          "front matter"
        ],
        "pubDate": "2022-08-30",
        "title": "Ordering front matter"
      },
      "url": "posts/2022/08/30/Ordering-Frontmatter.json"
    },
    {
      "content": "\n# 10:30 AM, SQL: Postgres\n\nPost: Friday, August 26, 2022, 10:30 AM\n\nIf you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 2,
        "pubDate": "2022-08-26",
        "series": "SQL",
        "title": "10:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/26/rosette-notes-2022-08-26_101030.json"
    },
    {
      "content": "\nPostgres 14 on Ubuntu 22.04 LTS\n===============================\n\nby R. S. Doiel, 2022-08-26\n\nThis is just a quick set of notes for working with Postgres 14 on an Ubuntu 22.04 LTS machine.  The goal is to setup Postgres 14 and have it available for personal work under a user account (e.g. jane.doe). \n\nAssumptions\n\n- include `jane.doe` is in the sudo group\n- `jane.doe` is the one logged in and installing Postgres for machine wide use\n- `jane.doe` will want to work with her own database by default\n\nSteps\n\n1. Install Postgres\n2. Confirm installation\n3. Add `jane.doe` user providing access\n\nBelow is the commands I typed to run to complete the three steps.\n\n~~~shell\nsudo apt install postgresql postgresql-contrib\nsudo -u createuser --interactive\njane.doe\ny\n~~~\n\nWhat we've accomplished is installing Postgres, we've create a user in Postgres DB environment called \"jane.doe\" and given \"jane.doe\" superuser permissions, i.e. the permissions to manage Postgres databases.\n\nAt this point we have a `jane.doe` Postgres admin user. This means we can run the `psql` shell from the Jane Doe account to do any database manager tasks. To confirm I want to list the databases available\n\n~~~shell\npsql \nSELECT datname FROM pg_database;\n\\quit\n~~~\n\nNOTE: This post is a distilation of what I learned from reading Digital Ocean's [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel",
        "byline": "R. S. Doiel",
        "daft": true,
        "number": 4,
        "pubDate": "2022-08-26",
        "series": "SQL Reflections",
        "title": "Postgres 14 on Ubuntu 22.04 LTS"
      },
      "url": "posts/2022/08/26/postgres-14-on-ubuntu-22.04-LTS.json"
    },
    {
      "content": "\nA Quick intro to PL/pgSQL\n========================\n\nPL/pgSQL is a procedure language extended from SQL. It adds flow control and local state for procedures, functions and triggers. Procedures, functions and triggers are also the compilation unit. Visually PL/pgSQL looks similar to the MySQL or ORACLE counter parts. It reminds me of a mashup of ALGO and SQL. Like the unit of compilation, the unit of execution is also procedure, function or trigger. \n\nThe Postgres documentation defines and explains the [PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql.html) and how it works.  This document is just a quick orientation with specific examples to provide context.\n\nHello World\n-----------\n\nHere is a \"helloworld\" procedure definition.\n\n```sql\n    CREATE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n       RAISE NOTICE 'Hello WORLD!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nLet's take a look this line by line.\n\n1. CREATE PROCEDURE defines the procedure and the starting and ending delimiter for the procedure (e.g. `AS $$` the procedure's text ends when `$$` is encountered an second time.\n2. DECLARE is the block where you would declare the variables used in the procedure, we have none in this example\n3. The BEGIN starts the actual procedure instructions\n4. The `RAISE NOTICE` line is how you can display output to the console when the procedure is run\n5. The END completes the procedure definition\n6. the `$$ LANGUAGE plpgsql;` concludes the text defining the procedure and tells the database engine that procedure is written in PL/pgSQL.\n\nWe can run the procedure using the \"CALL\" query.\n\n```sql\n    CALL helloworld()\n```\n\nNOTE: If you want to change the procedure you can \"DROP\" it first otherwise you'll get an error that it already exists.\n\n```sql\n    DROP PROCEDURE helloworld;\n```\n\nImproving my workflow\n---------------------\n\nSQL procedures are generally stored in the RDBMs in database environment. You can think of them as records in the system's database. Procedures and functions are created and can be dropped. While they can be manually typed in the database's shell it is easier to maintain them in plain text files outside the RDBM environment.  \n\n1. Write the procedure in a text file.\n2. Load the text file (e.g. FILENAME) into Postgres \n   a. outside the Postgres shell use `psql -f FILENAME` \n   b. inside the Postgres shell used `\\i FILENAME`\n3. Call the procedure to test it\n\nTo turn these steps into a look I use a \"CREATE OR REPLACE\" statement and be able to reload the updated procedure easier see [43.12. Tips for Developing in PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql-development-tips.html).  Note in the revised example the \"-- \" lines are comments.\n\nOur revised [helloworld](helloworld.plpgsql).\n\n```sql\n    --\n    -- Create (or replace) the new \"helloworld\" procedure.\n    -- NOTE: this can be run with \"CALL\"\n    --\n    CREATE OR REPLACE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n        RAISE NOTICE 'Hello World!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\n\nHi There\n--------\n\n[hithere](hithere.plpgsql) is similar to our helloworld example except it is a function that takes a parameter of the person's name. The function returns a \"VARCHAR\", so this should work as part of a select statement.\n\n```sql\n    --\n    -- This is a \"Hi There\" function. The function takes\n    -- a single parameter and forms a greeting.\n    --\n    CREATE OR REPLACE FUNCTION hithere(name varchar) RETURNS varchar AS $$\n    DECLARE\n      greeting varchar;\n    BEGIN\n        IF name = '' THEN\n            greeting := 'Hi there!';\n        ELSE\n            greeting := 'Hello ' || name || '!';\n        END IF;\n        RETURN greeting;\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nGiving it a try.\n\n```shell\n    SELECT hithere('Mojo Sam');\n```\n\nFurther reading\n---------------\n\n- [Conditionals](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONDITIONALS)\n- [Loops](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONTROL-STRUCTURES-LOOPS)\n- [Calling a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-CALLING-PROCEDURE)\n- [Early return from a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-RETURNING-PROCEDURE)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-24",
        "keywords": [
          "postgres",
          "sql",
          "psql",
          "plsql",
          "plpgsql"
        ],
        "number": 3,
        "pubDate": "2022-08-24",
        "series": "SQL Reflections",
        "title": "A Quick into to PL/pgSQL"
      },
      "url": "posts/2022/08/24/plpgsql-quick-intro.json"
    },
    {
      "content": "\n# 12:00 PM, SQL: Postgres\n\nPost: Wednesday, August 24, 2022, 12:00 PM\n\nI miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%'\n`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 3,
        "pubDate": "2022-08-24",
        "series": "SQL",
        "title": "12:00 PM, SQL: Postgres"
      },
      "url": "posts/2022/08/24/rosette-notes-2022-08-24_121200.json"
    },
    {
      "content": "\n# 11:30 AM, SQL: Postgres\n\nPost: Monday, August 22, 2022, 11:30 AM\n\nThree things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 4,
        "pubDate": "2022-08-22",
        "series": "SQL",
        "title": "11:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/22/rosette-notes-2022-08-22_111130.json"
    },
    {
      "content": "\nRosette Notes\n=============\n\nBy R. S. Doiel, 2022-08-19\n\n> A dance around two relational databases, piecing together similarities as with the tiny mosaic tiles of a guitar's rosette\n\nWhat follows are my preliminary notes learning Postgres 12 and 14.\n\nPostgres & MySQL\n----------------\n\nThis is a short comparison of some administrative commands I commonly use. The first column describes the task followed by the SQL to execute for Postgres 14.5 and then MySQL 8. The presumption is you're using `psql` to access Postgres and `mysql` to  access MySQL. Values between `<` and `>` should be replaced with an appropriate value.\n\n| Task                    | Postgres 14.5                     | MySQL 8           |\n|-------------------------|------------------------------------|-------------------|\n| show all databases      | `SELECT datname FROM pg_database;` | `SHOW DATABASES;` |\n| select a database       | `\\c <dbname>`                      | `USE <dbname>`    |\n| show tables in database | `\\dt`                              | `SHOW TABLES;`    |\n| show columns in table   | `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '<table_name>';` | `SHOW COLUMNS IN <table_name>` |\n\nReflections\n-----------\n\nThe Postgres shell, `psql`, provides the functionality of showing a list of tables via a short cut while MySQL choose to add the `SHOW TABLES` query. For me `SHOW ...` feels like SQL where as `\\d` or `\\dt` takes me out of SQL space. On the other hand given Postgres metadata structure the shortcut is appreciated and I often query for table names as I forget them. `\\dt` quickly becomes second nature and is shorter to type than `SHOW TABLES`. \n\nConnecting to a database with `\\c` in `psql` is like calling an \"open\" in programming language. The \"connection\" in `psql` is open until explicitly closed or the shell is terminated.  Like `USE ...` in the MySQL shell it make working with multiple database easy.  The difference are apparent when you execute a `DROP DATABASE ...` command. In `psql` you need to `CLOSE` the database first or the `DROP` will fail.  The MySQL shell will happily let you drop the current database you are currently using.\n\nThe challenge I've experienced learning `psql` after knowing MySQL is my lack of familiarity with the metadata Postgres maintains about databases and structures.  On the other hand everything I've learned about standards base SQL applies to managing Postgres once remember the database/table I need to work with.  A steeper learning curve from MySQL's `SHOW` but it also means writing external programs for managing Postgres databases and tables is far easier because everything is visible because that is how you manage Postgres. MySQL's `SHOW` is very convenient but at the cost of hiding some of its internal structures.\n\nBoth MySQL and Postgres support writing programs in SQL. They also support stored procedures, views and triggers. They've converged in the degree in which they have both implemented SQL language standards.  The differences are mostly in approach to managing databases.  There are some differences, necessitated by implementation choices, in the `CREATE DATABASE`, `CREATE TABLE` or `ALTER` statements but you can often use the basic form described in ANSI SQL and get the results you need. When doing performance tuning the dialect differences are more important.\n\nDump & Restore\n--------------\n\nBoth Postgres and MySQL provide command line programs for dumping a database. MySQL provides a single program where as Postgres splits it in two. Check the man pages (or website docs) for details in their options. Both sets of programs are highly configurable allowing you to dump just schema, just data or both with different expectations.\n\n| Postgres 14.5      | MySQL 8                         |\n|--------------------|---------------------------------|\n| `pg_dumpall`       | `mysqldump --all-databases`     |\n| `pg_dump <dbname>` | `mysqldump --database <dbname>` |\n\nThe `pg_dumpall` tool is designed to restore an entire database instance. It includes account and ownership information. `pg_dump` just focuses on the database itself. If you are taking a snapshot production data to use in a test `pg_dump` output is easier to work with. It captures the specific database with out entangling things like the `template1` database or database user accounts and ownership.\n\nYou can restore a database dump in both Postgres and MySQL. The tooling is a little different.\n\n| Postgres 14.5                   | MySQL 8                                      |\n|---------------------------------|----------------------------------------------|\n| `dropdb <dbname>`               | `mysql -execute 'DROP DATABASE <dbname>;'`   |\n| `createdb <dbname>`             | `mysql -execute 'CREATE DATABASE <dbname>;'` |\n| `psql -f <dump_filename>`       |`mysql <dbname> < <dump_filename>`            |\n\nNOTE: These instructions work for a database dumped with `pg_dump` for the Postgres example. In principle it is the same way you can restore from `pg_dumpall` but if you Postgres instance already exists then you're going to run into various problems, e.g. errors about `template1` db.\n\nLessons learned along the way\n-----------------------------\n\n2022-08-22\n\n8:00 - 11:30; SQL; Postgres; Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n\n2022-08-24\n\n11:00 - 12:00; SQL; Postgres; I miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%';`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n2022-08-26\n\n9:30 - 10:30; SQL; Postgres; If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n2022-09-19\n\n10:30 - 12:30; SQL; Postgres; Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-19",
        "keywords": [
          "postgres",
          "mysql",
          "sql",
          "psql"
        ],
        "number": 2,
        "pubDate": "2022-08-19",
        "series": "SQL Reflections",
        "title": "Rosette Notes: Postgres and MySQL",
        "updated": "2022-09-19"
      },
      "url": "posts/2022/08/19/rosette-notes.json"
    },
    {
      "content": "\nPttk and STN\n============\n\nBy R. S. Doiel, started 2022-08-15\n(updated: 2022-09-26, pdtk was renamed pttk)\n\nThis log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the \"blogit\" action that will translates the simple timesheet notation (aka STN) to a short blog post.  My \"short post\" interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.\n\n2022-08-15\n\n16:45 - 17:45; Golang; ptdk, stngo; Thinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n2022-08-14\n\n14:00 - 17:00; Golang; pdtk, stngo; Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n2022-08-12\n\n16:00 - 16:30; Golang; stngo; A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n2022-09-26\n\n6:30 - 7:30; Golang; pttk; renamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2022-08-15",
        "pubDate": "2022-08-15",
        "title": "PTTK and STN",
        "updated": "2022-09-26"
      },
      "url": "posts/2022/08/15/golang-development.json"
    },
    {
      "content": "\n# 5:45 PM, Golang: ptdk,  stngo\n\nPost: Monday, August 15, 2022, 5:45 PM\n\nThinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n",
      "data": {
        "keywords": [
          "ptdk",
          "stngo"
        ],
        "no": 4,
        "pubDate": "2022-08-15",
        "series": "Golang",
        "title": "5:45 PM, Golang: ptdk,  stngo"
      },
      "url": "posts/2022/08/15/golang-development-2022-08-15_170545.json"
    },
    {
      "content": "\n# 5:00 PM, Golang: pdtk,  stngo\n\nPost: Sunday, August 14, 2022, 5:00 PM\n\nToday I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n",
      "data": {
        "keywords": [
          "pdtk",
          "stngo"
        ],
        "no": 3,
        "pubDate": "2022-08-14",
        "series": "Golang",
        "title": "5:00 PM, Golang: pdtk,  stngo"
      },
      "url": "posts/2022/08/14/golang-development-2022-08-14_170500.json"
    },
    {
      "content": "\n# 4:30 PM, Golang: stngo\n\nPost: Friday, August 12, 2022, 4:30 PM\n\nA work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n",
      "data": {
        "keywords": [
          "stngo"
        ],
        "no": 2,
        "pubDate": "2022-08-12",
        "series": "Golang",
        "title": "4:30 PM, Golang: stngo"
      },
      "url": "posts/2022/08/12/golang-development-2022-08-12_160430.json"
    },
    {
      "content": "\n# Book review, \"Man and the Computer\"\n\nBy R. S. Doiel, 2025-02-10\n\nOpen Library has a wonderful collection of classic Computer related texts. This is a review of one of them.  \"Man and the Computer\" was written by [John G. Kemeny](https://en.wikipedia.org/wiki/John_G._Kemeny) and published in 1972. The book covers the evolution of the Dartmouth Time Sharing System (DTSS) and BASIC[^1].\n\n[^1]: Prof. Kemeny was co-developer of BASIC along with [Thomas Kurtz](https://en.wikipedia.org/wiki/Thomas_E._Kurtz) The author, Kemeny, as a math professor at Dartmouth and eventual became president of the university.\n\nIt is an interesting weekend read. You can read the text at [Open Library](https://openlibrary.org/books/OL5282840M/Man_and_the_computer). The book is short (160 pages or so). It is written for casual reading like a talk given to a small group. \n\nThe first part goes into the innovations that resulted for the undergraduate students who used, developed and extended the systems. The later part of the book covers what the implications of the system had been by the 1970s and what it suggestions for the future through 1990. Given that the book was written before DARPAnet, before Internet and before “Personal Computers” a surprising amount of the  Kemeny's predictions were on target and remaining relevant today. He anticipated Open Access and Cloud Computing and the benefits the were possible. My take away is it is a charming reflection of where computing was at for Dartmouth and its alumni in the start of the 1970s.\n\nStepping back to a bigger picture of computing in the 1960s and 1970s the book does have blind spots.  This is not surprising because the communications were so much more restrictive before the Internet in terms of technical exchange in computing. It is no wonder that there is but one line that mentions the innovations in time-sharing that occurred at RAND with the [Johniac Open Shop System](https://en.wikipedia.org/wiki/JOSS) (aka. JOSS-1, JOSS-2). They system have some parallels with the early DTSS/BASIC incarnations and may have preceded by as much as a few years[^2]. JOSS lead to other systems like CAL, PIL/I, FOCAL and MUMPs. The later, like JOSS, bare similarities to what would be developed at Dartmouth in the form of BASIC.\n\n[^2]: JOSS dates from approximately 1963 and DTSS was released in 1964. Kemeny was published in research memorandum in 1953 and probably talked about his ideas. https://en.wikipedia.org/wiki/JOSS\n\nIt is quite reasonable for this text to the have these blind spots. First it was not intended to be a history of computing (see the Preface of the book) but rather a high level look at what the promise was for people who could work interactively with a computer. I feel it gives insights into the early era where computing access was rapidly increasing and the sense of promise that carried. Well worth the reading time if you're a hobbyist or arm chair computer history buff.\n",
      "data": {
        "abstract": "A book review of a vintage computer publication, \"Man and the Computer\" by\nJohn G. Kemeny, published 1972, ISBN: 0684130092\nRead at the Open Library, <https://openlibrary.org/books/OL5282840M/Man_and_the_computer> \n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-02-10",
        "createDate": "2025-02-10",
        "keywords": [
          "book review",
          "computing",
          "basic",
          "time-sharing",
          "John G. Kemeny"
        ],
        "series": "books",
        "title": "Book review, \"Man and the Computer\""
      },
      "url": "posts/2025/02/10/Man_and_the_Computer.json"
    },
    {
      "content": "\n# Setting up my Raspberry Pi 500, a Portable Workstation\n\nBy R. S. Doiel, 2025-02-14\n\nI'm  writing this on a newly setup Raspberry Pi 500. So far I'm impressed. I was sceptical about moving from my Pi 400 but when it died I was forced to upgrade. It has been worth it.\n\nI've now had a chance to install the various pieces of software I regularly use. Even do some web browsing with it. Image some SD cards for a RISC OS project I'm working on. It feels much quicker than my Pi 400. I think the speed increase is in part the faster CPU but I suspect the program I am running is taking advantage of the 8 Gig RAM instead of the old 4. As I've testing this machine out I noticed I had stopped asking myself the question about launching more than one large program at a time (e.g. my browser is running and being used as I type this in VS Code). When I tried VS Code on my Pi 4B+ and 400 it just felt too sluggish. I'm typing this review in VS Code now on the Pi 500 is keeping up OK.\n\nWhen I purchased the Pi-500 and I also decided to go with the new Raspberry Pi Monitor. Together I have a complete portable workstation. While my previous powered WaveShare monitor was higher resolution the Pi Monitor it  also used allot more power.  With the combination of Pi Monitor and Pi-500 I am now contemplating exploring power consumption so I can find a one battery to power the monitor and Pi-500.\n\nHere's the spec and pricing for this new machine. Some parts I didn't purchase because I already owned them (e.g. the 27W Pi power supply) but I have included the list prices for those who might be interested.\n\nItem                                List Price    Notes                   \n----------------------------------  ------------  -----------------------\nRaspberry Pi 500 unit               $90.00        included  32G SD card  \nRaspberry Pi 27W Power supply       $13.65        used existing one      \nRaspberry Pi Mouse                  $9.25         used existing one      \nMicro HDMI to standard HDMI cable   $5.75         used existing one      \nRaspberry Pi Monitor                $100.00                              \nRaspberry Pi 15W Power supply       $8.00         for monitor            \nRaspberry Pi 128G SD Card           $16.95        upgraded storage       \n\n\n## Software Setup\n\nI created an \"image\" on my 128G SD Card using the Raspberry Pi Imager on another computer.  The imager lets you setup initial user account, WiFi configuration and whether you want SSH services running the first boot. I like using the vanilla 64 bit Raspberry Pi OS distribution. I picked the one for the Raspberry Pi 5 series.\n\nWhen I booted the Pi-500 connected to the Pi Monitor it seemed to cause the screen to cycle through and Pi splash page a few times. Once it completed its first boot it hasn't done that again. I am assuming that was some negotiation between the monitor and the Pi. After a quick click around test I rebooted the machine by doing a full shutdown and power off and the power back up.\n\n### Development and Writing Software\n\nI am planning to use the Pi-500 as a light weight development machine and as a machine for preparing updates for my blog. I installed the additional software below.\n\n- Pandoc 3 via installed via deb package from Pandoc GitHub releases\n- Rust installed via Rustup\n- PageFind installed via Cargo\n- Flatlake installed via Cargo\n- htmlq installed via Cargo\n- ncal installed via Cargo\n- Deno installed via website using CURL\n- Go installed via website's tar ball\n- jq installed via apt\n- SQLite3, libsqlite3-dev installed via apt\n- Hunspell and US English dictionary installed via apt\n\nThe \"build-essential\" package was already installed. I noticed that Git and GNU Make were immediately available with my first boot.\n\nI this setup as a portable  workstation. It feels quick and snappy but is small enough to toss in computer bag. I did test compile a few things. It is possible to peg the CPU but then again it's a little machine after all. I didn't get any warning lights or notifications like I used to on the Pi-400.\n\nWith a connection to my home WiFi network (not a fast connection) it took me about an hour or so to download and install all my extras. This was quicker than the last time I setup a Pi. Some of the time saved was the better hardware and net work performance but much of the time saved was due to the fact that I did not have to compile software from scratch. That was a change from the last time I setup a Pi up.  I guess  Pi and aarch64 processors are common enough that projects are now including it in their regular builds.\n\nIf  I pickup the right capacity battery I suspect I will have a lovely deconstructed Laptop to use as a portable workstation.\n\n",
      "data": {
        "abstract": "Quick notes on configuring a Raspberry Pi 500 as a portable workstation along with a price list.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-02-14",
        "dateCreated": "2025-02-14",
        "keywords": [
          "Raspberry Pi",
          "Workstation",
          "Review"
        ],
        "title": "Setting up my Raspberry Pi 500, a Portable Workstation"
      },
      "url": "posts/2025/02/14/Review_Pi-500_as_portable_workstation.json"
    },
    {
      "content": "\n# Working with Structured Data in Deno and TypeScript\n\nOne of the features in Go that I miss in TypeScript is Go's [DSL](https://en.wikipedia.org/wiki/Domain-specific_language \"Domain Specific Language\") for expressing data representations.  Adding JSON, YAML and XML support in Go is simple. Annotating a struct with a string expression. There is no equivalent feature in TypeScript. How do easily support multiple representations in TypeScript?\n\nLet's start with JSON. TypeScript has `JSON.stringify()` and `JSON.parse()`. So getting to JSON representation is trivial, just call the stringify method. Going from text to populated object is done with `JSON.parse`. But there is a catch.\n\nLet's take a simple object I'm defining called \"ObjectN\". The object has a single attribute \"n\". \"n\" holds a number. The initial values is set to zero. What happens when I instantiate my ObjectN then assign it the result from `JSON.parse()`.\n\n~~~TypeScript\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n}\nlet src = `{\"n\": 1}`;\nlet o: ObjectN = new ObjectN();\no = JSON.parse(src);\n// NOTE: This will fail, addThree method isn't available.\nconsole.log(o.addThree());\n~~~\n\nHuston, we have a problem. No \"addThree\" method. That is because JSON doesn't include method representation. What we really want to do is inspect the object returned by `JSON.parse()` and set the values in our ObjectN accordingly. Let's add a method called `fromObject()`.\n(type the following into the Deno REPL).\n\n~~~TypeScript\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n    fromObject(o: {[key: string]: any}): boolean {\n        if (o.n === undefined) {\n            return false;\n        }\n        // Validate that o.n is a number before assigning it.\n        const n = (new Number(o.n)).valueOf();\n        if (isNaN(n)) {\n            return false;\n        }\n        this.n = n;\n        return true;\n    }\n}\nlet src = `{\"n\": 1}`;\nlet o: ObjectN = new ObjectN();\nconsole.log(o.addThree());\no.fromObject(JSON.parse(src));\nconsole.log(o.addThree());\n~~~\n\nNow when we run this code we should see a \"3\" and then a \"4\" output. Wait, `o.fromObject(JSON.parse(src));` looks weird. Why not put `JSON.parse()` inside \"fromObject\"? Why not renamed it \"parse\"?\n\nI want to support many types of data conversion like YAML or XML. I can use my \"fromObject\" method with the result of produced from `JSON.parse()`, `yaml.parse()` and `xml.parse()`. One function works with the result of all three. Try adding this.\n\n~~~TypeScript\nimport * as yaml from 'jsr:@std/yaml';\nimport * as xml from \"jsr:@libs/xml\";\nsrc = `n: 2`;\no.fromObject(yaml.parse(src));\nconsole.log(o.addThree());\nsrc = `<n>3</n>`;\no.fromObject(xml.parse(src));\nconsole.log(o.addThree());\n~~~\n\nThat works!\n\nStill it would be nice to have a \"parse\" method too. How do I do that without winding up with a \"parseJSON()\", \"parseYAML()\" and \"parseXML()\"? What I really want is a \"parseWith\" method which accepts the text and a parse function. TypeScript expects type information about the function being passed. I solve that problem by including a \"ObjectParseType\" definition that works across the three parsing objects -- JSON, yaml and xml.\n\n~~~TypeScript\nimport * as yaml from 'jsr:@std/yaml';\nimport * as xml from \"jsr:@libs/xml\";\n\n// This defines my expectations of the parse function provide by JSON, yaml and xml.\ntype ObjectParseType = (arg1: string, arg2?: any) => {[key: string]: any} | unknown;\n\nclass ObjectN {\n    n: number = 0;\n    addThree(): number {\n        return this.n + 3;\n    }\n    fromObject(o: {[key: string]: any}) : boolean {\n        if (o.n === undefined) {\n            return false;\n        }\n        // Validate that o.n is a number before assigning it.\n        const n = (new Number(o.n)).valueOf();\n        if (isNaN(n)) {\n            return false;\n        }\n        this.n = n;\n        return true;\n    }\n    parseWith(s: string, fn: ObjectParseType): boolean {\n        return this.fromObject(fn(s) as unknown as {[key: string]: any});\n    }\n}\n\nlet o: ObjectN = new ObjectN();\nconsole.log(`Initial o.addThree() -> ${o.addThree()}`);\nconsole.log(`o.toString() -> ${o.toString()}`);\n\nlet src = `{\"n\": 1}`;\no.parseWith(src, JSON.parse);\nconsole.log(`parse with JSON, o.addThree() -> ${o.addThree()}`);\nconsole.log(`JSON.stringify(o) -> ${JSON.stringify(o)}`);\n\nsrc = `n: 2`;\no.parseWith(src, yaml.parse);\nconsole.log(`parse with yaml, o.addThree() -> ${o.addThree()}`);\nconsole.log(`yaml.stringify(o) -> ${yaml.stringify(o)}`);\n\nsrc = `<?xml version=\"1.0\"?>\n<n>3</n>`;\no.parseWith(src, xml.parse);\nconsole.log(`parse with xml, o.addThree() -> ${o.addThree()}`);\nconsole.log(`xml.stringify(o) -> ${xml.stringify(o)}`);\n~~~\n\nAs long as the parse method returns an object I can now update my ObjectN instance\nfrom the attributes of the object expressed as JSON, YAML, or XML strings. I like this approach because I can add validation and normalization in my \"fromObject\" method and use for any parse method that confirms to how JSON, YAML or XML parse works. The coding cost is the \"ObjectParseType\" type definition and the \"parseWith\" method boiler plate and defining a class specific \"fromObject\". Supporting new representations does require changes to my class definition at all.\n",
      "data": {
        "abstract": "A short discourse on working with structured data in TypeScript and easily\nconverting from JSON, YAML and XML representations.\n",
        "createDate": "2025-02-03",
        "keywords": [
          "Deno",
          "TypeScript",
          "Structured Data"
        ],
        "title": "Working with Structured Data in Deno and TypeScript"
      },
      "url": "posts/2025/02/03/working_with_structured_data.json"
    },
    {
      "content": "\n# Deno 2.1.7, Points of Friction\n\nBy R. S. Doiel, 2025-01-26\n\nI have run into a few points of friction in my journey with Deno coming from Go. I miss Go's standard \"io\" and \"bufio\" packages. With the Go code I'm porting TypeScript I'd often need to handle standard input or input from a named file interchangeably. Seems like this should be easy in Deno's TypeScript but there are a few bumps in the road.\n\nHere's the Go idiom I commonly use.\n\n~~~go\nvar err error\ninput := io.Stdin\nif inFilename != \"\" {\n    input, err := os.Open(inFilename)\n    if err !== nil {\n        // ... handle error\n    }\n    defer input.Close();\n}\n// Now I can just pass \"in\" around for processing.\n~~~\n\nConceptually this feels simple though verbose. I can pass around the \"input\" for processing in a way that is agnostic as to file or standard input. This type of Go code works equally on POSIX and Windows.\n\nDeno provide access to [standard input](https://docs.deno.com/api/deno/~/Deno.stdin). Deno supports streamable files. From the docs here's an simple example.\n\n~~~TypeScript\n// If the text \"hello world\" is piped into the script:\nconst buf = new Uint8Array(100);\nconst numberOfBytesRead = await Deno.stdin.read(buf); // 11 bytes\nconst text = new TextDecoder().decode(buf);  // \"hello world\"\n~~~\n\nSetting aside the buffer management code it seems simple and straight forward. It is easy to understand and you could wrap it in a function easily to hide the buffer management part. Yet it doesn't provide the same flexibility as the more verbose Go version. Surely there is an an idiomatic why of doing this in TypeScript already? \n\n## Stability Challenge\n\nDeno currently is a rapidly evolving platform. My first impulse was to reach for packages like `jsr:@std/fs` or `jsr:@sys/fs`. When I search for examples they mostly seem to reference specific versions of \"std/fs\" that are not available via jsr. So what's the \"right\" way to approach this?\n\n## Repl to the rescue.\n\nPoking around in the Deno repl I tried assigning `Deno.stdin` to a local variable. Playing with command line completion I realized it has most of the the methods you would get if you used `Deno.open()` to open a named file.\n\nHere's a little test I ran in the repl after creating a \"hellworld.txt\" text file.\n\n~~~deno\ndeno\nconst stdin = Deno.stdin;\nlet input = Deno.open('helloworld.txt')\nstdin.isTerminal();\ninput.isTerminal();\nstdin.valueOf();\ninput.valueOf();\nDeno.exit(0);\n~~~\n\nThe `valueOf()` reveals their type affiliation. It listed them as `Stdin {}` and `FsFile {}` respectively. I used TypeScript's typing system to let us implement \"mycat.ts\". You can assign multiple types to a variable with a `|` (pipe) symbol in TypeScript. \n\nUsed that result to write a simple cat file implementation.\n\n~~~TypeScript\nasync function catFile() {\n    let input : Stdin | FsFile = Deno.stdin;\n\n    if (Deno.args.length > 0) {\n        input = await Deno.open(Deno.args[0]);\n    }\n\n    const decoder = new TextDecoder();\n\n    // NOTE: the .readable function is available on both types of objects.\n    for await (const chunk of input.readable) {\n        console.log(decoder.decode(chunk));\n    }\n}\n\nif (import.meta.main) catFile();\n~~~\n\nYou can \"run\" this deno to see it in action. Try running it on your \"helloworld.txt\" file.\n\n~~~shell\ndeno run --allow-read mycat.ts helloworld.txt\n~~~\n\nYou can also read from standard input too. Try the command below type in some text then press Ctrl-D or Ctrl-Z if you're on Windows.\n\n~~~shell\ndeno run --allow-read mycat.ts\n~~~\n\nLooks like we have a nice solution. Now I can compile \"mycat.ts\".\n\n## trouble in paradise\n\nWhile you can \"run\" the script you can't compile it. It doesn't pass \"check\". This is the error I get with Deno 2.1.7.\n\n~~~shell\ndeno check mycat.ts\nCheck file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts\nerror: TS2304 [ERROR]: Cannot find name 'Stdin'.\n    let input : Stdin | FsFile = Deno.stdin;\n                ~~~~~\n    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:17\n\nTS2552 [ERROR]: Cannot find name 'FsFile'. Did you mean 'File'?\n    let input : Stdin | FsFile = Deno.stdin;\n                        ~~~~~~\n    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:25\n\n    'File' is declared here.\n    declare var File: {\n                ~~~~\n        at asset:///lib.deno.web.d.ts:622:13\n\nFound 2 errors.\n~~~\n\nIt seems like what works in the repl should also compile but that's isn't the case. I have an open question on Deno's discord help channel and am curious to find the \"correct\" way to handle this problem.\n\n## Update 2025-01-26, 5:00PM\n\nI heard back on Deno Discord channel for help.  With the help of [crowlKat](https://github.com/crowlKats) sorted the problem out.\n\nThe compile and runnable version of [mycat.ts](mycat.ts) looks like this.\n\n~~~typescript\nasync function main() {\n    let input : Deno.FsFile | any = Deno.stdin;\n\n    if (Deno.args.length > 0) {\n        input = await Deno.open(Deno.args[0]);\n    }\n\n    const decoder = new TextDecoder();\n\n    // NOTE: the .readable function is available on both types of objects.\n    for await (const chunk of input.readable) {\n        console.log(decoder.decode(chunk));\n    }\n}\n\nif (import.meta.main) main();\n~~~\n\nThe \"any\" type feels a little ugly but since I am assinging the default value is `Deno.stdin` it covers that case where the `Deno.FsFile` covers the case of a name file.  Where does this leave me? I have a nice clean idiom that does what I want for interacting with standard input or a file stream.  Not necessarily the fast thing on the planet but it works.\n\n\n",
      "data": {
        "abstract": "A short discussion of working with file input in TypeScript+Deno coming from the\nperspective of Go's idiomatic use of io buffers.\n",
        "byline": "R. S. Doiel",
        "createdDate": "2025-01-26",
        "keywords": [
          "deno",
          "text",
          "input"
        ],
        "title": "Deno 2.1.7, Points of Friction"
      },
      "url": "posts/2025/01/26/points_of_friction.json"
    },
    {
      "content": "\n# Deno 2.1.7, Project Idioms\n\nI've noticed a collection of file and code idioms I've been used in my recent Deno+TypeScript projects at work. I've captured them here as a future reference.\n\n## Project files\n\nMy project generally have the following files, these are derived from the [CodeMeta](https://codemeta.github.io) file using [CMTools](https://caltechlibrary.github.io/CMTools).\n\ncodemeta.json\n: Primary source of project metadata, used to generate various files\n\nCITATION.cff\n: used by GitHub for citations. version, dateModified, datePublished and releaseNotes\n\n\nabout.md\n: A project about page. This is generated based on the codemeta.json file.\n\nREADME.md, README\n: A general read me describing the project and pointing to INSTALL.md, user_manual.md as appropriate\n\nINSTALL.md\n: These are boiler plate description of how to install and compile the software\n\nuser_manual.md\n: This is an index document, a table of contents. It points to other document including Markdown versions of the man page(s).\n\nFor TypeScript projects I also include a following\n\nversion.ts\n: This hold project version information used in the TypeScript co-debase. It is generated from the codemeta.json via CMTools.\n\nhelptext.ts\n: This is where I place a function, `fmtHelp()`, for rendering response to the \"help\" command line option.\n\nI'm currently ambivalent about \"main.ts\" file which is created by `deno init`. My ambivalent is that most of my projects wind up producing more than one program from a shared code base. a single \"main.ts\" doesn't really fit that situation.\n\nThe command line tool will have a TypeScript with it's name. Inside this file I'll have a main function and use the Deno idiom `if (import.meta.main) main();` to invoke it. I don't generally put the command line  TypeScript in my \"mod.ts\" file since it's not going to work in a browser or be useful outside my specific project.\n\nmod.ts\n: I usually re-export modules here that maybe useful outside my project (or in the web browser).\n\ndeps.ts\n: I use this if there are allot of files consistently being imported across the project, otherwise I skip it.\n\n## What I put in Main\n\nI use the main function to define command line options, handle parameters such as data input, output and errors. It usually invokes a primary function modeled in the rest of the project code.\n\nHere is an example Main for a simple \"cat\" like program.\n\n~~~TypeScript\nimport { parseArgs } from \"jsr:@std/cli\";\nimport { licenseText, releaseDate, releaseHash, version } from \"./version.ts\";\nimport { fmtHelp, helpText } from \"./helptext.ts\";\n\nconst appName = \"mycat\";\n\nasync function main() {\n  const app = parseArgs(Deno.args, {\n    alias: {\n      help: \"h\",\n      license: \"l\",\n      version: \"v\",\n    },\n    default: {\n      help: false,\n      version: false,\n      license: false,\n    },\n  });\n  const args = app._;\n\n  if (app.help) {\n    console.log(fmtHelp(helpText, appName, version, releaseDate, releaseHash));\n    Deno.exit(0);\n  }\n  if (app.license) {\n    console.log(licenseText);\n    Deno.exit(0);\n  }\n  if (app.version) {\n    console.log(`${appName} ${version} ${releaseHash}`);\n    Deno.exit(0);\n  }\n\n  let input: Deno.FsFile | any = Deno.stdin;\n\n  // handle case of many file names\n  if (args.length > 1) {\n    for (const arg of args) {\n      input = await Deno.open(`${arg}`);\n      for await (const chunk of input.readable) {\n        const decoder = new TextDecoder();\n        console.log(decoder.decode(chunk));\n      }\n    }\n    Deno.exit(0);\n  }\n  if (args.length > 0) {\n    input = await Deno.open(Deno.args[0]);\n  }\n  for await (const chunk of input.readable) {\n    const decoder = new TextDecoder();\n    console.log(decoder.decode(chunk));\n  }\n}\n\nif (import.meta.main) main();\n~~~\n\n## helptext.ts\n\nThe following is an example of the [helptext.ts](helptext.ts) file for the demo [mycat.ts](mycat.ts).\n\n```TypeScript\nexport function fmtHelp(\n  txt: string,\n  appName: string,\n  version: string,\n  releaseDate: string,\n  releaseHash: string,\n): string {\n  return txt.replaceAll(\"{app_name}\", appName).replaceAll(\"{version}\", version)\n    .replaceAll(\"{release_date}\", releaseDate).replaceAll(\n      \"{release_hash}\",\n      releaseHash,\n    );\n}\n\nexport const helpText =\n  `%{app_name}(1) user manual | version {version} {release_hash}\n% R. S. Doiel\n% {release_date}\n\n# NAME\n\n{app_name}\n\n# SYNOPSIS\n\n{app_name} FILE [FILE ...] [OPTIONS]\n\n# DESCRIPTION\n\n{app_name} implements a \"cat\" like program.\n\n# OPTIONS\n\nOptions come as the last parameter(s) on the command line.\n\n-h, --help\n: display help\n\n-v, --version\n: display version\n\n-l, --license\n: display license\n\n\n# EXAMPLES\n~~~shell\n{app_name} README.md\n{app_name} README.md INSTALL.md\n~~~\n`;\n```\n\n## Generating version.ts\n\nThe [version.ts](version.ts) is generated form two files, [codemeta.json] and [LICENSE] using the CMTools, `cmt` command.\n\n~~~\ncmt codemeta.json veresion.ts\n~~~\n",
      "data": {
        "abstract": "Notes on some of the file and code idioms I'm using with Deno+TypeScript projects.",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "createDate": "2025-01-29",
        "keywords": [
          "Deno",
          "TypeScript",
          "Projects"
        ],
        "title": "Deno 2.1.7, Project Idioms"
      },
      "url": "posts/2025/01/29/project_idioms.json"
    },
    {
      "content": "\n\n# Go based Python modules\n\nBy R. S. Doiel, 2018-02-24\n\nThe problem: I have written a number of Go packages at work.\nMy colleagues know Python and I'd like them to be able to use the\npackages without resorting to system calls from Python to the\ncommand line implementations. The solution is create a C-Shared\nlibrary from my Go packages, using Go's _C_ package and combine it\nwith Python's _ctypes_ package.  What follows is a series of \nsimple recipes I used to understand the details of how that worked.\n\n\n## Example 1, libtwice.go and twice.py\n\nMany of the the examples I've come across on the web start by \nshowing how to run a simple math operation on the Go side with\nnumeric values traveling round trip via the C shared library layer. \nIt is a good place to start as you only need to consider type \nconversion between both Python's runtime and Go's runtime.  It \nprovides a simple illustration of how the Go *C* package, Python's\n*ctypes* module and the toolchain work together.\n\nIn this example we have a function in Go called \"twice\" it takes\na single integer, doubles it and returns the new value.  On\nthe Go side we create a _libtwice.go_ file with an empty `main()` \nfunction.  Notice that we also import the *C* package and use \na comment decoration to indicate the function we are exporting\n(see https://github.com/golang/go/wiki/cgo and \nhttps://golang.org/cmd/cgo/\nfor full story about Go's _C_ package and _cgo_).\nPart of the what _cgo_ and the *C* package does is use the \ncomment decoration to build the signatures for the function calls\nin the shared C library.  The Go toolchain does all the heavy \nlifting in making a *C* shared library based on comment \ndirectives like \"//export\". We don't need much for our twice\nfunction.\n\n```Go\n    package main\n    \n    import (\n    \t\"C\"\n    )\n    \n    //export twice\n    func twice(i int) int {\n    \treturn i * 2\n    }\n    \n    func main() {}\n```\n\nOn the python side we need to wrap our calls to our shared library\nbringing them into the Python runtime in a useful and idiomatically\nPython way. Python provides a few ways of doing this. In my examples\nI am using the *ctypes* package.  _twice.py_ looks like this--\n\n```python\n    import ctypes\n    import os\n    \n    # Set our shared library's name\n    lib_name='libtwice'\n    \n    # Figure out shared library extension\n    uname = os.uname().sysname\n    ext = '.so'\n    if uname == 'Darwin':\n        ext = '.dylib'\n    if uname == 'Windows':\n        ext = '.dll'\n    \n    # Find our shared library and load it\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n    \n    # Setup our Go functions to be nicely wrapped\n    go_twice = lib.twice\n    go_twice.argtypes = [ctypes.c_int]\n    go_twice.restype = ctypes.c_int\n    \n    # Now write our Python idiomatic function\n    def twice(i):\n        return go_twice(ctypes.c_int(i))\n    \n    # We run this test code if with: python3 twice.py\n    if __name__ == '__main__':\n        print(\"Twice of 2 is\", twice(2))\n```\n\nNotice the amount of lifting Python's *ctypes* does for us. It provides\nfor converting C based types to their Python counter parts. Indeed the\nadditional Python source here is focused around using that functionality\nto create a simple Python function called twice. This pattern of \nbringing in a low level version of our desired function and then \npresenting in a Pythonic one is common in more complex C based Python\nmodules.  In general we need *ctypes* to access and wrapping our \nshared library. The *os* module is used so we can find our C \nshared library based on the naming conventions of our host OS. \nFor simplicity I've kept the shared library (e.g. _libtwice.so_ \nunder Linux) in the same directory as the python module \ncode _twice.py_.\n\nThe build command for Linux looks like---\n\n```shell\n    go build -buildmode=c-shared -o libtwice.so libtwice.go\n```\n\nUnder Windows it would look like---\n\n```shell\n    go build -buildmode=c-shared -o libtwice.dll libtwice.go\n```\n\nand Mac OS X---\n\n```shell\n    go build -buildmode=c-shared -o libtwice.dynlib libtwice.go\n```\n\nYou can test the Python module with---\n\n```shell\n    python3 twice.py\n```\n\nNotice the filename choices. I could have called the Go shared\nlibrary anything as long as it wasn't called `twice.so`, `twice.dll`\nor `twice.dylib`. This constraint is to avoid a module name collision\nin Python.  If we had a Python script named `twice_test.py` and \nimport `twice.py` then Python needs to make a distinction between\n`twice.py` and our shared library. If you use a Python package\napproach to wrapping the shared library you would have other options\nfor voiding name collision.\n\nHere is an example of `twice_test.py` to make sure out import is\nworking.\n\n```python\n    import twice\n    print(\"Twice 3\", twice.twice(3))\n```\n\nExample 1 is our base recipe. The next examples focus on handling\nother data types but follow the same pattern.\n\n\n## Example 2, libsayhi.go and sayhi.py\n\nI found working with strings a little more nuanced. Go's concept of\nstrings are oriented to utf-8. Python has its own concept of strings \nand encoding.  Both need to pass through the C layer which assumes \nstrings are a char pointer pointing at contiguous memory ending \nin a null. The *sayhi* recipe is focused on moving a string from \nPython, to C, to Go (a one way trip this time). The example uses \nGo's *fmt* package to display the string. \n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export say_hi\n    func say_hi(msg *C.char) {\n    \tfmt.Println(C.GoString(msg))\n    }\n    \n    func main() { }\n```\n\nThe Go source is similar to our first recipe but our Python modules\nneeds to use *ctypes* to get you Python string into shape to be\nunpacked by Go.\n\n```python\n   import ctypes\n   import os\n   \n   # Set the name of our shared library\n   lib_name = 'libsayhi'\n\n   # Figure out shared library extension\n   uname = os.uname().sysname\n   ext = '.so'\n   if uname == 'Darwin':\n       ext = '.dylib'\n   if uname == 'Windows':\n       ext = '.dll'\n   \n   # Find our shared library and load it\n   dir_path = os.path.dirname(os.path.realpath(__file__))\n   lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n   \n   # Setup our Go functions to be nicely wrapped\n   go_say_hi = lib.say_hi\n   go_say_hi.argtypes = [ctypes.c_char_p]\n   # NOTE: we don't have a return type defined here, the message is \n   # displayed from Go\n   \n   # Now write our Python idiomatic function\n   def say_hi(txt):\n       return go_say_hi(ctypes.c_char_p(txt.encode('utf8')))\n   \n   if __name__ == '__main__':\n       say_hi('Hello!')\n```\n\nPutting things together (if you are using Windows or Mac OS X\nyou'll adjust name output name, `libsayhi.so`, to match the\nfilename extension suitable for your operating system).\n\n```bash\n    go build -buildmode=c-shared -o libsayhi.so libsayhi.go\n```\n\nand testing.\n\n```bash\n    python3 sayhi.py\n```\n\n\n## Example 3, libhelloworld.go and helloworld.py\n\nIn this example we send a Python string to Go (which expects utf-8)\nbuild our \"hello world\" message and then send it back to Python\n(which needs to do additional conversion and decoding).\n\nLike in previous examples the Go side remains very simple. The heavy\nlifting is done by the *C* package and the comment `//export`. We\nare using `C.GoString()` and `C.CString()` to flip between our native\nGo and C datatypes.\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export helloworld\n    func helloworld(name *C.char) *C.char {\n    \ttxt := fmt.Sprintf(\"Hello %s\", C.GoString(name))\n    \treturn C.CString(txt)\n    }\n    \n    func main() { }\n```\n\nIn the python code below the conversion process is much more detailed.\nPython isn't explicitly utf-8 like Go. Plus we're sending our Python \nstring via C's char arrays (or pointer to chars). Finally when we \ncomeback from Go via C we have to put things back in order for Python. \nOf particular note is checking how the byte arrays work then \nencoding/decoding everything as needed. We also explicitly set the result \ntype from our Go version of the helloworld function.\n\n```python\n    import ctypes\n    import os\n    \n    # Set the name of our shared library\n    lib_name = 'libhelloworld'\n\n    # Figure out shared library extension\n    uname = os.uname().sysname\n    ext = '.so'\n    if uname == 'Darwin':\n        ext = '.dylib'\n    if uname == 'Windows':\n        ext = '.dll'\n    \n    # Find our shared library and load it\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n    \n    # Setup our Go functions to be nicely wrapped\n    go_helloworld = lib.helloworld\n    go_helloworld.argtypes = [ctypes.c_char_p]\n    go_helloworld.restype = ctypes.c_char_p\n    \n    # Now write our Python idiomatic function\n    def helloworld(txt):\n        value = go_helloworld(ctypes.c_char_p(txt.encode('utf8')))\n        if not isinstance(value, bytes):\n            value = value.encode('utf-8')\n        return value.decode()\n    \n    \n    if __name__ == '__main__':\n        import sys\n        if len(sys.argv) > 1:\n            print(helloworld(sys.argv[1]))\n        else:\n            print(helloworld('World'))\n```\n\nThe build recipe remains the same as the two previous examples.\n\n```bash\n    go build -buildmode=c-shared -o libhelloworld.so libhelloworld.go\n```\n\nHere are two variations to test.\n\n```bash\n     python3 helloworld.py\n     python3 helloworld.py Jane\n```\n\n\n## Example 4, libjsonpretty.go and jsonpretty.py\n\nIn this example we send JSON encode text to the Go package,\nunpack it in Go's runtime and repack it using the `MarshalIndent()`\nfunction in Go's JSON package before sending it back as Python\nin string form.  You'll see the same encode/decode patterns as \nin our *helloworld* example.\n\nGo code\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"encoding/json\"\n    \t\"fmt\"\n    \t\"log\"\n    )\n    \n    //export jsonpretty\n    func jsonpretty(rawSrc *C.char) *C.char {\n    \tdata := new(map[string]interface{})\n    \terr := json.Unmarshal([]byte(C.GoString(rawSrc)), &data)\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \tsrc, err := json.MarshalIndent(data, \"\", \"    \")\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \ttxt := fmt.Sprintf(\"%s\", src)\n    \treturn C.CString(txt)\n    }\n    \n    func main() {}\n```\n\nPython code\n\n```python\n    import ctypes\n    import os\n    import json\n    \n    # Set the name of our shared library\n    lib_name = 'libjsonpretty'\n\n    # Figure out shared library extension\n    uname = os.uname().sysname\n    ext = '.so'\n    if uname == 'Darwin':\n        ext = '.dylib'\n    if uname == 'Windows':\n        ext = '.dll'\n\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n    \n    go_jsonpretty = lib.jsonpretty\n    go_jsonpretty.argtypes = [ctypes.c_char_p]\n    go_jsonpretty.restype = ctypes.c_char_p\n    \n    def jsonpretty(txt):\n        value = go_jsonpretty(ctypes.c_char_p(txt.encode('utf8')))\n        if not isinstance(value, bytes):\n            value = value.encode('utf-8')\n        return value.decode()\n    \n    if __name__ == '__main__':\n        src = '''\n    {\"name\":\"fred\",\"age\":25,\"height\":75,\"units\":\"inch\",\"weight\":\"239\"}\n    '''\n        value = jsonpretty(src)\n        print(\"Pretty print\")\n        print(value)\n        print(\"Decode into dict\")\n        o = json.loads(value)\n        print(o)\n```\n\nBuild command\n\n```shell\n    go build -buildmode=c-shared -o libjsonpretty.so libjsonpretty.go\n```\n\nAs before you can run your tests with `python3 jsonpretty.py`.\n\nIn closing I would like to note that to use these examples you Python3\nwill need to be able to find the module and shared library. For \nsimplicity I've put all the code in the same directory. If your Python\ncode is spread across multiple directories you'll need to make some \nadjustments.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-02-24",
        "keywords": [
          "Golang",
          "Python",
          "shared libraries"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Go based Python modules"
      },
      "url": "posts/2018/02/24/go-based-python-modules.json"
    },
    {
      "content": "\n\n# Go, Bleve and Library oriented software\n\nBy R. S. Doiel, 2018-02-19\n(updated: 2018-02-22)\n\nIn 2016, Stephen Davison, asked me, \"Why use Go and Blevesearch for\nour library projects?\" After our conversation I wrote up some notes so\nI would remember. It is now 2018 and I am revising these notes. I\nthink our choice paid off.  What follows is the current state of my\nreflection on the background, rational, concerns, and risk mitigation\nstrategies so far for using [Go](https://golang.org) and\n[Blevesearch](https://blevesearch.com) for Caltech Library projects.\n\n## Background\n\nI first came across Go a few years back when it was announced as an\nOpen Source project by Google at an Google I/O event (2012). The\noriginal Go authors were Robert Griesemer, Rob Pike, and Ken\nThompson. What I remember from that presentation was Go was a rather\nconsistent language with the features you need but little else.  Go\ndeveloped at Google as a response to high development costs for C/C++\nand Java in addition to challenges with performance and slow\ncompilation times.  As a language I would put Go between C/C++ and\nJava. It comes the ease of writing and reading you find in languages\nlike Python. Syntax is firmly in the C/C++ family but heavily\nsimplified. Like Java it provides many modern features including rich basic\ndata structures and garbage collection. It has a very complete standard\nlibrary and provides very good tooling.  This makes it easy to\ngenerate code level documentation, format code, test, efficiently profile, \nand debug.\n\nOften programming languages develop around a specific set of needs.\nThis is true for Go. Given the Google origin it should not be\nsurprising to find that Go's primary strengths are working with \nstructured data, I/O and concurrency. The rich standard\nlibrary is organized around a package concept. These include packages\nsupporting network protocols, file and socket I/O as well as various\nencoding and compression scheme. It has particularly strong support\nfor XML, JSON, CSV formatted data out of the box. It has a template\nlibrary for working with plain text formats as well as generating safe\nHTML. You can browse Go's standard library https://golang.org/pkg/.\n\nAn additional feature is Go's consistency. Go code that compiles under\nversion 1.0 still compiles under 1.10. Even before 1.0 code changes\nthat were breaking came with tooling to automatically updates existing\ncode.  Running code is a strong element of Go's evolution.\n\nGo is unsurprising and has even been called boring.  This turns out to\nbe a strength when building sustainable projects in a small team.\n\n\n## Why do I write Go?\n\nFor me Go is a good way to write web services, assemble websites,\ncreate search appliances and write command line (cli) utilities. When\na shell script becomes unwieldy Go is often what I turn to.  Go is\nwell suited to building tools as well as systems.  Go based command\nline tools are very easy to orchestrate with shell and Python.\n\nGo runs on all the platforms I actively use - Windows, Mac OS X, Linux\non both Intel and ARM (e.g. Raspberry Pi, Pine64). It has experimental\nsupport for Android and iOS.  I've used a tool called\n[GopherJS](http://gopherjs.org) to write web browser applications that\ntransform my command line tools into web tools with a friendlier user\ninterface (see our [BibTeX Tools](https://caltechlibrary.github.io/bibtex/webapp/)).\n\nGo supports cross compiling out of the box. This means a production\nsystem running on AWS, Google's compute engine or Microsoft's Azure\ncan be compiled from Windows, Mac OS or even a Raspberry Pi!\nDeployment is a matter of copying the (self contained) compiled binary\nonto the production system. This contrasts with other\nplatforms like Perl, PHP, Python, NodeJS and Ruby where you need to\ninstall not only your application code but all dependencies. While\ninterpretive languages retain an advantage of having a REPL, Go\nbased programs have advantages of fast compile times and easy deployment.\n\nIn many of the projects I've written in Go I've only required a few\n(if any) 3rd party libraries (packages in Go's nomenclature). This is\nquite a bit different from my experience with Perl, PHP, Python,\nNodeJS and Ruby. This is in large part a legacy of having grown up at\nGoogle before become an open source project. While the Go standard\npackages are very good there is a rich ecosystem for 3rd party\npackages for specialized needs. I've found I tend to rely only on a\nfew of them. The one I've used the most is\n[Bleve](http://blevesearch.com).\n\nBleve is a Go package for building search engines. When I originally\ncame across Bleve (around 2014), it was described as \"Lucene lite\". \n\"Lucene lite\" was an apt description, but I find it easier\nto use than Lucene. When I first used Bleve I embedded its\nfunctionality into the tools I used to process data and present web\nservices. It did not have much in the way of stand alone command line\ntooling.  Today I increasingly think of Bleve as \"Elastic Search\nlite\". It ships with a set of command line tools that include support\nfor building Bleve's indexes.  My current practice is to only embed the search\nportion of the packages. I can use the Bleve command line for the\nrest.  In 2018, Bleve is being actively developed, has a small vibrant\ncommunity and is used by [Couchbase](https://couchbase.com), a well\nestablished NoSQL player.\n\n\n## Who is using Go?\n\nMany companies use Go. The short list includes\nGoogle, Amazon, Netflix, Dropbox, Box, eBay, Pearsons and even\nWalmart and Microsoft. This came to my attention at developer conferences\nback in 2014.  People from many of these companies started\npresenting at conferences on pilot projects that had been successful\nand moved to production. Part of what drove adoption was the ease\nof development in Go along with good system performance. I also think\nthere was a growing disenchantment with alternatives like C++, C sharp\nand Java as well as the weight of the LAMP, Tomcat, and OpenStack.\n\nHighly visible Go based projects include\n\n+ [Docker](http://docker.org) and [Rocket](http://www.docker.com) - Containerization for running process in the cloud\n+ [Kubernettes](http://kubernetes.io/) and [Terraform](https://www.terraform.io/) - Container orchestration systems\n+ [Hugo](http://hugo.io) - the fast/popular static website generator, an alternative to Jekyll, for those who want speed\n+ [Caddy](https://caddyserver.com/) - a Go based web server trying to unseat Apache/NGinX focusing on easy of use plus speed\n+ [IPFS](http://ipfs.io) - a cutting edge distributed storage system based on block chains\n\n\n### Who is using Blevesearch?\n\nHere's some larger projects using Bleve.\n\n+ [Couchbase](http://www.couchbase.com), a NoSQL database platform are replacing Lucene with Bleve.  Currently the creator of Bleve works for them.\n+ [Hugo](http://hugo.io) can integrate with Bleve for search and index generation\n+ [Caddy](https://caddyserver.com/) integrates with Bleve to provide an embedded search capability\n\n\n## Managing risks\n\nIn 2014 Go was moving from bleeding to leading edge. Serious capital\nwas behind its adoption and it stopped being an exotic conference\nitem. In 2014 Bleve was definitely bleeding edge. By late 2015 and early\n2016 the program level API stabilized. People were piloting projects\nwith it. This included our small group at Caltech Library. In 2015\nnon-English language support appeared followed by a growing list\nof non-European languages in 2016. By mid 2016 we started to see \nmissing features like alternative sorting added. While Bleve isn't\nyet 1.0 (Feb. 2018) it is reliable. The primary challenge for the Bleve\nproject is documentation targeting the novice and non-Programmer users.\nBleve has proven effective as an indexing and search platform for \narchival, library, and data repository content.\n\nAdopting new software comes with risk. We have mitigated this in two ways.\n\n1. Identify alternative technology (a plan B)\n2. Architect our systems for easy decomposition and re-composition\n\nIn the case of Go, packages can be compiled to a C-Shared\nlibrary. This allows us to share working Go packages with languages\nlike Python, R, and PHP. We have included shared Go/Python modules\non our current road map for projects.\n\nFor Blevesearch the two alternatives are Solr and Elastic\nSearch. Both are well known, documented, and solid.  The costs would be\nrecommitting to a Java stack and its resource requirements. We have\nalready identified what we want to index and that could be converted\nto either platform if needed.  If we stick with Go but dropped \nBlevesearch we would swap out the Bleve specific code for Go packages \nsupporting Solr and Elastic Search.\n\n\nThe greatest risk in adopting Go for library and archive projects was \nknowledge transfer. We addressed this \nby knowledge sharing and insuring the Go codebase can \nbe used via command line programs.  Additionally \nwe are adding support for Go based Python modules.\nTraining also is available in the form of books, websites and\nonline courses ([lynda.com](https://www.lynda.com/Go-tutorials/Up-Running-Go/412378-2.html) offers a \"Up Running Go\" course).\n\n\n## What are the benefits?\n\nFor library and archives software we have found Go's benefits include\nimproved back end systems performance at a lower cost, ease of development, \nease of deployment, a rich standard library focused on the types of things \nneeded in library and archival software.  Go plays nice with\nother systems (e.g. I create an API based service in Go that can easily\nbe consumed by a web browser running JavaScript or Perl/PHP/Python\ncode running under LAMP). In the library and archives setting Go \ncan become a high performance duck tape. We get the performance and \nreliability of C/Java type systems with code simplicity \nsimilar to Python.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-02-19",
        "keywords": [
          "Golang",
          "Bleve",
          "search"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Go, Bleve and Library oriented software"
      },
      "url": "posts/2018/02/19/go-bleve-and-libraries.json"
    },
    {
      "content": "\n\n# Accessing Go from Julia\n\nBy R. S. Doiel, 2018-03-11\n\nThe problem: I've started exploring Julia and I would like to leverage existing\ncode I've written in Go. Essentially this is a revisit to the problem in my\nlast post [Go based Python Modules](https://rsdoiel.github.io/blog/2018/02/24/go-based-python-modules.html) \nbut with the language pairing of Go and Julia.\n\n\n## Example 1, libtwice.go, libtwice.jl and libtwice_test.jl\n\nIn out first example we send an integer value from\nJulia to Go and back via a C shared library (written in Go). While Julia doesn't\nrequire type declarations I will be using those for clarity. Like in my previous post\nI think this implementation this is a good starting point to see how Julia interacts with\nC shared libraries. Like before I will present our Go code, an explanation \nfollowed by the Julia code and commentary.\n\nOn the Go side we create a _libtwice.go_ file with an empty `main()` \nfunction.  Notice that we also import the *C* package and use \na comment decoration to indicate the function we are exporting\n(see https://github.com/golang/go/wiki/cgo and \nhttps://golang.org/cmd/cgo/\nfor full story about Go's _C_ package and _cgo_).\nPart of the what _cgo_ and the *C* package does is use the \ncomment decoration to build the signatures for the function calls\nin the shared C library.  The Go toolchain does all the heavy \nlifting in making a *C* shared library based on comment \ndirectives like \"//export\". We don't need much for our twice\nfunction.\n\n```Go\n    package main\n    \n    import (\n    \t\"C\"\n    )\n    \n    //export twice\n    func twice(i int) int {\n    \treturn i * 2\n    }\n    \n    func main() {}\n```\n\nLike in our previous Python implementation we need to build the C shared\nlibrary before using it from Julia. Here are some example Go build commands\nfor Linux, Windows and Mac OS X. You only need to run the one that applies\nto your operating system.\n\n```shell\n    go build -buildmode=c-shared -o libtwice.so libtwice.go\n    go build -buildmode=c-shared -o libtwice.dll libtwice.go\n    go build -buildmode=c-shared -o libtwice.dynlib libtwice.go\n```\n\nUnlike the Python implementation our Julia code will be split into two files. _libtwice.jl_ will\nhold our module definition and _libtwice_test.jl_ will hold our test code. In the\ncase of _libtwice.jl_ we will access the C exported function via a function named *ccall*. \nJulia doesn't require a separate module to be imported in order to access a C shared library.\nThat makes our module much simpler. We still need to be mindful of type conversion.  Both \nGo and Julia provide for rich data types and structs.  But between Go and Julia we have C \nand C's basic type system.  On the Julia side *ccall* and Julia's type system help us\nmanaging C's limitations.\n\nHere's the Julia module we'll call _libtwice.jl_.\n\n```Julia\n    module libtwice\n            \n    # We write our Julia idiomatic function\n    function twice(i::Integer)\n        ccall((:twice, \"./libtwice\"), Int32, (Int32,), i)\n    end\n\n    end\n```\n\nWe're will put the test code in a file named _libtwice\\_test.jl_. Since this isn't\nan establish \"Package\" in Julia we will use Julia's *include* statement to get bring the\ncode in then use an *import* statement to bring the module into our current name space.\n\n```Julia\n    include(\"libtwice.jl\")\n    import libtwice\n    # We run this test code for libtwice.jl\n    println(\"Twice of 2 is \", libtwice.twice(2))\n```\n\nOur test code can be run with\n\n```shell\n    julia libtwice_test.jl\n```\n\nNotice the amount of lifting that Julia's *ccall* does. The Julia code is much more compact\nas a result of not having to map values in a variable declaration. We still have the challenges \nthat Julia and Go both support richer types than C. In a practical case we should consider \nthe over head of running to two runtimes (Go's and Julia's) as well as whether or not \nimplementing as a shared library even makes sense. But if you want to leverage existing \nGo based code this approach can be useful.\n\nExample 1 is our base recipe. The next examples focus on handling\nother data types but follow the same pattern.\n\n\n## Example 2, libsayhi.go, libsayhi.jl and libsayhi_test.jl\n\nLike Python, passing strings passing to or from Julia and Go is nuanced. Go is expecting \nUTF-8 strings. Julia also supports UTF-8 but C still looks at strings as a pointer to an\naddress space that ends in a null value. Fortunately in Julia the *ccall* function combined with\nJulia's rich type system gives us straight forward ways to map those value. \nGo code remains unchanged from our Python example in the previous post. \nIn this example we use Go's *fmt* package to display the string. In the next example\nwe will round trip our string message.\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export say_hi\n    func say_hi(msg *C.char) {\n    \tfmt.Println(C.GoString(msg))\n    }\n    \n    func main() { }\n```\n\nThe Go source is the similar to our first recipe. No change from our\nprevious posts' Python example. It will need to be compiled to create our\nC shared library just as before. Run the go build line that applies to\nyour operating system (i.e., Linux, Windows and Mac OS X).\n\n```shell\n    go build -buildmode=c-shared -o libsayhi.so libsayhi.go\n    go build -buildmode=c-shared -o libsayhi.dll libsayhi.go\n    go build -buildmode=c-shared -o libsayhi.dylib libsayhi.go\n```\n\nOur Julia module looks like this.\n\n```julia\n    module libsayhi\n\n    # Now write our Julia idiomatic function using *ccall* to access the shared library\n    function say_hi(txt::AbstractString)\n        ccall((:say_hi, \"./libsayhi\"), Int32, (Cstring,), txt)\n    end\n\n    end\n```\n\nThis code is much more compact than our Python implementation.\n\nOur test code looks like\n\n```julia\n    include(\"./libsayhi.jl\")\n    import libsayhi\n    libsayhi.say_hi(\"Hello again!\")\n```\n\nWe run our tests with\n\n```shell\n    julia libsayhi_test.jl\n```\n\n\n## Example 3, libhelloworld.go and librhelloworld.cl and libhelloworld_test.jl\n\nIn this example we send a string round trip between Julia and Go. \nMost of the boiler plate we say in Python is gone due to Julia's type system. In\naddition to using Julia's *ccall* we'll add a *convert* and *bytestring* function calls\nto bring our __Cstring__ back to a __UTF8String__ in Julia.\n\nThe Go implementation remains unchanged from our previous Go/Python implementation. \nThe heavy lifting is done by the *C* package and the comment \n`//export`. We are using `C.GoString()` and `C.CString()` to flip between \nour native\nGo and C datatypes.\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export helloworld\n    func helloworld(name *C.char) *C.char {\n    \ttxt := fmt.Sprintf(\"Hello %s\", C.GoString(name))\n    \treturn C.CString(txt)\n    }\n    \n    func main() { }\n```\n\nAs always we must build our C shared library from the Go code. Below is\nthe go build commands for Linux, Windows and Mac OS X. Pick the line that\napplies to your operating system to build the C shared library.\n\n```shell\n    go build -buildmode=c-shared -o libhelloworld.so libhelloworld.go\n    go build -buildmode=c-shared -o libhelloworld.dll libhelloworld.go\n    go build -buildmode=c-shared -o libhelloworld.dylib libhelloworld.go\n```\n\nIn our Julia, _libhelloworld.jl_, the heavy lifting of type conversion\nhappens in Julia's type system and in the *ccall* function call. Additionally we need\nto handle the conversion from __Cstring__ Julian type to __UTF8String__ explicitly\nin our return value via a functions named *convert* and *bytestring*.\n\n```julia\n    module libhelloworld\n\n    # Now write our Julia idiomatic function\n    function helloworld(txt::AbstractString)\n        value = ccall((:helloworld, \"./libhelloworld\"), Cstring, (Cstring,), txt)\n        convert(UTF8String, bytestring(value))\n    end\n\n    end\n```\n\nOur test code looks similar to our Python test implementation.\n\n```julia\n    include(\"libhelloworld.jl\")\n    import libhelloworld\n \n    if length(ARGS) > 0\n        println(libhelloworld.helloworld(join(ARGS, \" \")))\n    else\n        println(libhelloworld.helloworld(\"World\"))\n    end\n```\n\nAs before we see the Julia code is much more compact than Python's.\n\n\n## Example 4, libjsonpretty.go, libjsonpretty.jl and libjsonpretty_test.jl\n\nIn this example we send JSON encode text to the Go package,\nunpack it in Go's runtime and repack it using the `MarshalIndent()`\nfunction in Go's JSON package before sending it back to Julia\nin C string form.  You'll see the same encode/decode patterns as \nin our *libhelloworld* example.\n\nGo code\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"encoding/json\"\n    \t\"fmt\"\n    \t\"log\"\n    )\n    \n    //export jsonpretty\n    func jsonpretty(rawSrc *C.char) *C.char {\n    \tdata := new(map[string]interface{})\n    \terr := json.Unmarshal([]byte(C.GoString(rawSrc)), &data)\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \tsrc, err := json.MarshalIndent(data, \"\", \"    \")\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \ttxt := fmt.Sprintf(\"%s\", src)\n    \treturn C.CString(txt)\n    }\n    \n    func main() {}\n```\n\nBuild commands for Linux, Windows and Mac OS X are as before, pick the one that matches\nyour operating system.\n\n```shell\n    go build -buildmode=c-shared -o libjsonpretty.so libjsonpretty.go\n    go build -buildmode=c-shared -o libjsonpretty.dll libjsonpretty.go\n    go build -buildmode=c-shared -o libjsonpretty.dylib libjsonpretty.go\n```\n\nOur Julia module code\n\n```Julia\n    module libjsonpretty\n\n    # Now write our Julia idiomatic function\n    function jsonpretty(txt::AbstractString)\n        value = ccall((:jsonpretty, \"./libjsonpretty\"), Cstring, (Cstring,), txt)\n        convert(UTF8String, bytestring(value))\n    end\n    \n    end\n```\n\nOur Julia test code\n\n```Julia\n    include(\"./libjsonpretty.jl\")\n    import libjsonpretty\n\n    src = \"\"\"{\"name\":\"fred\",\"age\":25,\"height\":75,\"units\":\"inch\",\"weight\":\"239\"}\"\"\"\n    println(\"Our origin JSON src\", src)\n    value = libjsonpretty.jsonpretty(src)\n    println(\"And out pretty version\\n\", value)\n```\n\nAs before you can run your tests with `julia libjsonpretty_test.jl`.\n\nIn closing I would like to note that to use these examples I am assuming your\nJulia code is in the same directory as your shared C library. Julia, like Python3,\nhas a feature rich module and Package system. If you are creating a serious Julia\nproject then you need to be familiar with how Julia's package and module system works\nand place your code and shared libraries appropriately.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-03-11",
        "keywords": [
          "Golang",
          "Julia",
          "shared libraries"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Accessing Go from Julia"
      },
      "url": "posts/2018/03/11/accessing-go-from-julia.json"
    },
    {
      "content": "\n\n# Review: Software Tools in Pascal\n\nBy R. S. Doiel, 2018-07-22\n(updated: 2018-07-22, 1:39 pm, PDT)\n\n\nThis book is by Brian W. Kernighan and P. J. Plauger. It is an\nexample of the type of books I find I re-read and want in my\npersonal library. The book covers software construction through \na series of programs written in pascal. It is about how these \nprograms work, how to approach problems and write sound software.\nI was surprised I did not know about this book when I was browsing \nthe [Open Library](https://openlibrary.org) this weekend.  While \nPascal was a popular in the 1980's it has faded for most people in the \nearly 21st century.  This review maybe a small bit of nostalgia. \nOn the other hand I suspect \n[\"Software Tools in Pascal\"](https://openlibrary.org/books/OL4258115M/Software_tools_in_Pascal)\nis one of the short list of computer books that will remain useful\nover the long run.\n\n\n## What's covered\n\nThe book is organized around specific programs and their implementations.\nThe implementations provided are simple and straight forward. Each\nsection is followed by a set of \"exercises\" that extend the ideas\nshown in the section. In this way you could derive the modern equivalent\nof these tools.\n\nThe topics you build tools for in the text are\nfilters, files, sorting, text patterns, editing, formatting, \nand macro processing.\n\nIf you want to follow the book along in Pascal then I think Free Pascal\navailable in many Debian distributions including Raspbian on the Raspberry\nPi is a good choice.  Likewise Wirth's Pascal is easy enough to port\nto other languages and indeed this would be a useful exercise when I\nre-read the book the next time.\n\nThe book presents a very nice set of text oriented programs to explore\nprogramming or re-connect with your programming roots.\n\n## Read the book\n\n<iframe width=\"165\" frameBorder=\"0\" height=\"400\" src=\"https://openlibrary.org/books/OL4258115M/Software_tools_in_Pascal/widget\"></iframe>\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-07-22",
        "keywords": [
          "Pascal",
          "programming",
          "book review"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Review: Software Tools in Pascal"
      },
      "url": "posts/2018/07/22/software-tools-in-pascal.json"
    },
    {
      "content": "\n\nInstalling Golang from Source on RPi-OS for arm64\n==========================================\n\nBy R. S. Doiel, 2022-02-18\n\nThis are my quick notes on installing Golang from source on the Raspberry Pi OS 64 bit.\n\n1. Get a working compiler\n\ta. go to https://go.dev/dl/ and download go1.17.7.linux-arm64.tar.gz\n\tb. untar the tarball in your home directory (it'll unpack to $HOME/go)\n\tc. `cd go/src` and `make.bash`\n2. Move go directory to go1.17\n3. Clone go from GitHub\n4. Compile with the downloaded compiler\n\ta. `cd go/src`\n\tb. `env GOROOT_BOOTSTRAP=$HOME/go1.17 ./make.bash`\n\tc. Make sure `$HOME/go/bin` is in the path\n\td. `go version`\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-02-18",
        "keywords": [
          "raspberry pi",
          "Raspberry Pi OS",
          "arm64"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 1,
        "series": "Raspberry Pi",
        "title": "Installing Golang from source on RPi-OS for arm64"
      },
      "url": "posts/2022/02/18/Installing-Go-from-Source-RPiOS-arm64.json"
    },
    {
      "content": "\nTurbo Oberon, the dream\n=======================\n\nby R. S. Doiel, 2022-07-30\n\nSometimes I have odd dreams and that was true last night through early this morning. The dream was set in the future. I was already retired. It was a dream about \"Turbo Oberon\".\n\n\"Turbo Oberon\" was an Oberon language. The language compiler was named \"TO\" in my dream. A module's file extension was \".tom\", in honor of Tom Lopez (Meatball Fulton) of ZBS. There were allot of ZBS references in the dream.\n\n\"TO\" was very much a language in the Oberon-07 tradition with minor extensions when it came to bringing in modules. It allowed for a multi path search for module names. You could also express a Module import as a string allowing providing paths to the imported module.\n\nCompilation was similar to Go. Cross compilation was available out of the box by setting a few environment variables. I remember answering questions about the language and its evolution. I remember mentioning in the conversation about how I thought Go felling into the trap of complexity like Rust or C/C++ before it. The turning point for Go was generics. Complexity was the siren song to be resisted in \"Turbo Oberon\". Complexity is seductive to language designers and implementers. I was only an implementer.\n\nEvolution wise \"TO\" was built initially on the Go tool chain. As a result it featured easily cross-compiled binaries and had a rich standard set of Modules like Go but also included portable libraries for implementing graphic user interfaces. \"Turbo Oberon\" evolved as a conversation between Go and the clean simplicity of Oberon-07. Two example applications \"shipped\" with the \"TO\" compiler. They were an Oberon like Operating System (stand alone and hosted) and a Turbo Pascal like IDE. The IDE was called \"toe\" for Turbo Oberon Editor. I don't remember the name of the OS implementation but it might have been \"toos\". I remember \"TO\" caused problems for search engines and catalog systems.\n\nI remember remarking in the dream that programming in \"Turbo Oberon\" was a little like returning to my roots when I first learned to program in Turbo Pascal. Except I could run my programs regardless of the operating system or CPU architecture. \"\"TO\" compiler supported cross compilation for Unix, macOS, Windows on ARM, Intel, RISC-V. The targets were inherited from Go implementation roots.\n\nIn my dream I remember forking Go 1.18 and first replacing the front end of the compiler. I remember it was a challenge understanding the implementation and generate a Go compatible AST. The mapping between Oberon-07 and Go had its challenges. I remember first sticking to a strict Oberon-07 compiler targeting POSIX before enhancing module imports. I remember several failed attempts at getting module imports \"right\". I remember being on the fence about a map data type and going with a Maps module.  I don't remember how introspection worked but saying it was based on an ETH paper for Oberon 2.  I remember the compiler, like Go, eventually became self hosting. It supported a comments based DSL to annotating RECORD types making encoding and decoding convenient, an influence of Go and it's tool chain.\n\nI believe the \"Turbo Oberon Editor\" came first and that was followed by the operating system implementation.\n\nI remember talking about a book that influenced me called, \"Operating Systems through compilers\" but don't know who wrote it. I remember a discussion about the debt owed to Prof. Wirth. I remember that the book showed how once you really understood building the compile you could then build the OS. There was a joke riffing on the old Lisp joke but rephrased, \"all applications evolve not to a Lisp but to an embedded OS\".\n\nIt was a pleasant dream, in the dream I was older and already retired but still writing \"TO\" code and having fun with computers. I remember a closing video shot showing me typing away at what looked like the old Turbo Pascal IDE. As Mojo Sam said in **Somewhere Next Door to Reality**, \"it was a sorta a retro future\".\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2022-07-30",
        "keywords": [
          "Oberon",
          "Wirth",
          "ETH",
          "dreams",
          "compilers",
          "operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Turbo Oberon, the dream"
      },
      "url": "posts/2022/07/30/Turbo-Oberon.json"
    },
    {
      "content": "\nArtemis Project Status, 2022\n============================\n\nIt's been a while since I wrote an Oberon-07 post and even longer since I've worked on Artemis. Am I done with Oberon-07 and abandoning Artemis?  No. Life happens and free time to just hasn't been available. I don't know when that will change.\n\nWhat's the path forward?\n------------------------\n\nSince I plan to continue working Artemis I need to find a way forward in much less available time. Time to understand some of my constraints. \n\n1. I work on a variety of machines, OBNC is the only compiler I've consistently been able to use across all my machines\n2. Porting between compilers takes energy and time, and those compilers don't work across all my machines\n3. When I write Oberon-07 code I quickly hit a wall for the things I want to do, this is what original inspired Artemis, so there is still a need for a collection of modules\n4. Oberon/Oberon-07 on Wirth RISC virtual machine is not sufficient for my development needs\n5. A2, while very impressive, isn't working for me either (mostly because I need to work on ARM CPUs)\n\nThese constraints imply Artemis is currently too broadly scoped. I think I need to focus on what works in OBNC for now. Once I have a clear set of modules then I can revisit portability to other compilers.\n\nWhat modules do I think I need? If I look at my person projects I tend to work allot with text, often structured text (e.g. XML, JSON, CSV). I also tend to be working with network services. Occasionally I need to interact with database (e.g. SQLite3, MySQL, Postgres).  Artemis should provide modules to make it easy to write code in Oberon-07 that works in those areas. Some of that I can do by wrapping existing C libraries. Some I can simply write from scratch in Oberon-07 (e.g. a JSON encoder/decoder). That's going to me my focus as my hobby time becomes available and then.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2022, R. S. Doiel",
        "date": "2022-07-27",
        "keywords": [
          "Oberon",
          "Oberon-07",
          "Artemis"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 22,
        "series": "Mostly Oberon",
        "title": "Artemis Project Status, 2022"
      },
      "url": "posts/2022/07/27/Artemis-Status-Summer-2022.json"
    }
  ]
}