{
  "content": "\n# LLM first impressions a few weeks in\n\nBy R.S. Doiel, 2025-03-30\n\nWriting code with an LLM is far cry from what the hype cycle promises. It requires care. An attention to detail. It imposes significant compute resources. Those impose significant amount of electricity consumption. Even cloud hosted LLM are slow beyond the first few iterations. If you want to find a happy medium between describing what want and how you want it done you need to commit to a non trivial amount of effort. Depending on your level of experience it may be faster to limit code generation to specific parts of a project. In early 2025 it maybe faster to code simple projects yourself.\n\nWhen I compare working with an LLM like Gemma, Llama, Phi, Mistral, Chat-GPT to traditional [RAD](https://en.wikipedia.org/wiki/Rapid_application_development \"Rapid Application Development\") the shiny of the current \"AI\" hype is diminished. RAD tools often are easier operate, been around so long we forget about them and use significantly less compute resources[^1]. RAD tools are venerable in 2025. The computational overhead as well as the complexity of running an integrated LLM environment that supports [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation \"Retrieval Augmented Generation\"), [agency](https://en.wikipedia.org/wiki/Software_agent \"software agent explained\") and [tools](https://www.forbes.com/councils/forbestechcouncil/2025/03/27/your-essential-primer-on-large-language-model-agent-tools/ \"A Forbes article on tool use with large language models\") is much more than writing a simple code generator[^2]. A popular one I tried is [Msty.app](https://mysty.app). It unfortunately is restricted to the x86 platform. So much for energy efficiency. How do we get more bang for the energy consumed? Does the LLM enhanced coding environment push things really forward? Are LLM yet another tool in the toolbox of our profession?\n\n[^1]: Delphi's RAD tooling and Lazarus' RAD tooling can run on desktop computers from a decade or more ago. The hardware vendors want to sell us \"AI computers\" and \"CPU\" because of the increase compute demands of running large LLM software. You can rent GPU as a service because the costs in terms of hardware and energy consumption from Big Tech's LLM obsession.\n\n[^2]: Code generation goes back to the at least to 1950 or 1960. Code generation was well established practice before the Go community picked it up over a decade ago to deal with the routine boiler plate. I routinely create code templates and run it through Pandoc. These I can even use on a Raspberry Pi without it breaking a sweat.\n\nWhat are the areas I've found helpful use LLM?\n\nMy hands and wrists aren't the same as when I started in my career. I am hoping that an LLM can reduce my typing and reduce the risk of [RSI](https://en.wikipedia.org/wiki/Repetitive_strain_injury \"repetitive strain injury\"). I am also hoping that I only need to do detailed reading after waiting for code to be generated (i.e. I'm taking advantage of the slowness of the LLM eval cycle). My hope is for lowered eye strain. Is that worth burning down rain forests? No it is not. Am I convinced that either benefit is true? No. I'm hoping this aspect of LLM usage get studied. I'm interested in tools that are a good ergonomic benefit for developers.\n\nWithout using an LLM my normal development approach is to write out a summary of what I want to do in a project. I clarify the modules I want and the operations I need to perform. Then I code tests before coding the modules.  For many people I think this approach appears upside down. When I started as a programmer I wrote code first to some general notion of what I thought the client wanted. I've found this inverted approach yields some documentation. Writing documentation first clarifies my software architecture and how I will organize the code needed. Writing tests first means I don't write more code than needed. The result is software I can look at a year or more later and still understand.\n\nThe first step in my current approach is well suited to adopting an LLM and using code generation.  The LLM I've tried are decent at writing minimal test code. Tooling can automate some of the loop of generating code that can actually run and pass tests. The process is not fast. It reminds me of the slow compilers of my youth. It works OK for the most part. You can speed things up with faster hardware. Are we reinventing Workstations with \"AI computers\"? Today you can rent LLM services and hardware in the cloud.\n\nAn intriguing relationship with LLM or \"prompt programming\" that isn't talked about much is the [Literate programming](https://en.wikipedia.org/wiki/Literate_programming) [Knuth](https://en.wikipedia.org/wiki/Donald_Knuth) advocated. The LLM I've tried have used a chat interface. An documented and editing approach may be better suited. Using a chat approach the responses are usually presented in Markdown with embedded code blocks. This is analogous to a simplified version of what Knuth created using TeX with embedded code blocks. With the LLM your prompts trigger a narrative of code and function. That feels familiar to me. The challenges of working with an LLM is much like the challenges of literate programming. An LLM might encourage us to think more about what we're trying to do and perhaps more objectively vet the resulting code without engaging our egos.\n\nA significant advantage of the LLM generated code is a byproduct of large training sets. The generated code tends to look average. It tends to avoid obfuscation. It mostly is reasonably reasonable. If the computing resources were significantly lower I'd feel more comfortable with this approach over the long run. The current state needs radical simplification. The LLM development environment is overly complex. That complexity only benefits consolidation in the cloud where we are rented the tools of our trade. LLM development environment's complexity and energy consumption weight heavily on me as I explore this approach. It is too early to tell if it should be a regular tool in the developers' toolbox of sustainable software practices.\n\n\n\n\n",
  "content_ast": {
    "children": [
      {
        "children": [
          {
            "type": "text",
            "value": "LLM first impressions a few weeks in"
          }
        ],
        "depth": 1,
        "type": "heading"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "By R.S. Doiel, 2025-03-30"
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "Writing code with an LLM is far cry from what the hype cycle promises. It requires care. An attention to detail. It imposes significant compute resources. Those impose significant amount of electricity consumption. Even cloud hosted LLM are slow beyond the first few iterations. If you want to find a happy medium between describing what want and how you want it done you need to commit to a non trivial amount of effort. Depending on your level of experience it may be faster to limit code generation to specific parts of a project. In early 2025 it maybe faster to code simple projects yourself."
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "When I compare working with an LLM like Gemma, Llama, Phi, Mistral, Chat-GPT to traditional "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "RAD"
              }
            ],
            "title": "Rapid Application Development",
            "type": "link",
            "url": "https://en.wikipedia.org/wiki/Rapid_application_development"
          },
          {
            "type": "text",
            "value": " the shiny of the current \"AI\" hype is diminished. RAD tools often are easier operate, been around so long we forget about them and use significantly less compute resources[^1]. RAD tools are venerable in 2025. The computational overhead as well as the complexity of running an integrated LLM environment that supports "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "RAG"
              }
            ],
            "title": "Retrieval Augmented Generation",
            "type": "link",
            "url": "https://en.wikipedia.org/wiki/Retrieval-augmented_generation"
          },
          {
            "type": "text",
            "value": ", "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "agency"
              }
            ],
            "title": "software agent explained",
            "type": "link",
            "url": "https://en.wikipedia.org/wiki/Software_agent"
          },
          {
            "type": "text",
            "value": " and "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "tools"
              }
            ],
            "title": "A Forbes article on tool use with large language models",
            "type": "link",
            "url": "https://www.forbes.com/councils/forbestechcouncil/2025/03/27/your-essential-primer-on-large-language-model-agent-tools/"
          },
          {
            "type": "text",
            "value": " is much more than writing a simple code generator[^2]. A popular one I tried is "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "Msty.app"
              }
            ],
            "type": "link",
            "url": "https://mysty.app"
          },
          {
            "type": "text",
            "value": ". It unfortunately is restricted to the x86 platform. So much for energy efficiency. How do we get more bang for the energy consumed? Does the LLM enhanced coding environment push things really forward? Are LLM yet another tool in the toolbox of our profession?"
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "[^1]: Delphi's RAD tooling and Lazarus' RAD tooling can run on desktop computers from a decade or more ago. The hardware vendors want to sell us \"AI computers\" and \"CPU\" because of the increase compute demands of running large LLM software. You can rent GPU as a service because the costs in terms of hardware and energy consumption from Big Tech's LLM obsession."
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "[^2]: Code generation goes back to the at least to 1950 or 1960. Code generation was well established practice before the Go community picked it up over a decade ago to deal with the routine boiler plate. I routinely create code templates and run it through Pandoc. These I can even use on a Raspberry Pi without it breaking a sweat."
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "What are the areas I've found helpful use LLM?"
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "My hands and wrists aren't the same as when I started in my career. I am hoping that an LLM can reduce my typing and reduce the risk of "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "RSI"
              }
            ],
            "title": "repetitive strain injury",
            "type": "link",
            "url": "https://en.wikipedia.org/wiki/Repetitive_strain_injury"
          },
          {
            "type": "text",
            "value": ". I am also hoping that I only need to do detailed reading after waiting for code to be generated (i.e. I'm taking advantage of the slowness of the LLM eval cycle). My hope is for lowered eye strain. Is that worth burning down rain forests? No it is not. Am I convinced that either benefit is true? No. I'm hoping this aspect of LLM usage get studied. I'm interested in tools that are a good ergonomic benefit for developers."
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "Without using an LLM my normal development approach is to write out a summary of what I want to do in a project. I clarify the modules I want and the operations I need to perform. Then I code tests before coding the modules.  For many people I think this approach appears upside down. When I started as a programmer I wrote code first to some general notion of what I thought the client wanted. I've found this inverted approach yields some documentation. Writing documentation first clarifies my software architecture and how I will organize the code needed. Writing tests first means I don't write more code than needed. The result is software I can look at a year or more later and still understand."
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "The first step in my current approach is well suited to adopting an LLM and using code generation.  The LLM I've tried are decent at writing minimal test code. Tooling can automate some of the loop of generating code that can actually run and pass tests. The process is not fast. It reminds me of the slow compilers of my youth. It works OK for the most part. You can speed things up with faster hardware. Are we reinventing Workstations with \"AI computers\"? Today you can rent LLM services and hardware in the cloud."
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "An intriguing relationship with LLM or \"prompt programming\" that isn't talked about much is the "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "Literate programming"
              }
            ],
            "type": "link",
            "url": "https://en.wikipedia.org/wiki/Literate_programming"
          },
          {
            "type": "text",
            "value": " "
          },
          {
            "children": [
              {
                "type": "text",
                "value": "Knuth"
              }
            ],
            "type": "link",
            "url": "https://en.wikipedia.org/wiki/Donald_Knuth"
          },
          {
            "type": "text",
            "value": " advocated. The LLM I've tried have used a chat interface. An documented and editing approach may be better suited. Using a chat approach the responses are usually presented in Markdown with embedded code blocks. This is analogous to a simplified version of what Knuth created using TeX with embedded code blocks. With the LLM your prompts trigger a narrative of code and function. That feels familiar to me. The challenges of working with an LLM is much like the challenges of literate programming. An LLM might encourage us to think more about what we're trying to do and perhaps more objectively vet the resulting code without engaging our egos."
          }
        ],
        "type": "paragraph"
      },
      {
        "children": [
          {
            "type": "text",
            "value": "A significant advantage of the LLM generated code is a byproduct of large training sets. The generated code tends to look average. It tends to avoid obfuscation. It mostly is reasonably reasonable. If the computing resources were significantly lower I'd feel more comfortable with this approach over the long run. The current state needs radical simplification. The LLM development environment is overly complex. That complexity only benefits consolidation in the cloud where we are rented the tools of our trade. LLM development environment's complexity and energy consumption weight heavily on me as I explore this approach. It is too early to tell if it should be a regular tool in the developers' toolbox of sustainable software practices."
          }
        ],
        "type": "paragraph"
      }
    ],
    "type": "root"
  },
  "data": {
    "abstract": "A first take of LLM use for coding projects.\n",
    "author": "R. S. Doiel",
    "byline": "R. S. Doiel, 2025-03-30",
    "dateCreated": "2025-03-30",
    "dateModified": "2025-03-30",
    "pubDate": "2025-03-30",
    "title": "LLM first impressions a few weeks in"
  }
}