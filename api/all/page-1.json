{
  "page": 1,
  "total_pages": 2,
  "has_more": true,
  "next_page": "all/page-2.json",
  "values": [
    {
      "content": "\n# Build your own static web server with Deno\n\nOne of things I have in my web toolbox is a static site web server. It only runs on localhost. It amazes me how often I wind up using it. PHP and Python can launch one easily from the command line but I have always found they were lacking. What I want is a simple web server that runs only on localhost. It can serve content from a specified directory and should handle common content types appropriately (e.g. JavaScript files are served as \"application/javascript\" not as \"text/plain\"). I should be able choose the port the server runs on. I should be able to specify a document root for the content I want to expose. It should default to a sensible location like the \"htdocs\" directory in my current working directory.\n\nWhen I started working with the web (when people used the NCSA web server), web servers were considered complex and hard to implement. I remember most network systems were presumed complex. Today most programming languages have some sort of library, module or package that makes implementing a web server trivial. This is true for JavaScript running under a JavaScript run time engine.\n\nDeno is a JavaScript and TypeScript runtime. I prefer Deno over other JavaScript runtimes like NodeJS. Deno runs sandboxed. This is similar to how the web browser treats JavaScript. Deno's standard library aligns with web browser implementation too. Deno has a good set of standard modules. Many modules can also be used browser side. \n\nI'll be using some Deno standard JavaScript modules in this post. The standard module \"@std/http/file-server\" provides most of what you need to implement a static content server. Two other modules will round things out in how I want my web server to behave. They are \"@std/fs/exists\" and \"@std/yaml/parse\".\n\nLet's build a simple but useful static web server and add it to our web toolbox.\n\nBefore I build my static web server I need some web content. I'm going to need an HTML file and a JavaScript file. This will provide content to test. The web content should be created in a directory called \"htdocs\". On macOS, Linux and Windows the command I run from in the terminal application to create the \"htdocs\" directory is `mkdir htdocs`. Using your my editor, I created the HTML file called \"helloworld.html\" inside the \"htdocs\" directory.\n\n~~~html\n<!DOCTYPE html>\n<head>\n  <script type=\"module\" src=\"helloworld.js\"></script>\n</head>\n<html lang=\"en-US\">\n  <body>Hello World</body>\n</html>\n~~~\n\nI created a \"helloworld.js\" inside the \"htdocs\" directory too.\n\n~~~JavaScript\nconst body = document.querySelector(\"body\");\nconst elem = document.createElement(\"div\");\nelem.innerText = 'Hello World 2!';\nbody.append(elem);\n~~~\n\nThis provides content to test my prototypes. Using these files I can make sure the prototype properly serves out a web page, handles a file listing and properly services the JavaScript.\n\nYour directory tree should look something like this.\n\n~~~shell\ntree htdocs\nhtdocs/\n├── helloworld.html\n└── helloworld.js\n\n1 directory, 2 files\n~~~\n\n## First prototype\n\nUsing a text editor, I create a file called `webserver_v1.js`. I need to do several things in JavaScript to build our static web server.\n\n1. import a function called `serveDir` from the \"@std/http/file-server\" module\n2. I need to set two constants, a port number and a root document path\n3. It is helpful to display the setting for the port and document root when the server starts up\n4. I can using Deno's built in `serve` method to handle inbound requests and then dispatch them to `serveDir`\n\nLet's start with the import, `\"@std/http/file-server`.  Notice that it starts with and \"@\". This indicates to the JavaScript runtime that the full URL to the module is defined by an import map. When you build a Deno project you can generate a file called `deno.json`. It can include an import map. The `deno add` command provides a really easy way to manage this mapping. As of Deno 2 the standard modules are available from [jsr.io](https://jsr.io), a reliable JavaScript registry. This includes our standard module `@std/http/file-server`. I can \"add\" it  to my project using the following command.\n\n~~~shell\ndeno add jsr:@std/http/file-server\n~~~\n\nIf the \"deno.json\" file does not exist this command will create it. If it does exist Deno will update it to reflect the new module. I can look inside the \"deno.json\" file after running this command and see my import map.\n\n~~~json\n{\n  \"imports\": {\n    \"@std/http\": \"jsr:@std/http@^1.0.18\"\n  }\n}\n~~~\n\nThe Deno runtime knows how to contact jsr.io and use it to retrieve the module requested.  By default it picks the current stable version. In my case that is v1.0.18. Deno updates happen pretty steadily through out the year. When I try this a month from now it'll probably be a different version number.\n\nNow that Deno is setup, I need to write my first prototype static web server.\n\n~~~JavaScript\n/**\n * webserver_v1.js - A simple static file server for serving files from the \"htdocs\" directory.\n */\nimport { serveDir } from \"@std/http/file-server\";\n\nconst port = 8000;\nconst rootPath = \"htdocs\"; // Serve files from the \"htdocs\" directory\n\nconsole.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);\n\n// Start a simple server\nDeno.serve({\n  port,\n}, async (req) => {\n  try {\n    // Serve files from the specified directory\n    return await serveDir(req, {\n      fsRoot: rootPath,\n      urlRoot: \"\",\n      showDirListing: true,\n      showDotfiles: false, // Exclude files starting with a period\n    });\n  } catch (err) {\n    console.error(err);\n    // Return a 404 response if something goes wrong\n    return new Response(\"404: Not Found\", { status: 404 });\n  }\n});\n~~~\n \nThe `Deno.serve` manages the inbound request and the async anonymous function handles the mapping to the file server module function called `serveDir`. A try catch wraps the `serveDir` function. If that function fails a 404 response is created and returned. Pretty simple.\n\nLet's see if the code we typed in works. Deno provides three helpful commands for working with your program code \n\n1. check \n2. lint\n3. fmt\n\nCheck reads the JavaScript (or TypeScript) file and makes sure it makes sense from the compilation point of view.  The lint command goes a step further. It checks to see if best practices have been followed. Lint is completely optional but check needs to pass before Deno will attempt to run or compile the program. The `fmt` command will format your source code in a standard way. I'm going to use check and lint.\n\n~~~shell\ndeno check webserver_v1.js\ndeno lint webserver_v1.js\n~~~\n\nAll went well. In both cases I see a line indicating it checked the file. If I had made errors check and lint would have complained and included lines describing errors.\n\nDeno can run our JavaScript and TypeScript files. To test my program I try the following. \n\n~~~shell\ndeno run webserver_v1.js\n~~~\n\nWhen I tried this I saw the following message. \n\n~~~shell\nServer running on http://localhost:8000/, serving htdocs\n┏ ⚠️  Deno requests net access to \"0.0.0.0:8000\".\n┠─ Requested by `Deno.listen()` API.\n┠─ To see a stack trace for this prompt, set the DENO_TRACE_PERMISSIONS environmental variable.\n┠─ Learn more at: https://docs.deno.com/go/--allow-net\n┠─ Run again with --allow-net to bypass this prompt.\n┗ Allow? [y/n/A] (y = yes, allow; n = no, deny; A = allow all net permissions) > \n~~~\n\nI type \"y\" and press enter. New lines appear.\n\n~~~shell\nServer running on http://localhost:8000/, serving htdocs\n✅ Granted net access to \"0.0.0.0:8000\".\nListening on http://0.0.0.0:8000/ (http://localhost:8000/)\n~~~\n\nI point my web browser to \"http://localhost:8000/\". Do I see anything? No. In my terminal window I see another prompt about permissions.\n\n~~~shell\n┏ ⚠️  Deno requests read access to \"htdocs\".\n┠─ Requested by `Deno.stat()` API.\n┠─ To see a stack trace for this prompt, set the DENO_TRACE_PERMISSIONS environmental variable.\n┠─ Learn more at: https://docs.deno.com/go/--allow-read\n┠─ Run again with --allow-read to bypass this prompt.\n┗ Allow? [y/n/A] (y = yes, allow; n = no, deny; A = allow all read permissions)\n~~~\n\nAgain answer \"y\". I then see something this in my terminal window.\n\n~~~shell\n[2025-06-30 16:27:31] [GET] / 200\nNo such file or directory (os error 2): stat '/Users/rsdoiel/Sandbox/Writing/Books/A_Simple_Web/htdocs/favicon.ico'\n[2025-06-30 16:27:31] [GET] /favicon.ico 404\n~~~\n\nI reload my web browser page, what do I see? A list of files. I know that file directory listing works.\nOne of the files is \"helloworld.html\".  I click on it. I my simple web page with the words \"Hello World\" and \"Hello World 2\". Yippee, I've created a static web server.\n\nYou might be wondering how I shutdown the web server. In the terminal window I press control and the letter c, aka \"Ctrl-C\". This will shuts down the web server. I can confirm it is shutdown in the web browser by reloading the page. I see an connection error page now.\n\nI don't want to answer questions about permissions each time I run my prototype. I can specify the permissions I want to grant on the command line.  I know from my test that my program needs \"net\" and \"read\" permissions. I can grant this using the following command.\n\n~~~shell\ndeno run --allow-net --allow-read webserver_v1.js\n~~~\n\nBetter yet I can compile our JavaScript program into an executable file. An executable is handy because I can run it without Deno being installed on a different computer as long as it runs the same operating system and has the same CPU type. Compiling to an executable makes this prototype similar to our tools in my web tool box. It let's me treat it just like my terminal application, text editor and web browser.\n\n~~~shell\ndeno compile --allow-net --allow-read webserver_v1.js\n~~~\n\nThis results in a file being created called \"webserver_v1\" (or on Windows, \"webserver_v1.exe\"). This file can be run from this directory or moved to another directory where I store other programs (e.g. `$HOME/bin` or `$HOME\\bin` on Windows).\n\n## Improving on v1\n\nWhile webserver_v1.js is helpful it could be more friendly. What if I want to use a different port number? What if I want to server out content my current directory or maybe I want to service content on a different mounted drive? I can do that by adding support for command line arguments.\n\n~~~JavaScript\n/**\n * webserver_v2.js - A simple static file server with configurable port and root directory.\n */\nimport { serveDir } from \"@std/http/file-server\";\n\nconst defaultPort = 8000;\nconst defaultRoot = \"htdocs\";\n\n// Parse command-line arguments\nconst args = Deno.args;\nlet rootPath = defaultRoot;\nlet port = defaultPort;\n\n// Check the command arguments and set the port and rootPath appropriately\nif (args.length > 0) {\n  // Check if the first argument is a port number\n  const portArg = parseInt(args[0], 10);\n  if (!isNaN(portArg)) {\n    port = portArg;\n  } else {\n    // If not a port number, assume it's the root path\n    rootPath = args[0];\n  }\n\n  // Check if the second argument is a root path\n  if (args.length > 1) {\n    rootPath = args[1];\n  }\n}\n\nconsole.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);\n\n// Start a simple server\nDeno.serve({\n  port,\n}, async (req) => {\n  try {\n    // Serve files from the specified directory\n    return await serveDir(req, {\n      fsRoot: rootPath,\n      urlRoot: \"\",\n      showDirListing: true,\n      showDotfiles: false, // Exclude files starting with a period\n    });\n  } catch (err) {\n    console.error(err);\n    // Return a 404 response if something goes wrong\n    return new Response(\"404: Not Found\", { status: 404 });\n  }\n});\n~~~\n\nI can compile that using the following deno compile command\n\n~~~shell\ndeno compile --allow-net --allow-read webserver_v2.js\n~~~\n\nWe can run the new webserver using the following command.\n\n~~~shell\n./webserver_v2 8001 .\n~~~\n\nPoint the web browser at <http://localhost:8001>. What do I see the directory? Yep, I see the files in my root directory of my project including the \"htdocs\" directory I created. Can I find and display \"helloworld.html\"? Yep and it works as in the first prototype. I shutdown the web server and then start it again using just the executable name.\n\nmacOS and Linux\n\n~~~shell\n./webserver_v2\n~~~\n\non Windows\n\n~~~shell\n.\\webserver_v2\n~~~\n\nWhat do you see? Can you find \"helloworld.html\"? Stop the web server. I copy \"helloworld.html\" to \"index.html\". After copying I restart the web server again.\n\nOn macOS and Linux\n\n~~~shell\ncp htdocs/helloworld.html htdocs/index.html\n./webserver_v2\n~~~\n\nOn Windows\n\n~~~pwsh\ncopy htdocs\\helloworld.html htdocs\\index.html\n.\\webserver_v2\n~~~\n\nI point the web browser at <http://localhost:8000>, what do I see? I don't see the file directory any more, I see the contents of  I copied into the \"index.html\" file, \"Hello World\" and \"Hello World 2\".\n\nCan this be improved?  It'd be nice to web able to just type \"webserver_v2\" and have the program using a default port and htdocs directory of my choice. That can be supported by using a configuration file. YAML is an easy to read and easy to type notation. It even supports comments which is nice in configuration files. YAML expresses the same types of data structures as JSON (JavaScript Object Notation). Below an example of a configuration file. I type it in and save it using the filename \"webserver.yaml\".\n\n~~~yaml\n# Set root path for web content to the current directory.\nhtdocs: .\n# Set the port number to listen on to 8002\nport: 8002\n~~~\n\nFrom the point of the view of my prototype it'll need to check if the \"webserver.yaml\" file exists before attempting to read it. Deno has a module for that. It'll also need to read the YAML, parse it and get an object that exposes my preferred settings. Deno has a standard model for working with YAML too. The modules I'm interested in are `@std/fs/exists` and `@std/yaml`. I'll need to \"add\" them to my deno project.\n\n~~~shell\ndeno add jsr:@std/fs/exists\ndeno add jsr:@std/yaml\n~~~\n\nTime for an improved version of the static web server. This prototype should be called, \"webserver_v3.js\".\n\n~~~JavaScript\n/**\n * webserver_v3.js - A simple static file server with configurable port and root directory via YAML.\n */\nimport { serveDir } from \"@std/http/file-server\";\nimport { parse } from \"@std/yaml/parse\";\nimport { exists } from \"@std/fs/exists\";\n\nconst defaultPort = 8000;\nconst defaultRoot = \"htdocs\";\n\n// Function to read and parse YAML configuration file\nasync function readConfigFile(filePath) {\n  try {\n    const fileContent = await Deno.readTextFile(filePath);\n    return parse(fileContent);\n  } catch (err) {\n    console.error(\"Error reading or parsing YAML file:\", err);\n    return null;\n  }\n}\n\n// Parse command-line arguments\nconst args = Deno.args;\nlet rootPath = defaultRoot;\nlet port = defaultPort;\n\nif (args.length > 0) {\n  // Check if the first argument is a port number\n  const portArg = parseInt(args[0], 10);\n  if (!isNaN(portArg)) {\n    port = portArg;\n  } else {\n    // If not a port number, assume it's the root path\n    rootPath = args[0];\n  }\n\n  // Check if the second argument is a root path\n  if (args.length > 1) {\n    rootPath = args[1];\n  }\n} else {\n  // Check for YAML configuration file\n  const configFilePath = \"webserver.yaml\";\n  if (await exists(configFilePath)) {\n    const config = await readConfigFile(configFilePath);\n    if (config) {\n      rootPath = config.htdocs || defaultRoot;\n      port = config.port || defaultPort;\n    }\n  }\n}\n\nconsole.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);\n\n// Start a simple server\nDeno.serve({\n  port,\n}, async (req) => {\n  try {\n    // Serve files from the specified directory\n    return await serveDir(req, {\n      fsRoot: rootPath,\n      urlRoot: \"\",\n      showDirListing: true,\n      showDotfiles: false, // Exclude files starting with a period\n    });\n  } catch (err) {\n    console.error(err);\n    // Return a 404 response if something goes wrong\n    return new Response(\"404: Not Found\", { status: 404 });\n  }\n});\n~~~\n\nLike before I compile it with the my desired permissions.\n\n~~~shell\ndeno compile --allow-net --allow-read webserver_v3.js\n./webserver_v3\n~~~\n\nI point the web browser at <http://localhost:8002>. What do I see? I see the contents of the index.html file. Can I display \"helloworld.html\" too? Yep. I remove the \"index.html\" file, then use my browser back button to go to the initial URL, yep I see a file directory listing again. Looks like this prototype works.\n\nI think I have a useful localhost static content web server. It's time to rename my working prototype, compile and install it so it is available in my toolbox.\n\n1. Copy `webserver_v3.js` to `webserver.js` \n2. Use `deno compile` to create an executable\n3. Create a \"$HOME/bin\" directory if necessary\n4. Move the executable to a location in the executable PATH with, example \"$HOME/bin\"\n5. Try running the program\n\nNOTE: If you are following along and have to create \"$HOME/bin\" then you may need to added to your environment's PATH.\n\nOn macOS and Linux\n\n~~~shell\ncp webserver_v3.js webserver.js\ndeno compile --allow-net --allow-read webserver.js\nmkdir -p $HOME/bin\nmv ./webserver $HOME/bin\nwebserver\n~~~\n\nOn Windows\n\n~~~shell\ncopy webserver_v3.js webserver.js\ndeno install --global --allow-net --allow-read webserver.js\nNew-Item -ItemType Directory -Path \"$HOME\\bin\" -Force\nmove webserver.exe $HOME\\bin\\\nwebserver\n~~~\n\nThere you have it. I have a new convenient static web server for serving static content on localhost.\n\n",
      "data": {
        "abstract": "This post discusses static web server implementation using Deno.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2025, R. S. Doiel",
        "dateCreated": "2025-06-30",
        "dateModified": "2025-07-01",
        "keywords": [
          "web service",
          "static web site",
          "JavaScript"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 8,
        "pubDate": "2025-06-30",
        "series": "Deno and TypeScript",
        "title": "Build a Static Web Server with Deno"
      },
      "url": "posts/2025/06/30/Build_a_Static_Web_Server.json"
    },
    {
      "content": "\n# Rethinking REST\n\n## How to embrace the read write abstraction using SQL databases\n\nBy R. S. Doiel, 2025-06-07\n\n[Roy Fielding](https://en.wikipedia.org/wiki/Roy_Fielding)'s 2000 dissertation describing [REST](https://en.wikipedia.org/wiki/REST) is a brilliant work. It revolutionized web services. I've spent a good chunk of my career implementing back end systems using a REST approach. REST's superpower is the mapping of HTTP methods to core database operations of create (POST), read (GET), update (PUT), and delete (DELETE). The has simplified machine to machine communication. That is a good thing.\n\nREST has a browser problem. A quarter century after Fielding presented REST, the web browser still requires JavaScript to talk directly to a REST service. The core problem is the REST methods are not defined in the semantics of HTML. They are only available in HTTP protocol layer. JavaScript plays the role of solving the mapping of actions to REST methods. I can program over that impedance server side, browser side or both. The penalty is increased complexity. I think this complexity unnecessarily.\n\n**What abstraction aligns with the grain of both web service and web browser?**\n\nSir Tim invented HTTP and HTML on a NeXT cube. The NeXT cube was a Unix system like the systems used by the Physicists at CERN where Sir Tim was employed. From Unix you can trace the concept of \"everything is a file\". File interaction can be boiled down to reads and writes. A second influence was the practice of using plain text to encode data. These characteristics influenced Sir Tim's choices when he invented HTTP and HTML. These characteristics inform the grain of the modern web.\n\nWhat are the challenges of building on a read write abstraction rather than the database abstraction of create, read, update and delete? Do we toss out the database completely? That would be a too high of a cost.  Databases solve some important problems. This includes managing concurrent access, data protections and versatile query support. Database are the right choice in most cases for web applications. **So how do I get to a read write (RW) abstraction? The database wants create, read, update and delete (CRUD)?**\n\nThe short answer is we already do it. It's just messy. Typically we do this server side. It can be implemented browser side using JavaScript. Sometimes both places. We may layer that step as a micro service or embedded in some monolithic monstrosity. It's there someplace. It doesn't need to be a mess.\n\nLet's consider that for a moment. Server side the web service receives a request containing web form data. The service decodes the web form, hopefully validates the contents, then figures out if it is a \"create\" or \"update\" in the database system before attempting either an `insert` or `update` operation. The database schema usually reflects the form data. If the form has repeating fields then you might have more than one table and need to maintain relationships between the tables. This can quickly become complex.\n\nServer side this complexity was answered via object relational models (ORM).  Browser side we've seen similar approaches to the ORM in the development of frameworks that \"bind\" data in to an object model that can be sent to a back end system (often a REST API). The problem with both the server side ORM and browser side data binding frameworks is they tend to add allot of complexity. Ultimately they wind up dictating the approach you take to solve problems. Over time the frameworks become more complex too as they try to be a generalize solution to complex schema implementations. This accrues another source of complexity. The price of either becomes loss of flexibility, loss of performances and often deep levels of knowledge about the framework or ORM.  The longer lived your application is the more likely that this will not end well. I believe we can avoid this by taking stock of where database systems and the web have evolved since 2000.\n\n\n**What am I proposing?**\n\nLet's look at the deepest layer in our stack, the relational database. Several changes have happen on the database side that I think can help us build web application aligned with the read write abstraction core to our web browsers.  The first is a concept called upsert. Upsert is the idea of combining the behavior of `insert` and `update` into one operation. The upsert gives us our write operation.\n\nWhat about the mapping of a web form's data?  The second change in relational database world is the wide adoption of JSON column support. We can treat web form contents as a JSON expression. Modern SQL can query the JSON columns along with the other supported data types.\n\nA third changed was the arrival of SQLite in 2000. SQLite is SQL engine that does not require a separate database management system. Since 2000 SQLite has grown in usage. It now is used more commonly than Microsoft SQL server, Oracle, MySQL or PostgreSQL. The old requirement of using a stand alone database management system as part of the web stack has now turned into an option.\n\nSQLite3 provides support for both JSON columns and upsert. The upsert concept is implemented as an `on conflict` clause in your `insert` statement.  SQLite3 also support SQL triggers. Using the JSON column, upsert and triggers the SQLite3 database can handle the mapping of data as well as mapping our read write (RW) operations to the database CRUD operations. Better yet SQLite3 is an embedded SQL engine so you don't have to run a database management system at all. \n\nUse of JSON columns can radically simplify your JSON schema for many use cases. The model I am suggesting can be used to implement simple content management systems, metadata managers and form processor systems. Here's a table design suitable to many simple web applications.\n\n~~~SQL\nCREATE TABLE IF NOT EXISTS data (\n   id TEXT NOT NULL PRIMARY KEY,\n   src JSON DEFAULT NULL,\n   updated DATETIME DEFAULT CURRENT_TIMESTAMP,\n   version INT DEFAULT 0\n);\n~~~\n\nThe `id` holds a unique identifier like a file path does in a file system. The `src` column holds our JSON source. The `updated` column records the ISO-8601 timestamp of when your object is updated.  You might be wondering about `version` column and a missing `created` column. SQL can be used to automate data versioning and reduce create and update into a write operation. This is done by adding a second table. The scheme change in the second table from the first is how the primary key is defined.\n\n~~~SQL\nCREATE TABLE IF NOT EXISTS data_history (\n   id TEXT NOT NULL,\n   src JSON DEFAULT NULL,\n   updated DATETIME DEFAULT CURRENT_TIMESTAMP,\n   version INT DEFAULT 0,\n   PRIMARY KEY (id, version)\n);\n~~~\n\nThe SQL engine (SQLite3) does the actual version management using an SQL trigger. The \"on conflict\" of an insert triggers an \"update\" operation. The \"update\" action then triggers the `write_data` action before it completes.\n\nHere is how our upsert is implemented.\n\n~~~SQL\nINSERT INTO data (id, src) values (?, ?) \nON CONFLICT (id) DO\n  UPDATE SET src = excluded.src\n  WHERE excluded.id = id;\n~~~\n\nThe `write_data` trigger is responsible for two things. Inserts a new row into the `data_history` table using the the current row's values. Next it updates the `data` table's `version` number and `updated` timestamp automatically.\n\n~~~SQL\nCREATE TRIGGER write_data BEFORE UPDATE OF src ON data\nBEGIN\n  -- Now insert a new version into data_history.\n  INSERT INTO data_history (id,src, updated, version)\n    SELECT id, src, updated, version FROM data WHERE id =id; \n  -- Handle updating the updated timestamp and version number\n  UPDATE data SET updated = datetime(), version = version + 1\n    WHERE old.id = new.id;\nEND; \n~~~\n\n\nSo when I insert a new object there is no conflict so a simple insert is performed on the `data` table.  The row's version and `upgrade` columns get populated by the schema defaults. The next time the row is update it triggers the `write_data` operation where the row is recorded (copied) to the `data_history` table before being updated to reflect the changed values.\n\nHow do you find out when a record was created without a column called created?\n\nIn the follow SQL we perform a left join with the `data_history` table. We filter the history table for a row with the same id but a version number of 0. If a row is found then the value of `data_history.updated` will not be null. A `ifnull` function can be used to pick that value otherwise we use the `data.updated` value from `data` table. Here is how that SQL would look.\n\n~~~SQL\nSELECT data.id as id, \n  data.src as src,\n  data.updated as updated,\n  ifnull(data_history.updated, data.updated) as created,\n  data.version\nFROM data LEFT JOIN data_history ON\n  ((data.id = data_history.id) and (data_history.version = 0))\nWHERE data.id = ?;\n~~~\n\nThe complexity of mapping CRUD to RW is now completely contained in the SQL engine. While I have use SQLite3 for this specific example in practice these features are available in most modern relational database management systems. It's matter of knowing the specifics of the dialect.\n\nIsn't this a whole lot of SQL to write? Perhaps. By leveraging JSON columns the needs to elaborate on this SQL are minimal. Effectively these four statements can function like an SQL component. I think the investment is small. It solves a large class of web application storage needs.  You could even use a template to automatically generate them. Once written your can re-use them as needed.\n\n**Why did I focus on SQLite3?**\n\nBecause reducing the layers in our web stack reduces complexity. With SQLite3 we don't need database management system running. It's one less thing to manage, monitor and defend. In a cloud environment it can mean renting one less service.\n\n**What layers remain? What are their responsibilities?**\n\nIn 1999 web applications had a data management component, a user management component and an authentication and authorization component. The point of the application was the data management component. You were required to implement the others to keep the data safe while it was on the Internet.\n\nToday authentication and authorization can be handled by single sign-on systems. In the academic and research settings you typically see combinations like Apache2 + Shibboleth or NginX + Shibboleth. On the commercial Internet you see systems like OpenID and OAuth2. For a decade or more the systems I've designed and implemented take advantaged of single sign-on.  My application doesn't have to have a user management component or an authentication and authorization component at all.\n\nI do need a layer that validates the inputs and returns the resources requested. I usually implement this as a \"localhost\" web service that relies on the \"front end\" web service for authentication and authorization. If my layer uses SQLite3 for data storage then the \"stack\" is just a \"front end\" web server providing authentication and authorization and a \"back end\" persistence layer providing validation, storage and retrieval.\n\nAn advantage of this simple stack is I can develop, test and improve the localhost web service and know it'll plug into the front end when I am ready for a production deployment.  The front end deals in requests and responses, the back end deals in requests and responses. Meanwhile I have all the advantages of a SQL database on the \"back end\".\n\nAre there times I might need more layers?  Sure.  If I was managing millions of objects I would not store them in a single SQLite database.I'd use a database management system like PostgreSQL.  If I need a rich full text search engine I might use Solr or Open Search for that. If I am storing large objects then I might have a middle ware that can speak S3 protocol to store or retrieve those objects. My point is those are no longer a requirement. Extra layers or parallel services are now only options. They are available if and only if I need them.\n\nExample.  If I want to basic full text search, SQL databases have index types that support this.  SQLite3 is included there.\nBy leveraging SQL triggers I can extract data from my stored JSON column and populate full text search columns or even other tables as needed.I can get allot of the advantages of a full text search before I reach for an external system like Solr.\n\nSo here are my take way items for you.\n\n1. The web and databases continue to evolve.\n2. Take advantage of the improvements to simplify your code and your implementations\n3. Evaluate if you really need that heavy stack when you build your next application\n4. Use the simplest of abstractions that solve the problem required\n5. Consider a simple data interaction model like read write before you reach for REST\n\nEnjoy.\n",
      "data": {
        "abstract": "I am re-thinking my reliance on REST's implementation of the CRUD abstraction in favor of the simpler\nread write file abstraction in my web application. This can be accomplished in SQL easily. This post\ncovers an example of doing this in SQLite3 while also implementing JSON object versioning.\n\nCoverted are implenting the write abstraction using an upsert operation based on `insert` and SQLite3's\n`on conflict` clause. The object versioning is implemented using a simple trigger on the JSON column.\nThe trigger maintains the version number and updated timestamp.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2025, R. S. Doiel",
        "dateCreated": "2025-05-31",
        "dateModified": "2025-06-09",
        "keywords": [
          "web service",
          "web applications",
          "web browsers",
          "REST",
          "read write web",
          "SQL",
          "SQLite3"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 7,
        "pubDate": "2025-06-08",
        "series": "SQL Reflections",
        "title": "Rethinking REST"
      },
      "url": "posts/2025/06/07/Rethinking-REST.json"
    },
    {
      "content": "\n# PowerShell and Edit on Windows, macOS and Linux\n\nBy R. S. Doiel, 2025-06-05\n\nOne of the challenges of multi platform support is the variance in tools. Unix and related operating systems are pretty unified these days. The differences are minor today as opposed to twenty years ago. If you need to support Windows too it's a whole different story. You can jump to Linux Subsystem for Windows but that is really like using a container inside Windows and doesn't solve the problem when you need to work across the whole system. \n\nWindows' shell experience is varied. Originally it was command com, essentially a enhanced CP/M shell. Much later as Windows moved beyond then replaced MS-DOS they invented PowerShell. Initially a Windows only system. Fast forward today things have change. PowerShell runs across Windows, macOS and Linux. It is even licensed under an MIT style license.\n\n- <https://github.com/PowerShell/PowerShell>\n\nPowerShell is intended as a system scripting language and as such is focused on the types of things you need too to manage a system. It has vague overtones of Java, .NET and F#. If you are familiar with those it probably feels familiar for me it wasn't familiar. Picking up PowerShell has boiled down to my thinking I can do X in Bash then doing a search to find out the equivalent in PowerShell 7 or above.  There are something examples out there that are Windows specific because there isn't a matching service under that other operating systems but if you focus on PowerShell itself rather than Windows particular feature it is very useful. It also means while you're picking up how Windows might approach something you can re-purpose that knowledge on the other operating systems. That's really handy for admin type tasks.\n\nOne of the things I've been playing with is creating a set of scripts that have a common name but deal with the specifics of the target OS. That way when I need to run a generalized task I can deploy the OS specific version to the platform but then start thinking about managing the heterogeneous environments in a unified way. E.g. scripts like \"require-reboot.ps1\", \"safe-to-reboot.ps1\", \"disk-is-used-by.ps1\".\n\nOnce you start getting serious about learning a system admin script language you also learn you need to vet the quality of your the scripts you are writing. On Unix I use a program called [shellcheck](https://www.shellcheck.net/) and [shfmt](https://github.com/patrickvane/shfmt) to format my scripts. How do you do that for Powershell?\n\nRecently I discovered recently is PSScriptAnalyzer. Like shellcheck it will perform static analysis on your script and let you know of lurking issues to be aware of. The beauty of it is I can evaluate a script in PowerShell on macOS and know that I've caught issues that would have pinched me if I ran it Linux or Windows.  That's kinds of sweet.\n\nYou need to [install PSScriptAnalyzer](https://learn.microsoft.com/en-us/powershell/utility-modules/psscriptanalyzer/overview?view=ps-modules) but it is easy to do under PowerShell.\n\n~~~pwsh\nInstall-Module -Name PSScriptAnalyzer -Force\n~~~\n\nor\n\n~~~pwsh\nInstall-PSResource -Name PSScriptAnalyzer -Reinstall\n~~~\n\nIf I want [run the analyzer](https://learn.microsoft.com/en-us/powershell/utility-modules/psscriptanalyzer/using-scriptanalyzer?view=ps-modules&source=recommendations) on a script called `installer.ps1` I'd run lit like\n\n~~~psh\nInvoke-ScriptAnalyzer -Path ./installer.ps1 -Settings PSGallery -Recurse\n~~~\n\nFormatting PowerShell scripts I am currently testing out [PowerShell-Beautifier](https://github.com/DTW-DanWard/PowerShell-Beautifier). It's a \"cmdlet\" and an easy install into PSGallery following the instructions in the GitHub repo.\n\nHere's an example of formatting the previous example so it uses tabs instead of spaces.\n\n~~~pwsh\nEdit-DTWBeautifyScript ./installer.ps1 -IndentType Tabs\n~~~\n\n# Now that I got a shell running across systems, what about an editor?\n\nMS-DOS acquired an editor called \"edit\" at some point in time (version 4 or 5?).  I remember it was a simple full screen non model editor. Recently Microsoft has created a similar editor that runs in a terminal on Windows and Linux. Like PowerShell it arrived as an Open Source Project. You don't mind some rough edges I have compiled it successfully on macOS. A few features are not implemented but it can open and save files.  It builds with the latest Rust (e.g. run `rustup update` if you haven't in a while) and generates an installable executable using Cargo. While I'm not likely to switch editors it's nice to have one that is really cross platform in doesn't require odd Unix adapter libraries to be installed to compile it. It's just a Rust program so like Go the build process is pretty consistent when you compile on Windows, Linux or macOS.\n\n- <https://github.com/Microsoft/edit>\n\nSo you have an administrative shell environment and a common editor. If you happen to have to document admin chores across systems at least now you can document one admin language and editor and know it'll run if installed.\n",
      "data": {
        "author": "R. S. Doiel",
        "dateCreated": "2025-06-05",
        "keywords": [
          "Windows",
          "macOS",
          "Linux"
        ],
        "pubDate": "2025-06-05",
        "title": "PowerShell and Edit for macOS, Linux and Windows"
      },
      "url": "posts/2025/06/05/PowerShell_and_Edit.json"
    },
    {
      "content": "\n# A quick note on types in Deno+TypeScript\n\nUnderstanding the plumbing of a program that is built with Deno in TypeScript can be challenging if you can't identify the type of variables or constants.  TypeScript inherits the JavaScript function, `typeof`. This works nicely for simple types like `string`, `boolean`, `number` but is  less useful when compared to a class or interface name of a data structure.\n\nThere are three approaches I've found helpful in my exploration of type metadata when working with Deno+TypeScript. (NOTE: in the following\nthe value `VARIABLE_OR_CONSTANT` would be replaced with the object you are querying for type metadata)\n\n`typeof`\n: This is good for simple types but when a type is an object you get `[object object]` response.\n\n`Object.protototype.toString.call(VARIABLE_OR_CONSTANT)`\n: This is what is behind the `typeof` function but can be nice to know. It returns the string representation of the `VARIABLE_OR_CONSTANT` you pass to it.\n\n`VARIABLE_OR_CONSTANT.constructor.name`\n: This will give you the name derived from the object's constructor, effectively the class name. It doesn't tell you if the the `VARIABLE_OR_CONSTANT` is an interface. If you construct an object as an object literal then the name returned will be `Object`.\n\nHere's the three types in action.\n\n~~~TypeScript\n  let fp = await Deno.open('README.md');\n  console.log(typeof(fp));\n  console.log(Object.prototype.toString.call(fp);\n  console.log(fp.constructor.name);\n  await fp.close()\n  \n  let t = { \"one\": 1 };\n  console.log(typeof(t));\n  console.log(Object.prototype.toString.call(t);\n  console.log(t.constructor.name);\n~~~\n\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-05-25",
        "keywords": [
          "TypeScript",
          "Deno"
        ],
        "pubDate": "2025-05-25",
        "series": "Deno+TypeScript",
        "title": "A quick note on types in Deno+TypeScript"
      },
      "url": "posts/2025/05/25/a_quick_notes_on_types.json"
    },
    {
      "content": "\n# New Life for Fielded Searches\n\nBy R. S. Doiel, 2025-04-10\n\n[Simon Willison](https://simonwillison.net/2025/Apr/9/an-llm-query-understanding-service/) posted an article yesterday that caught my eye. He was pointing out how even small language models can be used to breakdown a search query into fielded tokens.  Some of the earliest search engine, way before Google's one box, search engines were built on SQL databases. Full text search was tricky. Eventually full text search become a distinct service, e.g. [Solr](https://solr.apache.org). The full text search engine enabled the simple search to become the expected way to handle search. Later search engines like Google's use log analysis to improve this experience further. When you use a common search string like \"spelling of anaconda\", \"meaning of euphemism\", \"time in Hawaii\" these results are retrieved from a cache. The ones that are location/time sensitive can be handled by simple services that either populate the cache or return a result then populate the cache with a short expatriation time. Life in search land was good.  Then large language models hit the big time and the \"AI\" hyperbole cranked up to 11.\n\nThere has been flirtation with replacing full text engines or more venerable SQL databases with large language models.  There is a catch. Well many catches but let me focus on just one. The commercial large language models are a few years out of date. When you use a traditional search engine you expect the results to reflect recent changes. Take shopping a price from two years ago isn't has useful as today's price given the tariff fiasco. Assembling large language models takes allot of time, compute resources and energy. Updating them with today's changes isn't a quick process even if you can afford the computer and energy costs. So what do? One approach as been to allow the results of a large language model to have agency. Agency is to use a traditional search engine to get results.  They're serious challenges with this approach. These include performance, impact on other web services and of course security.\n\nWhat if the language model is used in the initial query evaluation stage?  This is what Simon's article is about. He points out that even a smaller language model can be used to successfully take a query string and break it down into a fielded JSON object. Let's call that a search object. The search object then can be run against a traditional full text search engine or a SQL engine. These of course can be provided locally on your own servers.  [Ollama](https://ollama.app) provides an easy JSON API on localhost that can be used as part of your query parsing stack. This may be leveling especially if your organization has collection specific content to search (e.g. a library, archive or museum).\n\nConstructing an language model enable stack could look something like this.\n\n1. front end web service (accepts the queries)\n2. Ollama is used to turn the raw query string into a JSON search object\n3. The JSON search object is then sent to your full text search engine or SQL databases to gather results\n4. results are formatted and returned to the browser.\n\nThe key point in Simon Willison's post is that you can use a smaller language model. This means you don't need more hardware to add the Ollama integration. You can shoe horn it into your existing infrastructure. \n\nThis pipeline is a simple to construct.  This trick part is finding the right model and evaluating the results and deciding when the LLM translation to a JSON search object is good enough. Worst case is the original query string can still be passed off to your full text engine. So far so good. A slightly more complex search stack with hopefully improved usefulness.\n\n## a few steps down the rabbit hole\n\nWhere I think things become interesting is when you consider where search processing can happen. In the old days the whole stack had to be on the server. Today that's isn't true.  The LLM piece might still be best running server side but the full text search engine can be provided along with your statically hosted website. You can even integrate with a statically hosted JSON API. In light of that let's revisit my sequence.\n\n1. Front end web service response with a search page to the browser\n2. Browser evaluates the search page, gets the query string\n3. The browser then sends it to the Ollama web service that is returns a JSON search object (fielded object)\n4. The browser applies the object to our static JSON API calculating some results, it also runs query string through the static site search getting results\n5. results are merged and displayed in the browser\n\nSo you might be wonder about the static site search (browser side search) and JSON API I mentioned. For static site search I've found [PageFind](https://pagefind.app) to be really handy. For the JSON API I'd go with [FlatLake](https://flatlake.app). The two eliminate much of what used to be required from dynamic websites like those provided by Drupal and WordPress. A key observation here is that your query string only leaves the browser once in order to get back the language model result. This is a step toward a more private search but it doesn't pass the goal of avoiding log traces of searches.\n\nI first encounter browser side search solution with Oliver Nightingale's [Lunrjs](https://lunrjs.com). I switch to PageFind because Cloud Cannon had the clever idea to partition the indexes and leverage WASM for delivering the search. PageFind has meant providing full text search for more than 10K objects.\n\n**Could a PageFind approach work for migrating the language model service browser side?**\n\nIf the answer were yes, then would be a huge win for privacy. It would benefit libraries, archives and museums by allowing them to host rich search experiences while also taking advantage of the low cost and defensibilty of static site deployments.\n\n",
      "data": {
        "abstract": "A day dreaming in response to a Simon Willison post on using language models\nto convert queries into fielded searches. In this post I extrapolate how this\ncould result in a more private search experience and allow for an enhanced\nsearch experience for static websites.\n",
        "author": "R. S. Doiel",
        "dateCreated": "2025-04-10",
        "dateModified": "2025-04-10",
        "keywords": [
          "search",
          "LLM",
          "Ollama",
          "PageFind",
          "FlatLake"
        ],
        "number": 3,
        "pubDate": "2025-04-10",
        "series": "Personal Search Engine",
        "title": "New Life for Fielded Search"
      },
      "url": "posts/2025/04/10/New_Life_for_Fielded_Search.json"
    },
    {
      "content": "\n# LLM first impressions a few weeks in\n\nBy R.S. Doiel, 2025-03-30\n\nWriting code with an LLM is a far cry from what the hype cycle promises. It requires care. An attention to detail. It imposes significant compute resources. The compute resources requires a significant amount of electricity consumption. It doesn't bring speed of usage. Even cloud hosted LLM are slow beyond the first few iterations. If you want to find a happy medium between describing what you want and how you want it done, you need to commit to a non trivial amount of effort. Depending on your level of experience it may be faster to limit code generation to specific parts of a project. It maybe faster to code simple projects yourself.\n\nWhen I compare working with an LLMs like Gemma, Llama, Phi, Mistral, Chat-GPT to traditional [RAD](https://en.wikipedia.org/wiki/Rapid_application_development \"Rapid Application Development\") tools the shiny of the current \"AI\" hype is diminished. RAD tools often are easier to operate. They have been around so long we have forgotten about them. They use significantly less compute resources. RAD tools are venerable in 2025.\n\nThe computational overhead as well as the complexity of running an integrated LLM environment that supports [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation \"Retrieval Augmented Generation\"), [agency](https://en.wikipedia.org/wiki/Software_agent \"software agent explained\") and [tools](https://www.forbes.com/councils/forbestechcouncil/2025/03/27/your-essential-primer-on-large-language-model-agent-tools/ \"A Forbes article on tool use with large language models\") is much more than a simple code generator. A case in point. The best front end I've tried so for in my LLM experiments is [Msty.app](https://mysty.app). It takes a desperate set of services and presents a near turn-key solution. You don't even need to install [Ollama](https://ollama.com) as it ships with it. It checks all the boxes, RAG, agency and tools. But this simplicity only masks the complexity. Msty is a closed source solution. It is only offered on the x86 platform. The x86 computers are known for their energy consumption. So that's a downer. There are more energy efficient ARM and RISC-V solutions out there.  A Raspberry Pi 500 can run Ollama and should also be able to run Msty.app. But since Msty.app is closed source, I can't download and compile an ARM version myself. I can't compile a version for my Windows ARM machine either. That machine has much more RAM than the Pi. It even has more RAM than the x86 Windows box I run Msty on. Unfortunately I'm out of luck, the best I can do is running Msty.app under emulation. Not ideal. Scaling up with Msty.app means using a remote hosted LLM solution. How much energy am I saving when I run remote? When things go South the complexity costs in diagnosing problems with distributed systems is more than running things on a single box. I understand their business model.  Providing a Freemium version is a classic loss leader. People who like Msty will pay to upgrade to paid plans. From a big picture perspective it makes less sense to me.\n\n- How do we get more bang for the energy consumed?\n- Does the LLM enhanced coding environment push development forward?\n- Are LLM yet an appropriate tool in the toolbox of our profession?\n- Why am I continuing to explore using LLM for code development?\n\nA few weeks in I am looking for reasons to continue exploring LLM for code generation. I have a much better understanding of how it could fit into my approach to software development. I see reason to be hopeful but I also continue to have serious reservations.\n\nMy hands and wrists aren't the same as when I started in my career. I am hoping that an LLM can reduce my typing and reduce the risk of [RSI](https://en.wikipedia.org/wiki/Repetitive_strain_injury \"repetitive strain injury\"). I am also hoping that I can reduce the need for close reading and therefore eye strain. I'm currently experimenting with taking advantage of the slowness of the LLM eval cycle. Accessibility features in the OS will read back parts of the screen for me. Are these features worth burning down forests, floods and other climate driven issues? No they are not. Am I convinced that these LLM benefits will pan out? No. I'm hoping this aspect of LLM usage gets studied. I'm generally interested in tools that are a good ergonomic benefit for developers over the long haul. We still need a livable planet regardless.\n\nWithout using an LLM my normal development approach is to write out a summary of what I want to do in a project. I clarify the modules I want and the operations I need to perform. I code tests before coding the modules.  For many people this approach appears upside down. I've found this inverted approach yields better documentation. Writing the documentation clarifies my software architecture and how I will organize the required code. Writing tests first means I don't write more code than needed. The result is software I can look at a year or more later and still understand.\n\nThe first step in my current approach is well suited to adopting an LLM for code generation.  The LLM I've tried are decent at writing minimal test code. A good front end to Ollama or other LLM runner can automate some of the development cycle. The process is not fast. Once you get beyond the trivial the process is slow. It is slow like the compilers I used in my youth. It works OK for the most part. I can speed things up a little with faster hardware. That cost is increased electricity consumption. \n\nAn intriguing analog to developing with an LLM environment is [Literate programming](https://en.wikipedia.org/wiki/Literate_programming) advocated by [Knuth](https://en.wikipedia.org/wiki/Donald_Knuth). The LLM I've tried have used a chat interface. Even if the chat conversation is read back there is allot of duplicate text to listen to. Current chat responses are usually presented in Markdown with embedded code blocks. This is feels like a simplified version of Knuth's approach using TeX with embedded code blocks. It feels familiar to me. The Chat UI model presents many of the challenges I remember trying out literate programming. For some people a literate approach resulted in better code. I'm not sure it was true for everyone. For some an LLM might encourage more thinking as dialog can be useful that way. It might be easier to be more objectively when vetting because the ego isn't challenged in the generated code. It is hard to say with certainty.\n\nI have found one significant advantage of the LLM generated code. It is a byproduct of large training sets. The generated code tends to look average. It tends to avoid obfuscation. It mostly is reasonably readable. If the computing resources were significantly lower I'd feel more comfortable with this approach over the long run. The current state needs radical simplification. The LLM development environment is overly complex. If it is running on my hardware why does it need to be implemented as a web service at all? I think allot of the complexity is intentional. It benefits consolidation of tooling in the cloud. The cloud where we rent the tools of our trade. LLM development environment's complexity and energy consumption weight heavily on me as I explore this approach. It is too early to tell if it should be a regular tool in my toolbox. It is too early to know if it is useful for sustainable software practices.\n\n\n\n\n",
      "data": {
        "abstract": "A first take of LLM use for coding projects.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2025-03-30",
        "dateCreated": "2025-03-30",
        "dateModified": "2025-03-30",
        "pubDate": "2025-03-30",
        "title": "LLM first impressions a few weeks in"
      },
      "url": "posts/2025/03/30/LLM_first_impressions_a_few_weeks_in.json"
    },
    {
      "content": "\n# Building Web Components using Large Language Models\n\nBy R. S. Doiel, 2025-03-13\n\nIn March I started playing around with large language models to create a couple web components. I settled on a paid subscription for Mistral's Le Chat. You can see the results of my work project at <https://github.com/caltechlibrary/CL-web-compoments>. The release at or before \"v0.0.4\" were generated using Mistral's Le Chat.\n\nThe current state of LLM offered online do not replace a human programmer. I'm sure that is surprising to those who thing of this as a solved problem. I found that is was my experience as a developer that I could detect the problems in the generated code. It was also required in coming up with the right prompts to get the code I wanted. \n\nThe key to success of using an LLM for code generation is domain knowledge. You need domain knowledge of the problem you're solving by creating new software and domain knowledge of the machine or run time that the software will target.\n\nThe ticket to working with an LLM using a chat interface is your domain knowledge. That's how you know what to ask. That starting point is useful when using the LLM explore **widely documented** topics and approaches. The LLM is not good at inferring novel solutions but rather at using what it has been trained on.  That has been the key with the current crop of LLM I tried.\n\nI think using an LLM to generate code alongside a human client has potential. The human client brings subject knowledge. The human software developer brings more domain knowledge. Between the two they can guide the LLM in generating useful code. Its a little like training a literalist four year old that has the ability to type. I think this three way collaboration has possibilities. The LLM may prove helpful in bridging the human client and software developer divide.\n\n## my experiment proceeded\n\nWorking with a chat interface and LLM is a non-trivial iterative process. It can be frustratingly slow and pedantic. I often felt I took two steps forward then a step backward. I am unsure if I arrived at the desire code faster using the LLM. I am unsure the result was better quality software.\n\nI spent two weeks of working with a Mistral's Le Chat LLM on two web components. I am happy with the results at this stage of development. I am not certain the web components will continue to be developed with an LLM. My experience left me with questions about how LLM generated code will help or hurt sustainable software.\n\n## Generating web components\n\nMy experiment focused on two web components I needed for a work project. The first successfully completed component was called `AToZUL` in \"[a_to_z_ul.js](https://raw.githubusercontent.com/caltechlibrary/CL-web-components/refs/heads/main/a_to_z_ul.js)”. This web component is intended to wrap a simple UL list of links. It turns the wrapped UL into an A to Z list. Taking the wrapping approach of a native HTML element was my idea not the LLM's. I asked the LLM to implement a fallback but each LLM I tried this with inevitably relied on JavaScript. This fails when JavaScript is unavailable in the browser[^1]. How can the LLM do better than rehashing of the training data?\n\n[^1]: By providing a non JavaScript implementation I can interact with the web page using terminal based browser or using simple web scraping libraries.\n\nThe second web component successfully generated is called `CSVTextarea` in the \"[csvtextarea.js](https://raw.githubusercontent.com/caltechlibrary/CL-web-components/refs/heads/main/csvtextarea.js)\". This web component wraps a TEXTAREA that has CSV data in it's inner text. The web component presents a simple table for data entry. It features auto complete for columns using DATALIST elements. The `CSVTextarea` emits \"focused\" and \"changed\" events when cells receive focus or change. Additional methods are implemented to push data into the component or retrieving data from the component. Both web components include optional attributes \"css-href\" to include CSS that overrides the default that ships with the component.\n\nAt the start of March a spent a couple days working with the free versions of several LLM. I found Chat-GPT to be useless. I found Claude and Mistral to be promising. In all cases I found the \"free\" versions to be crippled for generating web components. I settled on a paid subscription to Mistral's Le Chat. The code generation was nearly as good as Claude but less expensive per month.\n\nOut side of work hours I tested code generation for several personal projects as well as code porting.  I was unhappy with the results for porting code from Go to Deno 2.x and TypeScript. The LLM are not trained on recent data. They seem to be about two years out. This includes the paid ones. As the result the best I could do was generate code for Deno 1.x. \n\nTypeScript and Go have some strong parallels. I'd previously hand ported code. I compared my code with the LLM results and was surprised at the deficits in the generated code. I think this boils down to the training sets used. \n\nOf all the programming tasks I tried the best results for code generation were targeting JavaScript running in a web browser. This isn't surprising as the LLM likely trained on web data.\n\n## My development setup for the experiments\n\nI initially tried VSCode and CoPilot. For me it us annoying and highly unproductive[^2]. My reaction certainly is a reflection on my background. I prefer minimal IDE. I am happy with an editor that is limited to syntax highlighting, auto indent and line numbering. When I use VSCode I turn most features off. Your mileage may very with your preferences.\n\n[^2]: It was annoying enough that I initially dismissed using an LLM. My colleague, Tommy, however encouraged me to give it a more serious try. If you lived through the \"editor wars\" of the early ARPAnet get ready for them to reignite. We're going to see allot of organizations claiming superior dev setups for integrating LLM into IDE.\n\nI ran my tests using a terminal app, a text editor, a localhost web server and Firefox. The log output of the web server was available in one terminal window and another for my text editor. One browser tab was open to Mistral's Le Chat. The other tabs open to the HTML pages I used for testing the results of the generated code. It required a fair amount of cut and paste which is tedious. This is far from an ideal setup.\n\nI looked at [Ollama](https://ollama.com) to see about running the LLM from the command line.  Long run this seems like a better approach for me. Unfortunately to use Mistral.ai's trained models I would need to purchase a more expensive subscription. The price point is roughly the same as Anthropic's Claude, a closed sourced option. For now I am sticking with cut and paste.\n\nUsing Ollama there is the possibility of using Mistral's open source models and training them further on data I curate. At some point I'll give it a try. I remain concerned about the electric bill. If collectively we're going to run these systems they will need clean alternative sources of electricity. Otherwise we are in for an environmental impact like we saw with BitCoin and Block-Chains. I think this is a major problem in the LLM space. I don't think we can ignore it even when the \"AI\" hype cycle is hand waving it away.\n\nI liked using Ollama to test free models to understand their differences. I recommend this as an option if you are working from macOS or Windows. The quality of generated code varies considerably between models. It is also true that the speed and processing requirements varies considerably between models. I am sure this is why hardware vendors think they will be able to sell hardware with \"AI Chips\" built in. I'm skeptical.\n\nI think small language models targeting specific domains could really improve the use case for language models generating code. They could shine for specific tasks or programming languages. They might be reasonable to run on embedded platforms too. I think small language models are an overlooked area in the current \"AI\" hype cycle.\n\n## What I took away from this experiment\n\nI think a good front end developer[^3] could find an LLM useful as an \"assistant\". I think a novice developer will easily be lead astray. As a community of practitioners we should guard against the bias, \"computer must be correct\". This is not new. It happened when \"expert systems\" were the rage before the first AI winter. It'll be an easy trap to fall into for the public.\n\n[^3]: I am not a front end developer. I have spent most of my career writing back end systems and middleware.\n\nI have a great deal of concern about compromised training data. There is so much mischief possible. It has already been established that \"poisoning\" the LLM via training data and prompts will result in generating dubious code[^4]. I don't see much attention paid to the security and provenance of training data, let alone good tools to vet the generated code. This is a security bomb waiting to explode.\n\n[^4]: See [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1). Think of the decades waste on SQL injections then multiply that by magnitudes as people come to trust the results of LLM to build really complex things. \n\nToday's software systems are really complex. Reproducibility has become a requirement in mitigating the problem. This is why you've seen a rise in virtual environments. The LLM itself doesn't improve this situation. I've used the same text prompts with the same LLM but different chat session and gotten significantly different results. The LLM as presently implemented exacerbate the problems of reproducibility. \n\nTo an certain extent we can strengthen our efforts around quality assurance. The trouble I've found is this too is a domain where LLMs are being applied. If the quality assurance LLM is tainted we don't get the assurance we need. There also is the very human problem of typos in our prompts. That's a very deep rabbit hole by itself.\n\n## Lessons learned\n\nI got the best results by composing a long specification as an initial state prompt to kick off code generation. I still needed to fix bugs using short prompts interactively. With the CSVTextarea I threw away five versions before I got to something useful. Each chat session lasted a couple hours. They were spread out over many days.\n\nThere was a clear point when additional prompts don't improve the results of generated code. I found three cases where I lead the LLM down a rabbit hole.\n\n1. the terms I used weren't what the LLM was trained on so it couldn't respond the way I wanted\n2. the visible results, e.g. the web component failing to render, doesn't indicate the underlying problem, this leads to prompt churn\n3. their was a subtle assumption in the generated code I didn't pick up and correct early on\n\nA beneficial result of using an LLM to generate code is that it encourages reading the source. Reading code is generally not taught enough. You're going to be reading allot of code using the current crop of commercially available LLM. That is a good thing in my book.\n\nTo solve the rabbit hole problem I adopted the following practices. Keep all my prompts in a Markdown document, along with notes to myself. When I felt the LLM had gone down a rabbit whole have the LLM generate a \"comprehensive detailed specification\" of the code generated. Compare the LLM specification against my own prompts helped simplify things when starting over.\n\nBe willing to throw away the LLM results and start over. This is important in exploratory programming but also when you're using LLM generated code to solve a known problem. If you can tolerate the process of writing and refining the prompts in your native language the LLM will happily attempt to generate code for them. I'd love to get some English and Philosophy graduates using LLM for code generation. It'd be interesting to see how their skills may out perform those of traditionally educated software engineering graduates. I think the humanities fields that could benefit from a quantitative approach may find LLM to generate code to do analysis really compelling.\n\nWhile I like the results I got for this specific tests I remain on the fence about **general usefulness** of LLM in the area of code generation. I suspect it'll take time before the shared knowledge and practice in using them emerges. There is also the problem of energy consumption.  This feels like the whole \"proof of work\" problem consuming massive amounts of electricity in the block chain tech. That alone was enough to turn me off of block chain for most of its proposed applications. Hopefully alternatives will be developed to avoid that outcome with large language models.\n\n",
      "data": {
        "abstract": "Quick discussion of my recent experience bootstrapping the CL-web-components project\n",
        "author": "Doiel, R. S.",
        "byline": "R. S. Doiel",
        "dateCreated": "2025-03-13",
        "keywords": [
          "LLM",
          "Web Components",
          "JavaScript"
        ],
        "number": 2,
        "pubDate": "2025-03-13",
        "series": "Code Generation",
        "title": "Building Web Components using Large Language Models"
      },
      "url": "posts/2025/03/13/Building_Web_Component_using_an_LLM.json"
    },
    {
      "content": "\n# Moving beyond git template repositories with CodeMeta\n\nBy R. S. Doiel, 2025-01-31 (updated: 2025-02-03)\n\nA nice feature of GitHub is the ease in starting a new repository with a complete set of documents[^1]. This feature creates a problem. The \"template\" documents require editing. Then they require more editing to keep them from being stale. If you're serious about keeping documentation up to date then the copy edit work must be continuous. Copy edit work is tedious. Is there a path beyond the git repository templates that avoid stale [software artifacts](https://en.wikipedia.org/wiki/Artifact_(software_development))?\n\n[^1]: The feature is built around using another Git repository as a template. GitHub has a nice UI for this, essentially it is forking the \"template repository\" when you create your new repository.\n\nExisting development tools suggestion a solution. For decades software developers have had tools to extract documentation from the comments in source code.  [Knuth](https://en.wikipedia.org/wiki/Donald_Knuth) pioneered this with his [literate programming](https://en.wikipedia.org/wiki/Literate_programming) approach. A simplified take on this is [Javadoc](https://en.wikipedia.org/wiki/Javadoc) used in the Java programming community. Today most open source programming languages have similar tools available. In 2025 it is trivial to generate source code documentation directly from the comments in your source code. Can an approach like this be used for more of our software artifacts?\n\nI believe leveraging [CodeMeta](https://codemeta.github.io \"See the section titled, Motivation\"), developed by the research programming community, gives us a path forward for general software development.\n\n## How is CodeMeta relevant? \n\nThe CodeMeta data is meant to be machine readable and human manageable. That's a developer sweet spot. The CodeMeta object let's you track metadata like name, description, version, authorship and contributors. It includes where the project source repository is hosted and the license used. Over time the list of metadata attributes in the CodeMeta object has grown. Today we have a long list of a project's metadata that can be tracked and acted upon.\n\nWhen I started including CodeMeta files in my projects at Caltech Library I did so at software release. It was a task to do at the end of the process like the task of reviewing and updating the cloned documentation files. Treating the CodeMeta as an after thought created a burden. It was a disincentive to adopting CodeMeta. \n\nI think we make the CodeMeta object relevant to developers by treating it as primary source code. Editing it should be the first rather than last thing updated before a release. **An accurate CodeMeta file is actionable.** Ignored this and you ignore CodeMeta's super power.\n\nCan CodeMeta be used to help lower the documentation burden? Yes.\n\nIn 2025 there is enough information in a complete CodeMeta object to create structured documents. This includes documents like a README and INSTALL. Keeping track of the content as metadata can lower the effort in managing stale documentation. It may be enough to eliminate the need for git template repositories. At a minimum it can shrink what is needed from template repositories.\n\nHere's the steps I used to take setting up a project.\n\n1. Write the README\n2. Document how the software will work\n3. Write tests to confirm the software works\n4. Write the software\n5. Create/update my software artifacts to reflect the current state such as the codemeta.json file.\n\nRepeat as needed. The last step was always tedious. The longer a project is around the more artifacts need to be managed. It was easy to want to short cut that last step.\n\nHere's what I have been experimenting with.\n\n1. Create or update the CodeMeta file\n2. Document or update how the software should work\n3. Create or update tests to confirm the software works as documented\n4. Write or update the software to pass the tests\n5. **Generate** additional software artifacts from the CodeMeta document.\n\nStep five is automated. In practice step five can be integrated with your standard build processes. Us humans focus on steps one through four. Life just got a little easier for the busy developer.\n\nSeveral questions are suggested by this proposal.\n\n1. How do I make it easy to create and update the CodeMeta file?\n2. How do I generate the software artifacts?\n3. What artifacts can be generated in a reliable way?\n\nThe CodeMeta object is stored as a JSON document in your repository. That means you can readily build tooling around it.\n\nToday you can use the CodeMeta [generator](https://codemeta.github.io/codemeta-generator/) to bootstrap the creation of a CodeMeta file.  If you're willing to do some cut and paste work the generator can even be used to maintain your CodeMeta file.  \n\nWhat about generating our artifacts?\n\nFor the last few years I've relied on Pandoc templates, see [codemeta-pandoc-examples](https://github.com/caltechlibrary/codemeta-pandoc-examples). I use Pandoc and these templates to generate files like CITATION.cff, about.md, installer scripts and version files for the Python, Go and TypeScript programming languages. The trouble with this approach is Pandoc tends to be well knowing in the data science community but not in the general developer community. The Pandoc template language is obscure. This has lead me to believe new tools are needed beyond Pandoc and templates.\n\n***\n\n## CMTools\n\nI recently started prototyping two programs for working with [CodeMeta](https://codemeta.github.io) objects. The prototype code is available at [github.com/caltechlibrary/CMTools](https://github.com/caltechlibrary/CMTools). The two programs are for editing (`cme`) and for transforming (`cmt`) the CodeMeta object. I am currently testing the prototypes in selected Caltech Library projects.\n\n### What challenges have the prototypes raise?\n\nThe CodeMeta object is sprawling. `cme` was started as command line alternative to the generator.  It was initially designed with an interactive, prompt and response, user interface. It would iterate over all the top level attributes prompting for changes. This can be tedious. I quickly added support to shorten the process by including a list of specific attributes to edit. \n\n~~~shell\ncme codmeta.json                       # <-- iterate over all\ncme codemeta.json version dateModified # <-- just listed attributes\n~~~\n\nThe prompt and response approach works well for simple attribute types like name, description and version. The more complex attributes like author or contributor were challenging. To avoid the need to increase the types of prompts or be forced into a menu system I'm experimenting with using YAML to display the current value and accept YAML as the user response. YAML is much easier to type and copy edit than JSON. It is easy to transform into JSON. The downside is you need to know the structure and attribute names ahead of time. That gives `cme` a training cost.\n\nMulti line values are tricky to work with if you rely on standard input. To address this I added a feature to allow the use the editor of your choice.  If you are on macOS or Linux the default editor is nano. On Windows it is notepad.exe.  You can pick a different editor by setting the EDITOR environment variable.  In the example below I've chosen the [Micro Editor](https://micro-editor.github.io) for setting values. It didn't solve the problem of knowing the YAML attributes in advance but does make it easier to copy edit the YAML. Micro Editor is Open Source and available for macOS, Linux and Windows. Support for other editors could be added. Further prototyping and development work is needed to support alternatives to editing YAML.\n\nOn macOS and Linux\n\n~~~shell\nexport EDITOR=micro\ncme codemeta.json author -e\n~~~\n\nor on Windows\n\n~~~ps1\nSet-Variable EDITOR micro\ncme codemeta.json author -e\n~~~\n\nIn the 0.0.9 release of the `cme` prototype I added the ability to set the attribute values from the command line. This has helped in automated environments. CodeMeta attribute name is used as the key. An equal sign, \"=\", is the delimiter. What follows the equal sign is treated as the value. This works well for simple fields, e.g. version, dateModified.\n\n~~~shell\ncme codemeta.json version=\"0.0.2\" dateModified=\"2025-01-30\"\n~~~\n\nComplex attribute editing using this approach is very challenging.\n\n### What can CMTools generate?\n\nThe `cmt` prototype has limited abilities. It can render about.md and CITATION.cff files. It can generate \"version\" source code files for Python (version.py), Go (version.go), TypeScript (version.ts) and JavaScript (version.js). I am actively working on porting the remaining Pandoc templates from codemeta-pandoc-examples to `cmt`. README and INSTALL will be added after the template port is complete.\n\n~~~shell\ncmt codemeta.json about.md CITATION.cff version.py \\\\\n  README.md INSTALL.md installer.sh installer.ps1\n~~~\n\n## What's next?\n\nCMTools is at an early stage of development (January 2025). The project is focused finding the balance of editing and generating. Improvements will flow base on our usage.\n\nThe [v0.0.14 release](https://github.com/caltechlibrary/CMTools/releases/tag/v0.0.14) includes the basic features discussed in this post for both `cme` and `cmt`. RSD 2025-02-03\n",
      "data": {
        "abstract": "An exploration of using CodeMeta objects to generate of software artifacts as an alternative to Git template repositories.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "createDate": "2025-01-30",
        "keywords": [
          "software development",
          "programming",
          "CodeMeta"
        ],
        "pubDate": "2025-01-31",
        "series": "Code Generation",
        "series_no": 1,
        "title": "Moving beyond git template repositories with CodeMeta",
        "updateDate": "2025-02-03"
      },
      "url": "posts/2025/01/31/moving_beyond_git_templates.json"
    },
    {
      "content": "\n# Raspberry Pi 4 & 400 Power Supply Issues\n\nBy R. S. Doiel, 2024-11-20\n\nI have a Raspberry Pi 4 and Raspberry Pi 400. The former builds my personal website and my [Antenna](https://rsdoiel.github.io/antenna). I use the latter as a casual portable development box.\n\nOver the last year or two I noticed I would get voltage warnings popping up. Typical this was prevalent when I was compiling Go, Pandoc or Deno. The power supply I was using was the Official Raspberry Pi 4 power supply that came with the Raspberry Pi Desktop Kit and Raspberry Pi 400 Computer Kit.  At first I thought the power supplies were going bad or had become damaged.  I tried replacing the power supply with one of the extras I had picked up (power supplies can and do fail). Still had problems with low voltage.\n\nI did some digging but found nothing useful than the old recommendations of making sure the power supplies were appropriately and to spec. I did read somewhere in my searching that the official power supplies didn't have allot of headroom. This got me thinking.  The Raspberry Pi OS has been updated many times since I got these devices. Maybe there are more power demands happening than I realized.  I've also moved away from using the internal micro SD cards to using an external thumb drive to boot, that would place an additional power requirement on the system.\n\nI looked at [PiShop.us](https://www.pishop.us) which is linked from the Raspberry Pi website as an official retailer.  They had the official supplies that came with my kits but also had one that had a higher wattage rating. I ordered [Raspberry Pi 27W USB-C Power Supply White US](https://www.pishop.us/product/raspberry-pi-27w-usb-c-power-supply-white-us/). Connected it up to my Raspberry Pi 400 via the powered monitor. Booted from the thumb drive and then tried rebuilding Go from Go 1.19 that ships with Raspberry Pi OS compiling Go 1.21.9 and 1.23.3 from source. No voltage errors.  Looks like I just out grew the stock power supply.  If this remains working without problems I will order another one and use it with my old Pi Drive enclosure and my Pi 4. Keeping my fingers crossed.\n\n\n\n\n",
      "data": {
        "abstract": "Quick notes on some low voltage issues I ran into with my Raspberry Pi 4 and 400 using the stock power supply with thumb drives.\n",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-11-20",
        "createDate": "2024-11-20",
        "keywords": [
          "Raspberry Pi",
          "Power Supply",
          "Voltage errors"
        ],
        "pubDate": "2024-11-20",
        "title": "Raspberry Pi 4 & 400 Power Supply Issues"
      },
      "url": "posts/2024/11/20/power-supply-issues.json"
    },
    {
      "content": "\n# Two Rust command line tools for web work\n\nBy R. S. Doiel, 2024-11-06\n\nI've noticed that I'm using more tools written in the Rust language. I'd like to highlight two that have impressed me. Both are from [Cloud Cannon](https://cloudcannon.com). They make static website development interesting without unnecessarily increasing complexity.\n\nHistorically static site development meant limited interactivity browser side. Then JavaScript arrived. That enabled the possibility of an interactive static site. Two areas remained challenging. First was search. An effective search engine used to required running specialized software like [Solr](https://solr.apache.org) or using a SAAS[^1] search solution.  When the renaissance in static sites happened these options were seen as problematic.\n\n[^1]: SAAS, Software as a Service. This is how the web provides application software and application functionality. SAAS are convenient but often have significant drawbacks in terms of privacy and content ownership. SAAS dates back to pre-Google era. Early search engines like Alta Vista provided search as a service. Today many sites use Google, Bing or DuckDuckGo to provide search as a service.\n\nStatically hosted search arrived when [Oliver Nightingale](https://github.com/olivernn) created [LunrJS](https://lunrjs.com/). LunrJS provides a Solr like search experience run from within your web browser. You needed to write some JavaScript to generate indexes and of course JavaScript had to be available in the browser to run the search engine. In spite of having to write some JavaScript to perform indexing it was easier to setup and manage than Solr. LunrJS added benefit of avoiding running a server. Things were good but there were limitations.  If you wanted to index more than ten thousand or so objects the indexes grew to be quite large. This made search painfully slow. That's because your web browser needed to download the whole search index before search could run in the browser.  There were variations on Oliver's approach but none really seemed to solve the problem completely. Still for small websites LunrJS was very good.\n\nFast forward and a little over a decade ago Cloud Cannon emerged as a company trying to make static site content management easier.  One of the things they created was a tool called [PageFind](https://pagefind.app). Like LunrJS PageFind provides a search engine experience for static websites that runs in the browser.  But it includes a few important improvements. First you don't need to write a program to build your indexes. PageFind comes with a Rust based tool called, unsurprisingly, \"pagefind\". It builds the indexes for you with minimal configuration. The second difference from LunrJS is PageFind builds collection of partial indexes that can be loaded on demand. This means you can index sites with many more than 10K objects and still use a browser side search. That's huge. I've used it on sites with as many as hundred thousand objects. That's a magnitude difference content that can be searched! A very clever feature of PageFind is that you can combined indexes from multiple sites.  This means the search with my blog can also cover my GitHub project sites that use PageFind for their web documentation. Very helpful.\n\nPageFind does have limitations. It only indexes HTML documents. Unlike Solr it's not going to easily serve as a search engine for your collection of PDFs. At least without some effort on your part. Like LunrJS it also requires JavaScript to be available in the web browser. So if you're using RISC OS and a browser like NetSurf or Dillo, you're out of luck. Still it is a viable solution when you don't want to run a server and you don't want to rely on a SAAS solution like Google or Bing.\n\n> Wait! There's more from Cloud Cannon!\n\nIf you start providing JavaScript widgets for your website content you'll probably miss having a database backed JSON API. You can create one as part of your site rendering process but it is a big chore. Cloud Cannon, in their quest for a static site content management system, provides an Open Source solution for this too. It's called [FlatLake](https://flatlake.app). Like PageFind it is a command line tool. Instead of analyzing HTML documents FlatLake works on Markdown documents. More specifically Markdown documents with YAML front matter[^2]. It uses that to render a read only JSON API from the metadata in your documents front matter.  You define which attributes you want to expose in your API in a YAML file. FlatLake creates the necessary directory structure and JSON documents to reflect that. Want to create a tag cloud? Your JSON API can provided the data for that. You want to provide alternate views into your website such as indexes or article series views?  Again you now have a JSON API that aggregates your metadata to render that. Beyond some small amount of configuration FlatLake does the heavy lifting of generating and managing the JSON API for your site. It's consistent and predictable. Start a new site and add FlatLake and you get a familiar JSON API out of the box.\n\n[^2]: Front matter is a block of text at the top of your Markdown document usually expressed in YAML. Site building tools like [Pandoc](https://pandoc.org/chunkedhtml-demo/8.10-metadata-blocks.html#extension-yaml_metadata_block) can use the YAML block to populate and control templating. Similarly R-Markdown provides a similar functionality. FlatLake takes advantage of that. \n\n\n## PageFind and FlatLake in action\n\nMy personal website is indexed with PageFind.  The indexes are located at <https://rsdoiel.github.io/pagefind>. The search page is at <https://rsdoiel.github.io/search.html>. I index my content with the following command (which will run on macOS, Windows or Linux).\n\n~~~shell\npagefind --verbose --exclude-selectors=\"nav,header,footer\" --output-path ./pagefind --site .\n~~~\n\nThat command index all the HTML content excluding nav, header and footers.  The JavaScript in my [search.html](https://rsdoiel.github.io/search.html) page looks like this.\n\n~~~JavaScript\nlet pse = new PagefindUI({\n    element: \"#search\",\n    showSubResults: true,\n    highlightParam: \"highlight\",\n    mergeIndex: [{\n        bundlePath: \"https://rsdoiel.github.io/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/shorthand/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/pttk/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/skimmer/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/scripttools/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/fountain/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/osf/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/fdx/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/stngo/pagefind\",\n        bundlePath: \"https://rsdoiel.github.io/opml/pagefind\"\n    }]\n})\nwindow.addEventListener('DOMContentLoaded', (event) => {\n    let page_url = new URL(window.location.href),\n        query_string = page_url.searchParams.get('q');\n    if (query_string !== null) {\n        console.log('Query string: ' + query_string);\n        pse.triggerSearch(query_string);\n    }\n});\n~~~\n\nThis supports searching content in some of my GitHub project sites as well as my blog.\n\nOne of the things I am working on is updating how I render my website.  I have a home grown tool called [pttk](https://rsdoiel.github.io/pttk \"Plain Text Toolkit\") that includes a \"blogit\" feature. Blog it takes cares care of adding Markdown documents to my blog and generates a a JSON document that contains metadata from the Markdown documents.  That later feature is no longer needed with the arrival of FlatLake. FlatLake also has the advantage that I can define other metadata collections to include in my JSON API. The next incarnation of pttk will be simpler as the JSON API will be provided by FlatLake.\n\nFlatLake analyzes the Markdown documents in my website and build out a JSON API as folders and JSON documents. If I do this as the first step in rendering my site the rendering process can take advantage of that. It replace my \"blog.json\" file. It can even replace the directory traversal I previously needed to use with building the site. That's because I can take advantage of the exposed metadata in a highly consistent way. You can explore the JSON API being generated at <https://rsdoiel.github.io/api>. I haven't yet landed on my final API organization but when I do I'll be able to trim the code for producing my website significantly. Here's the outline I expect to follow.\n\n1. Run FlatLake on the Markdown content and update the JSON API content\n2. Read the JSON API and render my site\n3. Run PageFind and update my site indexes\n4. Deploy via GitHub Pages with `git` or `gh`\n\n## Installing PageFind and FlatLake\n\nBoth PageFind and FlatLake are written in Rust. If you have Rust installed on your machine then Cargo will do all your lifting.  When I have a new machine I install [Rust](https://www.rust-lang.org/) with [Rustup](https://rustup.rs). On an running Machine I run `rustup upgrade` to get the latest Rust.  I then install (or updated) PageFind and FlatLake with Cargo.\n\n~~~shell\nrustup upgrade\ncargo install pagefind\ncargo install flatlake\n~~~\n\nI've run PageFind on macOS, Windows, Linux. On ARM 64 and Intel CPUs. I even run it on Raspberry Pi!. Rust supports all these platforms so where Rust runs PageFind and FlatLake can follow.\n\nPageFind solves my search needs.  FlatLake is simplifying the tooling I use to generate my website. My plain text toolkit (pttk) needs to do much less. It feels close to the grail of static website management system built from simple precise tools. Git hands version control. Pandoc renders the Markdown to HTML. PageFind provides search and FlatLake provides the next generation JSON API. A Makefile or Deno task can knit things together into a publication system.\n",
      "data": {
        "abstract": "A quick review of a PageFind and FlatLake by Cloud Cannon. A brief description of how I use them.\n",
        "byline": "R. S. Doiel",
        "created": "2024-11-05",
        "keywords": [
          "web",
          "rust",
          "search",
          "JSON API",
          "Cloud Cannon"
        ],
        "pubDate": "2024-11-06",
        "title": "Rust tools for Web Work",
        "updated": "2024-11-06"
      },
      "url": "posts/2024/11/06/rust-tools-for-web-work.json"
    },
    {
      "content": "\n# When Deno+TypeScript, when Go?\n\nBy R. S. Doiel, 2024-11-06\n\nLast month I gave a [presentation](https://caltechlibrary.github.io/cold/presentations/presentation1.html) on a project written in [Deno](https://deno.com)+[TypeScript](https://typescriptlang.org). The project included code that needed to be shared server and browser side.  At the conclusion of my talk the question came up, \"When would I choose Go versus Deno+TypeScript for a project?\"[^1]. The answer I came up with at the time was I would choose Deno+TypeScript when I needed to share code between server and browser. That was the question that had lead me to Deno in the first place. Deno+TypeScript shared many of the advantages of Go[^2]. I like writing in Go. The weak point in my Go based projects I thought was limited to porting Go code to JavaScript when I needed it run browser side. I went home and slept well that night. Since then I have been reflecting a little more on the question. \n\n[^1]: I have presented allot of Go based projects to the SoCal Code4Lib group\n\n[^2]: Deno lets to cross compile TypeScript to binary executables. This, like Go, makes it trivial to develop and deploy. Deno provides tooling that seems inspired by the Go command.\n\nLearning Go isn't difficult and the learning resources are pretty good. I find Go well suited to the library and archive software domain. I believe typed languages are good for working with structured metadata. Go compiles to a binary and is trivial to deploy. Go has really good tooling making it easier to write better quality code.\n\nMy colleagues and I all know Python. Python is our language of collaboration. I'm the only one that knows Go on our team of four. The leap from Python to Go isn't huge but it is a leap. I made that leap because I needed the features that Go provided at the time[^3]. Since that time the uptake in libraries and archives of coding in Go has been minimal[^4].\n\n[^3]: This was over a decade ago, back when Go was very much a child of Robert Griesemer, Rob Pike and Ken Thompson.\n\n[^4]: After a decade I know only a half dozen or so Go programmers working in the library and archive domain.\n\nMy colleagues and I know JavaScript. Most of the people I've met through Code4Lib know JavaScript. TypeScript is a superset of JavaScript and Deno can compile it[^5]. The path from JavaScript to TypeScript is less of a leap and more of a stretch. Valid JavaScript is valid TypeScript after all.\n\n[^5]: Deno can also compile TypeScript/JavaScript making your project as easy to deploy as a Go project. TypeScript is a typed programming language so offers similar benefits when working with structured metadata. Deno has tooling inspired by the Go command's tooling.\n\nLearning Deno command is easy and not a big ask. It is certainly allot easier learning Deno than Git. Git knowledge has grown over the decade that I've known the Code4Lib community. I suspect Deno will be easier to adopt. When I take software sustainability into consideration I suspect those projects I write in Deno+TypeScript may out live the ones I've written in Go.\n\nIn hindsight I don't think my answer about sharing code between browser and server is the whole criteria. Libraries and archives tend to have a small team to zero software development staff. While Go is a very popular language in 2024 few writing software for libraries and archives know it. Go adoption in our community hasn't materialized.  In the meantime most of the people I've met via Code4Lib know JavaScript. The Go developers I know of in our community also know JavaScript.\n\nTypeScript is a small stretch to pickup if you know JavaScript. TypeScript has good, free, online documentation[^6]. TypeScript is by definition a superset of JavaScript. If your project requires participation by other developers and you choose Deno+TypeScript over Go you have a wider pool of possible helping hands. Deno+TypeScript gives most of the benefits that Go offers today. I think this is compelling for software sustainability. \n\n[^6]: See [www.typescriptlang.org](https://www.typescriptlang.org/docs/) for TypeScript and [MDN](https://developer.mozilla.org/en-US/docs/Web/JavaScript) for JavaScript.\n\nWhen would I pick Deno+TypeScript over Go? I now have three criteria questions I ask myself.\n\n- Do I need to share code between browser and server?\n  - If yes, maybe this is a Deno+TypeScript project.\n- Do I need to largest pool of programmers available to lend a hand?\n  - If yes, maybe this is a Deno+TypeScript project.\n- Do I want to require Go knowledge to participate in the project?\n  - If yes then it might be a Go project.\n\nIf my answers are \"yes\", \"yes\", \"no\" then the project should proceed with Deno+TypeScript. If I answer the last one is \"no\" then it shouldn't be a Go project.\n",
      "data": {
        "abstract": "Brief discussion of when I choose Deno+TypeScript versus Go for work projects.\n",
        "byline": "R. S. Doiel, 2024-11-06",
        "createDate": "2024-11-06",
        "keywords": [
          "Go",
          "TypeScript",
          "Programming Languages"
        ],
        "pubDate": "2024-11-06",
        "title": "When Deno+TypeScript, when Go?"
      },
      "url": "posts/2024/12/06/when_deno_when_go.json"
    },
    {
      "content": "\n# Limit and offset for row pruning\n\nBy R. S. Doiel, 2024-10-31\n\nI recently needed to prune data that tracked report requests and their processing status. The SQLite3 database table is called\n\"reports\" and has four columns.\n\n- `id` (uuid)\n- `created` (request date stamp)\n- `updated` (status updated date stamp)\n- `src` (a JSON column with the status details)\n\nThe problem is the generated report can be requested as needed. I wanted to maintain the request data for the most recent one. The \"src\" column has the report name and status. That is easily checked using the JSON notation supported by SQLite3 (v3.47.0). It's easy to get the most recent completed row with a simple SELECT statement using both an ORDER clause and LIMIT clause.\n\n~~~sql\nselect id\n  from reports\n  where src->>'report_name' = 'myreport' and\n        src->>'status' = 'completed'\n  order by updated desc\n  limit 1\n~~~\n\nThis gives me the key to the most recent record.  How do I get a list of he rows I want to prune?  The answer is to use the LIMIT cause with an OFFSET\nmodifier. The OFFSET let's us skip a certain number of rows before applying the limit.  In this case I want to skip one row and show the rest. This database table doesn't get that big so I can use a limit like one thousand. Here's what that looks like.\n\n~~~sql\nselect id\n  from reports\n  where src->>'report_name' = 'myreport' and\n        src->>'status' = 'completed'\n  order by updated desc\n  limit 1000 offset 1\n~~~\n\nNow that I have my list of ids I can combine it with a DELETE statement which has a WHERE clause. The WHERE clause will use the IN operator to iterate over the list of ids from my select statement.\n\nPutting it all together it looks like this.\n\n~~~sql\ndelete from reports\n  where id in (\n    select id\n      from reports\n      where src->>'report_name' = 'myreport' and\n            src->>'status' = 'completed'\n      order by updated desc\n      limit 1000 offset 1\n)\n~~~\n\nThe nice thing is I can run this regularly. It will never delete the most recent row because the offset value is one.\n\n",
      "data": {
        "abstract": "Noted are how to combine a select statement with limit and offset clauses with a delete statement to prune rows.",
        "byline": "R. S. Doiel",
        "created": "2024-10-31",
        "keywords": [
          "sql",
          "SQLite3"
        ],
        "pubDate": "2024-10-31",
        "title": "Limit and offset for row pruning"
      },
      "url": "posts/2024/10/31/limit_and_offset_for_row_pruning.json"
    },
    {
      "content": "\n# SQLite3 json_patch is a jewel\n\nBy R. S. Doiel, 2024-10-31\n\nIf you’re working with an SQLite3 database table and have JSON or columns you need to merge with other columns then the `json_path` function comes in really handy.\nI have a SQLite3 database table with four columns.\n\n- _key (string)\n- src (json)\n- created (datestamp)\n- updated (datestamp)\n\nOccasionally I want to return the `_key`, `created` and `updated` columns as part of the JSON held in the `src` column.  In SQLite3 it is almost trivial.\n\n~~~sql\nselect \n  json_patch(json_object('key', _key, 'updated', updated, 'created', created), src) as object\n  from data;\n~~~\n\n",
      "data": {
        "abstract": "Quick note about json_path function in SQLite3",
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel",
        "created": "2024-10-31",
        "keywords": [
          "sql",
          "SQLite3"
        ],
        "pubDate": "2024-10-31",
        "title": "SQLite3 json_patch is a jewel"
      },
      "url": "posts/2024/10/31/sqlite3_json_patch.json"
    },
    {
      "content": "\n# Quick tour of Deno 2.0.2\n\nBy R. S. Doiel\n\nI've been working with TypeScript this year using Deno. Deno has reached version 2.0. It has proven to be a nice platform for projects. Deno includes thoughtful tooling, good language support, ECMAScript module support and a [good standard library](https://jsr.io/@std).  As a TypeScript and JavaScript platform I find it much more stable and compelling than NodeJS. Deno has the advantage of being able to cross compile TypeScript to an executable which makes deployment of web services as easy for me as it is with Go.\n\n## Easy install with Cargo\n\nDeno is written in Rust. I like installing Deno via Rust's Cargo. You can installed Rust via [Rustup](https://rustup.rs). When I install Deno on a new machine I first check to make sure my Rust is the latest then I use Cargo to install Deno.\n\n~~~shell\nrustup update\ncargo install deno\n~~~\n\n## Easy Deno upgrade\n\nDeno is in active development. You'll want to run the latest releases.  That's easy using Deno. It has a self upgrade option.\n\n~~~shell\ndeno upgrade\n~~~\n\n## Exploring TypeScript\n\nWhen I started using Deno this year I wasn't familiar with TypeScript. Unlike NodeJS Deno can run TypeScript natively. Why write in TypeScript? TypeScript is a superset of JavaScript. That means if you know JavaScript you know most of TypeScript already. Where TypeScript differs is in the support for type safety and other modern language features. Writing TypeScript for Deno is a joy because it supports the web standard ECMAScript Models. That means the code I develop to run server side can be easily targetted to work in modern browsers too. TypeScript began life as a transpiled language targeting JavaScript. With Deno's emit module I can easily transpile my TypeScript to JavaScript. No more messying about with NodeJS and npm.\n\n## Exploring Deno\n\nAs a learning platform I find Deno very refreshing. Deno provides a [REPL](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop). That means you can easily try out TypeScript interactively. Deno is smart about when it runs \"programs\" versus running as a REPL. This is an improvement over NodeJS.\n\nDeno, like your web browser, runs TypeScript and JavaScript in a sand boxed environment. The REPL gives you full access to your machine but running programs via Deno requires you to give explicit permissions to resources like reading from your file system, accessing your environment, accessing the network or importing models from remote systems. This might sound tedious but Deno makes it easy in practice.\n\nDeno projects use a `deno.json` file for initialization. Creating the file is as easy as typing `deno init` in your project directory. Here's an example of setting up a `happy_deno` project.\n\n~~~shell\nmkdir happy_deno\ncd happy_deno\ndeno init\n~~~\n\nIf you list your directory you will see a `deno.json` file (Windows Powershell also supports \"ls\" to list directories).\n\n~~~shell\nls \n~~~\n\nThe init action created the following files.\n\n`deno.json`\n: The project configuration for Deno. It includes default tasks and module imports.\n\n`main.ts`\n: This is the \"main\" program for your project. It's where you'll add your TypeScript code.\n\n`main_test.ts`\n: This is a test program so you can test the code you've written in your \"main\" module.\n\nThe task action by itself will list currently defined tasks, e.g. `deno task` (the \"dev\" task\nwas defined by the init action).\n\n~~~shell\nAvailable tasks:\n- dev\n    deno run --watch main.ts\n~~~\n\nLooking at the `deno.json` file directly we see.\n\n~~~json\n{\n  \"tasks\": {\n    \"dev\": \"deno run --watch main.ts\"\n  }\n}\n~~~\n\nWhat does that do? The \"dev\" task will start deno using the \"run\" action passing it the \"watch\" option when running the file \"main.ts\". What does mean? The \"watch\" option will notice of the \"main.ts\" file changes on disk. It it changes it will re-run the \"main.ts\" program.  Save a change to \"main.ts\" in your editor deno and automagically it runs \"main.ts\" again. The really helps when you are write web services, the service automatically restarts.\n\nHere's an example of the output of running the \"dev\" task with the command `deno task dev`.\n\n~~~\nTask dev deno run --watch main.ts\nWatcher Process started.\nAdd 2 + 3 = 5\nWatcher Process finished. Restarting on file change...\n~~~\n\nUsing your editor, add a \"hello world\" log message to the code in \"main.ts\" so it looks like this.\n\n~~~typescript\nexport function add(a: number, b: number): number {\n  return a + b;\n}\n\n// Learn more at https://docs.deno.com/runtime/manual/examples/module_metadata#concepts\nif (import.meta.main) {\n  console.log(\"Add 2 + 3 =\", add(2, 3));\n  console.log(\"Hello World!\");\n}\n~~~\n\nSave your program and look what happens.\n\n~~~\nWatcher File change detected! Restarting!\nAdd 2 + 3 = 5\nHello World!\nWatcher Process finished. Restarting on file change...\n~~~\n\nAdding additional tasks is just a matter of editing the `deno.json` file and adding them to the `tasks` attributes.\n\nSee [deno task](https://docs.deno.com/runtime/reference/cli/task_runner/) documentation for details.\n\n### Modules in Deno\n\nTypeScript and JavaScript support \"modules\". Specifically Deno supports [ES](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules) modules. The nice thing about this is ES modules can be used with the same import export syntax in your web browser supports. Deno supports local modules and remote modules accessed via URL just like your browser. At work I have our project documentation hosted on GitHub. I can write a TypeScript modules there too. I can then import them into a another project just by using the URL.\n\nWhy is the significant? I don't need to rely on an external system like [npm](https://npmjs.com) for module repositories. All I need is a simple static website. Modules in the Deno community often use <https://jsr.io/> as a common module registery. This includes Deno's standard library modules.  Let's add the standard \"fs\" and \"path\" module to our happy deno project. Use Deno's \"add\" action.\n\n~~~shell\ndeno add jsr:@std/fs\ndeno add jsr:@std/path\n~~~\n\nIf you look at the `deno.json` now it should look something like this.\n\n~~~json\n{\n  \"tasks\": {\n    \"dev\": \"deno run --watch main.ts\"\n  },\n  \"imports\": {\n    \"@std/assert\": \"jsr:@std/assert@1\",\n    \"@std/fs\": \"jsr:@std/fs@^1.0.4\",\n    \"@std/path\": \"jsr:@std/path@^1.0.6\"\n  }\n}\n~~~\n\nTo quit my deno dev task I can press the control key and the \"c\" key (a.k.a. Ctrl-C) in my terminal window. \n\nI mentioned Deno runs programs in a sand box. That is because Deno tries to be secure by default. You must explicitly allow Deno to reach outside the sand box. One resource outside the sand box is the file system. If you use our remote modules we need to give Deno permission to do that too. See [security and permissions](https://docs.deno.com/runtime/fundamentals/security/) on Deno's documentation website for more details.\n\nTo allow reading files on the local file system with the \"dev\" task I would modify the \"dev\" command to look like.\n\n~~~\n    \"dev\": \"deno run --allow-read --watch main.ts\"\n~~~\n\nYou can include multiple permissions by adding the related \"allow\" option (E.g. `--allow-import`, `--allow-env`, `--allow-net`). It is important to realize that importing a moddel doesn't give you permission, you need to explicitly allow Deno to do that. When you compile a program the permissions you allow will also be allowed in the compiled version.\n\n### An exercise for the reader\n\nCreate a TypeScript file called [show_deno_json.ts](show_deno_json.ts). Read in and display the contents of the \"deno.json\" file in the same directory.\n\nHere's so links to documentation that may be helpful in finishing the exercise.\n\n- [reading files](https://docs.deno.com/examples/reading-files/)\n\nAdditional reading.\n\n- [fundamentals](https://docs.deno.com/runtime/fundamentals/)\n- [file system access](https://docs.deno.com/runtime/fundamentals/security/#file-system-access)\n- [standard modules](https://docs.deno.com/runtime/fundamentals/standard_library/)\n- [modules](https://docs.deno.com/runtime/fundamentals/modules/)\n- [deno.json](https://docs.deno.com/runtime/fundamentals/configuration/)\n- [security](https://docs.deno.com/runtime/fundamentals/security/)\n\n## Compiling TypeScript to executable code\n\nOne of the really nice things about Deno + TypeScript is that your development experience can be interactive like interpretive languages (e.g. Python, Lisp) and as convenient to deploy as a [Go](https://golang.org) executable. You can compile our \"main.ts\" file with the following command.\n\n~~~\ndeno compile --allow-read main.ts\n~~~\n\nListing my directory in our project I see the following.\n\n~~~shell\ndeno.json    deno.lock    happy_deno   main.ts      main_test.ts\n~~~\n\n~~~\n./happy_deno\n~~~\n\nNOTE: On a Windows the compiled program is named `happy_deno.exe`, to execute it I would type `.\\happy_deno.exe` in your Powershell session.\n\nBy default Deno uses the project directory for the executable name. You can explicitly set the executable name with a [command line option](https://docs.deno.com/runtime/getting_started/command_line_interface/). You can also use command line options with the compile action to [cross compile](https://en.wikipedia.org/wiki/Cross_compiler) your executable similar to how it is done with Go.\n\nWhy compile your program?  Well it runs slightly fast but more importantly you can now copy the executable to another machine and run it even if Deno isn't installed. This means you no longer have the version dependency problems I typically experience with deploying code from Python and NodeJS projects.  Like Go the Deno compiler is a cross compiler. That means I can compile versions for macOS, Windows and Linux on one machine then copy the platform specific executable to the machines where they are needed. Deno's compiler provides similar advantages to Go.\n\n## TypeScript to JavaScript with Deno\n\nJavaScript is a first class language in modern web browsers but TypeScript is not.  When TypeScript was invented it was positioned as a [transpiled](https://en.wikipedia.org/wiki/Source-to-source_compiler) language. Deno is a first class TypeScript environment but how do I get my TypeScript transpiled to JavaScript?  Deno provides an [emit](https://jsr.io/@deno/emit) module for that. With a five lines of TypeScript I can write a bundler to convert my TypeScript to JavaScript. I can even add running that as a task to my `deno.json` file. Here's an example of \"main_to_js.ts\".\n\n~~~typescript\nimport { bundle } from \"jsr:@deno/emit\";\nconst url = new URL(\"./main.ts\", import.meta.url);\nconst result = await bundle(url);\nconst { code } = result;\nconsole.log(code);\n~~~\n\nThe command I use to run `main_to_js.ts` is\n\n~~~shell\ndeno run --allow-read --allow-env main_to_js.ts\n~~~\n\nMy `deno.json` file will look like this with a \"transpile\" task.\n\n~~~json\n{\n  \"tasks\": {\n    \"dev\": \"deno run --allow-read --watch main.ts\",\n    \"transpile\": \"deno run --allow-read --allow-env main_to_js.ts\"\n  },\n  \"imports\": {\n    \"@std/assert\": \"jsr:@std/assert@1\",\n    \"@std/fs\": \"jsr:@std/fs@^1.0.4\",\n    \"@std/path\": \"jsr:@std/path@^1.0.6\"\n  }\n}\n~~~\n\nNow when I want to see the `main.ts` in JavaScript I can do `deno task transpile`.\n\n## Contrasting Deno + TypeScript with Go and Python\n\nFor me working in Go has been a pleasure in large part because of its tooling. The \"go\" command comes with module management, code formatter, linting, testing and cross compiler right out of the box. I like a garbage collected language. I like type safety. I like the ease which you can work with structured data. I've enjoyed programming with the excellent Go standard library while having the option to include third party modules if needed.\n\nDeno with TypeScript gives me most of what I like about Go out of the box. The `deno` command includes a task runner, module manager, testing, linting (aka check), cross compiler and formatter out of the box. TypeScript interfaces provide a similar experience to working with `struct` types in Go. Unlike Go you can work with Deno interactively similar to using the REPL in Python, Lisp or your favorite SQL client. I like the ES module experience of Deno better than Go's module experience.\n\nWhat makes Deno + TypeScript compelling over writing web services over Python is Deno's cross compiler.  Like Go I can compile executables for macOS, Windows and Linux on one box and target x86_64 and ARM 64 CPUs.No more need to manage virtual environments and no more sorting out things when virtual environments inevitably get crossed up. Copying an executable to the production machines is so much easier.  Many deployments boil down to an `scp` and restarting the services on the report machines. Example `scp myservice apps.example.edu:/serivces/bin/; ssh apps.example.edu \"systemctl restart myservice\"`.  It also means curl installs are trivial. All you need is an SH or Powershell script that can download a zip file, unpack it and copy it into the search path of the host system. Again the single self contained executable is a huge simplifier.\n\nOne feature I miss in Deno + TypeScript is the DSL in Go content strings embedded in struct type definitions. This makes it trivial to write converts for XML, JSON and YAML.  Allot of code in libraries and archives involves structured metadata and that feature ensures the structures definition are consistent between formats. I think adding to/from methods will become a chore at some point.\n\nIf you are working in Data Science domain I think Python still has the compelling code ecosystem. It works, it mature and there is lots of documentation and community out there. While you can run Deno from a [Jupyter notebook](https://docs.deno.com/runtime/reference/cli/jupyter/) I think it'll take a while for TypeScript/JavaScript to reach parity with Python for this application domain.\n\nSwitching from Go to Deno/TypeScript has been largely a matter of getting familiar with Deno, the standard library and remembering JavaScript while adding the TypeScript's type annotations. I've also had to learn TypeScript's approach to type conversions though that feels similar to Go. If I need the same functional code server side and browser side I think the Deno + TypeScript story can be compelling.\n\nPython, Rust, Go and Deno + TypeScript all support creating and running WASM modules.  Of those languages Rust has the best story and most complete experience. Deno runs a close second. Largely because it is written in Rust so what you learn about WASM in rust carries over nicely. The Python story is better than Go at this time. This is largely a result of how garbage collection is integrated into Go.  If I write a Go WASM module there is a penalty paid when you move between the Go runtime space and the hosts WASM runtime space. This will improve over time but it isn't something I've felt comfortable using in my day to day Go work (October 2024, Go v1.23.2).\n\nDeno makes TypeScript is a serious application language. I suspect more work projects to be implemented in TypeScript where shared server and browser code is needed. I has be useful exploring Deno and TypeScript.\n\n",
      "data": {
        "abstract": "A quick tour of Deno 2 and the features I enjoy. Deno includes thoughtful tooling, good language support,\nECMAScript module support and a good standard library. Deno has the advantage of being able to cross compile\nTypeScript to an executable which makes deployment of web services as easy for me as it is with Go.\n",
        "byline": "R. S. Doiell, 2024-10-18",
        "created": "2024-10-18",
        "keywords": [
          "development",
          "languages"
        ],
        "pubDate": "2024-10-18",
        "title": "Quick tour of Deno 2.0.2",
        "updated": "2024-10-21"
      },
      "url": "posts/2024/10/18/a-quick-tour-of-deno-2.json"
    },
    {
      "content": "\n# Web GUI and Deno\n\nBy R. S. Doiel, 2024-07-08\n\nI've been looking at various approaches to implement graphical interfaces for both Deno and other languages.  I had been looking primarily at webview bindings but then stumbled on [webui](https://webui.me). Both could be a viable way to implement a local first human user interface.\n\nHere's an example of the webview implementation of hello world.\n\n~~~typescript\nimport { Webview } from \"@webview/webview\";\n\nconst html = `\n<html>\n  <head></head>\n  <body>\n    <h1>Hello from deno v${Deno.version.deno}</h1>\n    <script>console.log(\"Hi There!\");</script>\n  </body>\n</html>\n`;\n\nconst webview = new Webview();\n\nwebview.navigate(`data:text/html,${encodeURIComponent(html)}`);\nwebview.run();\n~~~\n\nNow here is a functionally equivalent version implemented using webui.\n\n~~~typescript\nimport { WebUI } from \"https://deno.land/x/webui/mod.ts\";\n\nconst myWindow = new WebUI();\n\nmyWindow.show(`\n<html>\n  <head><script src=\"webui.js\"></script></head>\n  <body>\n    <h1>Hello from deno v${Deno.version.deno}</h1>\n    <script>console.log(\"Hi There!\");</script>\n    </body>\n</html>`);\n\nawait WebUI.wait();\n~~~\n\nLet's call these [thing1.ts](thing1.ts) and [thing2.ts](thing2.ts).  To run thing1 I need a little prep since I've used an `@` import. The command I need to map the `webview/webview` module is the `deno add` command.\n\n~~~shell\ndeno add @webview/webview\n~~~\n\nHere's how I check and run thing1.\n\n~~~shell\ndeno check thing1.ts\ndeno run -Ar --unstable-ffi thing1.ts\n~~~\n\nSince I didn't use an `@` import in the webui version I don't need to \"add\" it to Deno. I check and run thing2 similar to thing1.\n\n~~~shell\ndeno check thing2.ts\ndeno run -Ar --unstable-ffi thing2.ts\n~~~\n\nBoth will launch a window with our hello world message. Conceptually the code is similar but the details differ.  In the case of webview you are binding the interaction from the webview browser implementation. You populate your \"page\" using a data URL call (see `webview.navigate()`. Webview is a minimal web browser. It is similar to but not the same as evergreen web browsers like Firefox, Chrome, or Edge. Depending how var you want to push your CSS, JavaScript and HTML this may or may not be a problem.\n\nWebui uses a lighter weight approach. It focuses on a web socket connection between your running code and the user interface. It leaves the browser implementation to your installed browser (e.g. Chrome, Edge or Firefox). There is a difference in how I need to markup the HTML compared to the webview version. In the webui version I have a script element in the head. It loads \"webui.js\". This script is supplied by webui C level code. It \"dials home\" to connect your program code with the web browser handling the display. Webui at the C library level is functioning as a web socket server.\n\nConceptually I like the webui approach. My program code is a \"service\", webui manages the web socket layer and the web browser runs the UI. Web browsers are complex. In the web UI approach my application's binary isn't implementing one. In the webview approach I'm embedding one. Feels heavy. At a practical level of writing TypeScript it may not make much differences. When I compiled both thing1 and thing2 to binaries thing2 was approximately 1M smaller. Is that difference important? Not really sure.\n\nWhat about using webview or webui from other languages? Webview has been around a while. There are many bindings for the C++ code of webview and other languages.  Webui currently supports Rust, Go, Python, TypeScript/JavaScript (via Deno), Pascal as well as a few exotic ones. TypeScript was easy to use either. I haven't tried either out with Python or Go. I'll leave that for another day.\n",
      "data": {
        "abstract": "My notes on two Web GUI modules available for Deno.\n",
        "created": "2024-07-07",
        "keywords": [
          "Deno",
          "TypeScript",
          "webui",
          "webview"
        ],
        "pubDate": "2024-07-08",
        "title": "Web GUI and Deno"
      },
      "url": "posts/2024/07/08/webgui_and_deno.json"
    },
    {
      "content": "\n# Transpiling with Deno\n\n[Deno](https://deno.land) is a fun environment to work in for learning TypeScript.  As I have become comfortable writing server side TypeScript code I know I want to also be able to use some modules in JavaScript form browser side. The question is then how to you go from TypeScript to JavaScript easily with getting involved with a bunch-o-npm packages?  Turns the solution in deno is to use the [deno_emit](https://github.com/denoland/deno_emit/blob/main/js/README.md) module.  Let's say I have a TypeScript module called `hithere.ts`. I want to make it available as JavaScript so I can run it in a web browser. How do I use the `deno_emit` module to accomplish that?\n\n- Write a short TypeScript program\n  - include the transpiler module provided with emit\n  - use the transpiler to generate the JavaScript code\n  - output the JavaScript code\n\nHere's what `transpile.ts` might look like:\n\n~~~typescript\n/* Get the transpiler module from deno's emit */\nimport { transpile } from \"https://deno.land/x/emit/mod.ts\";\n\n/* Get the python to my CL.ts as a URL */\nconst url = new URL(\"./hithere.ts\", import.meta.url);\n/* Transpile the code returning a result */\nconst result = await transpile(url);\n\n/* Get the resulting code and write it to standard out */\nconst code = result.get(url.href);\nconsole.log(code);\n~~~\n\nHere's the `hithere.ts` module:\n\n~~~typescript\n/**\n * hithere takes a name and returns a string of \"hi there \", a name and \"!\". If the name is null\n * it returns \"Hello World!\".\n *\n * @param {string | null} name\n * @returns {string}\n */\nfunction hithere(name: string | null): string {\n\tif (name === null) {\n\t\treturn \"Hello World!\";\n\t}\n\treturn `hi there ${name}!`;\n}\n~~~\n\nTo compile the module I need to give transpile.ts some permissions.\n\n- --allow-read (so I can read my local module\n- --allow-env (the transpiler needs the environment)\n- --allow-net (the deno emit module is not hosted locally)\n\nThe command line could look like this.\n\n~~~shell\ndeno run --allow-read --allow-env --allow-net \\\n  transpile.ts\n~~~\n\nThe result is JavaScript. It still has my comments in the code but doesn't have the TypeScript specific\nannotations.\n\n~~~javascript\n/**\n * hithere takes a name and returns a string of \"hi there \", a name and \"!\". If the name is null\n * it returns \"Hello World!\".\n *\n * @param {string | null} name\n * @returns {string}\n */ function hithere(name) {\n  if (name === null) {\n    return \"Hello World!\";\n  }\n  return `hi there ${name}!`;\n}\n~~~\n",
      "data": {
        "created": "2024-07-03",
        "keywords": [
          "TypeScript",
          "JavaScript",
          "Deno"
        ],
        "pubDate": "2024-07-03",
        "software": [
          "Deno >= v1.44"
        ],
        "title": "Transpiling with Deno"
      },
      "url": "posts/2024/07/03/transpiling_with_deno.json"
    },
    {
      "content": "\n# Bootstrapping a Text Oriented Web\n\nBy R. S. Doiel, 2024-06-14\n\nFirst order of business is to shorten \"text oriented web\" to TOW.  It's easier to type and say.  I'm considering the bootstrapping process from three vantage points. \n\n1. content author\n2. the server software\n3. client software \n\nThe TOW approach is avoids invention in favor of reuse. HTTP protocol is well specified and proven. [Common Mark](https://commonmark.org) has a specification as does [YAML](https://yaml.org/). TOW documents are UTF-8 encoded. A TOW document is a composite of Common Mark with YAML blocks. TOW documents combined with HTTP provide a simplified hypertext platform. \n\n\n## Authoring TOW documents\n\nTOW seeks to simplify the content author experience. TOW removes most of the complexity of content management systems rendering processes. A TOW document only needs to be place in a directory supported by a TOW server. In that way it is as simple as [Gopher](https://en.wikipedia.org/wiki/Gopher_(protocol)). The content author should only need to know [Markdown](https://en.wikipedia.org/wiki/Markdown), specifically the [Common Markdown](https://commonmark.org/) syntax. If they want to create interactive documents or distribute metadata about their documents they will need to be comfortable creating and managing YAML blocks embedded in their Common Mark document. Use of YAML blocks is already a common practice in the Markdown community.\n\nDescribing content forms using YAML has several advantages. First it is much easier to read than HTML source. YAML blocks are not typically rendered by Markdown processor libraries. I can write a simple preprocessor which tenders the YAML content form as HTML. Since HTML is allowed in Markdown documents these could then be run through a standard Markdown to HTML converter.  In the specific case of Pandoc a filter could be written to perform the pre-processor step. It should be possible to always render a TOW document as an HTML5 document. This is deliberate, it should be possible to use the TOW documents to extrapolate a traditional website.\n\n## Server and client software\n\nTOW piggy backs on the HTTP protocol. A TOW document is a composite of Common Mark with embedded YAML blocks when needed. It differs from the existing WWW content only in its insistence that Common Mark and YAML be first class citizens forming a viable representation of a hypertext document. A TOW document URL looks the same as a WWW URL. The way TOW documents distinguish themselves from ordinary web content is via their content type, \"text/tow\" or \"text/x-tow\".  Content forms are sent to a TOW service using the content type \"application/yaml\" content type instead of the various urlencoded content types used by WWW forms. \n\nTOW browsers eschew client side programming. I have several reasons for specifying this. First the TOW concept is a response to current problems and practices in the WWW. I don't want to contribute to the surveillance economy. It also means that's what the client receives avoids on vector if hijacking that the WWW has battled over the years. Importantly this also keeps the TOW browser model very simple. The TOW browser renders TOW content once per load. TOW is following the path that [Gemini protocol](https://geminiprotocol.net/) and [Gemtext](https://hexdocs.pm/gemtext/Gemtext.html). Unlike Gemini it does not require a new protocol and leverages an existing markup. Like Gemini TOW is not replacing anything but only supplying an alternative.\n\nMy vision for implementing TOW is to use existing HTTP protocol. That means a TOW URL looks just like a WWW URL. How do I distinguish between WWW and TOW?  HTTP protocol supports headers. TOW native interaction should use the content type \"text/tow\" or \"text/x-tow\". Content forms submitted to a TOW native server should submit their content encoded as YAML and use the content type \"text/tow\" or \"text/x-tow\". This lets the server know that the reply should remain in \"text/tow\" or \"text/x-tow\".  A TOW enabled browser can be described as a browser that knows how to render TOW documents and submit YAML responses.\n\n## How to proceed?\n\nA TOW document needs to be render-able as HTML+CSS+JavaScript because that is what is available today to bootstrap TOW. The simplest TOW server just needs to be able to send TOW content to a requester with the correct content type header, e.g. \"text/tow\".  That means a server can be easily built in Go using the standard [net/http](https://gopkg.in/net/html) package. That same package could then be combined with a web server package to adapt it into a TOW server supporting translation to HTML+CSS+JavaScript during the bootstrap period.  If the TOW web server received a request where \"text/tow\" wasn't in the acceptable response list then it would return the TOW document translated to HTML+CSS+JavaScript.\n\nA TOW native browser could be built initially as a [PWA](https://en.wikipedia.org/wiki/Progressive_web_app). It just needs to render TOW native documents as HTML5+CSS+JavaScript and be able to send TOW content forms back as YAML using the \"text/tow\" content type. Other client approaches could be taken, e.g. write plugin for the [Dillo browser](https://dillo-browser.github.io/), build something on [Gecko](https://developer.mozilla.org/en-US/docs/Glossary/Gecko), build something on [WebKit](https://webkit.org/), or use [Electron](https://www.electronjs.org/). A PWA is probably good enough for proof of concept.\n\nA minimal TOW proof of concept would be the web service that can handle the translation of TOW documents to HTML+CSS+JavaScript. A complete proof of concept could be implemented TOW native support via [PWA](https://en.wikipedia.org/wiki/Progressive_web_app). \n\n1. tow2html5\n2. towtruck (built using tow2html5)\n3. towby (initially built as tow2html5 WASM module as PWA)\n\n## Proposed programs\n\ntow2html5\n: This can be implemented in Go as both a package and command line interface. The command line interface could function either in preprocessor mode (just translating the YAML forms into HTML5) or as a full processor using an existing Common Mark package. It could also be compiled to a WASM module to support implementing a TOW browser as PWA.\n\ntowtruck\n: This would be a simple web service that performed tow2html5 translation for tow document requests from non-TOW native browsers. If the accepted content type requested includes TOW native then it'd just hand back the TOW file untranslated. I would implemented this as a simple static HTTP web service running on localhost then use Lighttpd, Apache 2 or NginX for a front end web server. This simplifies the TOW native server.\n\ntowby\n: A [PWA](https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps) based TOW browser proof of concept\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-06-14",
        "created": "2024-06-14",
        "keywords": [
          "text oriented web",
          "tow"
        ],
        "pubDate": "2024-06-14",
        "title": "Bootstrapping a Text Oriented Web"
      },
      "url": "posts/2024/06/14/tow_bootstraping.json"
    },
    {
      "content": "\n# RISC OS 5.30, GCC 4.7 and Hello World\n\nBy R. S. Doiel, 2024-06-08 (updated: 2024-06-16)\n\nPresently am I learning RISC OS 5.30 on my Raspberry Pi Zero W. I want to write some programs and I learned C back in University. I am familiar with C on POSIX systems but not on RISC OS. These are my notes to remind myself how things work differently on RISC OS.\n\nI found two resources helpful. First James Hobson had a YouTUBE series on RISC OS and C programming. From this I learned about allocating more space in the Task Window via the Tasks window display of Next and Free memory. Very handy to know. Watching his presentation it became apparent he was walking through some\none's tutorial. This lead to some more DuckDuckGo searches and that is when I stumbled on Steve Fryatt's [Wimp Programming In C](https://www.stevefryatt.org.uk/risc-os/wimp-prog). James Hobson's series (showing visually) help and the detail of Steve Fryatt's tutorial helped me better understanding how things work on RISC OS.\n\nI think these both probably date from around 2016. Things have been evolving in RISC OS since then. I'm not certain that OSLib today plays the same role it played in 2016. Also in the case of Steve Fryatt's tutorial I'm not certain that the DDE and Norcroft compilers are essential in the say way. Since I am waiting on the arrival of the ePic SD Card I figured I'd get started using the\nGCC and tools available via Packman and see how far I can get.\n\n## Getting oriented\n\nWhat I think I need.\n\n1. Editor\n2. C compiler\n3. Probably some libraries\n\nYou need an editor fortunately RISC OS comes with two, `!Edit` and `!StrongED`. You can use both to create C files since they are general purpose text edits.\n\nYou need a C compiler, GCC 4.7.4 is available via Packman. That is a click,\nand download away so I installed that.\n\nI had some libraries already installed so I skipped installing additional ones since I wasn't sure what was currently required.\n\n## Pick a simple goal\n\nWhen learning a new system I find it helpful to set simple goals. It helps from feeling overwhelmed.\n\nMy initial goal is to understand how I can compile a program and run it in the Task Window of RISC OS. The Task Window is a command line environment for RISC OS much like a DOS Window was for MS Windows or the Terminal is for modern macOS.  My initial program will only use standard in and out. Those come with the standard library that ships with the compiler. Minimal dependencies simplifies things. That goes my a good simple intial goal.\n\n> I want to understand the most minimal requirements to compile a C program and run it in Task Window\n\n## Getting started\n\nThe program below is a simple C variation on the \"Hello World\"  program tought to beginner C programers.  I've added a minimal amount of parameter handlnig to se how that works in the Task Window environment. This program will say \"Hello World!\" but if you include parameters it will say \"Hi\" to those too.\n\nThe code looks like this.\n\n~~~C\n#include <stdio.h>\n\nint main(int argc, char *argv[]) {\n  int i = 0;\n  printf(\"Hello World!\\n\");\n  for (i = 1; i < argc; i++)  { \n       printf(\"Hi %s\\n\", argv[i]);\n  }\n  return 0;\n}\n~~~\n\nIn a POSIX system I would name this \"HelloWorld.c\". On RISC OS the \".\" (dot)\nis a directory delimiter. There seems to be two approaches to translating POSIX paths to RISC OS. Samba mounted resources seem to have a simple substitution translatio. A dot used for file extensions in POSIX becomes a slash. The slash directory delimiter becomes a dot. Looking at it from the POSIX side the translation is flipped. A POSIX path like \"Project/HelloWorld/HelloWorld.c\" becomes \"Project.HelloWorld.HelloWorld/c\" in RISC OS.\n\nIn reading of the RISC OS Open forums I heard discussions about a different approach that is more RISC OS centric. It looks like the convention in RISC OS is to put the C files in a directory called \"c\" and the header files in a directory called \"h\". Taking that approach I should instead setup my directory paths like \"Project.HelloWorld.c\" which in POSIX would be \"Project/HelloWorld/c\". It seems to make sense to follow the RISC OS convensions in this case as I am not planning to port my RISC OS C code to POSIX anytime soon and if I did I could easily write a mappnig program to do that. My path to \"HelloWorld\" C source should look like `$.Projects.C_Programming.c.HelloWorld`.\n\nAfter storting this bit out it is time to see if I can compile a simple program with GCC and run it in a Task Window. This is a summary of my initial efforts.\n\nFirst attempt steps\n\n1. Open Task Window\n2. run `gcc --version`\n\nThis failed. GCC wasn't visible to the task window. Without understanding what I was doing I decided maybe I need to launch `!GCC` in `$.Apps.Development` directory. I then tried `gcc --version` again in the Task Window and this time\nthe error was about not enough memory available. I looked the \"Tasks\" window and saw plenty of memory was free. I did NOT realise you could drag the red bar for \"next\" and increase the memory allocation for the next time you opened a Task Window. I didn't find that out until I did some searching and stumbled on James Hobson's videos after watching the recent WROCC Wakefield Show held in Bedford (2024).\n\n> A clever thing about RISC OS is the graphical elements are not strictly informational. Often they are actionable. Dragging is not limited to icons.\n\nSecond attempt steps\n\n1. Open the Tasks window, drag the memory (red bar) allocation to be more than 16K\n2. Open a new Task Window\n3. Find and Click on `!GCC`\n4. In the task window check the GCC version number\n5. Change the directory in the Task Window to where I saved \"HelloWorld\"\n6. Check the directory with \"cat\"\n7. Try to compile with `gcc HelloWorld -o app`, fails\n8. Check GCC options with `--help`\n9. Try to compiled with `gcc -x c HelloWorld -o app`, works\n\nThis sequence was more successful. I did a \"cat\" on the task window and saw I was not in the right folder where my \"HelloWorld\" was saved.  Fortunately James Hobson video shows any easy way of setting the working directory. I brought the window forward that held \"HelloWorld\". Then I used the middle mouse button (context menu) to \"set directory\". I then switched back to the Task Window and low and behold when I did a \"cat\" I could see my HelloWorld file.\n\nI  tried to compile \"HelloWorld\". In James Hobson video he shows how to do this but I couldn't really see what he typed.  When I tried this I got an error\nabout the file type not being determined.  Doing `gcc --help` listed the options\nand I spotted `-x` can be used to explicitly set the type from the GCC point of view. This is something to remember when using GCC. It's a POSIX program running\non RISC OS which is not a POSIX system.  GCC will expect files to have a POSIX references in some case and not others. There's a bit of trial and error around\nthis for me.\n\nNext I tried using the `-x c` option. I try recompiling and after a few moments\nGCC creates a \"app\" file in the current directory. On initial creation it is a Textfile but then the icon quickly switches to a \"App/ELF\" icon.  Double clicking the App icon displays hex code in the Task Window. Not what I was expected. Back in the Task Window I type the following.\n\n~~~shell\napp Henry Mable\n~~~\n\nAnd I get out put of\n\n~~~shell\nHello World!\nHi Henry\nHi Mable\n~~~\n\nMy program works at the CLI level in a Task Window. My initial goal has been met.\n\n## What I learned\n\n1. Remember that RISC OS is a fully GUI system, things you do in windows can change what happens in the whole environment\n2. Remember that the display elements in the GUI maybe actionable\n3. When I double clicked on `!GCC` what it did is add itself to the search path.\n\nI remember something from the Hobson video about setting that in `!Configure`, `!Boot` and picking the right boot configuration action.  I'll leave that for next time. I should also be able to script this in an Obey file and that might be a better approach.\n\nThere are some things I learned about StrongED that were surprising. StrongED's C mode functions like a \"folding\" editor. I saw a red arrow next to my \"main\" functions. If I click it the function folds up except for the function signature and opening curly bracket. Click it again the the arrow changes direction and the full function is visible again.\n\nThe \"build\" icon in StrongED doesn't invoke GCC at the moment. I think the build icon in the ribbon bar maybe looking for a Makefile. If so I need to install Make from Packman. This can be left for next time.\n\nI'd really like to change the editor colors as my eyes have trouble with white background. This too can be left for another day to figure out.\n\n## Next Questions\n\n1. How do I have the GCC compiled \"app\" so that I can double click in the file window and have it run without manually starting the Task Window and running it from there.  Is this a compiler option or do I need an Obey file?\n2. Which libraries do I need to install while I wait on the DDE from ePic to arrive so that I can write a graphical version of Hello World?\n\n## Updates\n\nI got a chance to read more about [Obey files](https://www.riscosopen.org/wiki/documentation/show/CLI%20Basics) and also clicked through the examples in the `SDSF::RISCOSPi.$.Apps.Development.!GCC` directory (shift double click to open the GCC directory. In that directory is an examples\nfolder which contains a Makefile for compile C programs in various forms.\nFrom there it was an easy stop to see how a simple Obey file could be used\nto create a `!Build` and `!Cleanup` scripts.\nwhere all the GCC setup lives). What follows are the two Obey files in the directory holding the \"c\" folder of HelloWorld.\n\nHere's `!build`\n\n~~~riscos\n| !Build will run GCC on c.HelloWorld to create !HelloWorld\nSet HelloWord$Dir <Obey$Dir>\nWimpSlot -min 16k\ngcc -static -O3 -s -O3 -o !HelloWorld c.HelloWorld\n~~~\n\nand `!Cleanup`\n\n~~~riscos\n| !Cleanup removes the binaries created with !Build\nSet HelloWorld$Dir <Obey$Dir>\nDelete !HelloWorld\n~~~\n\n",
      "data": {
        "abstract": "These are my notes on learning to program a Raspberry Pi Zero W\nunder RISC OS using GCC 4.7 and RISC OS 5.30\n",
        "created": "2024-06-08",
        "keywords": [
          "RISC OS",
          "Raspberry Pi",
          "GCC",
          "C Programming"
        ],
        "pubDate": "2024-06-08",
        "references": [
          "Steve Fryatt's tutorial <https://www.stevefryatt.org.uk/risc-os/wimp-prog>",
          "James Hobson's YouTUBE Video showing a summary of Steve Fryatt's tutorial"
        ],
        "title": "RISC OS 5.30, GCC 4.7 and Hello World"
      },
      "url": "posts/2024/06/08/riscos_gcc_and_hello.json"
    },
    {
      "content": "\n# Exploring RISC OS 5.30 on a Raspberry Pi Zero W\n\nBy R. S. Doiel, 2024-06-04\n\nBack on April, 27, 2024 [RISC OS Open](https://riscosopen.org) [announced](https://www.riscosopen.org/news/articles/2024/04/27/risc-os-5-30-now-available) the release of RISC OS 5.30. This release includes WiFi support for the Raspberry Pi Zero W. This may sound like a small thing. WiFi is taken for granted on many other operating systems.  This is the news I've been waiting for before diving into RISC OS. My Pi Zero W running RISC OS **just works** with my wireless network. That is wonderful.\n\n## RISC OS and Pi Zero W gives you a personal networked computer\n\nHere's my setup.\n\n- First generation Raspberry Pi Zero W (running RISC OS 5.30 with Pinboard2)\n- Raspberry Pi Keyboard and Mouse (the keyboard provides a nice USB hub for the Zero)\n- A powered portable monitory (the monitor provides power to the Pi Zero)\n\nFor additional disk storage I have a old Pi 3B+ with a 3.14G Western Digital hard drive. It is configured as a Samba file server running the bookworm release of Raspberry Pi OS[^1].\n\n[^1]: RISC OS 5.3 only supports SMB with LANMAN1. LANMAN1 is turned off by default for Samba on R-Pi OS bookworm.\n\n## Quick summary\n\nI've been playing around with RISC OS 5.30  on my Pi Zero for a couple weeks now. I've really come to enjoy using it. RISC OS is different from macOS and Windows in its approach to the graphical user interface. Like with the Oberon Operating System you need to accept that difference and shed your assumptions about how things should and do work. I've found the difference invigorating. My Pi Zero with RISC OS has become my \"fun desktop\" for recreational computing.\n\n## Diving in\n\nRISC OS is a small single user operating system. It requires minimal resources. It provides a rich graphical environment. Currently RISC OS 5.30 only runs on one core of an ARM CPU. The Raspberry Pi Zero has only one core so that's a nice fit.\n\nThere is a fair amount of information regarding RISC OS online. There are active user groups and communities too.\nSome of the documentation is quite good. One of the things to keep in mind if you search for \"RISC OS\" on the Internet is that RISC OS has forked. This is a little like the old Unix fork of BSD versus System V. They come from the same origins but have taken different paths and approaches.\n\n>RISC OS's closed fork runs on vintage hardware and emulators only. It does not appear to be actively developed and the version numbers associated include version four and six.\n>\n> The [RISC OS Open](https://www.riscosopen.org) (i.e. RISC OS 5.x) fork is Open Source. It is being actively developed. It is licensed using the Apache 2 Open Source license. It runs on most versions of the Raspberry Pi. It actually runs on many single board computers. See <https://www.riscosopen.org> for details. Today it seems to be a well run Open Source project.\n\nThe reasons for the fork are complicated. From what I've read they are due to how Acorn was broken up when the company ceased to operate as Acorn.  RISC OS as intellectual property wound up in two different companies. They had two different business models and were driven by maximizing profit in the diminished Acorn market. The net resulted was a divided RISC OS community. The division included a level of acrimony. Somehow RISC OS survived this. RISC OS 5 branch even survived a bumpy road to becoming a true Open Source operating system. Today RISC OS 5.30 is licensed under the widely used Apache 2.0 Open Source license. RISC OS 5 even have a small community of commercial software developers writing and updating software for it!\n\nIf, like me, you're starting your RISC OS 5.30 on a Raspberry Pi then you're in luck. The dust seems to have settled. I highly recommend first reading the User Guide found at the bottom of the [common](https://www.riscosopen.org/content/downloads/common) page in the downloads section of the RISC OS Open website. You can also buy a printed version of the User Guide. From there head over and explore RISC OS Open community forums at <https://www.riscosopen.org/forum/>. There is also a more general Acorn enthusiast community at [stardot.co.uk](https://www.stardot.org.uk/forums/). I found this particularly helpful in understanding the historical evolution of RISC OS. Stardot even has a presence of [GitHub](https://github.com/stardot).\n\nWhat follows are a semi-random set of points that I had to wrap my head around in getting oriented on RISC OS 5.30 on the Raspberry Pi Zero W.\n\n## Visibly different\n\nWhen I search for RISC OS on the [Raspberry Pi Forums](https://forums.raspberrypi.com/search.php?keywords=RISC+OS) many of the questions seemed to have more to do with user assumptions about how RISC OS works than about things actually not working. RISC OS is different. It comes from a different era. It was designed with a vastly different set of assumptions than POSIX systems like macOS, Linux, BSD, or Windows. Check your assumptions at the door.\n\n### The mouse and its pointer\n\nRISC OS is a three button mouse oriented system. The left and middle mouse buttons play very specific roles.  The left button is for \"action\" and the middle button provides a \"context menu\". RISC OS does not use \"application menu\" at the top of the screen or top of the window frame like macOS, Windows or Raspberry Pi OS. The \"context menu\" provides that functionality.\n\nWhen you point your mouse at a screen element and press the middle button you get the context menu for that thing.  If you single or double click on an item with the left button you are taking an action.\n\n- The left mouse button takes a action. Sometimes you use a single click and at others a double click\n- The middle mouse button brings up the context menu, on RISC OS this is replaces the need for an application menu\n\n### The Iconbar and Pinboard\n\nWhen you start RISC OS you will see two main visual elements. The desktop is called the \"Pinboard\". This is analogous to desktops on Windows, macOS and Raspberry Pi OS.  It contains a background image and icons.\n\nThe Iconbar is the other big visible element. It is found at the bottom of the screen. The Iconbar predates Window's task bar and may have inspired it.  The Iconbar is responsibly for allowing you to access system resources and the loaded applications and modules.\n\nOn the left side of the Iconbar are icons representing system resources. This includes the SD card holding RISC OS. On the right side of the Iconbar you'll find icons that represent running applications or modules.\n\nRISC OS is a single user operating system and relies on cooperative multitasking. An application or module can be loaded into memory and left in memory for fast reuse.  When you \"launch\" an application it loads into memory. To use the application you can either left double click on the icon in the Iconbar or open a file associated with the application. If you do the later then the application will automatically get added to the Iconbar if it was not already present. When you close an application's window you're not removing the application. To close an application you need to go to the Iconbar and use the context menu (remember that middle mouse button) to \"quit\" it.\n\nOn the left, system resources side, data storage is manipulated via the filer windows. You open a filer window by left double clicking on the resource in the Iconbar. The filer window on RISC OS plays a role similar to the file manager on Windows or a finder window on macOS.\n\n### Iconbar and file resources\n\nThe filer maintains metadata about files. This includes the file type and the last application used to save the file. The file type is explicit with RISC OS and is NOT based on a file extension like with macOS, Windows and Raspberry Pi OS. The file path notation is also significantly different from the POSIX path syntax.\n\nDouble click on a storage resource in the Iconbar opening a filer window. The window will list files, folders and applications. The file system is hierarchical.  An icon will indicate the file type. Some icons are folders, these are sub directories of the current filer window. Most files will have other icons associated based on file type. There are special \"files\" that begin with an exclamation symbol, \"!\". In POSIX this is referred as a \"bang\" symbol but in RISC OS it is called a \"pling\".  Filenames starting with the pling hold applications. Applications are implemented as a directory containing resources. Resources might be application's visual assets, configuration, scripts or executable binaries. Directories as application will be familiar to people running modern versions of macOS or who have used NeXTStep. RISC OS usage predated both those systems. As an end user you just click on the pling name to launch the application. It will then show up in the Iconbar ready for use.\n\nOn the right side of the Iconbar are your running applications. When you first startup RISC OS you'll see two running. First, on the far right is an icon representing the hardware you're running on. If you're on a Raspberry Pi it'll be raspberry icon.  If you're running on a Pine64 Pinebook it'll be a pine cone. Since I'm on a Raspberry Pi Zero I will called it a Raspberry icon. Use your mouse and move the mouse pointer over the Raspberry icon. Left double click on the icon. This starts up a dialog box containing information about system resources. If you single click the middle button on the Raspberry icon you will get a operating system context menu. In the context menu you'll see items for \"Configure\" and \"Task Window\". The Task window provides a command line interface for RISC OS. The configure menu lets you configure how RISC OS runs. This includes things like setting the desktop theme and configuring network services.\n\nPointing your mouse to the left of the Raspberry will place the cursor over an icon that looks like a computer monitor.  Double left clicking on this icon will open a dialog that lets you set the resolution of your monitory. Similarly a single middle click on the icon will open a context menu.  This is a major pattern in interacting with RISC OS on the Iconbar. In fact this is the general pattern of using the mouse through out the system. It's take an action (left) or select an action to take (middle).\n\nThe Now you should see it in the Iconbar towards the right.  Note it is only visible in the Iconbar right now. This is very different than double clicking on macOS Or Windows. If you move your mouse over the icon in the Iconbar and Left double click it'll open your applications main dialog or window (just like what happened when we click on the monitor or Raspberry icons). It's a two step process.\n\nHow do you get more applications on the Iconbar?\n\nUse the filer. The default applications are in the \"Apps\" icon visible on the Iconbar. Left double click on it. It'll open a filer window with an abbreviate list of applications. In that filer window look for an application called \"!Alarm\" (pling alarm). Double left click on that application's icon will cause it to appear in the Iconbar towards the right side. Congratulation you've loaded your first RISC OS application.\n\nYou will find more applications by looking at the contents of the SD Card. To do that double left click on the SD Card icon in the Iconbar. It'll open a filer window. You'll see a folder icon labeled \"Apps\". This is an actual directory on the file system. It is where applications are usually installed on RISC OS. Open that folder by double left clicking on it. You can low launch additional application by left double clicking on their pling names. These like Alarm will show up on the right side of the Iconbar.\n\nHaving things running on the Iconbar is convenient.  If you want to start a new document associated with one of the running application you just left double the icon on the Iconbar.  If you want an application to be processed by something running in the Iconbar you can draft the document from the filer window to the icon on the Iconbar. If you want to save that new document start the application from the Iconbar then use the context menu in the application window or press F3 to get the save dialog box. In the dialog box give your document a name and drag the icon of the file from the dialog box to a filer window where you want to store the document. That last step is important to note. Icons are actionable even when they appear in a dialog box! This is very unlike Windows, macOS or Raspberry Pi OS. The dragging the icon is what links data storage with the application. The next time you want to edit the file you just fine the file and left double click on it to open it. If the application isn't running then it'll be launched and added to the Iconbar automatically.\n\nRemember when you close your application's main window or dialog box it doesn't close the application. It only close the file you were working on. To actually unload the application (and free up additional memory) point your mouse at the icon in the Iconbar and middle click to see the context menu. One of the context menu items will be \"Quit\", this is how you fully shutdown the application.\n\n### Context menus\n\nWhat's all this about context menus? On other operating systems the context menu is called an application menu. It might be at the top of the application window (e.g. Windows) or at the top of the screen (e.g. macOS). On Windows and macOS the words \"context menu\" usually mean a special sub-menu in the application (e.g. like when you use a context menu to copy a link in your web browser). In RISC OS the context menu is the application menu. It is always accessed via the middle mouse button just as we saw in introduction to the Iconbar.\n\nSome applications like the \"!StrongEd\" editor do include a ribbon of icons that will make it easy to do various things. That's just a convenience. Other applications like \"!Edit\" don't provide this convenience. Personally I'm ambivalent to ribbon bars. I think they are most useful where you'd have to navigate down a level or two from the context menu to reach a common action you wanted to perform. There are things I don't like about the ribbon button approach. You loose window space to the ribbon. The second you are increasing the distance of mouse travel to take the action. On the other hand it beats traveling down three levels of nesting in the context menu. It's a trade off.\n\nWhat ever the case remember the middle button as picking an action and the left mouse button as taking the action.\n\n### Window frames\n\nWhen you open an application from the Iconbar it'll become a window (a square on the screen containing a dialog or application content). Looking at the top of of the window frame there is are some buttons and application title area. In the upper left of the window frame you see two buttons. From left to right, the first button you see will look like two squares overlaid. Reminds me of the symbols often used when copying text.  That button is not a copy button. The two overlapping squares indicate that the current window can be sent to the bottom (furthest behind) of the stack of overlapping windows. If you think of a bunch of paper loosely stack and being read it's like taking the top paper and putting it at the back of the stack as you finish reading it. I found this very intuitive and a better experience than how you would take similar actions on macOS and Windows.\n\nNext to the back window button is an button with an \"X\" on it. This button closes the window.\n\nTo the right of the close window button is the title bar. This usually shows the path to the document or data you the application is managing. You'll notice the path doesn't resemble a POSIX path or even a path as you'd find on DOS or CP/M.  I'll circle back to the RISC OS path notation in a bit. The path maybe followed by an asterisk. If the asterisk is present it indicates the data has been modified. If you try to close the window without saving a filer dialog will pop up giving a choice to discard changes, cancel closing or saving the changes. A heads up is saving is approach differently in RISC OS than on macOS or Windows.\n\nTo the right of the title bar you'll see minimize button which looks like a dash and a maximize button that looks like a square. These work similar to what you'd expect on macOS or Windows.\n\nA scroll bar is visible on the right side and bottom. These are nice and wide. Very easy to hit using the mouse unlike modern macOS and Windows scroll bars which are fickle. There is one on the bottom the lower right of the window. The lower right that you can use to stretch or squeeze the window easily.\n\nAlong with the middle mouse button presenting the context menu you'll also notice that RISC OS makes extensive use of dialog boxes to complete actions. If you see an Icon in the dialog it's not there as a branding statement. That icon can be dragged to another window such as the Filer window to save a document. This is a significant difference between RISC OS and other operating systems. The Icon has a purpose beyond identification and branding. Moving the Icon and dropping it usually tells the OS to link two things.\n\n### Some historical context and web browsing\n\nRISC OS has a design philosophy and historical implementation. As I understand it the original RISC OS was written in ARM assembly language with BBC BASIC used to orchestrate the pieces written in assembly. That was how it achieved a responsive fluid system running on minimal resources. Looking back at it historically I kinda of think of BBC BASIC as a scripting language for a visual interface built from assembly language modules. The fast stuff was done in assembly but the high level bits were BBC BASIC. Today C has largely taken over the assembly language duties. It even has taken over some of the UI work too (e.g. Pinboard2 is written in C). The nice thing is BBC BASIC remains integrated and available.\n\n> BBC BASIC isn't the BASIC I remember seeing in college.  BBC BASIC appears more thorough. It includes an inline ARM assembly language support. Check out Bruce Smith's books on ARM Assembly Language on RISC OS if you want to explore that.\n\nEven through RISC OS was originally developed with BBC BASIC and assembler it wasn't developed in a haphazard fashion. A very modular approach was taken. Extending the operating system often means writing a module.  The clarity of the approach as much as the tenacity of the community has enable RISC OS to survive long past the demise of Acorn itself. It has also meant that as things have shifted from assembly to C that the modularity has remained strong aspect of RISC OS design and implementation.\n\nRISC OS in spite of its over 30 years of evolution has a remarkably consistent feel. This is in part a historical accident but also a technical one in how the original system was conceived. That consistency is also one is one of its strengths. Humans tend to prefer consistency when they can get it. It is also the reasons you don't see allot of direct ports of applications from Unix, macOS (even single user system 7) or Windows. When I work on a Linux machine I expect the GUI to be inconsistent. The X Window systems was designed for flexibility and experimentation. The graphical experience only becomes consistent within the specific Linux distribution (e.g. Raspberry Pi OS). Windows also has a history of being pretty lose about it user interface. Windows user seem much happier to accept a Mac ported application then Mac users using a Windows ported one.\n\nWhy do I bring this up? Well like WiFi many people presume availability of evergreen browsers. Unlike WiFi I think I can live without Chrome and friends.  There is a browser called NetSurf that comes with RISC OS 5.30. You can view forums and community website with targeting RISC OS with it. If you're a follower of the Tidleverse you'll find NetSurf sufficient too. It's nice not running JavaScript. If you visit GitHub though it'll look very different. Some of it will not work.  Fortunately there is a new browser on the horizon called Iris. I suspect like the addition of native WiFi support you'll see a bump in usability for RISC OS when it lands.\n\n### Access external storage\n\nRISC OS supports access to external storage services. There are several file server types supported but I found SMB the easiest to get setup. I have a Raspberry Pi 3B+ file server running Samba on Raspberry Pi OS (bookworm). The main configuration change I needed to support talking to RISC OS was to enable LANMAN1 protocol. By default the current Samba shipping with bookworm has LANMAN1 protocol turned off (they are good reasons for this). This is added in the global section of the smb.conf file. I added the following line to turn on the protocol.\n\n~~~shell\nserver min protocol = LANMAN1\n~~~\n\nBecause of the vintage nature of the network calls supported and problems of WiFi being easy to sniff I opted to remove my personal directory from Samba services. I also used a different password to access Samba from my user account (see the smbpasswd manual page for how to do that).\n\nI don't store secret or sensitive stuff on RISC OS nor do I access it from RISC OS. Like running an DOS machine security on RISC OS isn't a feature of the operating system. For casual writing, playing retro games, not a big problem but I would not do my banking on it.\n\n### Connecting to my Raspberry Pi File Server\n\nOnce I had Samba configured to support LANMAN1 I was able to connect to it from RISC OS on the Pi Zero but it was tricky to figure out how at first.  The key was to remember to use the context menu and to use the `!OMNI` application found in the \"Apps\" folder on the Iconbar.\n\nFirst open the Apps folder, then right double click on `!OMNI`.  This will create what looks like a file server on the left side of the Iconbar (where other disk resources are listed).  If you place your mouse pointer over it and middle click the context menu will pop up. Then navigate to the protocol menu, followed by LanMan. Clicking on LanMan should bring up a dialog box that will let you set the connection name, the name of the file server as well as the login credentials for that server.  There is no OK button. When you press enter after entering your password the dialog box knows you're done and OMNI will try to make the connection. You'll see the mouse pointer turn into an hour glass as it negotiates the connection. If the connection is successful you'll see a new window open with the resource's available from that service.\n\nYou can save yourself time by \"saving\" the protocol setup via the context menu. To do that you point to the OMNI icon, pull up the context menu, select protocol then select save protocol.\n\n### RISC OS Paths\n\nThe RISC OS path semantics are NOT the same as POSIX.  The RISC OS file system is hierarchical but does not have a common root like POSIX. Today RISC OS supports several types of file systems and protocols. As a result the path has a vague resemblance to a URI.\n\nThe path starts with the protocol. This is followed by a double colon then the resource name. The resource name is followed by a period (a.k.a. dot) and then the dollar sign. The dot and dollar sign represents the root of the file system resource. The dot (period) is used as a path delimiter. It appears to be a practice to use a slash to indicate a file extension. This is important to remember. A path like `myfile.txt` on RISC OS means there is a folder called \"myfile\" and a file called \"txt\" in side it. On the other hand `myfile/txt` would translate to the POSIX form of `myfile.txt`. Fortunately the SMB client provided by OMNI handles this translation for us.\n\n## first impressions\n\nI find working on RISC OS refreshing. It is proving to be a good writing platform for me. I do have a short wish list. RISC OS seems like a really good platform for exploring the text oriented internet.  I would like to see both a gopher client and server implemented on RISC OS. Similarly I think Gemini protocol makes sense too.  I miss not having Pandoc available as I use that to render my writing to various formats on POSIX systems (e.g. html, pdf, ePub).\n\nWhat might I build in RISC OS?  I'm not sure yet though I have some ideas. I really like RISC OS as a writing platform. The OS itself reminds me of some of the features I like in Scrivener. I wonder if Scrivener's Pinboard got its inspiration from RISC OS?\n\nI've written Fountain, Open Screen Play, Final Draft conversion tools in Go. I'd like to have similar tools available along with a nice editor available on RISC OS. Not sure what the easiest approach to doing that is. I've ordered the [ePic](https://www.riscosopen.org/content/sales/risc-os-epic/epic-overview) SD card and that'll have the [DDE](https://www.riscosopen.org/content/sales/dde) for Raspberry Pi. It might be interesting to port OBNC Oberon-07 compiler to RISC OS and see if I could port my Go code to Oberon-07 in a sensible way.\n",
      "data": {
        "abstract": "In this post I talk about my exploration of using a Raspberry Pi Zero W\nas a desktop computer. This was made possible by the efficiency of \nRISC OS 5.30 which includes native WiFi support for Raspberry Pi computers.\n",
        "byline": "R. S. Doiel, 2024-06-04",
        "keywords": [
          "RISC OS",
          "Raspberry Pi"
        ],
        "pubDate": "2024-06-04",
        "title": "Exploring RISC OS 5.30 on a Raspberry Pi Zero W"
      },
      "url": "posts/2024/06/04/exploring_riscos.json"
    },
    {
      "content": "\n# Building Lagrange on Raspberry Pi OS\n\nThese are my quick notes on building the Lagrange Gemini browser on Raspberry Pi OS. They are based on instructions I found at <gemini://home.gegeweb.org/install_lagrange_linux.gmi>. These are in French and I don't speak or read French. My loss. The author kindly provided the specific command sequence in shell that I could read those. That was all I needed. When I read the site today I had to click through an expired certificate. That's why I think it is a good idea to capture the instructions here for the next time I need them.  I made single change to his instructions. I have cloned the repository from <https://github.com/skyjake/lagrange>.\n\n## Steps to build\n\n1. Install the programs and libraries in Raspberry Pi OS to build Lagrange\n2. Create a directory to hold the repository, then change into it\n3. Clone the repository\n4. Add a \"build\" directory to the repository and change into it\n5. Run \"cmake\" to build the release\n6. Run \"make\" in the build directory and install\n7. Test it out.\n\nWhen you clone the repository you want to clone recursively and get the release branch. Below is a transcript of the commands I typed in my shell to build Lagrange on my Raspberry Pi 4.\n\n~~~shell\nsudo apt install build-essential cmake \\\n     libsdl2-dev libssl-dev libpcre3-dev \\\n     zlib1g-dev libunistring-dev git\nmkdir -p src/github.com/skyjake && cd src/github.com/skyjake \ngit clone --recursive --branch release git@github.com:skyjake/lagrange.git\nmkdir -p lagrange/build && lagrange/build\ncmake ../ -DCMAKE_BUILD_TYPE=Release\nsudo make install\nlagrange &\n~~~\n\nThat's about it. It worked without a hitch. I'd like to thank Gérald Niel who I think created the page on gegeweb.org. I attempted to leave a thank you via the web form but couldn't get past the spam screener since I didn't understand the instructions. C'est la vie.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-05-10",
        "pubDate": "2024-05-10",
        "title": "Building Lagrange on Raspberry Pi OS"
      },
      "url": "posts/2024/05/10/building-lagrange-on-pi-os.json"
    },
    {
      "content": "\n# A quick review of Raspberry Pi Connect\n\nThe Raspberry Pi company has created a nice way to share a Pi Desktop. It is called Raspberry Pi Connect. It is built on the peer-to-peer capability of modern web browsers using [WebRTC](https://en.wikipedia.org/wiki/WebRTC). The connect service requires a Raspberry Pi 4, Raspberry Pi 400 or Raspberry Pi 5 running the [Wayland](https://en.wikipedia.org/wiki/Wayland_(protocol)) display server and Bookworm release of Raspberry Pi OS.\n\nWhen I read the [announcement](https://www.raspberrypi.com/news/raspberry-pi-connect/) I wondered, why create Raspberry Pi Connect? RealVNC has works fine.  RealVNC even has a service to manage your RealVNC setups.\n\nI think the answer has three parts. First it gives us another option for sharing a Pi Desktop. Second it is a chance to make things easier to use. Third if you can share a desktop using WebRTC then you can also provide additional services.\n\nFor me the real motivator is ease of use. In the past when I've used RealVNC between two private networks I've had to setup SSH tunneling. Not unmanageable but certainly not trivial.  I think this is where Raspberry Pi Connect shines. Setting up sharing is a three step process.\n\n1. Start up your Pi desktop, install the software\n2. Create a Raspberry Pi Connect account and register your Pi with the service\n3. On another machine point your web browser at the URL for Raspberry Pi connect and press the connect button\n\nThe next time you want to connect you just turn on your Pi and login. If I have my Pi desktop to auto login then I just turn the Pi on and when it finishes booting it is ready and waiting. On my other machine I point my web browser at the connect website, login and press the connection button.\n\nWhen I change computers I don't have to install VNC viewers. I don't have to worry about setting secure ssh tunnels. I point my web browser at the Raspberry Pi Connect site, login and press the connect button. The \"one less thing to worry about\" can make it feel much less cumbersome.\n\n## How does it work?\n\nThe Raspberry Pi Connect architecture is intriguing. It leverages [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API). WebRTC supports peer to peer real time connection between two web browsers running in separate locations across the internet. Making a WebRTC requires the two sites to use a URL to establish contact. From that location perform some handshaking and see if the peer connection can be establish a directly between the two locations. If the direct connection can't be established then a relay or proxy can be provided as a fallback. \n\nThe Raspberry Pi Connect site provides the common URL to contact. On the Pi desktop side a Wayland based service provides access to the Pi's desktop. On the other side you use a Web Browser to display and interact with the desktop. Ideally the two locations can establish a direct connection. If that is not possible then Raspberry Pi Connect hosts [TURN](https://en.wikipedia.org/wiki/Traversal_Using_Relays_around_NAT) in London as a fallback. A direct connection gives you a responsive shared desktop experience but if you're on the Pacific edge of North America or on a remote Pacific island then traffic being relayed via London can be a painfully slow experience.\n\nThe forum for the Raspberry Pi Connect has a [topic](https://forums.raspberrypi.com/viewtopic.php?t=370591&sid=61d7cdf3c03a7ead49e3da837b0d4f06) discussing the routing algorithm and choices. The short version is exerted below.\n\n> Essentially, when the connection was being established both sides provided their internet addresses (local, and WAN) - and when both sides tested their ability to talk to the other side, they failed. Only after this failure is the TURN server used.\n\n## Questions\n\nCan I replace RealVNC with Raspberry Pi Connect?\n\nIt depends. I still use Raspberry Pi 2, 3 and some Zeros. I'm out of luck using Pi Connect since these devices aren't supported. If you've already installed RealVNC and it's working well for you then sharing via Pi connect is less compelling.\n\nIf I was setting up a new set of Raspberry Pi 4/400 or 5s then I'd probably skip RealVNC and use Pi connect. It's feels much easier and unless the network situation forces you to route traffic through London is reasonably responsive.\n\nIs screen sharing the only thing Raspberry Pi Connect provides?\n\nI expect if Raspberry Pi Connect proves successful we'll see other enhancements. One of the ones mentioned in the forums was SSH services without the hassle of dealing with setting up tunnels. The folks in the Raspberry Pi company, foundation and community are pretty creative. It'll be interesting to see where this leads.\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-05-10",
        "keywords": [
          "raspberry pi"
        ],
        "pubDate": "2024-05-10",
        "title": "A quick review of Raspberry Pi Connect"
      },
      "url": "posts/2024/05/10/quick-review-rpi-connect.json"
    },
    {
      "content": "\n# Getting Started with Miranda\n\nI've been interested in exploring the Miranda programming language. Miranda influenced Haskell. Haskell was used for programs I use almost daily such as [Pandoc](https://pandoc.org) and [shellcheck](https://www.shellcheck.net/). I've given a quick review of [miranda.org.uk](https://miranda.org.uk) to get a sense of the language but to follow along with the [Miranda: The Craft of Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/Miranda_craft/) it is really helpful to have Miranda available on my machine. Today that machine is a Mac Mini, M1 processor, running macOS Sonoma (14.4.x) and the related Xcode C tool chain.  I ran into to minor hiccups in compilation and installation. Both easy to overcome but ones I will surely forget in the future. Thus I write myself another blog post.\n\n## Compilation\n\nFirst down load Miranda source code at <http://miranda.org.uk/downloads>. The version 2.066 is the most recent release I saw linked (2024-04-25), <http://www.cs.kent.ac.uk/people/staff/dat/ccount/click.php?id=11>. The [COPYING](https://www.cs.kent.ac.uk/people/staff/dat/miranda/downloads/COPYING) link shows the terms under which this source release is made available.\n\nNext you need to untar/gzip the tarball you downloaded. Try running `make` to see if it compiles. On my Mac Mini I got a compile error that looks like\n\n~~~shell\nmake\ngcc -w    -c -o data.o data.c\ndata.c:666:43: error: incompatible integer to pointer conversion passing 'word' (aka 'long') to parameter of type 'char *' [-Wint-conversion]\n                     else fprintf(f,\"%c%s\",HERE_X,mkrel(hd[x]));\n                                                        ^~~~~\n1 error generated.\nmake: *** [data.o] Error 1\n~~~\n\nWhile I'm rusty on C I read this as the C compiler being more strict today then it was back in the 1990s. That's a good thing generally.  Next I checked the compiler version. \n\n~~~shell\ngcc --version\nApple clang version 15.0.0 (clang-1500.3.9.4)\nTarget: arm64-apple-darwin23.4.0\nThread model: posix\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\n~~~\n\nI'm using clang and the website mentioned it should compile with clang for other platforms.  I reviewed the data.c file and notice other similar lines that invoked `mkrel(hd[x])` had a `(char *)` cast in front of `hd[x]`. This tells me that being explicit with the compiler might solve my problem. I edited line 666 of data.c to look like\n\n~~~C\n    else fprintf(f,\"%c%s\",HERE_X,mkrel((char *)hd[x]));\n~~~\n\nSave the file and then ran Make again. It compile cleanly. I gave at quick test run of the `mira` command creating an simple function called `addone`\n\n~~~miranda\nmira\n/edit\naddone a = a + 1\n:wq\naddone (addone (addone 3))\n6\n/q\n~~~\n\nMiranda seems to work. The Makefile comes with a an install rule but the install defaults doesn't really work with macOS (it wants to install into `/usr`).\nI'd rather it install into my home directory so I copied the Makefile to `miranda.mak` and change the lines setting `BIN`, `LIB` and `MAN` to the following\nlines.\n\n~~~Makefile\nBIN=$(HOME)/bin\nLIB=$(HOME)/lib#beware no spaces after LIB\nMAN=$(HOME)/man/man1\n~~~\n\nIn my `.profile` I set the `MIRALIB` variable to point at `$HOME/lib/miralib`. I opened a new terminal session and ran `mira` and the interpreter was up and running.\n\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-04-25",
        "keywords": [
          "functional",
          "miranda"
        ],
        "pubDate": "2024-04-25",
        "title": "Getting Started with Miranda"
      },
      "url": "posts/2024/04/25/getting-started.json"
    },
    {
      "content": "\n# A Text oriented web\n\nBy R. S. Doiel, 2024-02-25\n\nThe web is a busy place. There seems to be a gestalt resonant at the moment on the web that can be summarized by two phrases, \"back to basics\" and \"simplification\". It is not the first time I've seen this nor is it likely the last. This blog post describes a thought experiment about a simplification with minimal invention and focus on feature elimination. It's a way to think about the web status quo a little differently. My intention is to explore the implications of a more text centered web experience that could coexist as a subset of today's web.\n\n## The web's \"good stuff\"\n\nI think the following could form the \"good stuff\" in a Crockford[^1] sense of pairing things down to the essential.\n\n- the transport layer should remain HTTP but be limited to a few methods (GET, POST and HEAD) and the common header elements (e.g. length, content-type come to mind)\n- The trio of HTML, CSS and JavaScript is really complex, swap this out for Markdown augmented with YAML (Markdown and YAML already have a synergy in Markdown processors like Pandoc)\n- A Web form is expressed using GHYITS[^2], it is delimited in the Markdown document by the familiar \"`^---$`\" block element, web form content would be encoded as YAML in the body of the POST using the content type \"`application/x-yaml`\".\n- Content would be served using the `text/markdown; charset: utf-8` content type already commonly used to identify Markdown content distinct from `plain/text`\n\nI need a nice name for describing the arrangement of Markdown+YAML over HTTP arrangement. Is the descriptive acronym for \"text oriented web\", i.e. \"tow\", enough? Does it already have a meaning in software or the web? Would the protocol be \"tow://\"? I really need something a bit more clever and catchy if this is going to proceed beyond a thought experiment.\n\n[^1]: Douglas Crockford \"discovered\" JSON, see <https://en.wikipedia.org/wiki/Douglas_Crockford>\n\n[^2]: GHYITS, acronym, GitHub YAML Issue Template Syntax, see <https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/syntax-for-issue-forms>\n\n## Prototyping Options\n\nA proof of concept could be possible using off the self web server and web browser. The missing parts would be setting up the web server to add the `text/markdown; charset: utf-8` header for \"`.md`\" files and to handle processing POST with a content type of `application/x-yaml`. Client side could be implemented in a static web page via JavaScript or WASM module. The JS/WASM could convert the Markdown+YAML into HTML rendering the \"tow\" content. The web form on submit would be intercepted by the JavaScript event handler and reformulated as a POST with a content type of `application/x-yaml`.\n\nBuilding a \"tow\" server and client should be straight forward in Go (probably many languages). The the standard \"http\" package can be used to implement the specialized http server. The `yaml.v3` package to process the YAML POST data. Similar you should be able to create a text client for the command line or even a GUI client via [Fyne](https://fyne.io)\n\n## Exploratory Questions\n\n- What does it mean to have a more text oriented web?\n- What advantages could a text user interface have over a graphical user interface?\n- Can \"tow\" provide enough simple interactivity to support interactive fiction?\n- Could a simple specification be stated clearly in a few pages of text?\n- What possibilities open up when a web browser can send a data structure via YAML to a service?\n- Can we live with a simpler client than a modern evergreen web browser?\n- With a conversation interaction model of \"listener\" and a \"speaker\", does it make sense thinking in terms of client server architecture?\n- How hard is it to support both traditional website and this minimal \"tow\" site using the same corpus?\n- Can this be done sustainably?\n\n## Extrapolations\n\nFrom a thought experiment I can see how to implement this both from a proof of concept level but also from a service and viewer level. I think it even offers an opportunity to function in a peer to peer manner.  If we're focusing primarily on text then the storage requirements can be minimal and the service could even live in a database system like SQLite3 as a form of sandbox of content.  Leveraging HTTP/HTTPS means we don't need any special support for content traveling across the net. With a much smaller foot print you can scratch the itch of a simpler textual experience without the trackers, JavaScript ping backs, etc. It could re-emphasize the conversion versus broadcast metaphor popularized by the walled gardens.  It might provide a more satisifying experience on Mobile since the payloads delivered to the web browser could be much smaller.\n\n## What is needed to demonstrate a standalone \"tow\"?\n\n- A modified HTTP web server (easy to implement in Go and other languages)\n- A viewer/browser, possible to implement via Fyne in Go or as a text application/command line interface in Go\n\n## Why not Gopher or Gemini?\n\nTow is not there to replace anything, not Gopher, Not Gemini, the WWW. It is an exploration of a subset of the WWW protocols with a specific focused on textual interaction. I don't see why a server or browser couldn't support Gopher and Gemini as well as Tow. Given that Markdown can easily be rendered into Gem Text, and Markdown can be treated as plain text I suspect you should be able to support all three text rich systems from the same copy and easily derive a full HTML results if desired too.\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2024-02-25",
        "createdDate": "2024-02-25",
        "keywords": [
          "web",
          "text"
        ],
        "pubDate": "2024-02-25",
        "title": "A Text Oriented Web"
      },
      "url": "posts/2024/02/25/text_oriented_web.json"
    },
    {
      "content": "\n# Installing pgloader from source\n\nBy R. S. Doiel, 2024-02-01\n\nI'm working on macOS at the moment but I don't use Home Brew so the instructions to install pgloader are problematic for me. Except I know pgloader is a Lisp program and once upon a time I had three different Lisps running on a previous Mac.  So what follows is my modified instructions for bringing pgloader up on my current Mac Mini running macOS Sonoma 14.3 with Xcode already installed.\n\n## Getting your Lisps in order\n\npgloader is written in common list but the instructions at https://pgloader.readthedocs.io/en/latest/install.html specifically mention compiling with [SBCL](https://sbcl.org) which is one of the Lisps I've used in the past. But SBCL isn't (yet) installed on my machine and SBCL is usually compiled using SBCL but can be compiled using other common lists.  Enter [ECL](https://ecl.common-lisp.dev/), aka Embedded Common-Lisp. ECL compiles via a C compiler including the funky setup that macOS has. This means the prep for my machine should look something like\n\n1. Compile then install ECL\n2. Use ECL to compile SBCL\n3. Install SBCL\n4. Now that we have a working SBCL, follow the instructions to compile pgloader and install\n\nNOTE: pgloader requires some specific configuration of SBCL when SBCL is compiled\n\n## Getting ECL up and running\n\nThis recipe is straight forward. \n\n1. Review ECL's current website, find latest releases\n2. Clone the Git repository from GitLab for ECL\n3. Follow the install documentation and compile ECL then install it\n\nHere's the steps I took in the shell (I'm installing ECL, SBCL in my home directory)\n\n```\ncd\ngit clone https://gitlab.com/embeddable-common-lisp/ecl.git \\\n          src/gitlab.com/embeddable-common-lisp/ecl\ncd src/gitlab.com/embeddable-common-lisp/ecl\n./configure --prefix=$HOME\nmake\nmake install\n```\n\n## Getting SBCL up and running\n\nTo get SBCL up and running I grab the sources using Git then compile it with the options recommended by pgloader as well as the options to compile SBCL with another common lisp, i.e. ECL. (note: the `--xc-host='ecl'`)\n\n```\ncd\ngit clone git://git.code.sf.net/p/sbcl/sbcl src/git.code.sf.net/p/sbcl/sbcl\ncd git clone git://git.code.sf.net/p/sbcl/sbcl\nsh make.sh --with-sb-core-compression --with-sb-thread --xc-host='ecl'\ncd ./tests && sh ./run-tests.sh\ncd ..\ncd ./doc/manual && make\ncd ..\nenv INSTALL_ROOT=$HOME sh install.sh\n```\n\nAt this time SBCL should be available to compile pgloader.\n\n## Install Quicklisp\n\nQuicklisp is a package manager for Lisp. It is used by pgloader so also needs to be installed. We have two lisp on our system but since SBCL is the one I need to work for pgloader I install Quicklisp for SBCL.\n\n1. Check the [Quicklisp website](https://www.quicklisp.org/beta/) and see how things are done (it has been a long time since I did some lisp work)\n2. Follow the [instructions](https://www.quicklisp.org/beta/#installation) on the website to install Quicklisp for SBCL\n\nThis leaves me with the specific steps\n\n1. Use curl to download quicklisp.lisp\n2. Use curl to download the signature file\n3. Verify the signature file\n4. If OK, load into SBCL\n5. From the SBCL repl execute the needed commands\n\n```\ncurl -O https://beta.quicklisp.org/quicklisp.lisp\ncurl -O https://beta.quicklisp.org/quicklisp.lisp.asc\ngpg --verify quicklisp.lisp.asc quicklisp.lisp\nsbcl --load quicklisp.lisp\n```\n\nAt this point you're in SBCL repl. You need to issue the follow command\n\n```\n(quicklisp-quickstart:install)\n(quit)\n```\n\n\n## Compiling pgloader\n\nOnce you have SBCL and Quicklisp working you're now ready to look at the rest of the dependencies. Based on the what other Linux systems required I figure I need to have the following available\n\n- SQLite 3, libsqlite shared library (already installed)\n- unzip (already installed)\n- make (already installed)\n- curl (already installed)\n- gawk (already installed)\n- freetds-dev (not installed)\n- libzip-dev (not installed)\n\nTwo libraries aren't installed on my system. I use Mac Ports so doing a quick search both appear to be available.\n\n```\nsudo port search freetds\nsudo port search libzip\nsudo port install freetds libzip\n```\n\n\nOK, now I think I am ready to build pgloader. Here's what I need to do.\n\n1. Clone the git repo for pgloader\n2. Invoke make with the right options\n3. Test installation\n\n```\ncd\ngit git@github.com:dimitri/pgloader.git src/github.com/dimitri/pgloader\ncd src/github.com/dimitri/pgloader\nmake save\n./build/bin/pgloader -h\n```\n\nIf all works well I should see the help/usage text for pgloader. The binary executable\nis located in `./build/bin` so I can copy this into place in `$HOME/bin/` directory.\n\n```\ncp ./build/bin/pgloader $HOME/bin/\n```\n\nHappy Loading.\n, \"PostgreSQL\"\n\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-02-01",
        "keywords": [
          "SQL",
          "Postgres",
          "PostgreSQL",
          "MySQL",
          "pgloader",
          "lisp",
          "macos",
          "ecl",
          "sbcl"
        ],
        "number": 6,
        "pubDate": "2024-02-01",
        "series": "SQL Reflections",
        "title": "Installing pgloader from source"
      },
      "url": "posts/2024/02/01/installing-pgloader-from-source.json"
    },
    {
      "content": "\n# vis for vi and fun\n\nBy R. S. Doiel, 2024-01-31 (updated: 2024-02-02)\n\n\nI've been looking for a `vi` editor that my fingers would be happy with. I learned `vi` when I first encountered Unix in University (1980s). I was a transfer student so didn't get the \"introduction to Unix and Emacs\" lecture. Everyone used Emacs to edit programs but Emacs to me was not intuitive. I recall having a heck of a time figuring out how to exit the editor! I knew I needed to learn an editor and Unix fast to do my school work. I head to my college bookstore and found two spiral bound books [Unix in a Nutshell](https://openlibrary.org/works/OL8724416W?edition=key%3A/books/OL24392296M) and \"Vi/Ed in a Nutshell\". They helped remedy my ignorance. I spent the afternoon getting comfortable with Unix and learning the basics in Vi. It became my go to text editor. Somewhere along the line `nvi` came along I used that. Eventually `vim` replaced `nvi` as the default \"vi\" for most Linux system and adapted again.  I like one featured about `vim` over `nvi`. `vim` does syntax highlighting. I routinely get frustrate with `vim` (my old muscle memory throws me into the help systems, very annoying) so I tend to bounce between `nvi` and `vim` depending on how my eyes feel and frustration level. \n\n## vis, the vi I wished for\n\nRecently I stumbled on `vis`. I find it a  very interesting `vi` implementation. Like `vim` it mostly conforms to the classic mappings of a modal editor built on top of `ed`. But `vis` has some nice twists. First it doesn't try to be a monolithic systems like Emacs or `vim`. Rather then used an application specific scripting language (e.g. Emacs-lisp, vim-script) it uses Lua 5.2 as its configuration language. For me starting up `vis` feels like starting up `nvi`. It is quick and responsive where my typical `vim` setup feels allot like Visual Studio Code in that it's loading a whole bunch of things I don't use. \n\nHad `vis` just had syntax highlighting I don't know if I was would switched from `vim`. `neovim` is a better vim but I don't use it regularly and don't go out of my way to install it.  `vis` has one compelling feature that pushed me over the edge. One I didn't expect. `vis` supports [structured regular expressions](http://doc.cat-v.org/bell_labs/structural_regexps/se.pdf \"PDF paper explain structured regular expression by Rob Pike\"). This is the command language found in Plan 9 editors like [sam](http://sam.cat-v.org/) and [Acme](http://acme.cat-v.org/). The approach to regexp is oriented around streams of characters rather than lines of characters. It does this by supporting the concept of multiple cursors and operating on selections (note the plural) in parallel. This allows a higher degree of transformation, feels like a stream oriented AWK but with simpler syntax for the things you do all the time. It was easiest enough to learn that my finger quickly adapted to it. It does mean that in command mode my search and replace is different than what I used to type. E.g. changing CRLF to LF\n\n```\n:1,$x/\\r/ c//\n```\n\nversus\n\n```\n:1,$s/\\r//g\n```\n\nJust enough different to catch someone who is used to `vim` and `nvi` unaware.\n\n## Be careful what you wish for on Ubuntu\n\nWhen I decided I want to use `vis` as my preferred \"vi\" in went and installed it on all my work Ubuntu boxes. What surprised me was that when you install `vis` on an Ubuntu system it winds up becoming the default \"vi\". That posed a problem because I hadn't consulted with the other people who use those machines. I thought I would type `vis` instead of `vi` to use it. Fortunately Ubuntu also provides a means of fixing which alternative programs can be used for things like \"vi\".  I reverted the default \"vi\" to `vim` for my colleagues using the Ubuntu command `update-alternatives` (e.g. `sudo update-alternatives --config vi`). No surprises for them and I still get to use `vis`, I just type the extra \"s\". \n\n## Getting to know structured regular expressions and case swapping\n\nA challenge in making the switch to `vis` is learning a new approach to search and replace. Fortunately Marc Tanner gives you the phrases in his documentation.  Searching for \"structured regular expressions\" leads to Rob Pike's paper of the same name. The other thing Marc points out is his choices in implementing `vis`. `vis` is like `vi` meets the Sam editor of Plan 9 fame.  You can try Plan 9 Sam editor by installing [Plan 9 User Space](https://9fans.github.io/plan9port/). Understanding Sam made the transition to `vis` smoother. I recommend reading Rob Pike's paper on \"Structured Regular Expressions\"[^1], his \"Sam Tutorial\"[^2] then keeping the \"Sam Cheatsheet\"[^3] handy during the transition. The final challenge I ran into in making the switch is the old `vi` stand by for flipping case for letters in visual mode.  In the old `vi` you use the tilde key, `shift+~`. In `vis` you press `g` then `~` to change the case on a letter.  \n\n[^1]: Rob Pike's [\"structured regular expressions\"](http://doc.cat-v.org/bell_labs/structural_regexps/se.pdf \"PDF document\")\n[^2]: [Sam Tutorial](http://doc.cat-v.org/bell_labs/sam_lang_tutorial/sam_tut.pdf \"PDF document\")\n[^3]: [Sam Cheat Sheet](http://sam.cat-v.org/cheatsheet/ \"html document containing an image\")\n\n\n## A few \"thank you\" or \"how did I stumble on vis?\"\n\nI'd like to say thank you to [Marc André Tanner](https://github.com/martanne) for writing `vis`, [Glendix](https://www.glendix.org/) for highlighting it and to OS News contributor [pablo_marx](https://www.osnews.com/submissions/?user=pablo_marx) for the story [Glendix: Bringing the Beauty of Plan 9 to Linux](https://www.osnews.com/story/20588/glendix-bringing-the-beauty-of-plan-9-to-linux/). With this I find my fingers are happier.\n\n## Additional resources\n\n- [Marc André Tanner](https://www.brain-dump.org/projects/vis/)'s vis project page\n- [vis on GitHub](https://github.com/martanne/vis/)\n- [vis @ readthedocs](https://vis.readthedocs.io/en/master/vis.html)\n- [Vis Wiki](https://github.com/martanne/vis/wiki)\n- [GitHub Topic](https://github.com/topics/vis-editor)\n- [Plugin collection](https://erf.github.io/vis-plugins/)\n\n",
      "data": {
        "byline": "R. S. Doiel, 2024-01-31",
        "created": "2024-01-31",
        "keywords": [
          "editors",
          "vi"
        ],
        "pubDate": "2024-01-31",
        "title": "vis for vi and fun",
        "updated": "2024-02-02"
      },
      "url": "posts/2024/01/31/vis-for-vi-and-fun.json"
    },
    {
      "content": "\n# Updated recipe, compile PostgREST 12.0.2 (M1)\n\nby R. S. Doiel, 2024-01-04\n\nThese are my updated \"quick notes\" for compiling PostgREST v12.0.2 on a M1 Mac Mini using the current recommended\nversions of ghc, cabal and stack supplied [GHCup](https://www.haskell.org/ghcup).  When I recently tried to use\nmy previous [quick recipe](/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.md) I was disappointed it failed with errors like \n\n~~~\nResolving dependencies...\nError: cabal: Could not resolve dependencies:\n[__0] trying: postgrest-9.0.1 (user goal)\n[__1] next goal: optparse-applicative (dependency of postgrest)\n[__1] rejecting: optparse-applicative-0.18.1.0 (conflict: postgrest =>\noptparse-applicative>=0.13 && <0.17)\n[__1] skipping: optparse-applicative-0.17.1.0, optparse-applicative-0.17.0.0\n(has the same characteristics that caused the previous version to fail:\nexcluded by constraint '>=0.13 && <0.17' from 'postgrest')\n[__1] trying: optparse-applicative-0.16.1.0\n[__2] next goal: directory (dependency of postgrest)\n[__2] rejecting: directory-1.3.7.1/installed-1.3.7.1 (conflict: postgrest =>\nbase>=4.9 && <4.16, directory => base==4.17.2.1/installed-4.17.2.1)\n[__2] trying: directory-1.3.8.2\n[__3] next goal: base (dependency of postgrest)\n[__3] rejecting: base-4.17.2.1/installed-4.17.2.1 (conflict: postgrest =>\nbase>=4.9 && <4.16)\n\n...\n\n~~~\n\nSo this type of output means GHC and Cabal are not finding the versions of things they need\nto compile PostgREST. I then tried picking ghc 9.2.8 since the default.nix file indicated\na minimum of ghc 9.2.4.  The `ghcup tui` makes it easy to grab a listed version then set it\nas the active one.\n\nI made sure I was working in the v12.0.2 tagged release version of the Git repo for PostgREST.\nThen ran the usual suspects for compiling the project. I really wish PostgREST came with \ninstall from source documentation. It took me a while to think about looking in the default.nix\nfile for a minimum GHC version. That's why I am writing this update.\n\nA similar recipe can be used for building PostgREST on Linux.\n\n1. Upgrade [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)\n    a. Use the curl example command to install it or `gchup upgrade`\n    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)\n2. Make sure GHCup is pointing at the version PostgREST v12.0.2 needs, i.e. ghc v9.2.8. I chose to keep \"recommended\" versions of Cabal and Stack\n3. Clone <https://github.com/PostgREST/postgrest> to my local machine\n4. Check out the version you want to build, i.e. v12.0.2\n5. Run the \"usual\" Haskell build sequence with cabal\n    a. `cabal clean`\n    b. `cabal update`\n    c. `cabal build`\n    d. `cabal install` (I use the `--overwrite-policy=always` option to overwrite my old v11 postgrest install)\n\nHere's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).\n\n~~~\nghcup upgrade\nghcup tui\nmkdir -p src/github.com/PostgREST\ncd src/github.com/PostgREST\ngit clone git@github.com:PostgREST/postgrest\ncd postgrest\ngit checkout v12.0.2\ncabal clean\ncabal update\ncabal build\ncabal install --overwrite-policy=always\n~~~\n\nThis will install PostgREST in your `$HOME/.cabal/bin` directory. Make sure\nit is in your path (it should be if you've sourced the GHCup environment after you installed GHCup).\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "PostgREST",
          "M1"
        ],
        "pubDate": "2024-01-04",
        "title": "Updated recipe, compiling PostgREST 12.0.2 (M1)"
      },
      "url": "posts/2024/01/04/updated-recipe-compiling-postgrest_v12.0.2.json"
    },
    {
      "content": "\n# Building A to Z lists pages in Pandoc\n\nBy R. S. Doiel, 2023-10-18\n\nPandoc offers a very good template system. It avoids elaborate features in favor of a few simple ways to bring content into the page.  It knows how to use data specified in “front matter” (a YAML header to a Markdown document) as well as how to merge in JSON or YAML from a metadata file.  One use case that is common in libraries and archives that less obvious of how to handle is building A to Z lists or year/date oriented listings where you have a set of navigation links at the top of the page followed by a set of H2 headers with UL lists between them.  In JSON the typical data presentation would look something like\n\n```json\n{\n  \"a_to_z\": [ \"A\", \"B\"],\n  \"content\": {\n    \"A\": [\n      \"After a beautiful day\",\n      \"Afterlife\"\n    ],\n    \"B\": [\n      \"Better day after\",\n      \"Better Life\"\n    ]\n  }\n}\n```\n\nThe trouble is that while YAML’s outer dictionary (key/value map) works fine in Pandoc templates there is no way for the the for loop to handle maps of maps like we have above.  Pandoc templates really want to iterate over arrays of objects . That’s nice thing! It gives us more ways to transform the data to provide more flexibility in our template implementation. Here’s how I would restructure the previous JSON to make it easy to process via Pandoc’s template engine.  Note how I’ve taken our simple array of letters and turned them into an object with an “href” and “label” attribute. Similarly I’ve enhanced the “content” objects.\n\n```json\n{\n  \"a_to_z\": [ {\"href\": \"A\", \"label\": \"A\"}, {\"href\": \"B\", \"label\": \"B\"} ],\n  \"content\": [\n      {\"letter\": \"A\", \"title\": \"After a beautiful day\", \"id\": \"after-a-beautiful-day\"},\n      {\"title\": \"Afterlife\", \"id\": \"afterlife\"},\n      {\"letter\": \"B\", \"title\": \"Better day after\", \"id\": \"better-day-after\"},\n      {\"title\": \"Better Life\", \"id\": \"better-life\"}\n  ]\n}\n```\n\nThen the template can be structure something like\n\n```\n<menu>\n${for(a_to_z)}\n${if(it.href)}<li><a href=\"${it.href}\">${it.label}</a></li>${endif}\n${endfor}\n</menu>\n\n${for(content)}\n${if(it.letter)}\n\n## <a id=\"${it.letter}\" name=\"${it.letter}\">${it.letter}</a>\n\n${endif}\n- [${it.name}](${it.id})\n${endfor}\n\n```\n\nThere is one gotcha in A to Z list generation. A YAML parser may convert a bare “N” to “false” (and presumable “Y” will become “true”). This is really annoying. The way to avoid this is to add a space to the letter in your JSON output. This will insure that the “N” or “Y” aren’t converted to the boolean values “true” and “false”. Pandoc’s template engine is smart enough to trim leading and trailing spaces.\n\nFinally this technique can be used to produce lists and navigation that are based around years, months, or other iterative types but that is left as an exercise to the reader.\n\n\n\n",
      "data": {
        "keywords": [
          "Pandoc",
          "templates"
        ],
        "pubDate": "2023-10-18",
        "title": "Building A to Z list pages in Pandoc"
      },
      "url": "posts/2023/10/18/A-to-Z-lists.json"
    },
    {
      "content": "# skimmer\n\nBy R. S. Doiel, 2023-10-06\n\nI have a problem. I like to read my feeds in newsboat but I can't seem to get it working on a few machines I use.\nI miss having access to read feeds. Additionally there are times I would like to read my feeds in the same way\nI read twtxt feeds using `yarnc timeline | less -R`. Just get a list of all items in reverse chronological order.\n\nI am not interested in reinventing newsboat, it does a really good job, but I do want an option where newsboat isn't\navailable or is not not convenient to use.  This lead me to think about an experiment I am calling skimmer\n. Something that works with RSS, Atom and jsonfeeds in the same way I use `yarnc timeline | less -R`.  \nI'm also inspired by Dave Winer's a river of news site and his outline tooling. But in this case I don't want\nan output style output, just a simple list of items in reverse chronological order. I'm thinking of a more\nephemeral experience in reading.\n\nThis has left me with some questions.\n\n- How simple is would it be to write skimmer?\n- How much effort would be required to maintain it?\n- Could this tool incorporate support for other feed types, e.g. twtxt, Gopher, Gemini?\n\nThere is a Go package called [gofeed](https://github.com/mmcdole/gofeed). The README describes it\nas a \"universal\" feed reading parser. That seems like a good starting point and picking a very narrowly\nfocus task seems like a way to keep the experiment simple to implement.\n\n## Design issues\n\nThe reader tool needs to output to standard out in the same manner as `yarnc timeline` does. The goal isn't\nto be newsboat, or river of news, drummer, or Lynx but to present a stream of items usefully formatted to read\nfrom standard output.\n\nSome design ideas\n\n1. Feeds should be fetched by the same tool as the reader but that should be done explicitly (downloads can take a while)\n2. I want to honor that RSS does not require titles! I need to handle that case gracefully\n3. For a given list of feed URLs I want to save the content in a SQLite3 database (useful down the road)\n4. I'd like the simplicity of newsboat's URL list but I want to eventually support OPML import/export\n\n# Skimmer, a thin wrapper around gofeed\n\nIn terms of working with RSS, Atom and JSON feeds the [gofeed](https://github.com/mmcdole/gofeed) takes care of\nall the heavy lifting in parsing that content. The go http package provides a reliable client.\nThere is a pure Go package, [go-sqlite](), for integrating with SQLite 3 database. The real task is knitting this\ntogether and a convenient package.\n\nHere's some ideas about behavior.\n\nTo configure skimmer you just run the command. It'll create a directory at `$HOME/.skimmer` to store configuration\nmuch like newsboat does with `$HOME/.newsboat`.\n\n~~~\nskimmer\n~~~\n\nA default URL list to be created so when running the command you have something to fetch and read.\n\nSince fetching feed content can be slow (this is true of all news readers I've used) I think you should have to\nexplicitly say fetch.\n\n~~~\nskimmer -fetch\n~~~\n\nThis would read the URLs in the URL list and populate a simple SQLite 3 database table. Then running skimmer again \nwould display any harvested content (or running skimmer in another terminal session).\n\nSince we're accumulating data in a database there are some house keep chores like prune that need to be supported.\nInitial this can be very simple and if the experiment move forward I can improve them over time. I want something\nlike saying prune everything up to today.\n\n~~~\nskimmer -prune today\n~~~\n\nThere are times I just want to limit the number of items displayed so a limit options makes sense\n\n~~~\nskimmer -limit 10\n~~~\n\nSince I am displaying to standard out I should be able to output via Pandoc to pretty print the content.\n\n~~~\nskimmer -limit 50 | pandoc -t markdown -f plain | less -R\n~~~\n\nThat seems a like a good set of design features for an initial experiment.\n\n## Proof of concept implementation\n\nSpending a little time this evening. I've release a proof of concept on GitHub\nat <https://github.com/rsdoiel/skimmer>, you can read the initial documentation\nat [skimmer](https://rsdoiel.github.io/skimmer).\n\n",
      "data": {
        "keywords": [
          "feeds",
          "reader",
          "rss",
          "atom",
          "jsonfeed"
        ],
        "pubDate": "2023-10-06",
        "title": "Skimmer"
      },
      "url": "posts/2023/10/06/concept.json"
    },
    {
      "content": "\n# Quick recipe, compile Pandoc (M1)\n\nThese are my quick notes for compiling Pandoc on a M1 Mac Mini. I use a similar recipe for building Pandoc on Linux (NOTE: the challenges with libiconv and Mac Ports' libiconv below if you get a build error).\n\n1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)\n    a. Use the curl example command to install it\n    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)\n2. Make sure GHCup is pointing at the \"recommended\" versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)\n3. Clone <https://github.com/jgm/pandoc> to your local machine\n4. Check out the version you want to build (e.g. 3.1.4)\n5. Run the \"usual\" Haskell build sequence with cabal per Pandoc's installation documentation for building from source\n    a. `cabal clean`\n    b. `cabal update`\n    c. `cabal install pandoc-cli`\n\nHere's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).\n\n~~~\ncurl --proto '=https' --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh\nsource $HOME/.gchup/env\nghcup tui\nmkdir -p src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\ngit clone git@github.com:jgm/pandoc\ncd pandoc\ngit checkout 3.1.4\ncabal clean\ncabal update\ncabal install pandoc-cli\n~~~\n\nThis will install Pandoc in your `$HOME/.cabal/bin` directory. Make sure\nit is in your path (it should be if you've sourced the GHCup environment after you installed GHCup).\n\n## libiconv compile issues\n\nIf you use Mac Ports it can confuse Cabal/Haskell which one to link to. You'll get an error talking about undefined symbols and iconv.  To get a clean compile I've typically worked around this issue by removing the Mac Ports installed libiconv temporarily (e.g. `sudo port uninstall libiconv`, an using the \"all\" option when prompted).  After I've got a clean install of Pandoc then I re-install libiconv for those Ports based applications that need it. Putting libiconv back is important, as Mac Ports version of Git expects it.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "Pandoc",
          "GHCup",
          "M1"
        ],
        "pubDate": "2023-07-05",
        "title": "Quick recipe, compiling Pandoc (M1)"
      },
      "url": "posts/2023/07/05/quick-recipe-compiling-Pandoc-M1.json"
    },
    {
      "content": "\n# Quick recipe, compile PostgREST (M1)\n\nThese are my quick notes for compiling PostgREST on a M1 Mac Mini. I use a similar recipe for building PostgREST on Linux.\n\n1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)\n    a. Use the curl example command to install it\n    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)\n2. Make sure GHCup is pointing at the \"recommended\" versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)\n3. Clone <https://github.com/PostgREST/postgrest> to your local machine\n4. Check out the version you want to build (e.g. v11.1.0)\n5. Run the \"usual\" Haskell build sequence with cabal\n    a. `cabal clean`\n    b. `cabal update`\n    c. `cabal build`\n    d. `cabal install`\n\nHere's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).\n\n~~~\ncurl --proto '=https' --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh\nsource $HOME/.gchup/env\nghcup tui\nmkdir -p src/github.com/PostgREST\ncd src/github.com/PostgREST\ngit clone git@github.com:PostgREST/postgrest\ncd postgrest\ncabal clean\ncabal update\ncabal build\ncabal install\n~~~\n\nThis will install PostgREST in your `$HOME/.cabal/bin` directory. Make sure\nit is in your path (it should be if you've sourced the GHCup environment after you installed GHCup).\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "PostgREST",
          "M1"
        ],
        "pubDate": "2023-07-05",
        "title": "Quick recipe, compiling PostgREST (M1)"
      },
      "url": "posts/2023/07/05/quick-recipe-compiling-PostgREST-M1.json"
    },
    {
      "content": "\n# gsettings command\n\nOne of the things I find annoying about Ubuntu Desktop defaults is that when I open a new application it opens in the upper left corner. I then drag it to the center screen and start working. It's amazing how a small inconvenience can grind on you over time.  When I've search the net for changing this behavior the usual suggestions are \"install gnome-tweaks\". This seems ham-handed. I think continue searching and eventually find the command below. So I am making a note of the command here in my blog so I can find it latter.\n\n~~~\ngsettings set org.gnome.mutter center-new-window true\n~~~\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "Ubuntu Desktop",
          "Gnome",
          "gsettings"
        ],
        "pubDate": "2023-05-20",
        "title": "gsettings command"
      },
      "url": "posts/2023/05/20/gsettings-commands.json"
    },
    {
      "content": "\n# First Personal Search Engine Prototype\n\nBy R. S. Doiel, 2023-08-10\n\nI've implemented a first prototype of my personal search engine which\nI will abbreviate as \"pse\" from here on out. I implemented it using \nthree [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) scripts\nrelying on [sqlite3](https://sqlite.org), [wget](https://en.wikipedia.org/wiki/Wget) and [PageFind](https://pagefind.app) to do the heavy lifting.\n\nBoth Firefox and newsboat store useful information in sqlite databases.  Firefox's `moz_places.sqlite` holds both all the URLs visited as well as those that are associated with bookmarks (i.e. the SQLite database `moz_bookmarks.sqlite`).  I had about 2000 bookmarks, less than I thought with many being stale from link rot. Stale page URLs really slow down the harvest process because of the need for wget to wait on various timeouts (e.g. DNS, server response, download times).  The \"history\" URLs would make an interesting collection to spider but you'd probably want to have an exclude list (e.g. there's no point in saving queries to search engines, web mail, shopping sites). Exploring that will wait for another prototype.\n\nThe `cache.db` associated with Newsboat provided a rich resource of content and much fewer stale links (not surprising because I maintain that URL list more much activity then reviewing my bookmarks).  Between the two I had 16,000 pages. I used SQLite 3 to query the url values from the various DB into sorting for unique URLs into a single text file one URL per line.\n\nThe next thing after creating a list of pages I wanted to search was to download them into a directory using wget.  Wget has many options, I choose to enable timestamping, create a protocol directory and then a domain and path directory for each item. This has the advantage of being able to transform the Path into a URL later.\n\nOnce the content was harvested I then used PageFind to index the all the harvested content. Since I started using PageFind originally the tool has gained an option called `--serve` which provides a localhost web service on port 1414.  All I needed to do was add an index.html file to the directory where I harvested the content and saved the PageFind indexes. Then I used PageFind to again to provide a localhost based personal search engine.\n\nWhile the total number of pages was small (16k pages) I did find interesting results just trying out random words. This makes the prototype look promising.\n\n## Current prototype components\n\nI have simple Bash script that gets the URLs from both Firefox bookmarks and Newsboat's cache then generates a single text file of unique URLs I've named \"pages.txt\".\n\nI then use the \"pages.txt\" file to harvest content with wget into a tree structure like \n\n- htdocs\n    - http (all the http based URLs I harvest go in here)\n    - https (all the https based URLs I harvest go in here)\n    - pagefind (this holds the PageFind indexes and JavaScript to implement the search UI)\n    - index.html (this holds the webpage for the search UI using the libraries in `pagefind`)\n\nSince I'm only downloaded the HTML the 16k pages does not take up significant disk space yet.\n\n## Prototype Implementation\n\nHere's the bash scripts I use to get the URLs, harvest content and launch my localhost search engine based on PageFind.\n\nGet the URLs I want to be searchable. I use to environment variables\nfor finding the various SQLite 3 databases (i.e. PSE_MOZ_PLACES, PSE_NEWSBOAT).\n\n~~~\n#!/bin/bash\n\nif [ \"$PSE_MOZ_PLACES\" = \"\" ]; then\n    printf \"the PSE_MOZ_PLACES environment variable is not set.\"\n    exit 1\nfi\nif [ \"$PSE_NEWSBOAT\" = \"\" ]; then\n    printf \"the PSE_NEWSBOAT environment variable is not set.\"\n    exit 1\nfi\n\nsqlite3 \"$PSE_MOZ_PLACES\" \\\n    'SELECT moz_places.url AS url FROM moz_bookmarks JOIN moz_places ON moz_bookmarks.fk = moz_places.id WHERE moz_bookmarks.type = 1 AND moz_bookmarks.fk IS NOT NULL' \\\n    >moz_places.txt\nsqlite3 \"$PSE_NEWSBOAT\" 'SELECT url FROM rss_item' >newsboat.txt\ncat moz_places.txt newsboat.txt |\n    grep -E '^(http|https):' |\n    grep -v '://127.0.' |\n    grep -v '://192.' |\n    grep -v 'view-source:' |\n    sort -u >pages.txt\n~~~\n\nThe next step is to have the pages. I use wget for that.\n\n~~~\n#!/bin/bash\n#\nif [ ! -f \"pages.txt\" ]; then\n    echo \"missing pages.txt, skipping harvest\"\n    exit 1\nfi\necho \"Output is logged to pages.log\"\nwget --input-file pages.txt \\\n    --timestamping \\\n    --append-output pages.log \\\n    --directory-prefix htdocs \\\n    --max-redirect=5 \\\n    --force-directories \\\n    --protocol-directories \\\n    --convert-links \\\n    --no-cache --no-cookies\n~~~\n\nFinally I have a bash script that generates the index.html page, an Open Search Description XML file, indexes the harvested sites and launches PageFind in server mode.\n\n~~~\n#!/bin/bash\nmkdir -p htdocs\n\ncat <<OPENSEARCH_XML >htdocs/pse.osdx\n<OpenSearchDescription xmlns=\"http://a9.com/-/spec/opensearch/1.1/\"\n                       xmlns:moz=\"http://www.mozilla.org/2006/browser/search/\">\n  <ShortName>PSE</ShortName>\n  <Description>A Personal Search Engine implemented via wget and PageFind</Description>\n  <InputEncoding>UTF-8</InputEncoding>\n  <Url rel=\"self\" type=\"text/html\" method=\"get\" template=\"http://localhost:1414/index.html?q={searchTerms}\" />\n  <moz:SearchForm>http://localhost:1414/index.html</moz:SearchForm>\n</OpenSearchDescription>\nOPENSEARCH_XML\n\ncat <<HTML >htdocs/index.html\n<html>\n<head>\n<link\n  rel=\"search\"\n  type=\"application/opensearchdescription+xml\"\n  title=\"A Personal Search Engine\"\n  href=\"http://localhost:1414/pse.osdx\" />\n<link href=\"/pagefind/pagefind-ui.css\" rel=\"stylesheet\">\n</head>\n<body>\n<h1>A personal search engine</h1>\n<div id=\"search\"></div>\n<script src=\"/pagefind/pagefind-ui.js\" type=\"text/javascript\"></script>\n<script>\n    window.addEventListener('DOMContentLoaded', function(event) {\n\t\tlet page_url = new URL(window.location.href),\n    \t    query_string = page_url.searchParams.get('q'),\n      \t\tpse = new PagefindUI({ element: \"#search\" });\n\t\tif (query_string !== null) {\n\t\t\tpse.triggerSearch(query_string);\n\t\t}\n    });\n</script>\n</body>\n</html>\nHTML\n\npagefind \\\n--source htdocs \\\n--serve\n~~~\n\nThen I just language my web browser pointing at `http://localhost:1414/index.html`. I can even pass the URL a `?q=...` query string if I want.\n\nFrom a functionality point of view this is very bare bones and I don't think 16K pages is enough to make it compelling (I think I need closer to 100K for that).\n\n## What I learned from the prototype so far\n\nThis prototype suffers from several limitations.\n\n1. Stale links in my pages.txt make the harvest process really really slow, I need to have a way to avoid stale links getting into the pages.txt or have them removed from the pages.txt\n2. PageFind's result display uses the pages I downloaded to my local machine. It would be better if the result link was translated to point at the actual source of the pages, I think this can be done via JavaScript in my index.html when I setup the PageFind search/results element. Needs more exploration\n\n16K pages is a very tiny corpus. I get interesting results from my testing but not good enough to make me use first.  I'm guessing I need a corpus of at least 100K pages to be compelling for first search use.\n\nIt is really nice having a localhost personal search engine. It means that I can keep working with my home network connection is problematic. I like that. Since the website generated for my localhost system is a \"static site\" I could easily replicate that to net and make it available to other machines.\n\nRight now the big time sync is harvesting content to index. I'm not certain yet how much space disk space will be needed for my 100K page target corpus.\n\nSetting up indexing and the search UI were the easiest part of the process.  PageFind is so easy to work with compare to enterprise search applications.\n\n## Things to explore\n\nI can think of several ways to enlarge my search corpus. The first is there are a few websites I use for reference that are small enough to mirror. Wget provides a mirror function. Working from a \"sites.txt\" list I could mirror those sites periodically and have their content available for indexing.\n\nWhen experimenting with the mirror option I notice I wind up with PDF that are linked in the pages being mirrored.  If I used the Unix find command to locate all the PDF I could use another tool to extract there text.  Doing that would enlarge my search beyond plain text and HTML.  I would need to think this through as ultimately I'd want to be able to recover the path to the PDF when those results are displayed.\n\nAnother approach would be to work with my full web browsers' history as\nwell as it's bookmarks. This would significantly expand the corpus. If I did this I could also check the \"head\" of the HTML for references to feeds that could be folded into my feed link harvests. This would have the advantage of capture content from sources I find useful to read but would catch blog posts I might have skipped due to limited reading time.\n\nI use Pocket to read the pages I find interesting in my feed reader.  Pocket has an API and I could get some additional interesting pages from it. Pocket also has various curated lists and they might have interesting pages to harvest and index. I think the trick would be to use those suggests against an exclude list of some sort. E.g. Makes not sense to try to harvest paywall stuff or commercial sites more generally. One of the values I see in pse is that it is a personal search engine not a replacement for commercial search engines generally.\n\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 2,
        "pubDate": "2023-03-10",
        "series": "Personal Search Engine",
        "title": "First Personal Search Engine Prototype",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/10/first-prototype-pse.json"
    },
    {
      "content": "\n# Prototyping a personal search engine\n\nBy R. S. Doiel, 2023-03-07\n\n> Do we really need a search engine to index the \"whole web\"? Maybe a curated subset is better.\n\nAlex Schreoder's post [A Vision for Search](https://alexschroeder.ch/wiki/2023-03-07_A_vision_for_search) prompted me to write up an idea I call a \"personal search engine\".   I've been thinking about a \"a personal search engine\" for years, maybe a decade.\n\nWith the current state of brokenness in commercial search engines, especially with the implosion of the commercial social media platforms, we have an opportunity to re-think search on a more personal level.\n\nThe tooling around static site generation where a personal search is an extension of your own website suggests a path out of the quagmire of commercial search engines.  Can techniques I use for my own site search, be extended into a personal search engine?\n\n## A Broken Cycle\n\nSearch engines happened pretty early on in the web. If my memory is correct they showed up with the arrival of support for [CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface \"Common Gateway Interface\") in early web server software. Remembering back through the decades I see a pattern.\n\n1. Someone comes up with a clever way to index web content and determine relevancy\n2. They index enough the web to be interesting and attract early adopters\n3. They go mainstream, this compels them to have a business model, usually some form of ad-tech\n4. They index an ever larger portion of the web, the results from the search engine starts to degrade\n5. The business model becomes the primary focus of the company, the indexing gets exploited (e.g. content farms, page hijacking), the search results degrade.\n\nStage four and five can be summed up as the \"bad search engine stage\". When things get bad enough a new search engine comes on the scene and the early adopters jump ship and the cycle repeats. This was well established by the time some graduate students at Stanford invented page rank. I think it is happening now with search integrated ChatGPT.\n\nI think we're at the point in the cycle where there is an opportunity for something new. Maybe even break the loop entirely.\n\n## How do I use search?\n\nMy use of search engines can be described in four broad categories.\n\n1. Look for a specific answer queries\n    - `spelling of <VALUE>`\n    - `meaning of <VALUE>`\n    - `location of <VALUE>`\n    - `convert <UNIT> from <VALUE> to <UNIT>`\n2. Shopping queries\n    - pricing an item\n    - finding an item\n    - finding marketing material on an item\n3. Look for subject information\n    - a topic search\n    - news event\n    - algorithms\n4. Look for information I know exists\n    - technical documentation\n    - an article I read\n    - an article I want to read next\n\nMost of my searches are either subject information or retrieving something I know exists. Both are particularly susceptible to degradation when the business model comes to dominate the search results.\n\nA personal search engine for me would address these four types of searches before I reach for alternatives. In the mean time I'm stuck attempting to mitigate the bad search experience as best I can.\n\n## Mitigating the bad search engine experience\n\n> As commercial search engines degrade I rely on a given website's own search more\n\nI've noticed the follow search behavior practice changes in my own web usage.  For shopping I tend to go to the vendors I trust and use their searches on their websites.  To learn about a place, it's Wikipedia and if I trying to get a sense of going there I'll probably rely on an Open Street Map to avoid the ad-tech in commercial services. I dread using the commercial maps because usage correlates so strongly with the spam I encounter the next time I use an internet connected device.\n\nFor spelling and dictionary I can use Wiktionary. Location information I use Wikipedia and Open Street Maps. Weather info I have links into [NOAA](https://www.weather.gov/) website. Do I really need to use the commercial services?\n\nIt seems obvious that the commercial services for me are at best a fallback experience. They are no longer the \"go to\" place on the web to find stuff. I miss the convenience of using my web browsers URL box as a search box but the noise of the commercial search engines means the convenience is not worth the cost.\n\nWhat I would really like is a search service that integrated **my trusted sources** with a single search box but without the noise of the commercial sites. Is this possible? How much work would it be?\n\nI think a personal (or even small group) search engine is plausible and desirable. I think we can build a prototype with some off the shelf parts.\n\n## Observations\n\n1. I only need to index a tiny subset of the web, I don't want a web crawler that needs to be monitored and managed\n2. The audience of the search engine is me and possibly some friends\n3. There are a huge number of existing standards, protocols and structured data formats and practices I could leverage to mange building a search corpus and for indexing.\n4. Static site generators have moved site search from services (often outsourced to commercial search engines) to browser based solutions (e.g. [PageFind](https://pagefind.app), [LunrJS](https://lunrjs.com))\n5. A localhost site could stage pages for indexing and I could leverage my personal website to expose my indexes to my web devices (e.g. my phone).\n6. Tools like wget can mirror websites and that could also be used to stage content for a personal search engine\n7. There is a growing body of Open Access data, journal articles and books, these could be indexed and made available in a personal search engine with some effort\n\n## Exploring a personal search engine concept\n\nWhen I've brought up the idea of \"a personal search engine\" over the years with colleagues I've been consistently surprise at the opposition I encounter.  There are so many of reasons not to build something, including a personal search engine. That has left me thinking more deeply about the problem, a good thing in my experience.  I've synthesized that resistance into three general questions. Keeping those questions in mind will be helpful in evaluating the costs in time for prototyping a personal search engine and ultimately if the prototype should turn into an open source project.\n\n1. How would a personal search engine know/discover \"new\" content to include?\n2. Search engines are hard to setup and maintain (e.g. Solr, Opensearch), why would I want to spend time doing that?\n3. Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nConstraints can be a good thing to consider as well. Here's some constraints I think will be helpful when considering a prototype implementation.\n\n- I maintain my personal website using a Raspberry Pi 400. The personal search engine needs to respect the limitations of that device.\n- I'd like to be able to access my personal search engine from all my networked devices (e.g. my phone when I am away from home)\n- I have little time to prototype or code anything\n- I need to explain the prototype easily if I want others to help expand on the ideas\n- If it breaks I need to easily fix it\n\nI believe recent evolution of static site generation and site search offer an adjacent technology that can be leverage to demonstrate a personal search engine as a prototype. The prototype of a personal search engine could be an extension of my existing website.\n\n## Addressing challenges\n\n### How can a personal search engine know about new things?\n\nThe first challenge boils down to discovering content you want to index. What I'm describing is a personal search engine. I'm not trying to \"index the whole web\" or even a large part of it. I suspect the corpus I regularly search probably is in the neighborhood of a 100,000 pages or so. Too big for a bookmark list but magnitudes smaller than search engine deployments commonly see in an enterprise setting. I also am NOT suggesting a personal search engine will replace commercial search engines or even compete with them. What I'm thinking of is an added tool, not a replacement.\n\nCurating content is labor intensive. This is why Yahoo evolved from a curated web directory to become a hybrid web directory plus search engine before its final demise.  I don't want to have to change how I currently find content on the web. When I do stumble on something interesting I need a mechanism to easily add it to my personal search engine. Fortunately I think my current web reading habits can function like a [mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk).\n\nMost \"new\" content I find isn't from using a commercial search engine. When I look at my habits I find two avenues for content discovery dominate. I come across something via social media (today that's RSS feeds provided via Mastodon and Yarn Social/Twtxt) or from RSS, Atom and JSON feeds of blogs or websites I follow. Since the social media platforms I track support RSS I typically read all this content via newsboat which is a terminal based feed reader. I still find myself using the web browser's bookmark feature. It's just the bookmarks aren't helpful if they remain only in my web browser.  I also use [Pocket](https://getpocket.com) to read things later. I think all these can serve as a \"link discovery\" mechanism for a personal search engine. It's just a matter of collecting the URLs into a list of content I want to index, staging the content, index it and publish the resulting indexes on my personal website using a browser based search engine to query them.\n\nThis link discovery approach is different from how commercial search engines work.  Commercial engines rely on crawlers that retrieve a web page, analyze the content, find new links in the page then recursively follows those to scan whole domains and websites.  Recursive crawlers aren't automatic. It's easy for them to get trapped in link loops and often they can be a burden to the sites they are crawling (hence robot.txt files suggesting to crawlers what needs to be avoided).  I don't need to index the whole web, usually not even whole websites.  I'm interested in page level content and I can get a list of web pages from by bookmarks and the feeds I follow.\n\n\nA Quick digression:\n\nBlogs, in spite of media hype, haven't \"gone away\".  Presently we're seeing a bit of a renaissance with projects like [Micro.blog](https://micro.blog) and [FeedLand](http://docs.feedland.org/about.opml \"this is a cool project from Dave Winer\"). The \"big services\" like [WordPress](https://wordpress.com), [Medium](https://medium.com), [Substack](https://substack.com) and [Mailchimp](https://mailchimp.com/) provide RSS feeds for their content. RSS/Atom/JSON feed syndication all are alive and well at least for the sites I track and content I read. I suspect this is the case for others.  What is a challenge is knowing how to find the feed URL.  But even that I've notice is becoming increasingly predictable. I suspect given a list of blog sites I could come up with a way of guessing the feed URL in many cases even without an advertised URL in the HTML head or RSS link in the footer.\n\n### Search engines are hard to setup and maintain, how can that be made easier?\n\nI think this can be addressed in several ways. First is splitting the problem of content retrieval, indexing and search UI.  [PageFind](https://pagefind.app) is the current static site search I use on my blog.  It does a really good job at indexing blog content will little configuration. PageFind is clever about the indexes it builds.  When PageFind indexes a site is builds a partitioned index. Each partition is loaded by the web browser only when the current search string suggests it is needed. This means you can index a large number of pages (e.g. 100,000 pages) before it starts to feel sluggish. Indexing is fast and can be done on demand after harvesting the new pages you come across in your feeds. If the PageFind indexes are saved in my static site directory (a Git repository) I can implement the search UI there implementing the personal search engine prototype. The web browser is the search engine and PageFind tool is the indexer. The harvester is built by extracting interesting URLs from the feeds I follow and the current state of my web browsers' bookmarks and potentially from content in Pocket. Note the web browser bookmarks are synchronized across my devices so if I encounter an interesting URL in the physical world I can easily add it my personal search engine too the next time I process the synchronized bookmark file.\n\n### Indexing and search engines are resource intensive, isn't that going to bog down my computer?\n\nEnterprise Search Engine Software is complicated to setup, very resource intensive and requires upkeep. For me Solr, Elasticsearch, Opensearch falls into the category \"day job\" duty and I do not want that burden for my personal search engine. Fortunately I don't need to run Solr, Elasticsearch or Opensearch. I can build a decent search engine using [PageFind](https://pagefind.app).  PageFind is simple to configured, simple to index with and it's indexes scale superbly for a browser based search engine UI. Hosting is reduced to the existing effort I put into updating my personal blog and automating the link extraction from the feeds I follow and my web browsers' current bookmark file.\n\nI currently use PageFind for web content I mirror to a search directory locally for off line reading. From that experience I know it can handle at least 100,000 pages. I know it will work on my Raspberry Pi 400. I don't see a problem in a prototype personal search engine assuming a corpus in the neighborhood of 100,000 pages.\n\n\n## Sketching the prototype\n\nHere's a sketch of a prototype of \"a personal search engine\" built on PageFind.\n\n1. Generate a list of URLs pointing at pages I want to index (this can be done by mining my bookmarks and feed reader content).\n2. Harvest and stage the pages on my local file system, maintaining a way to associated their actual URL with the staged copy\n3. Index with PageFind and save the resulting indexes my local copy of my personal website\n4. Have a page on my personal website use these indexes to implement a search and results page\n\nThe code that I would need to be implemented is mostly around extracting URL from my browser's bookmark file and my the feeds managed in my feed reader. Since newsboat is open source and it stores it cached feeds in a SQLite3 database in principle I could use the tables in that database to generate a list of content to harvest for indexing. I could write a script that combines the content from my bookmarks file and newsboat database rendering a flat list to harvest, stage and then index with PageFind. A prototype could be done in Bash or Python without too much of an effort.\n\nOne challenge remains after harvesting and staging is solved. It would be nice to use my personal search engine as my default search engine. After all I am already curating the content. I think this can be done by supporting the [Open Search Description](https://developer.mozilla.org/en-US/docs/Web/OpenSearch) to make my personal search engine a first class citizen in my browser URL bar. Similarly I could turn the personal search engine page into a PWA so I can have it on my phone's desktop along the other apps I commonly use.\n\nObservations that maybe helpful for a successful prototype\n\n1. I don't need to crawl the whole web just the pages that interest me\n2. I don't need to create or monitor a recursive web crawler\n3. I avoid junk because I'm curating the sources through my existing web reading practices\n4. I am targeting a small search corpus, approximately 100,000 pages or so\n5. I am only indexing HTML, Pagefind can limit the elements it indexes\n\nA prototype of a personal search engine seems possible. The challenge will be finding the time to implement it.\n\n",
      "data": {
        "author": "R. S. Doiel",
        "keywords": [
          "personal search engine",
          "search",
          "indexing",
          "web",
          "pagefind"
        ],
        "number": 1,
        "pubDate": "2023-03-07",
        "series": "Personal Search Engine",
        "title": "Prototyping a personal search engine",
        "updated": "2023-11-29"
      },
      "url": "posts/2023/03/07/prototyping-a-personal-search-engine.json"
    },
    {
      "content": "\n# SQL query to CSV, a missing datatool\n\nBy R. S. Doiel, 2023-01-13\n\nUpdate: 2023-03-13\n\nAt work we maintain allot of metadata related academic and research publications in SQL databases. We use SQL to query the database and export what we need in tab delimited files. Often the exported data includes a column containing publication or article titles.  Titles in library metadata can be a bit messy. They contain a wide set of UTF-8 characters include math symbols and various types of quotation marks. The exported tab delimited data usually needs clean up before you can import it successfully into a spreadsheet.\n\nIn the worst cases we debug what the problem is then write a Python script to handle the tweak to fix things.  This results in allot of extra work and slows down the turn around for getting reports out quickly. This is particularly true of data stored in MySQL 8 (though we also use SQLite 3 and Postgres).\n\nThis got me thinking about how to get a clean export (tab or CSV) from our SQL databases today.  It would be nice if you provided a command line tool with a data source string (e.g. in a config file or the environment), a SQL query and the tool would use that to render a CSV or tab delimited file to standard out or a output file. It would work something like this.\n\n```\n    sql2csv -o eprint_status_report.csv -config=$HOME/.my.cnf \\\n\t    'SELECT eprintid, title, eprint_status FROM eprint' \n```\n\nThe `sql2csv` would take the results of the query and write to the CSV file.\n\nThe nice thing about this approach is that I could support the three relational databases we use -- i.e. MySQL 8, Postgres and SQLite3 with one common tool so my Bash scripts that run the reports would be very simple rather than specialized to one database system or the other.\n\nI hope to experiment with this approach in the next release of [datatools](https://github.com/caltechlibrary/datatools), an open source project maintained at work.\n\n## update\n\nJon Woodring pointed out to me today that both SQLite3 and PostgreSQL clients can output to CSV without need of an external tool. Wish MySQL client did that! Instead MySQL client supports tab delimited output. I'm still concidering sql2csv due to the ammount work I do with MySQL database but I'm not sure if it will make it into to the datatools project or now since I suspect our MySQL usage will decline overtime as more projects are built with PostgreSQL and SQLite3.\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "sql",
          "csv",
          "tab delimited"
        ],
        "pubDate": "2023-01-03",
        "title": "SQL query to CSV, a missing datatool",
        "updateDate": "2023-03-13"
      },
      "url": "posts/2023/01/03/sql-to-csv-a-missing-datatool.json"
    },
    {
      "content": "\n# Go and MySQL timestamps\n\nBy R. S. Doiel, 2022-12-12\n\nThe Go [sql](https://pkg.go.dev/database/sql) package provides a nice abstraction for working with SQL databases. The underlying drivers and DBMS can present some quirks that are SQL dialect and driver specific such as the [MySQL driver](github.com/go-sql-driver/mysql).  Sometimes that is not a big deal. [MySQL](https://dev.mysql.com) can maintain a creation timestamp as well as a modified timestamp easily via the SQL schema definition for the field. Unfortunately if you need to work with the MySQL timestamp at a Go level (e.g. display the timestamp in a useful way) the int64 provided via the driver isn't compatible with the `int64` used in Go's `time.Time`. To work around this limitation I've found it necessary to convert the MySQL timestamp to a formatted string using [DATE_FORMAT](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_date-format \"DATE_FORMAT is a MySQL date/time function returning a string value\") and from the Go side convert the formatted string into a `time.Time` using `time.Parse()`. Below is some Golang pseudo code showing this approach.\n\n```\n// Format used by MySQL strings representing date/times\nconst MySQLTimestamp = \"2006-01-02 15:04:05\"\n\n// GetRecordUpdate takes a configuration with a db attribute previously\n// opened and an id string returning a record populated with id and updated values where updated is an attribute of type time.Time. We use MySQL's\n// `DATE_FORMAT()` function to convert the timestamp into a string and\n// Go's `time.Parse()` to convert the string into a `time.Time` value.\nfunc GetRecordUpdate(cfg, id string) {\n\tstmt := `SELECT id, DATE_FORMAT(updated, \"%Y-%m-%d %H:%i:%s\") FROM some_tabl WHERE id = ?`\n\trow, err := cfg.db.Query(stmt, id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer row.Close()\n\trecord := new(Record)\n\tif row.Next() {\n\t\tvar updated string\n\t\tif err := row.Scan(&record.ID, &updated); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trecord.Updated, err = time.Parse(MySQLTimestamp, updated)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\terr = row.Err()\n\treturn record, err\n}\n```\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "golang",
          "sql",
          "timestamps"
        ],
        "pubDate": "2022-12-12",
        "title": "Go and MySQL timestamps"
      },
      "url": "posts/2022/12/12/Go-and-MySQL-Timestamps.json"
    },
    {
      "content": "\n# Progress and time remaining\n\nBy R. S. Doiel, 2022-11-05\n\nI often find myself logging output when I'm developing tools.  This is typically the case where I am iterating over data and transforming it. Overtime I've come to realize I really want a few specific pieces of information for non-error logging (e.g. `-verbose` which monitors progress as well as errors).\n\n- percentage completed\n- estimated time allocated (i.e. time remaining)\n\nTo do that I need three pieces of information.\n\n1. the count of the current iteration(e.g. `i`)\n2. the total number of iterations required (e.g. `tot`)\n3. The time just before I started iterating(e.g. `t0`)\n\nThe values for `i` and `tot` let me compute the percent completed. The percent completed is trivial `(i/tot) * 100.0`. Note on the first pass (i.e. `i == 0`) you can skip the percentage calculation.\n\n\n```golang\nimport (\n\t\"time\"\n\t\"fmt\"\n)\n\n// Show progress with amount of time running\nfunc progress(t0 time.Time, i int, tot int) string {\n    if i == 0 {\n        return \"\"\n    }\n\tpercent := (float64(i) / float64(tot)) * 100.0\n\tt1 := time.Now()\n\t// NOTE: Truncating the duration to seconds\n\treturn fmt.Sprintf(\"%.2f%% %v\", percent, t1.Sub(t0).Truncate(time.Second))\n}\n```\n\nHere's how you might use it.\n\n```golang\n\ttot := len(ids)\n\tt0 := time.Now()\n\tfor i, id := range ids {\n\t\t// ... processing stuff here ... and display progress every 1000 records\n\t\tif (i % 1000) == 0 {\n\t\t\tlog.Printf(\"%s records processed\", progress(t0, i, tot))\n\t\t}\n\t}\n```\n\nAn improvement on this is to include an time remaining. I need to calculated the estimated time allocation (i.e. ETA). I know `t0` so I can estimate that with this formula `estimated time allocation = (((current running time since t0)/ the number of items processed) * total number of items)`[^1]. ETA adjusted for time running gives us time remaining[^2]. The first pass of the function progress has a trivial optimization since we don't have enough delta t0 to compute an estimate. Calls after that are computed using our formula.\n\n[^1]: In code `(rt/i)*tot` is estimated time allocation\n\n[^2]: Estimated Time Remaining, in code `((rt/i)*tot) - rt`\n\n```golang\nfunc progress(t0 time.Time, i int, tot int) string {\n\tif i == 0 {\n\t\treturn \"0.00 ETR Unknown\"\n\t}\n\t// percent completed\n\tpercent := (float64(i) / float64(tot)) * 100.0\n\t// running time\n    rt := time.Now().Sub(t0)\n    // estimated time allocation - running time = time remaining\n    eta := time.Duration((float64(rt)/float64(i)*float64(tot)) - float64(rt))\n    return fmt.Sprintf(\"%.2f%% ETR %v\", percent, eta.Truncate(time.Second))\n}\n```\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "keywords": [
          "programming",
          "golang",
          "log info"
        ],
        "pubDate": "2022-12-05",
        "title": "Progress and time remaining"
      },
      "url": "posts/2022/12/05/progress-and-time-remaining.json"
    },
    {
      "content": "\n# Pandoc, Pagefind and Make\n\nRecently I've refresh my approach to website generation using three programs.\n\n- [Pandoc](https://pandoc.org)\n- [Pagefind](https://pagefind.app) for providing a full text search of documentation\n- [GNU Make](https://www.gnu.org/software/make/)\n    - [website.mak](website.mak) Makefile\n\nPandoc does the heavy lifting. It renders all the HTML pages, CITATION.cff (from the projects [codemeta.json](codemeta.github.io \"codemeta.json is a metadata documentation schema for documenting software projects\")) and rendering an about.md file (also from the project's codemeta.json). This is done with three Pandoc templates. Pandoc can also be used to rendering man pages following a simple page recipe.\n\nI've recently adopted Pagefind for indexing the HTML for the project's website and providing the full text search UI suitable for a static website. The Pagefind indexes can be combined with your group or organization's static website providing a rich cross project search (exercise left for another post).\n\nFinally I orchestrate the site construction with GNU Make. I do this with a simple dedicated Makefile called [website.mak](#website.mak).\n\n\n## website.mak\n\nThe website.mak file is relatively simple.\n\n```makefile\n#\n# Makefile for running pandoc on all Markdown docs ending in .md\n#\nPROJECT = PROJECT_NAME_GOES_HERE\n\nMD_PAGES = $(shell ls -1 *.md) about.md\n\nHTML_PAGES = $(shell ls -1 *.md | sed -E 's/.md/.html/g') about.md\n\nbuild: $(HTML_PAGES) $(MD_PAGES) pagefind\n\nabout.md: .FORCE\n        cat codemeta.json | sed -E 's/\"@context\"/\"at__context\"/g;s/\"@type\"/\"at__type\"/g;s/\"@id\"/\"at__id\"/g' >_codemeta.json\n        if [ -f $(PANDOC) ]; then echo \"\" | pandoc --metadata title=\"About $(PROJECT)\" --metadata-file=_codemeta.json --template codemeta-md.tmpl >about.md; fi\n        if [ -f _codemeta.json ]; then rm _codemeta.json; fi\n\n$(HTML_PAGES): $(MD_PAGES) .FORCE\n\tpandoc -s --to html5 $(basename $@).md -o $(basename $@).html \\\n\t\t--metadata title=\"$(PROJECT) - $@\" \\\n\t    --lua-filter=links-to-html.lua \\\n\t    --template=page.tmpl\n\tgit add $(basename $@).html\n\npagefind: .FORCE\n\tpagefind --verbose --exclude-selectors=\"nav,header,footer\" --bundle-dir ./pagefind --source .\n\tgit add pagefind\n\nclean:\n\t@if [ -f index.html ]; then rm *.html; fi\n\t@if [ -f README.html ]; then rm *.html; fi\n\n.FORCE:\n```\n\nOnly the \"PROJECT\" value needs to be set. Typically this is just the name of the repository's base directory.\n\n## Pandoc, filters and templates\n\nWhen write my Markdown documents I link to Markdown files instead of the HTML versions. This serves two purposes. First GitHub can use this linking directory and second if you decide to repurposed the website as a Gopher or Gemini resource\nyou don't linking to the Markdown file makes more sense.  To convert the \".md\" names to \".html\" when I render the HTML I use a simple Lua filter called [links-to-html.lua](https://stackoverflow.com/questions/40993488/convert-markdown-links-to-html-with-pandoc#49396058 \"see the stackoverflow answer that shows this technique\").\n\n```lua\n# links-to-html.lua\nfunction Link(el)\n  el.target = string.gsub(el.target, \"%.md\", \".html\")\n  return el\nend\n```\n\nThe \"page.tmpl\" file provides a nice wrapper to the Markdown rendered as HTML by Pandoc. It includes the site navigation and project copyright information in the wrapping HTML. It is based on the default Pandoc page template with some added markup for navigation and copyright info in the footer. I also update the link to the CSS to conform with our general site branding requirements. You can generate a basic template using Pandoc.\n\n```shell\npandoc --print-default-template=html5\n```\n\nI also use Pandoc to generate an \"about.md\" file describing the project and author info.  The content of the about.md is taken directly from the project's codemeta.json file after I've renamed the \"@\" JSON-LD fields (those cause problems for Pandoc). You can see the preparation of a temporary \"_codemeta.json\" using `cat` and `sed` to rename the fields. This is I use a Pandoc template to render the Markdown from.\n\n```pandoc\n---\ntitle: $name$\n---\n\nAbout this software\n===================\n\n$name$ $version$\n----------------\n\n$if(author)$\n### Authors\n\n$for(author)$\n- $it.givenName$ $it.familyName$\n$endfor$\n$endif$\n\n$if(description)$\n$description$\n$endif$\n\n\n$if(license)$- License: $license$$endif$\n0$if(codeRepository)$- GitHub: $codeRepository$$endif$\n$if(issueTracker)$- Issues: $issueTracker$$endif$\n\n\n$if(programmingLanguage)$\n### Programming languages\n\n$for(programmingLanguage)$\n- $programmingLanguage$\n$endfor$\n$endif$\n\n$if(operatingSystem)$\n### Operating Systems\n\n$for(operatingSystem)$\n- $operatingSystem$\n$endfor$\n$endif$\n\n$if(softwareRequirements)$\n### Software Requiremets\n\n$for(softwareRequirements)$\n- $softwareRequirements$\n$endfor$\n$endif$\n\n$if(relatedLink)$\n### Related Links\n\n$for(relatedLink)$\n- [$it$]($it$)\n$endfor$\n$endif$\n```\n\nThis same technique can be repurposed to render a CITATION.cff if needed.\n\n## Pagefind\n\nPagefind provides three levels of functionality. First it will generate indexes for a full text search of your\nproject's HTML pages. It also builds the necessary search UI for your static site. I include the search UI via a\nMarkdown document that embeds the HTML markup described at [Pagefind.app](https://pagefind.app/docs/)'s Getting started\npage.  When I invoke Pagefind I use the `--bundle-dir` option to be \"pagefind\" rather than \"_pagefind\".  The reason is GitHub Pages ignores the \"_pagefind\" (probably ignores all directories with \"_\" prefix).\n\nIf you need a quick static web server while you're writing and developing your documentation website Pagefind can\nprovide that using the `--serve` option. Assuming you're in your project's directory then something like this should do the trick.\n\n```shell\n    pagefind --source . --bundle-dir=pagefind --serve\n```\n",
      "data": {
        "keywords": [
          "Pandoc",
          "Pagefind",
          "make",
          "static site"
        ],
        "pubDate": "2022-11-30",
        "title": "Pandoc, Pagefind and Make"
      },
      "url": "posts/2022/11/28/pandoc-pagefind-and-make.json"
    },
    {
      "content": "\n# Initial Impression of Pagefind\n\nBy R. S. Doiel, 2022-11-21\n\nI'm interested in site search that does not require using server side services (e.g. Solr/Elasticsearch/Opensearch). I've used [LunrJS](https://lunrjs.com) on my person blog site for several years.  The challenge with LunrJS is indexes become large and that limits how much your can index and still have a quick loading page. [Pagefind](https://pagefind.app) addresses the large index problem. The search page only downloads the portion of the indexes it needs. The index and search functionality are compiled down to WASM files. This does raise challenges if you're targeting older web browsers.\n\nPagefind is a [rust](https://www.rust-lang.org/) application build using `cargo` and `rustc`. Unlike the documentation on the [Pagefind](https://pagefind.app) website which suggests installing via `npm` and `npx` I recommend installing it from sources using the latest release of cargo/rustic.  For me I found getting the latest cargo/rustc is easiest using [rustup](https://rustup.rs/). Pagefind will not compile using older versions of cargo/rustc (e.g. the example currently available from Mac Ports for M1 Macs).\n\nHere's the steps I took to bring Pagefind up on my M1 Mac.\n\n1. Install cargo/rust using rustup\n2. Make sure `$HOME/.cargo/bin` is in my PATH\n3. Clone the Pagefind Git repository\n4. Change to the repository directory\n5. Build and install pagefind\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\ngit clone git@github.com git@github.com:CloudCannon/pagefind.git src/github.com/CloudCannon/pagefind\ncd src/github.com/CloudCannon/pagefind\ncargo install pagefind --features extended\n```\n\nNext steps were\n\n1. Switch to my local copy of my website\n2. Build my site in the usual page\n3. Update my `search.html` page to use pagefind\n4. Index my site using pagefind\n5. Test my a local web server\n\nTo get the HTML/JavaScript needed to embed pagefind in your search page see [Getting Started](https://pagefind.app/docs/). The HTML/JavaScript fragment is at the top of the page. After updating `search.html` I ran the pagefind command[^1].\n\n```\npagefind --verbose --bundle-dir ./pagefind --source .\n```\n\nThe indexing is wicked fast and it gives you nice details. I verified everything worked as expected using `pttk ws` static site web server. I then published my website. You can see the results at <http://rsdoiel.sdf.org/search.html> and <https://rsdoiel.github.io/search.html>\n\n[^1]: I specified the bundle directory because GitHub pages had a problem with the default `_pagefind`.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-21",
        "keywords": [
          "site search",
          "pagefind",
          "rust",
          "cargo",
          "rustup",
          "M1",
          "macOS"
        ],
        "pubDate": "2022-11-21",
        "title": "Initial Impressions of Pagefind"
      },
      "url": "posts/2022/11/21/initial-impressions-pagefind.json"
    },
    {
      "content": "\nBrowser based site search\n=========================\n\nBy R. S. Doiel, 2022-11-18\n\nI recently read Brewster Kahle’s 2015 post about his vision for a [distributed web](https://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/). Many of his ideas have carried over into [DWeb](https://wiki.mozilla.org/Dweb), [Indie Web](https://indieweb.org/), [Small Web](https://benhoyt.com/writings/the-small-web-is-beautiful/), [Small Internet](https://cafebedouin.org/2021/07/28/the-small-internet/) and the like. A point he touches on is site search running in the web browser.\n\nI've use this approach in my own website relying on [LunrJS](https://lunrjs.com) by Oliver Nightingale. It is a common approach for small sites built using Markdown and [Pandoc](https://pandoc.org).  In the Brewster article he mentions [js-search](https://github.com/cebe/js-search), an implementation I was not familiar with. Like LunrJS the query engine runs in the browser via JavaScript but unlike LunrJS the indexes are built using PHP rather than JavaScript. The last couple of years I've used [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) to generating indexes for my own website site while using LunrJS for the browser side query engine. Today I check to see what the [Hugo](https://gohugo.io/tools/search/) community is using and found [Pagefind](https://github.com/cloudcannon/pagefind). Pagefind looks impressive. There was a presentation on at [Hugo Conference 2022](https://hugoconf.io/). It takes building a Lucene-like index several steps further. I appears to handle much larger indexes without requiring the full indexes to be downloaded into the browser.  It seems like a good candidate for prototyping personal search engine.\n\nHow long have been has browser side search been around? I do not remember when I started using. I explored seven projects on GitHub that implemented browser side site search. This is an arbitrary selection projects but even then I had no idea that this approach dates back a over decade!\n\n| Project | Indexer | query engine | earliest commit[^1] | recent commit[^2] |\n|---------|---------|--------------|:-------------------:|:-----------------:|\n| [LunrJS](https://github.com/olivernn/lunr.js) | JavaScript | JavaScript | 2011 | 2020 |\n| [Fuse.io](https://github.com/krisk/Fuse) | JavaScript/Typescript | JavaScript/Typescript | 2012 | 2022 |\n| [search-index](https://github.com/fergiemcdowall/search-index) | JavaScript | JavaScript | 2013 | 2016 |\n| [js-search](https://github.com/cebe/js-search) (cebe) | PHP | JavaScript | 2014 | 2022 |\n| [js-search](https://github.com/bvaughn/js-search) (bvaughn)| JavaScript | JavaScript | 2015 | 2022 |\n| [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) | Python | Python or JavaScript | 2018 | 2022 |\n| [Pagefind](https://github.com/cloudcannon/pagefind) | Rust | WASM and JavaScript | 2022 | 2022 |\n\n[^1]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n[^2]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-18",
        "keywords": [
          "search",
          "web browser",
          "dweb",
          "static site",
          "lunrjs",
          "pagefind"
        ],
        "pubDate": "2022-11-18",
        "title": "Browser based site search"
      },
      "url": "posts/2022/11/18/browser-side-site-search.json"
    },
    {
      "content": "\nTwitter's pending implosion\n===========================\n\nBy R. S. Doiel, 2022-11-11\n\nIt looks like Twitter continues to implode as layoffs and resignations continue. If bankers, investors and lenders call in the loans [bankruptcy appears to be possible](https://www.reuters.com/technology/twitter-information-security-chief-kissner-decides-leave-2022-11-10/). So what's next?\n\n\nThe problem\n-----------\n\nTwitter has been troubled for some time. The advertising model corrodes content. Twitter is effectively a massive RSS-like distribution system. It has stagnated as the APIs became more restrictive. The Advertising Business Model via [Ad-tech](https://pluralistic.net/tag/adtech/ \"per Cory Doctorow 'ad-fraud'\") encourages decay regardless of system.  Non-Twitter examples include commercial search engines (e.g. Google, Bing et el). Their usefulness usefulness declines over time. I believe this due to the increase in \"noise\" in the signal. The \"noise\" is driven be business models. That usually boils down to content who's function is to attract your attention so it can be sold for money. A corollary is [fear based journalism](https://medium.com/@oliviacadby/fear-mongering-journalisms-downfall-aac1f4f5756d). That has even caught the attention of a [Pope](https://www.9news.com.au/world/fear-based-journalism-is-terrorism-pope/4860b502-5dbb-4eef-abcf-57582445fc2c). Not fun.\n\nI suspect business models don't encourage great content. Business models are generally designed to turn a profit. They tend to get refined and tuned to that purpose. The evolution of Twitter and Google's search engine would make good case studies in that regard.\n\n\nA small hope\n------------\n\nI don't know what is next but I know what I find interesting. I've looked at Mastodon a number of times. It's not going away but the W3C activity pub spec is horribly complex. Complexity slows adoption. It reminds me of SGML. Conceptually interesting but in practice was too heavy. It did form inspiration for HTML though, and that has proven successful. What gives me hope is that Mastodon has survived. I think casting a wide net is interesting. The wider net is something I've heard called the \"small web\".\n\nThe small web\n-------------\n\nFor a number of years there has been a slowly growing  \"small web\" movement. I think it is relatively new term, I didn't find it in [Wikipedia](https://en.wikipedia.org/w/index.php?search=small+web&ns0=1 \"today is 2022-11-11\") when I looked today. As I see it the \"small web\" has been driven by a number of things. It is not a homogeneous movement but rather a collection of various efforts and communities.  I think it likely to continue to evolve. At times evolve rapidly. Perhaps it will coalesce at some point.  Here's what appears to me to be the common motivations as of 2022-11-11.\n\n- desire for simplicity\n- desire for authenticity\n- lower resource footprint\n- text as a primary but not exclusive medium\n- hyperlinks encouraged\n- a space where you're not a product\n- desire for decentralization (so you're not a product)\n- a desire to have room to grow (because you're not a product)\n\nThe \"small web\" is a term I've seen pop up in Gopherspace, among people who promote [Gemini](https://gemini.circumlunar.space/), [Micro blogging](https://micro.blog \"as an example of micro blogging\") and in the [Public Access Unix](https://sdf.org) communities.\"small web\" as a term does not return useful results in the commercial search engines I've checked. Elements seem to be part of [DWeb](https://getdweb.net/) which Mozilla is [championing](https://wiki.mozilla.org/Dweb). Curiously in spite of the hype and marketing I don't see \"small web\" in [web 3.0](https://www.forbes.com/advisor/investing/cryptocurrency/what-is-web-3-0/). I think blockchain has proven environmentally dangerous and tends to compile down to various forms of [grift](https://pluralistic.net/2022/05/27/voluntary-carbon-market/).\n\n\nSmall web\n---------\n\nWhat does \"small web\" mean to me?  I think it means\n\n- simple protocols that are flexible and friendly to tool creation\n- built on existing network protocols with a proven track record (e.g. IPv4, IPv6)\n- decentralized by design as was the early Internet\n- low barrier to participation\n    - e.g. a text editor, static site providing a URL to a twtxt file\n- text centric (at least for the moment)\n- integrated with the larger Internet, i.e. supports hyper links\n- friendly to distributed personal search engines (e.g. LunrJS running over curated set of JSONfeeds or twtxt urls)\n- \"feed\" oriented discovery based on simple formats (e.g. [RSS 2.0](https://cyber.harvard.edu/rss/rss.html), [JSONfeed](https://www.jsonfeed.org/), [twtxt](https://twtxt.readthedocs.io/en/latest/), [OPML](https://en.wikipedia.org/wiki/OPML), even [Gophermaps](https://en.wikipedia.org/wiki/Gopher_(protocol) \"see Source code of a menu title\"))\n- sustainable and preservation friendly\n    - example characteristics\n        - clone-able (e.g. as easy as cloning a Git Repo)\n        - push button update to Internet Archive's way back machine\n        - human and machine readable metadata\n\nI think the \"small web\" already exists. Examples include readable personal websites hosted as \"static pages\" via GitHub and S3 buckets are a good examples of prior art in a \"small web\".  Gopherspace is a good example of the \"small web\". I think the various [tilde communities](https://tilde.club) hosted on [Public Access Unix](https://en.wikipedia.org/wiki/SDF_Public_Access_Unix_System) are examples. Even the venerable \"bloggosphere\" of [Wordpress](https://wordpress.com) and the newer [Micro.blog](https://micro.blog/) is evidence that the \"small web\" already is hear. [Dave Winer](https://scripting.com)'s [Feedland](http://feedland.org/) is a good example of innovation in the \"small web\" happen today.  [Yarn.social](https://yarn.social) built on twtxt file format is very promising. I would argue right now the \"small web\" is the internet that already exists outside the walled gardens of Google, Meta/Facebook, Twitter, TikTok, Pinterest, Slack, Trello, Discord, etc.\n\nI think it is significant that the \"small web\" existed before the Pandemic. It continued to thrive during it. It is likely to evolve beyond it. The pending shift has already happening as it is already populated by \"early adopters\" and appears to be growing into larger community participation.  For the \"main stream\" it is waiting to be \"discovered\" or perhaps \"re-discovered\" depending on your point of view.\n\nHow do you participate?\n-----------------------\n\nYou may already be participating in the \"small web\".  Do you blog? Do your read feeds? Do you use a non-soloed social media platform like Mastodon? Do you use Gopher? The \"small web\" is defined by choice and is characterized by simplicity. It is a general term. You're the navigator not an algorithm tuned to tune someone a profit. If you are not sure where to start you can join a communities like [sdf.org](https://sdf.org) and get started there. You can explore [Gopherspace](https://floodgap.com) via a WWW proxy. You can create a static website and host a [twtxt](https://twtxt.readthedocs.io/en/latest/) file on GitHub or a [Yarn Pod](https://yarn.social). You can create a site via [Micro.blog](https://micro.blog) or [Feedland](http://feedland.org). You can blog. You can read RSS feeds or read twtxt feed with [twtxt](https://twtxt.readthedocs.io/en/latest/user/intro.html), [twet](https://github.com/quite/twet) or [yarn.social](https://yarn.social). You participate by stepping outside the walled gardens and seeing the larger \"Internet\".\n\nI think the important thing is to realize the alternatives are already here, you don't need to wait for invention, invitation or permission. You can move beyond the silos today. You don't need to have your attention captured then bought and sold. It's not so much a matter of \"giving up\" a silo but rather stepping outside one and breathing some fresh air.\n\nThings to watch\n---------------\n\n- [Feedland](https://feedland.org)\n- [yarn.social](https://yarn.social) and [twtxt](https://twtxt.readthedocs.io/en/latest/)\n- [Micro.blog](https://micro.blog/)\n- [Mastodon](https://joinmastodon.org/)\n- [Gopherspace](http://gopher.floodgap.com/gopher/gw?a=gopher%3A%2F%2Fgopher.floodgap.com%2F1%2Fworld), see [Gopherspace in 2020](https://cheapskatesguide.org/articles/gopherspace.html) as a nice orientation to see the internet through lynx and text\n- Even [Project Gemini](https://gemini.circumlunar.space/)\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "keywords": [
          "small web",
          "twtxt",
          "micro blogging",
          "social networks"
        ],
        "pubDate": "2022-11-11",
        "title": "Twitter's pending implosion"
      },
      "url": "posts/2022/11/11/Twitter-implosion.json"
    },
    {
      "content": "\nCompiling Pandoc from source\n============================\n\nBy R. S. Doiel, 2022-11-07\n\nI started playing around with Pandoc's __pandoc-server__ last Friday. I want to play with the latest version of Pandoc.  When I gave it a try this weekend I found that my Raspberry Pi 400's SD card was too small. This lead me to giving the build process a try on my Ubuntu desktop. These are my notes about how I going about building from scratch.  I am not a Haskell programmer and don't know the tool chain or language. Take everything that follows with a good dose of salt but this is what I did to get everything up and running. I am following the compile from source instructions in Pandoc's [INSTALL.md](https://github.com/jgm/pandoc/blob/master/INSTALL.md)\n\nI'm running this first on an Intel Ubuntu box because I have the disk space available there. If it works then I'll try it directly on my Raspberry Pi 400 with an upgrade SD card and running the 64bit version of Raspberry Pi OS.\n\nI did not have Haskell or Cabal installed when I started this process.\n\nSteps\n-----\n\n1. Install __stack__ (it will install GHC)\n2. Clone the GitHub repo for [Pandoc](https://github.com/jgm/pandoc)\n3. Setup __stack__ for Pandoc\n4. Build and test with __stack__\n5. Install __stack__ install\n6. Make a symbolic link from __pandoc__ to __pandoc-server__\n\n```\nsudo apt update\nsudo apt search \"haskell-stack\"\nsudo apt install \"haskell-stack\"\nstack upgrade\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup \nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nThis step takes a long time and on the Raspberry Pi it'll take allot longer.\n\nThe final installation of Pandoc was in my `$HOME/.local/bin` directory. Assuming this is early in your path this can allow you to experiment with a different version of Pandoc from the one installed on your system. \n\nI also wanted to try the latest of __pandoc-server__.  This was not automatically installed and is not mentioned in the INSTALL.md file explicitly. But looking at the discussion of running __pandoc-server__ in CGI mode got me thinking. I then checked the installation on my Ubuntu box for the packaged version of pandoc-server and saw that is was a symbolic link.  Adding a `ln` command to my build instruction solved the problem.\n\nI decided to try compiling Pandoc on my M1 mac.  First I needed to get __stack__ installed. I use Mac Ports but it wasn't in the list of available packages.  Fortunately the Haskell Stack website provides a shell script for installation on Unixes. I wanted to install __stack__ in my home `bin` directory not `/usr/bin/slack`. So after reviewing the downloaded install script I found the `-d` option for changing where it installs to. It indicated I need to additional work with __xcode__.\n\n```\ncurl -sSL https://get.haskellstack.org/ > stack-install.sh\nmore stack-install.sh\nsh stack-install.sh -d $HOME/bin\n```\n\nThe __stack__ installation resulted in a message in this form.\n\n```\nStack has been installed to: $HOME/bin/stack\n\nNOTE: You may need to run 'xcode-select --install' and/or\n      'open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg'\n      to set up the Xcode command-line tools, which Stack uses.\n\nWARNING: '$HOME/.local/bin' is not on your PATH.\n    Stack will place the binaries it builds in '$HOME/.local/bin' so\n    for best results, please add it to the beginning of PATH in your profile.\n```\n\nI already had xcode setup for compiling Go so those addition step was not needed.  I only needed to add `$HOME/.local/bin` to my search path.\n\nI then followed the steps I used on my Ubuntu Intel box.\n\n```\ngit clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc\ncd src/github.com/jgm/pandoc\nstack setup\nstack build\nstack test\nstack install\nln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server\n```\n\nNow when I have a chance to update my Raspberry Pi 400 to a suitable sized SD Card (or external drive) I'll be ready to compile a current version of Pandoc from source.\n\nAdditional notes\n----------------\n\n[stack](https://docs.haskellstack.org/en/stable/) is a Haskell build tool. It setups up an Haskell environment per project. If a project requires a specific version of the Haskell compiler it'll be installed and made accessible for the project. In this way it's a bit like having a specific environment for Python. The stack website indicates that it targets cross platform development in Haskell which is nice.  Other features of stack remind me of Go \"go\" command in that it can build things or Rust's \"cargo\" command. Like __cargo__ it can update itself which is nice. That is what I did after installing the Debian package version used by Ubuntu. Configuration of a \"stack\" project uses YAML files. Stack uses __cabal__, Haskell's older build tool but subsumes __cabal-install__ for setting up __cabal__ and __ghc__. It appears from my reading that __stack__ addresses some of the short falls __cabal__ originally had and specifically focusing on reproducible compiles. This is important in sharing code as well as if you want to integrate automated compilation and testing. It maintains a project with \"cabal files\" so there is the ability to work with older non-stack code if I read the documentation correctly. Both __cabal__ and __stack__ seem to be evolving in parallel taking different approaches but influencing one another. Both systems use \"cabal files\" for describing projects and dependencies as of 2022. The short version of [Why Stack](https://docs.haskellstack.org/en/stable/#why-stack) can be found the __stack__ website.\n\n[Hackage](https://hackage.haskell.org/) is a central repository of Haskell packages. \n\n[Stackage](https://www.stackage.org/) is a curated subset of Hackage packages. It appears to be the preferred place for __stack__ to pull from.\n\n\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-11-07",
        "keywords": [
          "pandoc",
          "pandoc-server",
          "pandoc-citeproc",
          "haskell-stack",
          "cabal",
          "ghc"
        ],
        "pubDate": "2022-11-07",
        "title": "Compiling Pandoc from source"
      },
      "url": "posts/2022/11/07/compiling-pandoc-from-source.json"
    },
    {
      "content": "\nFeeds, formats, and plain text\n==============================\n\nBy R. S. Doiel, 2022-11-01\n\nThere has been a proliferation of feed formats. My personal preferred format is RSS 2.0. It's stable and proven the test of type. Atom feeds always felt a little like, \"not invented here so we're inventing it again\", type of thing. The claim was they could support read/write but so can RSS 2.0 specially with the namespace possibilities. The innovative work [Dave Winer](https://scripting.com) has done in the past and is doing today with [Feedland](https://feedland.org) is remarkably impressive.\n\nIn my experience the format of the feed is less critical than the how to author the metadata.  Over the last several years I've moved to static hosting as my preferred way of hosting a website. My writing is typically in Markdown or Fountain formats and frontmatter like used in RMarkdown has proven very convenient. The \"blogit\" command that started out from an idea in [mkpage](https://github.com/caltechlibrary/mkpage \"Make Page, a Pandoc preprocessor and tool set\") has been implemented in [pttk](https://github.com/rsdoiel/pttk \"Plain Text Toolkit\"). So for me metadata authoring makes sense in the front matter. That has the advantage that Pandoc can leverage the information in its templates (that is what I use to render HTML, man pages and the occasional PDF). It also is a food source for data to include in a feed.\n\nI've recently become aware of a really simple text format called [twtxt](https://twtxt.readthedocs.io/en/latest/). This simple format is meant for micro blogging but is also useful as a feed source and format. Especially in terms of rendering content for Gopherspace which I've re-engaged in recently. [Yarn.social](https://yarn.social) has built an entire ecosystem around it. Very impressive. The format is so simple it can be done with a pipe and the \"echo\" command in the shell.  It looks promising in terms for personal search ingest as well.\n\nOne of the formats that Dave Winer supports in Feedland and is used in the micro blogging community he has connected with is [jsonfeeds](https://www.jsonfeed.org/). It is lightweight and to me feels allot like RSS 2.0 without the XML-isms that go along with it.  I'm playing with the idea that in pttk it'll be the standard feed format and that from it I can then render our traditional feed friends of RSS 2.0 and Atom.\n\nI've looked at the ActivityPub from the Mastodon community but like [James Mill](https://prologic.github.io/prologic/ \"aka prologic\") I find it too complex. What is needed is something simple, really simple.  That's why I've been looking closely at Gopherspace again. The Gophermap can function as a bookmark file, a \"home page\" a list of feeds. A little archaic but practical in its simplicity. The only challenges I've run into has been figuring out that expectations of the Gopher server software. Currently I've settled on [gophernicus](https://gophernicus.org) as that is was it supported at [sdf.org](https://sdf.org) where I have a gopher \"hole\".\n\nAs pttk grows and I explore where I can take simple text processing I'm not targeting Gopherspace, twtxt and static websites. I've looked at [Gemini](https://gemini.circumlunar.space/docs/specification.gmi) but haven't grokked the point yet.  Their choice of yet another markup for content seems problematic at best. For me gopher solves the problems that would make me look at Gemini and I can use most any structured text I want. The text just needs to be readable easily by humans. The Gophermap provides can be enhanced menus much like \"index.html\" pages have become (a trunk that branches and eventually leads to a leaf). \n\n[OPML](http://home.opml.org/) remains a really nice outline data format.  It's something I'd like to eventually integrate with pttk. It can be easily represented as JSON. Just need to figure what problem I am trying to solve by using it.  Share a list of feeds is the classic case but looking at twtxt as well as the [newsboat](https://newsboat.org/) URL list makes me think it is more than I need. We'll see.  It is certainly reasonable to generate from a simpler source. If I ever write a personal search engine (something I've been thinking about to nearly a decade) it'd be a good way to share curated indexes sources as well as sources to crawl.  I just need to think that through more.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "plain text",
          "small internet",
          "rss",
          "jsonfeed",
          "gopher"
        ],
        "pubDate": "2022-11-01",
        "title": "feeds, formats and plain text"
      },
      "url": "posts/2022/11/01/Feeds-formats-and-plain-text.json"
    },
    {
      "content": "\nInstalling Cargo/Rest on Raspberry Pi 400\n=========================================\n\nOn my Raspberry Pi 400 I'm running the 64bit Raspberry Pi OS.\nThe version of Cargo and Rustc are not recent enough to install\n[ncgopher](https://github.com/jansc/ncgopher). What worked for\nme was to first install cargo via the instructions in the [The Cargo Book](https://doc.rust-lang.org/cargo/getting-started/installation.html). \n\n~~~shell\ncurl https://sh.rustup.rs -sSf | sh\n~~~\n\nAn important note is if you previously installed a version of Cargo/Rust\nvia the debian package system you should uninstall it before running the\ninstructions above from the Cargo Book.\n\nWith this version I was able to install __ncgopher__ using the simple\nrecipe of \n\n~~~shell\ncargo install ncgopher\n~~~\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "keywords": [
          "64bit",
          "Rapsberry Pi OS",
          "Cargo",
          "rustc"
        ],
        "pubDate": "2022-10-31",
        "title": "Installing Cargo/Rust on Raspberry Pi 400"
      },
      "url": "posts/2022/11/01/installing-cargo-rust-r400.json"
    },
    {
      "content": "\n# 7:30 AM, Oberon Language: A minimum SYSTEM module\n\nPost: Tuesday, October 18, 2022, 7:30 AM\n\nIt occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.\n\n",
      "data": {
        "keywords": [
          "A minimum SYSTEM module"
        ],
        "no": 1,
        "pubDate": "2022-10-18",
        "series": "Oberon Language",
        "title": "7:30 AM, Oberon Language: A minimum SYSTEM module"
      },
      "url": "posts/2022/10/18/Wishlist-Oberon-in-2023-2022-10-18_070730.json"
    },
    {
      "content": "\nWish list for Oberon in 2023\n===========================\n\nNext year will be ten years since Prof. Wirth and Paul Reed released [Project Oberon 2013](https://www.projectoberon.com).  It took me most of that decade to stumble on the project and community.  I am left wondering if Prof. Wirth and Paul Reed sat down today what would they design? I think only minor changes are needed and those mostly around assumptions.\n\nOberon-07 changing assumptions\n------------------------------\n\nThe language of Oberon-07 doesn't need to change. I do think the assumptions of the compiler are worth revisiting.  A CHAR should not be assumed to be an eight bit byte.  A CHAR should represent a character or symbol in a language. Many if not most of the Oberon community speaks language other than American English and that which is trivially represented in seven or eight bit ASCII.  While changing the representation assumption in Oberon-07 does increase complexity I feel restrict the character presentation of a CHAR to eight bits puts us on the side of \"too simple\" in the equation of \"Simpler but not to simple\".\n\nI am concerned about the assumption of an INTEGER as 32 bits. Increasingly I've seen single board computer implementations that are 64 bits.  Today feels allot like when I started in computing where personal computers were shifting from eight or sixteen bits to thirty two.  I suspect increasingly we will find that eight, sixteen and thirty two bit computers are relegated to the realm of specialized computers. While supporting these other widths will remain important I think shifting assumptions to sixty four bit machines makes sense now. Is a 32 bit machine \"too simple\" in our equation of \"simpler but not too simple\"?\n\n\n\nOberon as Operating System\n--------------------------\n\nThe operating system I still find liberating in 2023 as when I first was able to use it.  The challenge in 2023 though is the three button mouse. I think the supporting the historic mouse remains important but that the viewers should also support navigation via the keyboard and easily support touch systems that lack a mouse.  Being backward compatibly while adopting an enhance UI would make things more complex bit if care is taken I think that it can be done while keep the equation balanced as \"simpler, but not too simple\".\n\nTransforming my assumptions in 2023\n-----------------------------------\n\nI think the Artemis Project should presume that the representation of CHAR and INTEGER may change and probably should change. The portable modules should support compiling Oberon-07 programs on non-Oberon 2014 Systems without change.  I am skeptical that I can create a module system that provides a base line with the historic Oberon implementations. I think the Oakwood modules are just too limited. I think the assumption is I need implementations for Project Oberon 2013 modules as the base line perhaps enhanced with a few additional modules to supporting networking, UTF-8, JSON, and XML. The goal I think is that using Artemis on a non-Oberon System should facilitate bootstrapping an Oberon System 2023 should one come to exist.\n\nErrata\n------\n\n7:00 - 7:30; Oberon Language; A minimum SYSTEM module; It occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-10-16",
        "keywords": [
          "Oberon",
          "Oberon-07",
          "Oberon System",
          "Artemis"
        ],
        "pubDate": "2022-10-16",
        "title": "Wish list for Oberon in 2023"
      },
      "url": "posts/2022/10/16/Wishlist-Oberon-in-2023.json"
    },
    {
      "content": "\n# 7:30 AM, Gopher: Setup\n\nPost: Monday, October 10, 2022, 7:30 AM\n\nAccount verified, Yippee!\n\n",
      "data": {
        "keywords": [
          "Setup"
        ],
        "no": 1,
        "pubDate": "2022-10-10",
        "series": "Gopher",
        "title": "7:30 AM, Gopher: Setup"
      },
      "url": "posts/2022/10/10/getting-things-setup-2022-10-10_070730.json"
    },
    {
      "content": "\n\nGetting things setup\n====================\n\nBy R. S. Doiel, 2022-10-09\n\nI'm digging my [gopherhole on sdf.org](gopher://sdf.org:70/0/users/rsdoiel)\nas I wait for my validation to go through.  The plan is to migrate content\nfrom rsdoiel.github.io to here and host it in a Gopher context.  It's\ninteresting learning my way around sdf.org. Reminds me of my student days\nwhen I first had access to a Unix system.  Each Unix has it own flavors and\neven for the same Unix type/version each system has it's own particular\nvariation. Unix lends itself to customization and that why one system can\n\"feel\" or \"look\" different than the next.\n\nI'm trying to remember how to use Pico (vi isn't available yet).\nDiscovering how far \"mkgopher\" can go (very far it turns out).\n\nI'm looking forward to validation so I can have access to Git and\n\"move in\" to this gopherspace in a more sustainable way.\n\nThings to read and do\n---------------------\n\n- wait to be validated\n- learn [gitia](https://git.sdf.org) and setup up a mirror my personal projects and blog\n- read up on [gophernicus](https://www.gophernicus.org/) (the gopher server used by sdf.org)\n- [twenex project](https://www.twenex.org/), sounds interesting,\n  I remember accessing a TOPS-20 system at Whitesands in New Mexico\n  once upon a time.\n- figure out to access comp.lang.oberon if it is available on sdf.org\n- figure out, after validation, if I can compile OBNC for working on\n  Artemis and Oberon-07 code projects\n\nUpdates\n-------\n\n7:30 - 7:30; Gopher; Setup; Account verified, Yippee!\n",
      "data": {
        "author": "rsdoiel@sdf.org (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-10-09",
        "keywords": [
          "gopher",
          "public unix"
        ],
        "pubDate": "2022-10-09",
        "title": "Getting things setup"
      },
      "url": "posts/2022/10/09/getting-things-setup.json"
    },
    {
      "content": "\nThinking about Gopher\n=====================\n\nBy R. S. Doiel, 2022-09-28\n\nLast weekend I visited the [Gophersphere](gopher://gopher.floodgap.com \"Floodgap is a good starting point for Gopher\") for the first time in a very long time. I'm happy to report it is still alive an active. It remains fast, lower resource consuming. This resulted in finding a Gopher protocol package in Go and adding light weight Gopher server to [pttk](https://rsdoiel.github.io/pttk) my current vehicle for experimenting with plain text writing tools.\n\nI've been thinking allot this past half week about where to explore in Gopher. The biggest issue I ran into turned out to be easily solve. Gopher protocol is traditionally served over port 70 but if you're running a \\*nix if you are just experimenting on localhost it is easier to use port 7000 (analogous to port 80 becoming 8000 or 8080 in the websphere). But some Gopher clients will only serve port 70. Two clients work very well at 7000 and they are Lynx (the trusty old console web browser) and one written in Rust called [ncgopher](https://github.com/jansc/ncgopher). The latter I find I use most of the time. It also supports Gemini sites though I am less interested in Gemini at the movement.  Gopher has a really nice sweet spot of straight forward implementation for both client and server. It would be a good exercise for moving from beginner to intermediate programming classes as you would be introducing network programming, a little parsing and the client server application models. It's a really good use case of looking back (Gopher is venerable in Internet age) and looking forward (a radical simplification of distributing readable material and related files).\n\nConstraints and creativity\n--------------------------\n\nThe simplicity and limitations of Gopher are inspiring. The limitations are particularly important as they are good constraints that help focus where to innovate. Gopher is a protocol ripe for software innovation precisely because of it's constraints.\n\nGophermaps is a good example. The Go package [git.mills.io/prologic/go-gopjer](https://git.mills.io/prologic/go-gopher) supports easily building servers that have Gophermaps the way of structuring the Gopher menus (aka selectors in Gopher parlance). A Gophermaps is a plain text file where you have lines that start with a Gopher supported document type (see [Gopher protocol](https://en.wikipedia.org/wiki/Gopher_(protocol) for details) a label followed by a tab character, a relative path followed by a tab character, a hostname followed by a tab character and a port number.  Very simple to parse.  The problem is Gopher clients expect all the fields for them to interpret them as a linked resource (e.g. a text file, binary file, image, or another Gopher selector). When I first encountered Gopher at USC so many years ago (pre-Mosaic, pre-Netscape) Gophermaps selectors are trivial to setup and you could build a service that supported ftp and Gopher in the same directory structure. All the \"development\" of a gopher site was done directly on the server in the directories where the files would live. Putting in all values seemed natural. Today I don't develop on a \"production server\" if I can avoid it. My writing is done on a small pool of machines at home, each with its own name. Explicitly writing a hostname and port with the eye to publishing to a public site then becomes a game of running `sed` to correct hostname and ports across the updated Gophermap files.\n\n> Gopher selectors form \"links\" to navigate through a Gopher site or through the Gophersphere depending on what they point at\n\nWithout changing the protocol you could modify the go-gopher package's function for presenting a Gophermap where the hostname port is assumed to the gopher server name and port if it was missing. Another approach would be to translate a place holder value. This would facilitate keeping your Gopher site under version control (e.g. Git and GitHub) while allowing you to easily deploy a version of the site in a public setting or in your development setting.  The constraint of the Gophermap definition as needed by the protocol doesn't mean it forces a cumbersome choice on your writing process.\n\nSimilarly the spaces versus tabs (invisible by default in many editors) because a non-issue by adopting editors that support [editorconfig](https://editorconfig.org) or even making the server slightly more complex in correctly identifying when to convert spaces to tabs expanding them out to a Gopher selectors.\n\nClient sites there are also many opportunities.  [Little Gopher Client](http://runtimeterror.com/tools/gopher/) pulls out the selectors its finds into a nice tree (like a bookmark tree) in a left panel and puts the text in the main window.  ncgopher let's you easily bookmark things and has a very clean, easy on the eyes reading experience in the console. In principle you the client could look at the retrieved selector and choose to display different file types based on the file extension as well as the selector type retrieved. This would let you include a richer experience in the Gophersphere for light weight markup like Commonmark files while still running nicely on Gopher protocol. Lots of room to innovate because the protocol is simple, limited and stable after all these years.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "draft": true,
        "pubDate": "2022-09-28",
        "title": "Thinking about Gopher"
      },
      "url": "posts/2022/09/28/thinking-about-gopher.json"
    },
    {
      "content": "\nRust development notes\n======================\n\nby R. S. Doiel, 2022-09-27\n\nI recently wanted to try [ncgopher](https://github.com/jansc/ncgopher) which is a [rust](https://rust-lang.org) based application. I was working on a an M1 Mac mini. I use [Mac Ports](https://www.macports.org) for my userland applications and installed [cargo](https://doc.rust-lang.org/cargo/) to pickup the rust compiler and build tool\n\n```shell\nsudo port install cargo\n```\n\nAll went well until I tried to build ncgopher and got an error as follows\n\n```\ncargo build --release\n    Updating crates.io index\nerror: Unable to update registry `crates-io`\n\nCaused by:\n  failed to fetch `https://github.com/rust-lang/crates.io-index`\n\nCaused by:\n  failed to authenticate when downloading repository: git@github.com:rust-lang/crates.io-index\n\n  * attempted ssh-agent authentication, but no usernames succeeded: `git`\n\n  if the git CLI succeeds then `net.git-fetch-with-cli` may help here\n  https://doc.rust-lang.org/cargo/reference/config.html#netgit-fetch-with-cli\n\nCaused by:\n  no authentication available\nmake: *** [build] Error 101\n```\n\nThis seemed odd as I could run `git clone git@github.com:rust-lang/crates.io-index` successfully. Re-reading the error message a dim light went on. I checked the cargo docs and the value `net.git-fetch-with-cli` defaults to false. That meant that cargo was using its own embedded git. OK, that makes sense but how do I fix it. I had no problem using cargo installed via ports on an Intel iMac so what gives? When cargo got installed on the M1 there was now `.cargo/config.toml` file. If you create this and set the value of `git-fetch-with-cli` to true then the problem resolves itself.\n\nIt was good that the error message provided a lead. It's also good that cargo has nice documentation. My experience though still left the taste of [COIK](https://www.urbandictionary.com/define.php?term=coik). Not sure how to improve the situation. It's not really a cargo bug (unless config.taml should be always created), it's not a rust bug and I don't even think it is a ports packaging bug.  If I was a new developer just getting familiar with git I don't think I would have known how to solve my problem even with the documentation provided. Git is something that has always struggled with COIK. While I like it it does make things challenging.\n\nIf I wind up playing with rust more then I'll add somemore notes here in the future.\n\nMy `$HOME/.cargo/config.toml` file looks like to have cargo use the git cli instead of the built in rust library.\n\n```\n[net]\ngit-fetch-with-cli = true\n```\n\n\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "rust",
          "cargo",
          "M1",
          "macOS",
          "ports"
        ],
        "pubDate": "2022-09-27",
        "title": "Rust development notes"
      },
      "url": "posts/2022/09/27/rust-development-notes.json"
    },
    {
      "content": "\n# 7:30 AM, Golang: pttk\n\nPost: Monday, September 26, 2022, 7:30 AM\n\nrenamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n\n",
      "data": {
        "keywords": [
          "pttk"
        ],
        "no": 1,
        "pubDate": "2022-09-26",
        "series": "Golang",
        "title": "7:30 AM, Golang: pttk"
      },
      "url": "posts/2022/09/26/golang-development-2022-09-26_070730.json"
    },
    {
      "content": "\nPostgreSQL dump and restore\n===========================\n\nThis is a quick note on easily dumping and restoring a specific database\nin Postgres 14.5.  This example has PostgreSQL running on localhost and\n[psql](https://www.postgresql.org/docs/current/app-psql.html) and\n[pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) are both available.\nOur database administrator username is \"admin\", the database to dump is called \"collections\". The SQL dump\nfile will be named \"collections-dump-2022-09-19.sql\".\n\n```shell\n\tpg_dump --username=admin --column-inserts \\\n\t    collections >collections-dump-2022-09-19.sql\n```\n\nFor the restore process I follow these steps\n\n1. Using `psql` create an empty database to restore into\n2. Using `psql` replay (import) the dump file in the new database to restoring the data\n\nThe database we want to restore our content into is called \"collections_snapshot\"\n\n```shell\n\tpsql -U dbadmin\n\t\\c postgres\n\tDROP DATABASE IF EXISTS collections_snapshot;\n\tCREATE DATABASE collections_snapshot;\n\t\\c collections_snapshots\n\t\\i ./collections-dump-2022-09-19.sql\n\t\\q\n```\n\nOr if you want to stay at the OS shell level\n\n```shell\n\tdropdb collections_snapshot\n\tcreatedb collections_snapshot\n\tpsql -U dbadmin --dbname=collections_snapshot -f ./collections-dump-2022-09-19.sql\n```\n\n\nNOTE: During this restore process `psql` will display some output. This is normal. The two\ntypes of lines output are shown below.\n\n```sql\n\tINSERT 0 1\n\tALTER TABLE\n```\n\nIf you want to stop the input on error you can use the `--set` option to set the error behavior\nto abort the reload if an error is encountered.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-09-19",
        "draft": true,
        "keywords": [
          "PostgreSQL"
        ],
        "pubDate": "2022-09-19",
        "title": "PostgreSQL dump and restore"
      },
      "url": "posts/2022/09/19/PostgreSQL-Dump-and-Restore.json"
    },
    {
      "content": "\n# 12:30 PM, SQL: Postgres\n\nPost: Monday, September 19, 2022, 12:30 PM\n\nSetting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 1,
        "pubDate": "2022-09-19",
        "series": "SQL",
        "title": "12:30 PM, SQL: Postgres"
      },
      "url": "posts/2022/09/19/rosette-notes-2022-09-19_121230.json"
    },
    {
      "content": "\nOrdering Front Matter\n=====================\n\nBy R. S. Doiel, 2022-08-30\n\nA colleague of mine ran into an interesting Pandoc behavior. He was combining a JSON metadata document and a converted word document and wanted the YAML front matter to have a specific order of fields (makes it easier for us humans to quickly scan it and see what the document was about).\n\nThe order he wanted in the front matter was\n\n- title\n- interviewer\n- interviewee\n- abstract\n\nThis was for a collection of oral histories. When my friend use Pandoc's `--metadata-json` to read the JSON metadata it rendered the YAML fine except the attributes were listed in alphabetical order.\n\nWe found a solution by getting Pandoc to treat the output not as Markdown plain text so that we could template the desired order of attributes.\n\nHere's the steps we used.\n\n1. create an empty file called \"empty.txt\" (this is just so you pandoc doesn't try to read standard input and processes\nyou metadata.json file with the template supplied)\n2. Create a template with the order you want (see below)\n3. Use pandoc to process your \".txt\" file and your JSON metadata file using the template (it makes it tread it as plain text even though we're going to treat it as markdown later)\n4. Append the content of the word file and run pandoc over your combined file as you would normally to generate your HTML\n\n\nThis is the contents of our [metadata.json](metadata.json) file.\n\n```json\n    {\n        \"title\": \"Interview with Mojo Sam\", \n        \"interviewee\": \"Mojo Sam\", \n        \"interviewer\": \"Tom Lopez\",\n        \"abstract\": \"Interview in three sessions over sevaral decases, 1970 - 20020. The interview was conducted next door to reality via a portal in Old Montreal\"\n    }\n```\n\n[frontmatter.tmpl](frontmatter.tmpl) is the template we used to render ordered front matter.\n\n```\n    ---\n    $if(title)$title: \"$title$\"$endif$\n    $if(interviewee)$interviewee: \"$interviewee$\"$endif$\n    $if(interviewer)$interviewer: \"$interviewer$\"$endif$\n    $if(abstract)$abstract: \"$abstract$\"$endif$\n    ---\n```\n\nHere's the commands we used to generate a \"doc.txt\" file with the \nfront matter in the desired order. Not \"empty.txt\" is just an empty\nfile so Pandoc will not read from standard input and just work with the\nJSON metadata and our template.\n\n```\ntouch empty.txt\npandoc --metadata-file=metadata.json --template=frontmatter.tmpl empty.txt\n```\n\nThe output of the pandoc command looks like this.\n\n```\n    ---\n    title: \"Interview with Mojo Sam\"\n    interviewee: \"Mojo Sam\"\n    interviewer: \"Tom Lopez\"\n    abstract: \"Interview in three sessions over sevaral decases, 1970 -\n    20020. The interview was conducted next door to reality via a portal in\n    Old Montreal\"\n    ---\n```\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "keywords": [
          "pandoc",
          "front matter"
        ],
        "pubDate": "2022-08-30",
        "title": "Ordering front matter"
      },
      "url": "posts/2022/08/30/Ordering-Frontmatter.json"
    },
    {
      "content": "\nPostgres 14 on Ubuntu 22.04 LTS\n===============================\n\nby R. S. Doiel, 2022-08-26\n\nThis is just a quick set of notes for working with Postgres 14 on an Ubuntu 22.04 LTS machine.  The goal is to setup Postgres 14 and have it available for personal work under a user account (e.g. jane.doe). \n\nAssumptions\n\n- include `jane.doe` is in the sudo group\n- `jane.doe` is the one logged in and installing Postgres for machine wide use\n- `jane.doe` will want to work with her own database by default\n\nSteps\n\n1. Install Postgres\n2. Confirm installation\n3. Add `jane.doe` user providing access\n\nBelow is the commands I typed to run to complete the three steps.\n\n~~~shell\nsudo apt install postgresql postgresql-contrib\nsudo -u createuser --interactive\njane.doe\ny\n~~~\n\nWhat we've accomplished is installing Postgres, we've create a user in Postgres DB environment called \"jane.doe\" and given \"jane.doe\" superuser permissions, i.e. the permissions to manage Postgres databases.\n\nAt this point we have a `jane.doe` Postgres admin user. This means we can run the `psql` shell from the Jane Doe account to do any database manager tasks. To confirm I want to list the databases available\n\n~~~shell\npsql \nSELECT datname FROM pg_database;\n\\quit\n~~~\n\nNOTE: This post is a distilation of what I learned from reading Digital Ocean's [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel",
        "byline": "R. S. Doiel",
        "daft": true,
        "number": 4,
        "pubDate": "2022-08-26",
        "series": "SQL Reflections",
        "title": "Postgres 14 on Ubuntu 22.04 LTS"
      },
      "url": "posts/2022/08/26/postgres-14-on-ubuntu-22.04-LTS.json"
    },
    {
      "content": "\n# 10:30 AM, SQL: Postgres\n\nPost: Friday, August 26, 2022, 10:30 AM\n\nIf you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 2,
        "pubDate": "2022-08-26",
        "series": "SQL",
        "title": "10:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/26/rosette-notes-2022-08-26_101030.json"
    },
    {
      "content": "\nA Quick intro to PL/pgSQL\n========================\n\nPL/pgSQL is a procedure language extended from SQL. It adds flow control and local state for procedures, functions and triggers. Procedures, functions and triggers are also the compilation unit. Visually PL/pgSQL looks similar to the MySQL or ORACLE counter parts. It reminds me of a mashup of ALGO and SQL. Like the unit of compilation, the unit of execution is also procedure, function or trigger. \n\nThe Postgres documentation defines and explains the [PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql.html) and how it works.  This document is just a quick orientation with specific examples to provide context.\n\nHello World\n-----------\n\nHere is a \"helloworld\" procedure definition.\n\n```sql\n    CREATE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n       RAISE NOTICE 'Hello WORLD!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nLet's take a look this line by line.\n\n1. CREATE PROCEDURE defines the procedure and the starting and ending delimiter for the procedure (e.g. `AS $$` the procedure's text ends when `$$` is encountered an second time.\n2. DECLARE is the block where you would declare the variables used in the procedure, we have none in this example\n3. The BEGIN starts the actual procedure instructions\n4. The `RAISE NOTICE` line is how you can display output to the console when the procedure is run\n5. The END completes the procedure definition\n6. the `$$ LANGUAGE plpgsql;` concludes the text defining the procedure and tells the database engine that procedure is written in PL/pgSQL.\n\nWe can run the procedure using the \"CALL\" query.\n\n```sql\n    CALL helloworld()\n```\n\nNOTE: If you want to change the procedure you can \"DROP\" it first otherwise you'll get an error that it already exists.\n\n```sql\n    DROP PROCEDURE helloworld;\n```\n\nImproving my workflow\n---------------------\n\nSQL procedures are generally stored in the RDBMs in database environment. You can think of them as records in the system's database. Procedures and functions are created and can be dropped. While they can be manually typed in the database's shell it is easier to maintain them in plain text files outside the RDBM environment.  \n\n1. Write the procedure in a text file.\n2. Load the text file (e.g. FILENAME) into Postgres \n   a. outside the Postgres shell use `psql -f FILENAME` \n   b. inside the Postgres shell used `\\i FILENAME`\n3. Call the procedure to test it\n\nTo turn these steps into a look I use a \"CREATE OR REPLACE\" statement and be able to reload the updated procedure easier see [43.12. Tips for Developing in PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql-development-tips.html).  Note in the revised example the \"-- \" lines are comments.\n\nOur revised [helloworld](helloworld.plpgsql).\n\n```sql\n    --\n    -- Create (or replace) the new \"helloworld\" procedure.\n    -- NOTE: this can be run with \"CALL\"\n    --\n    CREATE OR REPLACE PROCEDURE helloworld() AS $$\n    DECLARE\n    BEGIN\n        RAISE NOTICE 'Hello World!';\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\n\nHi There\n--------\n\n[hithere](hithere.plpgsql) is similar to our helloworld example except it is a function that takes a parameter of the person's name. The function returns a \"VARCHAR\", so this should work as part of a select statement.\n\n```sql\n    --\n    -- This is a \"Hi There\" function. The function takes\n    -- a single parameter and forms a greeting.\n    --\n    CREATE OR REPLACE FUNCTION hithere(name varchar) RETURNS varchar AS $$\n    DECLARE\n      greeting varchar;\n    BEGIN\n        IF name = '' THEN\n            greeting := 'Hi there!';\n        ELSE\n            greeting := 'Hello ' || name || '!';\n        END IF;\n        RETURN greeting;\n    END;\n    $$ LANGUAGE plpgsql;\n```\n\nGiving it a try.\n\n```shell\n    SELECT hithere('Mojo Sam');\n```\n\nFurther reading\n---------------\n\n- [Conditionals](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONDITIONALS)\n- [Loops](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONTROL-STRUCTURES-LOOPS)\n- [Calling a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-CALLING-PROCEDURE)\n- [Early return from a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-RETURNING-PROCEDURE)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-24",
        "keywords": [
          "postgres",
          "sql",
          "psql",
          "plsql",
          "plpgsql"
        ],
        "number": 3,
        "pubDate": "2022-08-24",
        "series": "SQL Reflections",
        "title": "A Quick into to PL/pgSQL"
      },
      "url": "posts/2022/08/24/plpgsql-quick-intro.json"
    },
    {
      "content": "\n# 12:00 PM, SQL: Postgres\n\nPost: Wednesday, August 24, 2022, 12:00 PM\n\nI miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%'\n`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 3,
        "pubDate": "2022-08-24",
        "series": "SQL",
        "title": "12:00 PM, SQL: Postgres"
      },
      "url": "posts/2022/08/24/rosette-notes-2022-08-24_121200.json"
    },
    {
      "content": "\n# 11:30 AM, SQL: Postgres\n\nPost: Monday, August 22, 2022, 11:30 AM\n\nThree things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n",
      "data": {
        "keywords": [
          "Postgres"
        ],
        "no": 4,
        "pubDate": "2022-08-22",
        "series": "SQL",
        "title": "11:30 AM, SQL: Postgres"
      },
      "url": "posts/2022/08/22/rosette-notes-2022-08-22_111130.json"
    },
    {
      "content": "\nRosette Notes\n=============\n\nBy R. S. Doiel, 2022-08-19\n\n> A dance around two relational databases, piecing together similarities as with the tiny mosaic tiles of a guitar's rosette\n\nWhat follows are my preliminary notes learning Postgres 12 and 14.\n\nPostgres & MySQL\n----------------\n\nThis is a short comparison of some administrative commands I commonly use. The first column describes the task followed by the SQL to execute for Postgres 14.5 and then MySQL 8. The presumption is you're using `psql` to access Postgres and `mysql` to  access MySQL. Values between `<` and `>` should be replaced with an appropriate value.\n\n| Task                    | Postgres 14.5                     | MySQL 8           |\n|-------------------------|------------------------------------|-------------------|\n| show all databases      | `SELECT datname FROM pg_database;` | `SHOW DATABASES;` |\n| select a database       | `\\c <dbname>`                      | `USE <dbname>`    |\n| show tables in database | `\\dt`                              | `SHOW TABLES;`    |\n| show columns in table   | `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '<table_name>';` | `SHOW COLUMNS IN <table_name>` |\n\nReflections\n-----------\n\nThe Postgres shell, `psql`, provides the functionality of showing a list of tables via a short cut while MySQL choose to add the `SHOW TABLES` query. For me `SHOW ...` feels like SQL where as `\\d` or `\\dt` takes me out of SQL space. On the other hand given Postgres metadata structure the shortcut is appreciated and I often query for table names as I forget them. `\\dt` quickly becomes second nature and is shorter to type than `SHOW TABLES`. \n\nConnecting to a database with `\\c` in `psql` is like calling an \"open\" in programming language. The \"connection\" in `psql` is open until explicitly closed or the shell is terminated.  Like `USE ...` in the MySQL shell it make working with multiple database easy.  The difference are apparent when you execute a `DROP DATABASE ...` command. In `psql` you need to `CLOSE` the database first or the `DROP` will fail.  The MySQL shell will happily let you drop the current database you are currently using.\n\nThe challenge I've experienced learning `psql` after knowing MySQL is my lack of familiarity with the metadata Postgres maintains about databases and structures.  On the other hand everything I've learned about standards base SQL applies to managing Postgres once remember the database/table I need to work with.  A steeper learning curve from MySQL's `SHOW` but it also means writing external programs for managing Postgres databases and tables is far easier because everything is visible because that is how you manage Postgres. MySQL's `SHOW` is very convenient but at the cost of hiding some of its internal structures.\n\nBoth MySQL and Postgres support writing programs in SQL. They also support stored procedures, views and triggers. They've converged in the degree in which they have both implemented SQL language standards.  The differences are mostly in approach to managing databases.  There are some differences, necessitated by implementation choices, in the `CREATE DATABASE`, `CREATE TABLE` or `ALTER` statements but you can often use the basic form described in ANSI SQL and get the results you need. When doing performance tuning the dialect differences are more important.\n\nDump & Restore\n--------------\n\nBoth Postgres and MySQL provide command line programs for dumping a database. MySQL provides a single program where as Postgres splits it in two. Check the man pages (or website docs) for details in their options. Both sets of programs are highly configurable allowing you to dump just schema, just data or both with different expectations.\n\n| Postgres 14.5      | MySQL 8                         |\n|--------------------|---------------------------------|\n| `pg_dumpall`       | `mysqldump --all-databases`     |\n| `pg_dump <dbname>` | `mysqldump --database <dbname>` |\n\nThe `pg_dumpall` tool is designed to restore an entire database instance. It includes account and ownership information. `pg_dump` just focuses on the database itself. If you are taking a snapshot production data to use in a test `pg_dump` output is easier to work with. It captures the specific database with out entangling things like the `template1` database or database user accounts and ownership.\n\nYou can restore a database dump in both Postgres and MySQL. The tooling is a little different.\n\n| Postgres 14.5                   | MySQL 8                                      |\n|---------------------------------|----------------------------------------------|\n| `dropdb <dbname>`               | `mysql -execute 'DROP DATABASE <dbname>;'`   |\n| `createdb <dbname>`             | `mysql -execute 'CREATE DATABASE <dbname>;'` |\n| `psql -f <dump_filename>`       |`mysql <dbname> < <dump_filename>`            |\n\nNOTE: These instructions work for a database dumped with `pg_dump` for the Postgres example. In principle it is the same way you can restore from `pg_dumpall` but if you Postgres instance already exists then you're going to run into various problems, e.g. errors about `template1` db.\n\nLessons learned along the way\n-----------------------------\n\n2022-08-22\n\n8:00 - 11:30; SQL; Postgres; Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is \"REPLACE\" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used \"?\" with SQLite and MySQL but with Postgres I need to use \"$1\", \"$2\", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.\n\n\n2022-08-24\n\n11:00 - 12:00; SQL; Postgres; I miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%';`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's \"SHOW TABLES\". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.\n\n2022-08-26\n\n9:30 - 10:30; SQL; Postgres; If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \\[Quickstart\\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.\n\n2022-09-19\n\n10:30 - 12:30; SQL; Postgres; Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel, 2022-08-19",
        "keywords": [
          "postgres",
          "mysql",
          "sql",
          "psql"
        ],
        "number": 2,
        "pubDate": "2022-08-19",
        "series": "SQL Reflections",
        "title": "Rosette Notes: Postgres and MySQL",
        "updated": "2022-09-19"
      },
      "url": "posts/2022/08/19/rosette-notes.json"
    },
    {
      "content": "\n# 5:45 PM, Golang: ptdk,  stngo\n\nPost: Monday, August 15, 2022, 5:45 PM\n\nThinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n",
      "data": {
        "keywords": [
          "ptdk",
          "stngo"
        ],
        "no": 4,
        "pubDate": "2022-08-15",
        "series": "Golang",
        "title": "5:45 PM, Golang: ptdk,  stngo"
      },
      "url": "posts/2022/08/15/golang-development-2022-08-15_170545.json"
    },
    {
      "content": "\nPttk and STN\n============\n\nBy R. S. Doiel, started 2022-08-15\n(updated: 2022-09-26, pdtk was renamed pttk)\n\nThis log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the \"blogit\" action that will translates the simple timesheet notation (aka STN) to a short blog post.  My \"short post\" interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.\n\n2022-08-15\n\n16:45 - 17:45; Golang; ptdk, stngo; Thinking through what a \"post\" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's \"end\" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that \"title\" is what I use for other posts. Maybe I should just use a command line option to provide a title?\n\n2022-08-14\n\n14:00 - 17:00; Golang; pdtk, stngo; Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n2022-08-12\n\n16:00 - 16:30; Golang; stngo; A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n2022-09-26\n\n6:30 - 7:30; Golang; pttk; renamed \"pandoc toolkit\" (pdtk) to \"plain text toolkit\" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.\n",
      "data": {
        "author": "R. S. Doiel",
        "byline": "R. S. Doiel, 2022-08-15",
        "pubDate": "2022-08-15",
        "title": "PTTK and STN",
        "updated": "2022-09-26"
      },
      "url": "posts/2022/08/15/golang-development.json"
    },
    {
      "content": "\n# 5:00 PM, Golang: pdtk,  stngo\n\nPost: Sunday, August 14, 2022, 5:00 PM\n\nToday I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the \"right\" place for rendering simple timesheet notation as blog posts is in the the \"blogit\" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.\n\n",
      "data": {
        "keywords": [
          "pdtk",
          "stngo"
        ],
        "no": 3,
        "pubDate": "2022-08-14",
        "series": "Golang",
        "title": "5:00 PM, Golang: pdtk,  stngo"
      },
      "url": "posts/2022/08/14/golang-development-2022-08-14_170500.json"
    },
    {
      "content": "\n# 4:30 PM, Golang: stngo\n\nPost: Friday, August 12, 2022, 4:30 PM\n\nA work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?\n\n",
      "data": {
        "keywords": [
          "stngo"
        ],
        "no": 2,
        "pubDate": "2022-08-12",
        "series": "Golang",
        "title": "4:30 PM, Golang: stngo"
      },
      "url": "posts/2022/08/12/golang-development-2022-08-12_160430.json"
    },
    {
      "content": "\n\nOPML to Markdown and back\n=========================\n\nBy R. S. Doiel 2016-05-28\n\n## Overview\n\nI wrote a Go language package to sort [OPML](http://dev.opml.org/spec2.html) outlines. \nI wrote this because my preferred [feed reader ](http://goread.io) supports manual \nsorting but not automatic alpha sorting by the _outline_ element's _text_ attribute. \n\n## Observations\n\nOut of the box the OPML 2 Spec provides attributes indicating inclusion of other OPML files,\nscripts, basic metadata (create, modified, authorship), and even directory structures.\n\n[Fargo](http://fargo.io) allows user defined attributes to be applied to the _outline_ \nelement in OPML. This could be used in support some of the \n[Scrivener](https://www.literatureandlatte.com/scrivener.php)\nfeatures I miss such as describing how to render a project to various formats such as\nrtf, pdf, ePub, web pages or even [Final Draft fdx](https://www.finaldraft.com/) files.\n\nI write allot of Markdown formatted text.  Markdown is simple to index, \nsearch and convert into useful formats. Markdown is not good at expressing more\ncomplex structures such as metadata. Website generators that use markdown often\nrequire a preamble or _front matter_ in the markdown to provide any metadata. This\nleaves your document head cluttered and less human readable.\n\nAnother approach is to include a parallel document with the metadata.  It occurred to me \nthat an OPML file could easily hold that metadata. It can even hold Markdown content.\nThe trouble with OPML is that it is not quick to edit by hand.\n\n    Is there a round trip semantic mapping between OPML and Markdown?\n\n\n## Germination of an idea\n\nEntering a web link in Fargo the link is URL encoded and saved in the _text_ attribute of the \n_outline_ element.\n\nThe source view of a web links in Fargo's _outline_ element looks like\n\n```OPML\n    <outline text=\"&gt; href=&quot;http://example.org&quot;&lt;My example.org&gt;/a&lt;\" />\n```\n\nThat _outline_ element might render in Markdown as\n\n```\n    + [My element.org](http://example.org)\n```\n\nThe steps to create the Markdown view are simple\n\n1. URL decode the _text_ attribute\n2. Convert HTML to Markdown\n\nMaking a round trip could be done by\n\n3. Convert Markdown into HTML\n4. For each _li_ element covert to an _outline_ element URL encoding the inner HTML of the _li_\n\nSo far so good. What about something more complex?\n\n\nHere's an _outline_ element example from http://hosting.opml.org/dave/spec/directory.opml \n\n```OPML\n    <outline text=\"Scripting News sites\" created=\"Sun, 16 Oct 2005 05:56:10 GMT\" type=\"link\" url=\"http://hosting.opml.org/dave/mySites.opml\"/>\n```\n\nTo me that should look like \n\n```\n    + [Scripting News Sites](http://hosting.opml.org/dave/mySites.opml)\n```\n\nWhat about the _created_ attribute? Could we render this case as an additional set of anchors using data uri?\n\nThis suggest a rule like\n\n+ if the _text_ attribute contains HTML markup\n    + URL decode into HTML\n    + Convert HTML to Markdown\n+ else render attributes as additional anchors using data URI\n\nThis might work as follows. \n\n```OPML\n    <outline text=\"Scripting News sites\" \n        created=\"Sun, 16 Oct 2005 05:56:10 GMT\" \n        type=\"link\" \n        url=\"http://hosting.opml.org/dave/mySites.opml\"/>\n```\n\nWould become \n\n```Markdown\n    + [Scripting News Sites](http://hosting.opml.org/dave/mySites.opml) [type](data:text/plain;link) [created](data:text/date;Sun, 16 Oct 2005 05:56:10 GMT)\n```\n\nIn HTML this would look like\n\n```HTML\n    <li><a href=\"http://histing.opml.org/dave/mySites.opml\">Scripting News Sites</a>\n        <a href=\"data:text/plain;link\">type</a>\n        <a href=\"data:text/date;Sun, 16 Oct 2005 05:56:10 GMT\">created</a></li>\n```\n\n### Markdown to OPML\n\nComing back to OPML from Markdown then becomes\n\n+ Convert Markdown to HTML\n+ For each _li_ element inspect anchors, \n    + if anchors contain data URI then map _outline_ element\n    + else URL encode and embed in _outline_ _text_ attribute\n\nIs this viable? Does it have any advantages?\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-05-28",
        "keywords": [
          "golang",
          "opml",
          "markdown"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "OPML to Markdown and back"
      },
      "url": "posts/2016/05/28/OPML-to-Markdown-and-back.json"
    },
    {
      "content": "\n\n# Instant Articles, Accelerated Mobile Pages, Twitter Cards and Open Graph\n\nBy R. S. Doiel 2016-05-30\n\n## The problem\n\nThe web has gotten slow. In [2016](http://httparchive.org/trends.php) the \naverage page weight is in multi-megabytes and the average number of network \nrequests needed to deliver the content is counted in \nthe hundreds. In the mix are saturated networks and a continued public \nexpectation of responsiveness (web wisdom suggests you have about 3 seconds \nbefore people give up).  The odd thing is we've known how to build fast \nwebsites for a [decade](https://www.stevesouders.com/) or so.  \nCollectively we don't build them [fast](https://www.sitepoint.com/average-page-weight-increased-another-16-2015/). \n\n\n## Meet the new abstractions\n\nCorporations believe they have the answer and they are providing us \nwith another set of abstractions. In a few years maybe these will \nget distilled down to a shared common view but in the mean time disc \ncosts remain reasonably priced and generating these new forms of \npages or feeds is a template or so away.\n\n+ [Twitter Cards](https://dev.twitter.com/cards/overview) and [Open Graph](http://ogp.me/)\n  + Exposing your content via social media, search results or embedded in pages via an aside element\n+ [Accelerated Mobile Pages](https://www.ampproject.org/) (also called AMP)\n  + A simplification in content delivery to improve web reading experience\n  + Its usefulness is it proscribes an approach to leverage what we have\n  + AMP works well with Twitter Cards, Open Graph and can leverage Web Components\n+ [Instant Articles](https://instantarticles.fb.com/)\n  + a format play to feed the walled garden of Facebook for iOS and Android devices\n\n\n## The players \n\n### Twitter Cards and Open Graph\n\nTwitter's Titter Cards and Facebook's Open Graph offer approaches to \nbuild off of our existing meta elements in an HTML page's document \nhead.  They are named space to avoid collisions but supporting both \nwill still result in some content duplication. The k-weight \ndifference in the resulting HTML pages isn't too bad. \n\nAdopting either or both is a matter of adjusting how your render your \nweb page's head block.  It is easy enough to do manually but easier \nstill using some sort of template system that renders the appropriate \nmeta elements based on the content type and contents in the page \nbeing rendered.  \n\nGoogle and other search engines can leverage this richer meta \ndata and integrate it into their results. Google's Now application can \nrender content cards based on either semantic. It also appears that \ncontent cards are being leverage selectively for an aside and related \ncontent on Google search results pages. You could even built this into \nyour own indexing process for use with the Solr or Elasticsearch.\n\nContent Cards offer intriguing opportunity for web crawlers and search \nengines.  This is particularly true when combined with mature feed \nformats like RSS, OPML, Atom and the maturing efforts in the linked \ndata community around JSON-LD.\n\n\n### AMP - Accelerated Mobile Pages\n\nThe backers of AMP (not to be confused with Apache+MySQL+PHP) are largely\npublishers including major news outlets and web media\ncompanies in the US and Europe. This is an abridged list from 2015--\n\n+ BBC\n+ Atlantic Media\n+ Vox Media\n+ Conde Nast\n+ New York Times\n+ Wall Street Journal\n+ The Daily Mail\n+ Huffington Post\n+ Gannet\n+ The Guardian\n+ The Economist\n+ The Financial Times\n\nIn additional to the publishers there is participation by tech companies\nsuch as Google, Pinterest, Twitter, LinkedIn and Wordpress.com.  Accelerated\nMobile Pages offer benefits for web crawlers and search engines supporting\nsurfacing content is clearly and enabling easier distinction from \nadvertisements. \n\n\n### Instant Articles\n\nIn additional to Open Graph Facebook has put forward [Instant Articles](https://developers.facebook.com/docs/instant-articles).\nLike AMP it is targeting content delivery for mobile. Unlike AMP Instant Articles is an\nexplicit binding into Facebook's walled garden only exposing the content on supported\nversions of iOS and Android. You don't see Instant Articles in your Facebook timeline or when  \nyou browse from a desktop web browser.  Unlike the previous\nexamples you actually need to sign up to participate in the Instant Article publishing\nprocess.  Sign up cost is having a Facebook account, being approved by Facebook and compliance\nwith their terms of service. Facebook does provide some publishing tools, publishing controls\nas well as some analytics. They do allow 3rd party ads as well as encourage access to\ntheir advertising network.  Once approved the burden on your content manage process \nappears manageable.  \n\nYou can submit Instant Articles via a modified RSS feed or directly through their API. \nIn this sense the overhead is about the same as that for implementing support for Twitter Cards\nOpen Graph, and AMP. Facebook does a good job of quickly propagating changes to your\nInstant Articles across their platform. That's nice.\n\nWhy go through the trouble? If you're a content producer and your audience lives on Facebook\nFacebook commands the attention of a lot of eye balls.  Instant Articles provides \nanother avenue to reach them.  For some Facebook effectively controls the public view of the \nweb much as America Online and Prodigy did decades ago. [Dave Winer](https://twitter.com/davewiner) \nhas written extensively on how he implemented Instant Article support along with \nsome very reasoned pros and cons for doing so. The landscape is evolving and \n[Dave's river of news](http://scripting.com) is worth following.\n\n\n## Impact on building content\n\nThese approaches require changes in your production of your HTML and RSS sent to the browser.\nTwitter Cards and Open Graph change what you put in the HEAD element of the HTML\npages.  AMP proscribes what you should put in the BODY element of the webpage.\nInstant Articles tweaks your RSS output.  Not surprisingly the major content management \nsystems Wordpress and Drupal have plugins for this.  All can be implemented via your template \nsystem or page generation process.\n\n\n## Whither adopt?\n\nBecause these approaches boil down to content assembly the adoption risk \nis low.  If your audience views Twitter, Facebook or Google search results \nthen it is probably worth doing.  All allow you to continue to publish your \nown content and own your URLs as opposed to being a tenant on one or another \nplatform. That benefits the open web.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-05-30",
        "keywords": [
          "structured data",
          "amp",
          "opengraph",
          "twitter",
          "google",
          "facebook",
          "instant pages"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Instant Articles, Accelerated Mobile Pages, Twitter Cards and Open Graph"
      },
      "url": "posts/2016/05/30/amp-cards-and-open-graph.json"
    },
    {
      "content": "\n\nHow to make a Pi-Top more Raspbian\n==================================\n\nBy R. S. Doiel, 2016-07-04\n\nI have a first generation Pi-Top.  I like the idea but found I didn't use it much due to a preference for\nbasic Raspbian. With the recent Pi-TopOS upgrades I realized getting back to basic Raspbian was relatively\nstraight forward.\n\n## The recipe\n\n1. Make sure you're running the latest Pi-TopOS based on Jessie\n2. Login into your Pi-Top normally\n3. From the Pi-Top dashboard select the \"Desktop\" icon\n4. When you see the familiar Raspbian desktop click on the following things\n\t+ Click on the Raspberry Menu (upper left corner)\n\t+ Click on Preferences\n\t+ Click on Raspberry Pi Configuration\n5. I made the following changes to my System configuration\n\t+ Under *Boot* I selected \"To CLI\"\n\t+ I unchecked *login as user \"pi\"*\n6. Restart your Pi Top\n\t+ Click on Raspberry Menu in the upper left of the desktop\n\t+ Click on shutdown\n\t+ Select *reboot*\n7. When you restart you'll see an old school console login, login as the pi user using your Pi-Top password\n8. Remove the following program use the *apt* command\n\t+ ceed-universe\n\t+ pt-dashboard\n\t+ pt-splashscreen\n\n```\n    sudo apt purge ceed-universe pt-dashboard pt-splashscreen\n```\n\nNote: pi-battery, pt-hub-controller, pt-ipc, pt-speaker are hardware drivers specific to your Pi-Top so you probably\nwant to keep them.\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-07-04",
        "keywords": [
          "Raspberry Pi",
          "Pi-Top",
          "Rasbian",
          "Raspberry Pi OS",
          ":operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "How to make a Pi-Top more Raspbian"
      },
      "url": "posts/2016/07/04/How-To-Make-A-PiTop-More-Raspbian.json"
    },
    {
      "content": "\n\n# Exploring Bash for Windows 10 Pro\n\nBy R. S. Doiel 2016-08-15\n\n    UPDATE (2016-10-27, RSD): Today trying to compile Go 1.7.3 under \n    Windows 10 Pro I've am getting compile errors when the \n    assembler is being built.  I can compile go1.4.3 but see errors \n    in some of the tests results.\n\n## Initial Setup and configuration\n\nI am running Windows 10 Pro (64bit) Anniversary edition under Virtual Box. The VM was upgraded from an earlier version of Windows 10 Pro (64bit). The VM was allocated 4G or ram, 200G disc and simulating 2 cores.  After the upgrade I took the following steps\n\n+ Search with Bing for \"Bash for Windows\" \n    + Bing returns http://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/\n+ Switch on developer mode for Windows\n+ Turned on Linux Subsystem Beta (searched for \"Turning on Features\")\n+ Reboot\n+ Search for \"Bash\" and clicked on \"Run Bash command\"\n+ Answered \"y\"\n+ Waited for download and extracted file system\n+ When prompted setup developer account with username/password\n    + Documentation can be found at https://aka.ms/wsldocs\n+ Exit root install shell\n+ Search for \"Bash\" locally\n+ Launched \"Bash on Ubuntu on Windows\"\n+ Authenticate with your username/password\n\n\n## Setting up Go under Bash for Windows 10\n\nWith Bash installed these are the steps I took to compile Go\nunder Bash on Ubuntu on Windows.\n\n```shell\n    sudo apt-get update && sudo apt-get upgrade -y\n    sudo apt-get autoremove\n    sudo apt-get install build-essential clang git-core unzip zip -y\n    export CGO_ENABLE=0\n    git clone https://github.com/golang/go go1.4\n    git clone https://github.com/golang/go go\n    cd go1.4\n    git checkout go1.4.3\n    cd src\n    ./all.bash\n    cd\n    export PATH=$PATH:$HOME/go1.4/bin\n    cd go\n    git checkout go1.7\n    cd src\n    ./all.bash\n    cd\n    export PATH=$HOME/go/bin:$HOME/bin:$PATH\n    export GOPATH=$HOME\n```\n\nNote some tests failing during compilation in both 1.4.3 and 1.7. They mostly failed\naround network sockets.  This is probably a result of the limitations in the Linux subsystem\nunder Windows.\n\nIf successful you should be able to run `go version` as well as install additional Go based software\nwith the usual `go get ...` syntax.\n\nIn your `.bashrc` or `.profile` add the following\n\n```shell\n    export PATH=$HOME/go/bin:$HOME/bin:$PATH\n    export GOPATH=$HOME\n```\n\n\n## Improved vim setup\n\nI like the vim-go packages for editing Go code in vim. They are easy to setup.\n\n```shell\n     mkdir -p ~/.vim/autoload ~/.vim/bundle \n     curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim\n     git clone https://github.com/fatih/vim-go.git ~/.vim/bundle/vim-go\n```\n\nExample $HOME/.vimrc\n\n```vimrc\n    execute pathogen#infect()\n    syntax on\n    filetype plugin on\n    set ai\n    set nu\n    set smartindent\n    set tabstop=4\n    set shiftwidth=4\n    set expandtab\n    let &background = ( &background == \"dark\"? \"light\" : \"dark\" )\n    let g:vim_markdown_folding_disabled=1\n```\n\nColor schemes are browsable at [vimcolors.com](http://vimcolors.com). They can be installed in\n$HOME/.vim/colors.\n\n1. git clone and place the colorscheme\n2. place the *.vim file holding the color scheme into $HOME/.vim/colors\n3. start vim and at the : do colorscheme NAME where NAME is the scheme you want to try\n\nYou can find the default shipped color schemes in /usr/share/vim/vimNN/colors where vimNN is the version number\ne.g. /usr/share/vim/vim74/colors.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-08-15",
        "keywords": [
          "Golang",
          "Windows",
          "Bash",
          "Linux Subsystem"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Exploring Bash for Windows 10 Pro"
      },
      "url": "posts/2016/08/15/Setting-up-Go-under-Bash-for-Windows-10.json"
    },
    {
      "content": "\n\nFrom Markdown and Bash to mkpage\n================================\n\nBy R. S. Doiel 2016-08-16\n\nWhen I started maintaining a website on GitHub a few years ago my needs\nwere so simple I hand coded the HTML.  Eventually I adopted \na markdown processor for maintaining the prose. My \"theme\" was a\nCSS file and some HTML fragments to wrap the markdown output. If I needed \ninteractivity I used JavaScript to access content via a web API. \nLife was simple, all I had to learn to get started was Git and how to\npopulate a branch called \"gh-pages\".\n\n\n## Deconstructing Content Management Systems\n\nRecently my website needs have grown. I started experimenting with static\nsite generators thinking an existing system would be the right fit. \nWhat I found were feature rich systems that varied primarily in \nimplementation language and template engine. Even though I wasn't\nrequired to run Apache, MySQL and PHP/Perl/Python/Ruby/Tomcat it felt \nlike the static site generators were racing to fill a complexity \nvacuum. In the end they were interesting to explore but far more\nthan I was willing to run. I believe modern content management systems can\nbe deconstruct into something simpler.\n\nSome of the core elements of modern content management systems are\n\n+ creation and curation of data sources (including metadata)\n+ transforming data sources if needed\n+ mapping a data source to appropriate template set\n+ rendering template sets to produce a final website\n\nModern static site generators leave creation and curation to your \ntext editor and revision control system (e.g. vi and git). \n\nMost static site generators use a simplified markup. A populate one is\ncalled [Markdown](https://en.wikipedia.org/wiki/Markdown). This \"markup\"\nis predictable enough that you can easily convert the results to HTML and\nother useful formats with tools like [pandoc](http://pandoc.org/). In most \nstatic site generators your content is curated in Markdown and when the \npages are built it is rendered to HTML for injection into your website's \ntemplate and theme.\n\nMapping the data sources to templates, combining the templates and rendering \nthe final website is where most systems introduce a large amount of complexity.\nThis is true of static site generators like [Jekill](https://jekyllrb.com) and \n[Hugo](https://gohugo.io).\n\n\n## An experimental deconstruction\n\nI wanted a simple command line tool that would make a single web page.\nIt would take a few data sources and formats and run them through a\ntemplate system. The template system needed to be simple but support\nthe case where data might not be available. It would be nice if it handled\nthe case of repetitious data like that used in tables or lists. Ideally\nI could render many pages from a single template assuming a simple website\nand layout.\n\n### A single page generator\n\n[mkpage](https://github.com/rsdoiel/mkpage) started as an experiment in\nbuilding a simple single page generator. It's responsibilities\ninclude mapping data sources to the template, transforming data if needed\nand rendering the results. After reviewing the websites I've setup in\nthe last year or two I realized I had three common types of data.\n\n1. Plain text or content that did not need further processing\n2. Markdown content (e.g. page content, navigation lists)\n3. Occasionally I include content from JSON feeds\n\nI also realized I only needed to handle three data sources.\n\n1. strings\n2. files\n3. web resources\n\nEach of these sources might provide plain text, markdown or JSON data formats.\n\nThat poses the question of how to express the data format and the data \nsource when mapping the content into a template. The web resources are\neasy in the sense that the web responses include content type information.\nFiles can be simple too as the file extension indicates their\nformat (e.g. \".md\" for Markdown, \".json\" for JSON documents). What remained\nwas how to identify a text string's format.  I opted for a prefix ending in \na colon (e.g. \"text:\" for plain text, \"markdown:\" for markdown \nand \"json:\" for JSON). This mapping allows for a simple key/value\nrelationship to be expressed easily on the command line.\n\n### mkpage in action\n\nDescribing how to build \"mypage.html\" from \"mypage.md\" and \"nav.md\" \n(containing links for navigating the website) is as easy as typing\n\n```shell\n    mkpage \"content=mypage.md\" \"navigation=nav.md\" page.tmpl > mypage.html\n```\n\nIn this example the template is called \"page.tmpl\" and we redirect the \noutput to \"mypage.html\".\n\n\nAdding a custom page title is easy too.\n\n```shell\n    mkpage \"title=text:My Page\" \\\n        \"content=mypage.md' \"navigation=nav.md\" \n        page.tmpl \\\n        > mypage.html\n```\n\nLikewise integrating some JSON data from weather.gov is relatively straight\nforward. The hardest part is discovering the [URL](http://forecast.weather.gov/MapClick.php?lat=34.0522&lon=118.2437&DFcstType=json) \nthat returns JSON!  Notice I have added a weather field and the URL. When data\nis received back from weather.gov it is JSON decoded and then passed to the\ntemplate for rendering using the \"range\" template function.\n\n```shell\n    mkpage \"title=My Page\" \\\n        \"content=mypage.md\" \\\n        \"navigation=nav.md\" \\\n        \"weather=http://forecast.weather.gov/MapClick.php?lat=34.0522&lon=118.2437&DFcstType=json\" \\\n        page.tmpl \\\n        > mypage.html\n```\n\nWhat is *mkpage* doing?\n\n1. Reading the data sources and formats from the command line\n2. Transforming the Markdown and JSON content appropriately\n3. Applying them to the template (e.g. page.tmpl)\n4. Render the results to stdout\n\nBuilding a website then is only a matter of maintaining navigation in\n*nav.md* and identifying the pages needing to be created. I can easily \nautomated that using the Unix find, grep, cut and sort. Also with find \nI can iteratively process each markdown file applying a \ntemplate and rendering the related HTML file.  This can be done for a site \nof a few pages (e.g. about, resume and cv) to more complex websites like \nblogs and repository activities.\n\nHere's an example template that would be suitable for the previous\ncommand line example. It's mostly just HTML and some curly bracket notation \nsprinkled in.\n\n```html\n    <!DOCTYPE html>\n    <html>\n    <head>\n        {{with .title}}<title>{{- . -}}</title>{{end}}\n        <link rel=\"stylesheet\" href=\"css/site.css\">\n    </head>\n    <body>\n        <nav>\n        {{ .navigation }}\n        </nav>\n        <section>\n        {{ .content }}\n        </section>\n        <aside>\n        Weather Demo<br />\n        <ul>\n        {{range .weather.data.text}}\n            <li>{{ . }}</li>\n        {{end}}\n        </ul>\n        </aside>\n\n    </body>\n    </html>\n```\n\nYou can find out more about [mkpage](https://github.com/rsdoiel/mkpage)\n[rsdoiel.github.io/mkpage](https://rsdoiel.github.io/mkpage).\n\nTo learn more about Go's text templates see \n[golang.org/pkg/text/template](https://golang.org/pkg/text/template/). \n\nIf your site generator needs are more than *mkpage* I suggest [Hugo](https://gohugo.io). \nIt's what I would probably reach for if I was building a large complex organizational\nsite or news site.\n\nIf you're looking for an innovative and rich author centric content system\nI suggest Dave Winer's [Fargo](http://fargo.io) outliner and [1999.io](https://1999.io).\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-08-16",
        "keywords": [
          "Bash",
          "Markdown",
          "site generator",
          "mkpage",
          "pandoc"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "From Markdown and Bash to mkpage"
      },
      "url": "posts/2016/08/16/From-Markdown-and-Bash-to-mkpage.json"
    },
    {
      "content": "\n\n# Android, Termux and Dev Environment\n\nBy R. S. Doiel 2016-09-20\n\nRecently I got a new Android 6 tablet. I got a case with a tiny Bluetooth keyboard. I started wondering if I could use it as a development device when on the road. So this is my diary of that test.\n\n## Challenges\n\n1. Find a way to run Bash without rooting my device\n2. See if I could use my normal web toolkit\n\t+ curl\n\t+ jq\n\t+ sed\n\t+ grep\n3. See if I could compile or add my own custom Golang programs\n4. Test setup by running a local static file server, mkpage and update my website\n\n## Searching for Android packages and tools of my toolbox\n\nAfter searching with Duck Duck Go and Google I came across the [termux](https://termux.com). Termux provides a minimal Bash shell environment with support for adding\npackages with _apt_ and _dpkg_.  The repositories visible to *termux* include\nmost of the C tool chain (e.g. clang, make, autoconf, etc) as well as my old Unix favorites _curl_, _grep_, _sed_, _gawk_ and a new addition to my toolkit _jq_.  Additionally you'll find recent versions (as of Sept. 2016) versions of _Golang_, _PHP_, _python_, and _Ruby_.\n\nThis quickly brought me through step 3.  Installing _go_, _git_, and _openssh_ completed what I needed to test static site development with some of the tools in our incubator at [Caltech Library](https://caltechlibrary.github.io).\n\n## Setting up for static site development\n\nAfter configuring _git_, adding my public key to GitHub and running _go get_ on my\ncustom static site tools I confirmed I could build and test static websites from my Android tablet using *Termux*.\n\nHere's the list of packages I installed under *Termux* to provide a suitable shell environment for writing and website constructions.\n\n```shell\n    apt install autoconf automake bash-completion bc binutils-dev bison \\\n        bzip2 clang cmake coreutils ctags curl dialog diffutils dos2unix \\\n        expect ffmpeg findutils gawk git gnutls golang grep gzip \\\n\timagemagick jq less lynx m4 make-dev man-dev nano nodejs \\\n        openssh patch php-dev python readline-dev rlwrap rsync ruby-dev \\\n        sed sensible-utils sharutils sqlite tar texinfo tree unzip vim \\\n        w3m wget zip\n```\n\nThis then allowed me to setup my *golang* environment variables and install\nmy typical custom written tools\n\n```shell\n    export PATH=$HOME/bin:$PATH\n    export GOPATH=$HOME\n    export GOBIN=$HOME/bin\n    go get github.com/rsdoiel/shelltools/...\n    go get github.com/caltechlibrary/mkpage/...\n    go get github.com/caltechlibrary/md2slides/...\n    go get github.com/caltechlibrary/ws/...\n```\n\nFinally pulled down some content to test.\n\n```shell\n    cd\n    mkdir Sites\n    git clone https://github.com/rsdoiel/rsdoiel.github.io.git Sites/rsdoiel.github.io\n    cd  Sites/rsdoiel.github.io\n    ws\n```\n\nThis started the local static site webserver and I pointed by Firefox for Android at http://localhost:8000 and saw a local copy of my personal website. From there I wrote this article and updated it just as if I was working on a Raspberry Pi or standard Linux laptop.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-09-20",
        "keywords": [
          "Bash",
          "cURL",
          "jq",
          "sed",
          "grep",
          "search",
          "golang",
          "Android"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Android, Termux and Dev Environment"
      },
      "url": "posts/2016/09/20/Android-Termux-Dev-environment.json"
    },
    {
      "content": "\n\n# Cross compiling Go 1.8.3 for Pine64 Pinebook\n\nBy R. S. Doiel 2017-06-16\n\nPine64's Pinebook has a 64-bit Quad-Core ARM Cortex A53 which is \nnot the same ARM processor found on a Raspberry Pi 3. As a \nresult it needs its own compiled version of Go. Fortunately cross \ncompiling Go is very straight forward. I found two helpful Gists\non GitHub discussing compiling Go for a 64-Bit ARM processor. \n\n+ [conoro's gist](https://gist.github.com/conoro/4fca191fad018b6e47922a21fab499ca)\n+ [truedat101's gist](https://gist.github.com/truedat101/5898604b1f7a1ec42d65a75fa6a0b802)\n\nI am using a Raspberry Pi 3, raspberrypi.local, as my cross compile \nhost. Go 1.8.3 is already compiled and available.  Inspired by the \ngists I worked up with this recipe to bring a Go 1.8.3 to my Pinebook.\n\n```shell\n    cd\n    mkdir -p gobuild\n    cd gobuild\n    git clone https://github.com/golang/go.git go1.8.3\n    cd go1.8.3\n    git checkout go1.8.3\n    export GOHOSTARCH=arm\n    export GOARCH=arm64\n    export GOOS=linux\n    cd src\n    ./bootstrap.bash\n```\n\nAfter the bootstrap compile is finished I switch to my Pinebook,\ncopy the bootstrap compiler to my Pinebook and use it to compile\na new go1.8.3 for Pine64.\n\n```shell\n    cd\n    scp -r raspberrypi.local:gobuild/*.tbz ./\n    tar jxvf go-linux-arm64-bootstrap.tbz\n    export GOROOT=go-linux-arm64-bootstrap\n    go-linux-arm64-bootstrap/bin/go version\n    unset GOROOT\n    git clone https://github.com/golang/go\n    cd go\n    git checkout go1.8.3\n    export GOROOT_BOOTSTRAP=$HOME/go-linux-arm64-bootstrap\n    cd src\n    ./all.bash\n```\n\n_all.bash_ will successfully compile _go_ and _gofmt_ but fail on \nthe tests. It's not perfect but appears to work as I explore\nbuilding Go applications on my Pinebook. Upcoming Go releases should\nprovide better support for 64 bit ARM.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2016, R. S. Doiel",
        "date": "2016-06-16",
        "keywords": [
          "Golang",
          "Pine64",
          "ARM"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Cross compiling Go 1.8.3 for Pine64 Pinebook"
      },
      "url": "posts/2017/06/16/cross-compiling-go.json"
    },
    {
      "content": "\n\n# NodeJS, NPM, Electron\n\nBy R. S. Doiel 2017-10-20\n\nElectron is an app platform leveraging web technologies. Conceptually it is a\nmashup of NodeJS and Chrome browser. [Electron](https://electron.atom.io/) site\nhas a nice starter app. It displays a window with Electron version info and\n'hello world'.\n\nBefore you can get going with _Electron_ you need to have a\nworking _NodeJS_ and _NPM_. I usually compile from source and this\nwas my old recipe (adjusted for v8.7.0).\n\n```shell\n    cd\n    git clone https://github.com/nodejs/node.git\n    cd node\n    git checkout v8.7.0\n    ./configure --prefix=$HOME\n    make && make install\n```\n\nTo install an _Electron Quick Start_ I added the additional steps.\n\n```shell\n    cd\n    git clone https://github.com/electron/electron-quick-start\n    cd electron-quick-start\n    npm install\n    npm start\n```\n\nNotice _Electron_ depends on a working _node_ and _npm_.  When I\ntried this recipe it failed on `npm install` with errors regarding\ninternal missing node modules.\n\nAfter some fiddling I confirmed my node/npm install failed because\nI had install the new version of over a partially installed previous\nversion. This causes the node_modules to be populated with various\nconflicting versions of internal modules.\n\nSorting that out allowed me to test the current version of\n*electron-quick-start* cloned on 2017-10-20 under _NodeJS_ v8.7.0.\n\n## Avoiding Setup Issues in the future\n\nThe *Makefile* for _NodeJS_ includes an 'uninstall' option. Revising\nmy _NodeJS_ install recipe above I now do the following to setup a machine\nto work with _NodeJS_ or _Electron_.\n\n```shell\n    git clone git@github.com:nodejs/node.git\n    cd node\n    ./configure --prefix=$HOME\n    make uninstall\n    make clean\n    make -j 5\n    make install\n```\n\nIf I am on a device with a multi-core CPU (most of the time) you can speed\nup the make process using a `-j CPU_CORE_COUNT_PLUS_ONE` option (e.g. `-j 5`\nfor my 4 core x86 laptop).\n\nOnce _node_ and _npm_ were working normally the instructions in the\n*electron-quick-start* worked flawlessly on my x86.\n\nI have tested the node install recipe change on my Pine64 Pinebook, on \nseveral Raspberry Pi 3s as well as my x86 Ubuntu Linux laptop.\n\nI have not gotten Electron up on my Pine64 Pinebook or Raspberry Pi's yet. \n`npm install` outputs errors suggesting that it is expecting an x86 architecture.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2017, R. S. Doiel",
        "date": "2017-10-20",
        "keywords": [
          "Javascript",
          "NodeJS",
          "Electron"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "NodeJS, NPM, Electron"
      },
      "url": "posts/2017/10/20/node-npm-electron.json"
    },
    {
      "content": "\n\n# Harvesting my Gists from GitHub\n\nBy R. S. Doiel 2017-12-10\n\nThis is a just quick set of notes on harvesting my Gists on GitHub so I\nhave an independent copy for my own website. \n\n## Assumptions\n\nIn this gist I assume you are using Bash on a POSIX system (e.g. Raspbian \non a Raspberry Pi) with the standard compliment of Unix utilities (e.g. cut, \nsed, curl). I also use Stephen Dolan's [jq](https://github.com/stedolan/jq)\nas well as Caltech Library's [datatools](https://github.com/caltechlibrary/datatools).\nSee the respective GitHub repositories for installation instructions.\nThe gist harvest process was developed against GitHub's v3 API\n(see developer.github.com). \n\nIn the following examples \"USER\" is assumed to hold your GitHub user id \n(e.g. rsdoiel for https://github.com/rsdoiel).\n\n## Getting my basic profile\n\nThis retrieves the public view of your profile.\n\n```shell\n    curl -o USER \"https://api.github.com/users/USER\"\n```\n\n## Find the urL for your gists\n\nGet the gists url from `USER.json.\n\n```shell\n    GISTS_URL=$(jq \".gists_url\" \"USER.json\" | sed -E 's/\"//g' | cut -d '{' -f 1)\n    curl -o gists.json \"${GISTS_URL}\"\n```\n\nNow `gists.json` should hold a JSON array of objects representing your Gists.\n\n## Harvesting the individual Gists.\n\nWhen you look at _gists.json_ you'll see a multi-level JSON structure.  It has been\nformatted by the API so be easy to scrape.  But since this data is JSON and Caltech Library\nhas some nice utilities for working with JSON I'll use *jsonrange* and *jq* to pull out a list\nof individual Gists URLS.\n\n```shell\n    jsonrange -i gists.json | while read I; do \n        jq \".[$I].files\" gists.json | sed -E 's/\"//g'\n    done\n```\n\nExpanding this we can now curl each individual gist metadata to find URL to the raw file.\n\n\n```shell\n    jsonrange -i gists.json | while read I; do \n        jq \".[$I].files\" gists.json | jsonrange -i - | while read FNAME; do\n            jq \".[$I].files[\\\"$FNAME\\\"].raw_url\" gists.json | sed -E 's/\"//g'; \n        done;\n    done\n```\n\nNow that we have URLs to the raw gist files we can use curl again to fetch each.\n\nWhat do we want to store with our harvested gists?  The raw files, metadata\nabout the Gist (e.g. when it was created), the Gist ID. Putting it all together\nwe have the following script.\n\n```shell\n    #!/bin/bash\n    if [[ \"$1\" = \"\" ]]; then\n        echo \"USAGE: $(basename \"$0\") GITHUB_USERNAME\"\n        exit 1\n    fi\n\n    USER=\"$1\"\n    curl -o \"$USER.json\" \"https://api.github.com/users/$USER\"\n    if [[ ! -s \"$USER.json\" ]]; then\n        echo \"Someting went wrong getting https://api.github.cm/users/${USER}\"\n        exit 1\n    fi\n\n    GISTS_URL=$(jq \".gists_url\" \"$USER.json\" | sed -E 's/\"//g' | cut -d '{' -f 1)\n    curl -o gists.json \"${GISTS_URL}\"\n    if [[ ! -s gists.json ]]; then\n        echo \"Someting went wrong getting ${GISTS_URL}\"\n        exit 1\n    fi\n\n    # For each gist harvest our file\n    jsonrange -i gists.json | while read I; do\n        GIST_ID=$(jq \".[$I].id\" gists.json | sed -E 's/\"//g')\n        mkdir -p \"gists/$GIST_ID\"\n        echo \"Saving gists/$GIST_ID/metadata.json\"\n        jq \".[$I]\" gists.json > \"gists/$GIST_ID/metadata.json\"\n        jq \".[$I].files\" gists.json | jsonrange -i - | while read FNAME; do\n            URL=$(jq \".[$I].files[\\\"$FNAME\\\"].raw_url\" gists.json | sed -E 's/\"//g')\n            echo \"Saving gist/$GIST_ID/$FNAME\"\n            curl -o \"gists/$GIST_ID/$FNAME\" \"$URL\"\n        done;\n    done\n```\n\n\n\n\n\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2017, R. S. Doiel",
        "date": "2017-12-10",
        "keywords": [
          "GitHub",
          "Gists",
          "JSON"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Harvesting my Gists from GitHub"
      },
      "url": "posts/2017/12/10/harvesting-my-gists-from-github.json"
    },
    {
      "content": "\n\n# Raspbian Stretch on DELL E4310 Laptop\n\nby R. S. Doiel 2017-12-18\n\nToday I bought a used Dell E4310 laptop. The E4310 is an \"old model\" now\nbut certainly not vintage yet.  It has a nice keyboard and reasonable \nscreen size and resolution. I bought it as a writing machine. I mostly\nwrite in Markdown or Fountain depending on what I am writing these days.\n\n## Getting the laptop setup\n\nThe machine came with a minimal bootable Windows 7 CD and an blank \ninternal drive. Windows 7 installed fine but was missing the network \ndrivers for WiFi.  I had previously copied the new [Raspbian Stretch](https://www.raspberrypi.org/blog/raspbian-stretch/) ISO to a USB drive. While\nthe E4310 didn't support booting from the USB drive Windows 7 does make\nit easy to write to a DVRW. After digging around and finding a blank disc\nI could write to it was a couple of mouse clicks and a bit of waiting \nand I had new bootable Raspbian Stretch CD.\n\nBooting from the Raspbian Stretch CD worked like a charm. I selected \nthe graphical install which worked well though initially the trackpad \nwasn't visible so I just used keyboard navigation to setup the install.\nAfter the installation was complete and I rebooted without the install\ndisc everything worked except the internal WiFi adapter.\n\nI had several WiFi dongles that I use with my Raspberry Pis so I \nborrowed one and with that was able to run the usual `sudo apt update \n&& sudo apt upgrade`.\n\nWhile waiting for the updates I did a little web searching and found \nwhat I needed to know on the Debian Wiki (see\nhttps://wiki.debian.org/iwlwifi?action=show&redirect=iwlagn).  Following\nthe instructions for *Debian 9 \"Stretch\"* ---\n\n```shell\n    sudo vi /etc/apt/sources.list.d/non-free.list \n    # adding the deb source line from the web page\n    sudo apt update && sudo apt install fireware-iwlwifi\n    sudo modprobe -r iwlwifi; sudo modprobe iwlwifi\n    sudo shutdown -r now\n```\n\nAfter that everything came up fine.\n\n## First Impressions\n\nFirst, I like Raspbian Pixel. It was fun on my Pi but on an Intel box\nwith 4Gig RAM it is wicked fast.  Pixel is currently my favorite flavor \nof Debian GNU/Linux. It is simple, minimal with a consistent UI for \nan X based system. Quite lovely. \n\nIf you've got an old laptop you'd like to breath some life into \nRaspbian Stretch is the way to go.\n\n\n### steps for my install process\n\n+ Booted from a minimal Windows 7 CD to get a basic OS minus networking\n+ Used Windows 7 and the internal DVD-RW to create a Raspbian Stretch CD\n+ Booted from the Raspbian Stretch CD and installed Raspbian replacing Windows 7\n+ Used a spare WiFi dongle initially to fetch the non-free iwlwifi modules\n+ Updated my source list, re-run apt update and upgrade\n+ Rebooted and everything came up and is running\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2017, R. S. Doiel",
        "date": "2017-12-18",
        "keywords": [
          "Raspbian",
          "Raspberry Pi OS",
          "amd64",
          "i386",
          "operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Raspbian Stretch on DELL E4310 Laptop"
      },
      "url": "posts/2017/12/18/raspbian-stretch-on-amd64.json"
    },
    {
      "content": "\n\n# Go, Bleve and Library oriented software\n\nBy R. S. Doiel, 2018-02-19\n(updated: 2018-02-22)\n\nIn 2016, Stephen Davison, asked me, \"Why use Go and Blevesearch for\nour library projects?\" After our conversation I wrote up some notes so\nI would remember. It is now 2018 and I am revising these notes. I\nthink our choice paid off.  What follows is the current state of my\nreflection on the background, rational, concerns, and risk mitigation\nstrategies so far for using [Go](https://golang.org) and\n[Blevesearch](https://blevesearch.com) for Caltech Library projects.\n\n## Background\n\nI first came across Go a few years back when it was announced as an\nOpen Source project by Google at an Google I/O event (2012). The\noriginal Go authors were Robert Griesemer, Rob Pike, and Ken\nThompson. What I remember from that presentation was Go was a rather\nconsistent language with the features you need but little else.  Go\ndeveloped at Google as a response to high development costs for C/C++\nand Java in addition to challenges with performance and slow\ncompilation times.  As a language I would put Go between C/C++ and\nJava. It comes the ease of writing and reading you find in languages\nlike Python. Syntax is firmly in the C/C++ family but heavily\nsimplified. Like Java it provides many modern features including rich basic\ndata structures and garbage collection. It has a very complete standard\nlibrary and provides very good tooling.  This makes it easy to\ngenerate code level documentation, format code, test, efficiently profile, \nand debug.\n\nOften programming languages develop around a specific set of needs.\nThis is true for Go. Given the Google origin it should not be\nsurprising to find that Go's primary strengths are working with \nstructured data, I/O and concurrency. The rich standard\nlibrary is organized around a package concept. These include packages\nsupporting network protocols, file and socket I/O as well as various\nencoding and compression scheme. It has particularly strong support\nfor XML, JSON, CSV formatted data out of the box. It has a template\nlibrary for working with plain text formats as well as generating safe\nHTML. You can browse Go's standard library https://golang.org/pkg/.\n\nAn additional feature is Go's consistency. Go code that compiles under\nversion 1.0 still compiles under 1.10. Even before 1.0 code changes\nthat were breaking came with tooling to automatically updates existing\ncode.  Running code is a strong element of Go's evolution.\n\nGo is unsurprising and has even been called boring.  This turns out to\nbe a strength when building sustainable projects in a small team.\n\n\n## Why do I write Go?\n\nFor me Go is a good way to write web services, assemble websites,\ncreate search appliances and write command line (cli) utilities. When\na shell script becomes unwieldy Go is often what I turn to.  Go is\nwell suited to building tools as well as systems.  Go based command\nline tools are very easy to orchestrate with shell and Python.\n\nGo runs on all the platforms I actively use - Windows, Mac OS X, Linux\non both Intel and ARM (e.g. Raspberry Pi, Pine64). It has experimental\nsupport for Android and iOS.  I've used a tool called\n[GopherJS](http://gopherjs.org) to write web browser applications that\ntransform my command line tools into web tools with a friendlier user\ninterface (see our [BibTeX Tools](https://caltechlibrary.github.io/bibtex/webapp/)).\n\nGo supports cross compiling out of the box. This means a production\nsystem running on AWS, Google's compute engine or Microsoft's Azure\ncan be compiled from Windows, Mac OS or even a Raspberry Pi!\nDeployment is a matter of copying the (self contained) compiled binary\nonto the production system. This contrasts with other\nplatforms like Perl, PHP, Python, NodeJS and Ruby where you need to\ninstall not only your application code but all dependencies. While\ninterpretive languages retain an advantage of having a REPL, Go\nbased programs have advantages of fast compile times and easy deployment.\n\nIn many of the projects I've written in Go I've only required a few\n(if any) 3rd party libraries (packages in Go's nomenclature). This is\nquite a bit different from my experience with Perl, PHP, Python,\nNodeJS and Ruby. This is in large part a legacy of having grown up at\nGoogle before become an open source project. While the Go standard\npackages are very good there is a rich ecosystem for 3rd party\npackages for specialized needs. I've found I tend to rely only on a\nfew of them. The one I've used the most is\n[Bleve](http://blevesearch.com).\n\nBleve is a Go package for building search engines. When I originally\ncame across Bleve (around 2014), it was described as \"Lucene lite\". \n\"Lucene lite\" was an apt description, but I find it easier\nto use than Lucene. When I first used Bleve I embedded its\nfunctionality into the tools I used to process data and present web\nservices. It did not have much in the way of stand alone command line\ntooling.  Today I increasingly think of Bleve as \"Elastic Search\nlite\". It ships with a set of command line tools that include support\nfor building Bleve's indexes.  My current practice is to only embed the search\nportion of the packages. I can use the Bleve command line for the\nrest.  In 2018, Bleve is being actively developed, has a small vibrant\ncommunity and is used by [Couchbase](https://couchbase.com), a well\nestablished NoSQL player.\n\n\n## Who is using Go?\n\nMany companies use Go. The short list includes\nGoogle, Amazon, Netflix, Dropbox, Box, eBay, Pearsons and even\nWalmart and Microsoft. This came to my attention at developer conferences\nback in 2014.  People from many of these companies started\npresenting at conferences on pilot projects that had been successful\nand moved to production. Part of what drove adoption was the ease\nof development in Go along with good system performance. I also think\nthere was a growing disenchantment with alternatives like C++, C sharp\nand Java as well as the weight of the LAMP, Tomcat, and OpenStack.\n\nHighly visible Go based projects include\n\n+ [Docker](http://docker.org) and [Rocket](http://www.docker.com) - Containerization for running process in the cloud\n+ [Kubernettes](http://kubernetes.io/) and [Terraform](https://www.terraform.io/) - Container orchestration systems\n+ [Hugo](http://hugo.io) - the fast/popular static website generator, an alternative to Jekyll, for those who want speed\n+ [Caddy](https://caddyserver.com/) - a Go based web server trying to unseat Apache/NGinX focusing on easy of use plus speed\n+ [IPFS](http://ipfs.io) - a cutting edge distributed storage system based on block chains\n\n\n### Who is using Blevesearch?\n\nHere's some larger projects using Bleve.\n\n+ [Couchbase](http://www.couchbase.com), a NoSQL database platform are replacing Lucene with Bleve.  Currently the creator of Bleve works for them.\n+ [Hugo](http://hugo.io) can integrate with Bleve for search and index generation\n+ [Caddy](https://caddyserver.com/) integrates with Bleve to provide an embedded search capability\n\n\n## Managing risks\n\nIn 2014 Go was moving from bleeding to leading edge. Serious capital\nwas behind its adoption and it stopped being an exotic conference\nitem. In 2014 Bleve was definitely bleeding edge. By late 2015 and early\n2016 the program level API stabilized. People were piloting projects\nwith it. This included our small group at Caltech Library. In 2015\nnon-English language support appeared followed by a growing list\nof non-European languages in 2016. By mid 2016 we started to see \nmissing features like alternative sorting added. While Bleve isn't\nyet 1.0 (Feb. 2018) it is reliable. The primary challenge for the Bleve\nproject is documentation targeting the novice and non-Programmer users.\nBleve has proven effective as an indexing and search platform for \narchival, library, and data repository content.\n\nAdopting new software comes with risk. We have mitigated this in two ways.\n\n1. Identify alternative technology (a plan B)\n2. Architect our systems for easy decomposition and re-composition\n\nIn the case of Go, packages can be compiled to a C-Shared\nlibrary. This allows us to share working Go packages with languages\nlike Python, R, and PHP. We have included shared Go/Python modules\non our current road map for projects.\n\nFor Blevesearch the two alternatives are Solr and Elastic\nSearch. Both are well known, documented, and solid.  The costs would be\nrecommitting to a Java stack and its resource requirements. We have\nalready identified what we want to index and that could be converted\nto either platform if needed.  If we stick with Go but dropped \nBlevesearch we would swap out the Bleve specific code for Go packages \nsupporting Solr and Elastic Search.\n\n\nThe greatest risk in adopting Go for library and archive projects was \nknowledge transfer. We addressed this \nby knowledge sharing and insuring the Go codebase can \nbe used via command line programs.  Additionally \nwe are adding support for Go based Python modules.\nTraining also is available in the form of books, websites and\nonline courses ([lynda.com](https://www.lynda.com/Go-tutorials/Up-Running-Go/412378-2.html) offers a \"Up Running Go\" course).\n\n\n## What are the benefits?\n\nFor library and archives software we have found Go's benefits include\nimproved back end systems performance at a lower cost, ease of development, \nease of deployment, a rich standard library focused on the types of things \nneeded in library and archival software.  Go plays nice with\nother systems (e.g. I create an API based service in Go that can easily\nbe consumed by a web browser running JavaScript or Perl/PHP/Python\ncode running under LAMP). In the library and archives setting Go \ncan become a high performance duck tape. We get the performance and \nreliability of C/Java type systems with code simplicity \nsimilar to Python.\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-02-19",
        "keywords": [
          "Golang",
          "Bleve",
          "search"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Go, Bleve and Library oriented software"
      },
      "url": "posts/2018/02/19/go-bleve-and-libraries.json"
    },
    {
      "content": "\n\n# Go based Python modules\n\nBy R. S. Doiel, 2018-02-24\n\nThe problem: I have written a number of Go packages at work.\nMy colleagues know Python and I'd like them to be able to use the\npackages without resorting to system calls from Python to the\ncommand line implementations. The solution is create a C-Shared\nlibrary from my Go packages, using Go's _C_ package and combine it\nwith Python's _ctypes_ package.  What follows is a series of \nsimple recipes I used to understand the details of how that worked.\n\n\n## Example 1, libtwice.go and twice.py\n\nMany of the the examples I've come across on the web start by \nshowing how to run a simple math operation on the Go side with\nnumeric values traveling round trip via the C shared library layer. \nIt is a good place to start as you only need to consider type \nconversion between both Python's runtime and Go's runtime.  It \nprovides a simple illustration of how the Go *C* package, Python's\n*ctypes* module and the toolchain work together.\n\nIn this example we have a function in Go called \"twice\" it takes\na single integer, doubles it and returns the new value.  On\nthe Go side we create a _libtwice.go_ file with an empty `main()` \nfunction.  Notice that we also import the *C* package and use \na comment decoration to indicate the function we are exporting\n(see https://github.com/golang/go/wiki/cgo and \nhttps://golang.org/cmd/cgo/\nfor full story about Go's _C_ package and _cgo_).\nPart of the what _cgo_ and the *C* package does is use the \ncomment decoration to build the signatures for the function calls\nin the shared C library.  The Go toolchain does all the heavy \nlifting in making a *C* shared library based on comment \ndirectives like \"//export\". We don't need much for our twice\nfunction.\n\n```Go\n    package main\n    \n    import (\n    \t\"C\"\n    )\n    \n    //export twice\n    func twice(i int) int {\n    \treturn i * 2\n    }\n    \n    func main() {}\n```\n\nOn the python side we need to wrap our calls to our shared library\nbringing them into the Python runtime in a useful and idiomatically\nPython way. Python provides a few ways of doing this. In my examples\nI am using the *ctypes* package.  _twice.py_ looks like this--\n\n```python\n    import ctypes\n    import os\n    \n    # Set our shared library's name\n    lib_name='libtwice'\n    \n    # Figure out shared library extension\n    uname = os.uname().sysname\n    ext = '.so'\n    if uname == 'Darwin':\n        ext = '.dylib'\n    if uname == 'Windows':\n        ext = '.dll'\n    \n    # Find our shared library and load it\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n    \n    # Setup our Go functions to be nicely wrapped\n    go_twice = lib.twice\n    go_twice.argtypes = [ctypes.c_int]\n    go_twice.restype = ctypes.c_int\n    \n    # Now write our Python idiomatic function\n    def twice(i):\n        return go_twice(ctypes.c_int(i))\n    \n    # We run this test code if with: python3 twice.py\n    if __name__ == '__main__':\n        print(\"Twice of 2 is\", twice(2))\n```\n\nNotice the amount of lifting Python's *ctypes* does for us. It provides\nfor converting C based types to their Python counter parts. Indeed the\nadditional Python source here is focused around using that functionality\nto create a simple Python function called twice. This pattern of \nbringing in a low level version of our desired function and then \npresenting in a Pythonic one is common in more complex C based Python\nmodules.  In general we need *ctypes* to access and wrapping our \nshared library. The *os* module is used so we can find our C \nshared library based on the naming conventions of our host OS. \nFor simplicity I've kept the shared library (e.g. _libtwice.so_ \nunder Linux) in the same directory as the python module \ncode _twice.py_.\n\nThe build command for Linux looks like---\n\n```shell\n    go build -buildmode=c-shared -o libtwice.so libtwice.go\n```\n\nUnder Windows it would look like---\n\n```shell\n    go build -buildmode=c-shared -o libtwice.dll libtwice.go\n```\n\nand Mac OS X---\n\n```shell\n    go build -buildmode=c-shared -o libtwice.dynlib libtwice.go\n```\n\nYou can test the Python module with---\n\n```shell\n    python3 twice.py\n```\n\nNotice the filename choices. I could have called the Go shared\nlibrary anything as long as it wasn't called `twice.so`, `twice.dll`\nor `twice.dylib`. This constraint is to avoid a module name collision\nin Python.  If we had a Python script named `twice_test.py` and \nimport `twice.py` then Python needs to make a distinction between\n`twice.py` and our shared library. If you use a Python package\napproach to wrapping the shared library you would have other options\nfor voiding name collision.\n\nHere is an example of `twice_test.py` to make sure out import is\nworking.\n\n```python\n    import twice\n    print(\"Twice 3\", twice.twice(3))\n```\n\nExample 1 is our base recipe. The next examples focus on handling\nother data types but follow the same pattern.\n\n\n## Example 2, libsayhi.go and sayhi.py\n\nI found working with strings a little more nuanced. Go's concept of\nstrings are oriented to utf-8. Python has its own concept of strings \nand encoding.  Both need to pass through the C layer which assumes \nstrings are a char pointer pointing at contiguous memory ending \nin a null. The *sayhi* recipe is focused on moving a string from \nPython, to C, to Go (a one way trip this time). The example uses \nGo's *fmt* package to display the string. \n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export say_hi\n    func say_hi(msg *C.char) {\n    \tfmt.Println(C.GoString(msg))\n    }\n    \n    func main() { }\n```\n\nThe Go source is similar to our first recipe but our Python modules\nneeds to use *ctypes* to get you Python string into shape to be\nunpacked by Go.\n\n```python\n   import ctypes\n   import os\n   \n   # Set the name of our shared library\n   lib_name = 'libsayhi'\n\n   # Figure out shared library extension\n   uname = os.uname().sysname\n   ext = '.so'\n   if uname == 'Darwin':\n       ext = '.dylib'\n   if uname == 'Windows':\n       ext = '.dll'\n   \n   # Find our shared library and load it\n   dir_path = os.path.dirname(os.path.realpath(__file__))\n   lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n   \n   # Setup our Go functions to be nicely wrapped\n   go_say_hi = lib.say_hi\n   go_say_hi.argtypes = [ctypes.c_char_p]\n   # NOTE: we don't have a return type defined here, the message is \n   # displayed from Go\n   \n   # Now write our Python idiomatic function\n   def say_hi(txt):\n       return go_say_hi(ctypes.c_char_p(txt.encode('utf8')))\n   \n   if __name__ == '__main__':\n       say_hi('Hello!')\n```\n\nPutting things together (if you are using Windows or Mac OS X\nyou'll adjust name output name, `libsayhi.so`, to match the\nfilename extension suitable for your operating system).\n\n```bash\n    go build -buildmode=c-shared -o libsayhi.so libsayhi.go\n```\n\nand testing.\n\n```bash\n    python3 sayhi.py\n```\n\n\n## Example 3, libhelloworld.go and helloworld.py\n\nIn this example we send a Python string to Go (which expects utf-8)\nbuild our \"hello world\" message and then send it back to Python\n(which needs to do additional conversion and decoding).\n\nLike in previous examples the Go side remains very simple. The heavy\nlifting is done by the *C* package and the comment `//export`. We\nare using `C.GoString()` and `C.CString()` to flip between our native\nGo and C datatypes.\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export helloworld\n    func helloworld(name *C.char) *C.char {\n    \ttxt := fmt.Sprintf(\"Hello %s\", C.GoString(name))\n    \treturn C.CString(txt)\n    }\n    \n    func main() { }\n```\n\nIn the python code below the conversion process is much more detailed.\nPython isn't explicitly utf-8 like Go. Plus we're sending our Python \nstring via C's char arrays (or pointer to chars). Finally when we \ncomeback from Go via C we have to put things back in order for Python. \nOf particular note is checking how the byte arrays work then \nencoding/decoding everything as needed. We also explicitly set the result \ntype from our Go version of the helloworld function.\n\n```python\n    import ctypes\n    import os\n    \n    # Set the name of our shared library\n    lib_name = 'libhelloworld'\n\n    # Figure out shared library extension\n    uname = os.uname().sysname\n    ext = '.so'\n    if uname == 'Darwin':\n        ext = '.dylib'\n    if uname == 'Windows':\n        ext = '.dll'\n    \n    # Find our shared library and load it\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n    \n    # Setup our Go functions to be nicely wrapped\n    go_helloworld = lib.helloworld\n    go_helloworld.argtypes = [ctypes.c_char_p]\n    go_helloworld.restype = ctypes.c_char_p\n    \n    # Now write our Python idiomatic function\n    def helloworld(txt):\n        value = go_helloworld(ctypes.c_char_p(txt.encode('utf8')))\n        if not isinstance(value, bytes):\n            value = value.encode('utf-8')\n        return value.decode()\n    \n    \n    if __name__ == '__main__':\n        import sys\n        if len(sys.argv) > 1:\n            print(helloworld(sys.argv[1]))\n        else:\n            print(helloworld('World'))\n```\n\nThe build recipe remains the same as the two previous examples.\n\n```bash\n    go build -buildmode=c-shared -o libhelloworld.so libhelloworld.go\n```\n\nHere are two variations to test.\n\n```bash\n     python3 helloworld.py\n     python3 helloworld.py Jane\n```\n\n\n## Example 4, libjsonpretty.go and jsonpretty.py\n\nIn this example we send JSON encode text to the Go package,\nunpack it in Go's runtime and repack it using the `MarshalIndent()`\nfunction in Go's JSON package before sending it back as Python\nin string form.  You'll see the same encode/decode patterns as \nin our *helloworld* example.\n\nGo code\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"encoding/json\"\n    \t\"fmt\"\n    \t\"log\"\n    )\n    \n    //export jsonpretty\n    func jsonpretty(rawSrc *C.char) *C.char {\n    \tdata := new(map[string]interface{})\n    \terr := json.Unmarshal([]byte(C.GoString(rawSrc)), &data)\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \tsrc, err := json.MarshalIndent(data, \"\", \"    \")\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \ttxt := fmt.Sprintf(\"%s\", src)\n    \treturn C.CString(txt)\n    }\n    \n    func main() {}\n```\n\nPython code\n\n```python\n    import ctypes\n    import os\n    import json\n    \n    # Set the name of our shared library\n    lib_name = 'libjsonpretty'\n\n    # Figure out shared library extension\n    uname = os.uname().sysname\n    ext = '.so'\n    if uname == 'Darwin':\n        ext = '.dylib'\n    if uname == 'Windows':\n        ext = '.dll'\n\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))\n    \n    go_jsonpretty = lib.jsonpretty\n    go_jsonpretty.argtypes = [ctypes.c_char_p]\n    go_jsonpretty.restype = ctypes.c_char_p\n    \n    def jsonpretty(txt):\n        value = go_jsonpretty(ctypes.c_char_p(txt.encode('utf8')))\n        if not isinstance(value, bytes):\n            value = value.encode('utf-8')\n        return value.decode()\n    \n    if __name__ == '__main__':\n        src = '''\n    {\"name\":\"fred\",\"age\":25,\"height\":75,\"units\":\"inch\",\"weight\":\"239\"}\n    '''\n        value = jsonpretty(src)\n        print(\"Pretty print\")\n        print(value)\n        print(\"Decode into dict\")\n        o = json.loads(value)\n        print(o)\n```\n\nBuild command\n\n```shell\n    go build -buildmode=c-shared -o libjsonpretty.so libjsonpretty.go\n```\n\nAs before you can run your tests with `python3 jsonpretty.py`.\n\nIn closing I would like to note that to use these examples you Python3\nwill need to be able to find the module and shared library. For \nsimplicity I've put all the code in the same directory. If your Python\ncode is spread across multiple directories you'll need to make some \nadjustments.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-02-24",
        "keywords": [
          "Golang",
          "Python",
          "shared libraries"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Go based Python modules"
      },
      "url": "posts/2018/02/24/go-based-python-modules.json"
    },
    {
      "content": "\n\n# Accessing Go from Julia\n\nBy R. S. Doiel, 2018-03-11\n\nThe problem: I've started exploring Julia and I would like to leverage existing\ncode I've written in Go. Essentially this is a revisit to the problem in my\nlast post [Go based Python Modules](https://rsdoiel.github.io/blog/2018/02/24/go-based-python-modules.html) \nbut with the language pairing of Go and Julia.\n\n\n## Example 1, libtwice.go, libtwice.jl and libtwice_test.jl\n\nIn out first example we send an integer value from\nJulia to Go and back via a C shared library (written in Go). While Julia doesn't\nrequire type declarations I will be using those for clarity. Like in my previous post\nI think this implementation this is a good starting point to see how Julia interacts with\nC shared libraries. Like before I will present our Go code, an explanation \nfollowed by the Julia code and commentary.\n\nOn the Go side we create a _libtwice.go_ file with an empty `main()` \nfunction.  Notice that we also import the *C* package and use \na comment decoration to indicate the function we are exporting\n(see https://github.com/golang/go/wiki/cgo and \nhttps://golang.org/cmd/cgo/\nfor full story about Go's _C_ package and _cgo_).\nPart of the what _cgo_ and the *C* package does is use the \ncomment decoration to build the signatures for the function calls\nin the shared C library.  The Go toolchain does all the heavy \nlifting in making a *C* shared library based on comment \ndirectives like \"//export\". We don't need much for our twice\nfunction.\n\n```Go\n    package main\n    \n    import (\n    \t\"C\"\n    )\n    \n    //export twice\n    func twice(i int) int {\n    \treturn i * 2\n    }\n    \n    func main() {}\n```\n\nLike in our previous Python implementation we need to build the C shared\nlibrary before using it from Julia. Here are some example Go build commands\nfor Linux, Windows and Mac OS X. You only need to run the one that applies\nto your operating system.\n\n```shell\n    go build -buildmode=c-shared -o libtwice.so libtwice.go\n    go build -buildmode=c-shared -o libtwice.dll libtwice.go\n    go build -buildmode=c-shared -o libtwice.dynlib libtwice.go\n```\n\nUnlike the Python implementation our Julia code will be split into two files. _libtwice.jl_ will\nhold our module definition and _libtwice_test.jl_ will hold our test code. In the\ncase of _libtwice.jl_ we will access the C exported function via a function named *ccall*. \nJulia doesn't require a separate module to be imported in order to access a C shared library.\nThat makes our module much simpler. We still need to be mindful of type conversion.  Both \nGo and Julia provide for rich data types and structs.  But between Go and Julia we have C \nand C's basic type system.  On the Julia side *ccall* and Julia's type system help us\nmanaging C's limitations.\n\nHere's the Julia module we'll call _libtwice.jl_.\n\n```Julia\n    module libtwice\n            \n    # We write our Julia idiomatic function\n    function twice(i::Integer)\n        ccall((:twice, \"./libtwice\"), Int32, (Int32,), i)\n    end\n\n    end\n```\n\nWe're will put the test code in a file named _libtwice\\_test.jl_. Since this isn't\nan establish \"Package\" in Julia we will use Julia's *include* statement to get bring the\ncode in then use an *import* statement to bring the module into our current name space.\n\n```Julia\n    include(\"libtwice.jl\")\n    import libtwice\n    # We run this test code for libtwice.jl\n    println(\"Twice of 2 is \", libtwice.twice(2))\n```\n\nOur test code can be run with\n\n```shell\n    julia libtwice_test.jl\n```\n\nNotice the amount of lifting that Julia's *ccall* does. The Julia code is much more compact\nas a result of not having to map values in a variable declaration. We still have the challenges \nthat Julia and Go both support richer types than C. In a practical case we should consider \nthe over head of running to two runtimes (Go's and Julia's) as well as whether or not \nimplementing as a shared library even makes sense. But if you want to leverage existing \nGo based code this approach can be useful.\n\nExample 1 is our base recipe. The next examples focus on handling\nother data types but follow the same pattern.\n\n\n## Example 2, libsayhi.go, libsayhi.jl and libsayhi_test.jl\n\nLike Python, passing strings passing to or from Julia and Go is nuanced. Go is expecting \nUTF-8 strings. Julia also supports UTF-8 but C still looks at strings as a pointer to an\naddress space that ends in a null value. Fortunately in Julia the *ccall* function combined with\nJulia's rich type system gives us straight forward ways to map those value. \nGo code remains unchanged from our Python example in the previous post. \nIn this example we use Go's *fmt* package to display the string. In the next example\nwe will round trip our string message.\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export say_hi\n    func say_hi(msg *C.char) {\n    \tfmt.Println(C.GoString(msg))\n    }\n    \n    func main() { }\n```\n\nThe Go source is the similar to our first recipe. No change from our\nprevious posts' Python example. It will need to be compiled to create our\nC shared library just as before. Run the go build line that applies to\nyour operating system (i.e., Linux, Windows and Mac OS X).\n\n```shell\n    go build -buildmode=c-shared -o libsayhi.so libsayhi.go\n    go build -buildmode=c-shared -o libsayhi.dll libsayhi.go\n    go build -buildmode=c-shared -o libsayhi.dylib libsayhi.go\n```\n\nOur Julia module looks like this.\n\n```julia\n    module libsayhi\n\n    # Now write our Julia idiomatic function using *ccall* to access the shared library\n    function say_hi(txt::AbstractString)\n        ccall((:say_hi, \"./libsayhi\"), Int32, (Cstring,), txt)\n    end\n\n    end\n```\n\nThis code is much more compact than our Python implementation.\n\nOur test code looks like\n\n```julia\n    include(\"./libsayhi.jl\")\n    import libsayhi\n    libsayhi.say_hi(\"Hello again!\")\n```\n\nWe run our tests with\n\n```shell\n    julia libsayhi_test.jl\n```\n\n\n## Example 3, libhelloworld.go and librhelloworld.cl and libhelloworld_test.jl\n\nIn this example we send a string round trip between Julia and Go. \nMost of the boiler plate we say in Python is gone due to Julia's type system. In\naddition to using Julia's *ccall* we'll add a *convert* and *bytestring* function calls\nto bring our __Cstring__ back to a __UTF8String__ in Julia.\n\nThe Go implementation remains unchanged from our previous Go/Python implementation. \nThe heavy lifting is done by the *C* package and the comment \n`//export`. We are using `C.GoString()` and `C.CString()` to flip between \nour native\nGo and C datatypes.\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"fmt\"\n    )\n    \n    //export helloworld\n    func helloworld(name *C.char) *C.char {\n    \ttxt := fmt.Sprintf(\"Hello %s\", C.GoString(name))\n    \treturn C.CString(txt)\n    }\n    \n    func main() { }\n```\n\nAs always we must build our C shared library from the Go code. Below is\nthe go build commands for Linux, Windows and Mac OS X. Pick the line that\napplies to your operating system to build the C shared library.\n\n```shell\n    go build -buildmode=c-shared -o libhelloworld.so libhelloworld.go\n    go build -buildmode=c-shared -o libhelloworld.dll libhelloworld.go\n    go build -buildmode=c-shared -o libhelloworld.dylib libhelloworld.go\n```\n\nIn our Julia, _libhelloworld.jl_, the heavy lifting of type conversion\nhappens in Julia's type system and in the *ccall* function call. Additionally we need\nto handle the conversion from __Cstring__ Julian type to __UTF8String__ explicitly\nin our return value via a functions named *convert* and *bytestring*.\n\n```julia\n    module libhelloworld\n\n    # Now write our Julia idiomatic function\n    function helloworld(txt::AbstractString)\n        value = ccall((:helloworld, \"./libhelloworld\"), Cstring, (Cstring,), txt)\n        convert(UTF8String, bytestring(value))\n    end\n\n    end\n```\n\nOur test code looks similar to our Python test implementation.\n\n```julia\n    include(\"libhelloworld.jl\")\n    import libhelloworld\n \n    if length(ARGS) > 0\n        println(libhelloworld.helloworld(join(ARGS, \" \")))\n    else\n        println(libhelloworld.helloworld(\"World\"))\n    end\n```\n\nAs before we see the Julia code is much more compact than Python's.\n\n\n## Example 4, libjsonpretty.go, libjsonpretty.jl and libjsonpretty_test.jl\n\nIn this example we send JSON encode text to the Go package,\nunpack it in Go's runtime and repack it using the `MarshalIndent()`\nfunction in Go's JSON package before sending it back to Julia\nin C string form.  You'll see the same encode/decode patterns as \nin our *libhelloworld* example.\n\nGo code\n\n```go\n    package main\n    \n    import (\n    \t\"C\"\n    \t\"encoding/json\"\n    \t\"fmt\"\n    \t\"log\"\n    )\n    \n    //export jsonpretty\n    func jsonpretty(rawSrc *C.char) *C.char {\n    \tdata := new(map[string]interface{})\n    \terr := json.Unmarshal([]byte(C.GoString(rawSrc)), &data)\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \tsrc, err := json.MarshalIndent(data, \"\", \"    \")\n    \tif err != nil {\n    \t\tlog.Printf(\"%s\", err)\n    \t\treturn C.CString(\"\")\n    \t}\n    \ttxt := fmt.Sprintf(\"%s\", src)\n    \treturn C.CString(txt)\n    }\n    \n    func main() {}\n```\n\nBuild commands for Linux, Windows and Mac OS X are as before, pick the one that matches\nyour operating system.\n\n```shell\n    go build -buildmode=c-shared -o libjsonpretty.so libjsonpretty.go\n    go build -buildmode=c-shared -o libjsonpretty.dll libjsonpretty.go\n    go build -buildmode=c-shared -o libjsonpretty.dylib libjsonpretty.go\n```\n\nOur Julia module code\n\n```Julia\n    module libjsonpretty\n\n    # Now write our Julia idiomatic function\n    function jsonpretty(txt::AbstractString)\n        value = ccall((:jsonpretty, \"./libjsonpretty\"), Cstring, (Cstring,), txt)\n        convert(UTF8String, bytestring(value))\n    end\n    \n    end\n```\n\nOur Julia test code\n\n```Julia\n    include(\"./libjsonpretty.jl\")\n    import libjsonpretty\n\n    src = \"\"\"{\"name\":\"fred\",\"age\":25,\"height\":75,\"units\":\"inch\",\"weight\":\"239\"}\"\"\"\n    println(\"Our origin JSON src\", src)\n    value = libjsonpretty.jsonpretty(src)\n    println(\"And out pretty version\\n\", value)\n```\n\nAs before you can run your tests with `julia libjsonpretty_test.jl`.\n\nIn closing I would like to note that to use these examples I am assuming your\nJulia code is in the same directory as your shared C library. Julia, like Python3,\nhas a feature rich module and Package system. If you are creating a serious Julia\nproject then you need to be familiar with how Julia's package and module system works\nand place your code and shared libraries appropriately.\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-03-11",
        "keywords": [
          "Golang",
          "Julia",
          "shared libraries"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Accessing Go from Julia"
      },
      "url": "posts/2018/03/11/accessing-go-from-julia.json"
    },
    {
      "content": "\n\n# Review: Software Tools in Pascal\n\nBy R. S. Doiel, 2018-07-22\n(updated: 2018-07-22, 1:39 pm, PDT)\n\n\nThis book is by Brian W. Kernighan and P. J. Plauger. It is an\nexample of the type of books I find I re-read and want in my\npersonal library. The book covers software construction through \na series of programs written in pascal. It is about how these \nprograms work, how to approach problems and write sound software.\nI was surprised I did not know about this book when I was browsing \nthe [Open Library](https://openlibrary.org) this weekend.  While \nPascal was a popular in the 1980's it has faded for most people in the \nearly 21st century.  This review maybe a small bit of nostalgia. \nOn the other hand I suspect \n[\"Software Tools in Pascal\"](https://openlibrary.org/books/OL4258115M/Software_tools_in_Pascal)\nis one of the short list of computer books that will remain useful\nover the long run.\n\n\n## What's covered\n\nThe book is organized around specific programs and their implementations.\nThe implementations provided are simple and straight forward. Each\nsection is followed by a set of \"exercises\" that extend the ideas\nshown in the section. In this way you could derive the modern equivalent\nof these tools.\n\nThe topics you build tools for in the text are\nfilters, files, sorting, text patterns, editing, formatting, \nand macro processing.\n\nIf you want to follow the book along in Pascal then I think Free Pascal\navailable in many Debian distributions including Raspbian on the Raspberry\nPi is a good choice.  Likewise Wirth's Pascal is easy enough to port\nto other languages and indeed this would be a useful exercise when I\nre-read the book the next time.\n\nThe book presents a very nice set of text oriented programs to explore\nprogramming or re-connect with your programming roots.\n\n## Read the book\n\n<iframe width=\"165\" frameBorder=\"0\" height=\"400\" src=\"https://openlibrary.org/books/OL4258115M/Software_tools_in_Pascal/widget\"></iframe>\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2018-07-22",
        "keywords": [
          "Pascal",
          "programming",
          "book review"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Review: Software Tools in Pascal"
      },
      "url": "posts/2018/07/22/software-tools-in-pascal.json"
    },
    {
      "content": "\n\nFreeDOS to Oberon System 3\n==========================\n\nBy R. S. Doiel, 2019-07-28\n\n>    UPDATE: (2021-02-26, RSD) Under VirtualBox 6.1 these\n>    instructions still fail. My hope is to revise these \n>    instructions when I get it all sorted out.\n>\n>    Many links such as the ftp site at ETH Oberon are \n>    no more. I've updated this page to point at Wayback machine\n>    or included content in here where I cannot find it else where.\n>\n>    UPDATE: (2021-02-19, RSD) Under VirtualBox 6.1 these instructions \n>    fail. For VirtualBox I’ve used FreeDOS 1.3rc3 Live CD installing \n>    the “Plain DOS” without problems.\n>\n>    UPDATE: (2021-03-16, RSD) After reviewing my post, correcting\n>    some mistakes I finally was able to get FreeDOS up and running\n>    on VirtualBox 6.1. This allows NativeOberon 2.3.6 to be brought\n>    up by booting the \"oberon0.dsk\" virtual floppy and following\n>    the instructions included. You need to know how to use\n>    the Oberon mouse and the way commands work in Oberon.\n\nWhat follows are notes on getting a FreeDOS 1.2[^1] and \nthen Native Oberon[^2] running under VirtualBox 6.0. You might \nwonder why these two are together. While it was\neasy to run the Native Oberon installation process that process\nassumes you have a properly partitioned hard disk and VirtualBox\nseems to skip that process. I found taking advantage of FreeDOS\nsimplified things for me.\n\nMy goal was running Oberon System 3, but setting up a Virtual Box\nwith FreeDOS 1.2 gave me a virtual machine that functions like a \n1999 era PC. From there all the steps in the Oberon instructions\njust worked.\n\n## Creating FreeDOS 1.2 Virtual Box\n\nI've been doing a bit if computer history reading and decided to\nbring up some older systems as a means to understand where\nthings were.  The first computers I had access to were 8080, 8086\nmachines running MS DOS based. My first computer programming language\nwas Turbo Pascal. Feeling a bit nostalgic I thought it would be\ninteresting to use it again and see what I remembered from the days\nof old. While PC and MS DOS no longer exist as commercial productions\nan wonderful group of Open Source hackers have brought new life into\nDOS with FreeDOS 1.2[^3]. You'll find many of your old familiar commands\nbut also some nice improvements. You can even run it under VirtualBox\nwhich is what I proceeded to do.\n\n### VirtualBox 6.0 setup\n\nThe [FreeDOS](https://freedos.org) website includes a CD ROM image\nthat you can use to install it. There are couple small hitches though\nto get it working under VirtualBox. First go to the [download](https://freedos.org/download) page and download the [CDROM \"standard\" installer\"](http://www.freedos.org/download/download/FD12CD.iso).\n\nWhile that is downloading you can setup your VirtualBox machine.\nFirst to remember is DOS compared to today's operating systems is\nfrugal in its hardware requirements. As a result I picked very modest\nsettings for my virtual machine. \n\n1. Launch VirtualBox\n2. From the menu, pick Machine then pick new\n3. Name your machine (e.g. \"FreeDOS 1.2\"), select the type: \"Other\" and Operating system of \"DOS\"\n4. Set memory size as you like, I just accepted the default 32MB\n5. Hard disk, pick \"Create a virtual hard disc now\"\n6. Hard disk file type, pick \"VHD (Virtual Hard Disk)\"\n7. Storage on physical hard disk, I picked Dynamically allocated both either is fine\n8. File location and size, I accepted the default location and size\n9. Before starting my FreeDOS box I made a couple of changes using \"settings\" menu icon\n    a. Display, I picked bumped memory up to 128M and picked VBoxSVGA with 33D acceleration (for games)\n    b. Storage, I added a second floppy drive (empty)\n    c. Network, I picked attached to NAT\n10. When looking at my virtual machine's detail page I clicked on the Optical drive (empty), click \"choose disc image\" and pointed at the downloaded installed CD\n11. Click Start.\n12. At \"Welcome to FreeDOS 1.2\" blue screen, hit TAB key\n13. You will see a line that begins with a boot instruction. Add a space than add the word \"raw\" (without quotes) press enter\n14. Follow the install instructions, when you get to \"Drive C: does not appear to be partitioned\" dialog, pick \"Yes - Partition drive C:\"\n15. On the next screen pick \"Yes - Please reboot now\"\n16. When at the \"Welcome to FreeDOS 1.2\" screen hit TAB again\n17. Once again add a space and type \"raw\" to the command then press enter\n18. Pick \"Yes - continue with the installation\"\n19. Pick \"Yes - Please erase and format drive C:\"\n20. At this point its a normal FreeDOS install\n21. When the install is done and reboots \"eject\" the virtual CD form the \"Optical Drive\" in the VirtualBox panel, then choose \"boot from system disk\",you now should have a working FreeDOS on VirtualBox\n\n## Native Oberon System 3 on Virtual Box\n\nNative Oberon can be found at http://www.ethoberon.ethz.ch/native/.\nThere is a related ftp site[^4] where you can download the necessary\nfiles for the stand alone version. \n\nHere's the steps I used in my Mac to download Native Oberon and\ninto a file on my desktop called \"NativeOberon-Standalone\". Open\nthe macOS Terminal application. I assume you've got a Unix\ncommand called [wget](https://en.wikipedia.org/wiki/Wget)\nalready installed[^5].\n\n> NOTE: The ETH ftp server is no more. I've included Web Archive\n> links and links to my own copies of the files needed to\n> install Native Oberon 2.3.6 in the paragraphs that follow.\n> RSD, 2021-03-16\n\n```bash\n\n    cd\n    mkdir -p Desktop/NativeOberon-Standalone\n    cd Desktop/NativeOberon-Standalone\n    wget ftp://ftp.ethoberon.ethz.ch/ETHOberon/Native/StdAlone/\n\n```\n\nClone your FreeDOS Box first. You'll want to do a \"Full Clone\". You'll\nalso want to \"remove\" any optical disks or floppies. You do that from\nthe virtual boxes' detail page and clicking on the drive and picking the\n\"Remove disk from virtual drive\" in the popup menu.\n\nAt this point we have a a virtual machine that is very similar to an \n1999 era PC installed with MS DOS.  [Native Oberon](http://web.archive.org/web/20190929033749/http://www.ethoberon.ethz.ch/native/) Normally you'd\ninstall [Native Oberon via 1.44MB floppy disks](/blog/2019/07/28/NativeOberon-StnAlone-2.3.6.zip \"Zip file of individual floppies\"). \nWe can simulate that with our Virtual machine.\nIn the folder of you downloaded there is disc called \"oberon0.dsk\". That\ncan go in our first floppy drive. But how to we get the rest of the \nfiles onto a virtual floppies? This wasn't obvious to me at first.\n\nThe Oberon install disks were organized as follows\n\n| PACKAGE    | FILENAME     | SIZE  | DSK   |\n| ---------- | ------------ | ----- | ----- |\n| Oberon-0      | [oberon0.dsk](oberon0.dsk \"boot disk\")  |          | 0 | \n| Gadgets       | [gadgets.arc](gadgets1.arc \"a modified gadgets.arc to fit 1.4 floppy\")  | 1.4  2.9 | 1 | \n| Documentation | [docu.arc](docu.arc \"documentation\")     | 1.3  2.5 | 2 | \n| Applications  | [apps.arc](apps.arc \"applications\")     | 1.3  2.8 | 3 | \n| Tutorials     | [tutorial.arc](tutorial.arc \"tutorial\") | 0.3  0.8 | 4 | \n| Pr3Fonts      | [pr3fonts.arc](pr3fonts.arc \"fonts\") | 0.3  0.6 | 4 | \n| Pr6Fonts      | [pr6fonts.arc](pr6fonts.arc \"fonts\") | 0.5  1.8 | 4 | \n| Source1       | [source1.arc](source1.arc \"Source Code\")  | 0.9  2.5 | 5 | \n| Source2       | [source2.arc](source2.arc \"Source Code\")  | 1.2  3.5 | 6 | \n| Source3       | [source3.arc](source3.arc \"Source Code\")  | 0.6  1.7 | 7 | \n\n\nIt turns out you can create 1.44MB Fat16 disc images from the\nVirtual Box 6.0 floppy drive link.  When you click on the floppy\ndrive in the details page you have a choice that includes \"create a new floppy disc\". Select this, find the disc a filename like \"disk1\". Click\non the virtual floppy disk in the Virtual Box and \"remove\"\nthe disc then create disk2, disk3, etc. In each the empty disc image\nfiles places the files from the table above. These image files can then\nbe opened on your host operating system and files copied to them. \nIt's a tedious process but this gives you something the Oberon System \ncan read and install from. Originally I just put all the files into an \nISO CD ROM image but I could not figure out how to mount that from this\nversion of Oberon. Now when you start up your Oberon V3 virtual machine\nyou can install the rest of the software like Gadgets.\n\n\n[^1]: FreeDOS is an Open Source implementation of PC/MS DOC\n\n[^2]: Native Oberon is a 1990's version of Oberon System running on i386\n\n[^3]: Download FreeDOS from http://freedos.org/download\n\n[^4]: Download Native Oberon Stand Alone from [ftp://ftp.ethoberon.ethz.ch/ETHOberon/Native/StdAlone](NativeOberon-StdAlone-2.3.6.zip \"Zip of what used to be available in that directory at ftp.ethoberon.ethz.ch\")\n\n[^5]: wget is easily installed with [HomeBrew](https://brew.sh/) or [Mac Ports](https://www.macports.org/)\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2018, R. S. Doiel",
        "date": "2019-07-28",
        "keywords": [
          "FreeDOS",
          "Oberon System"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "FreeDOS 1.2 to Oberon System 3",
        "updated": "2021-03-16"
      },
      "url": "posts/2019/07/28/freedos-to-oberon-system-3.json"
    },
    {
      "content": "\n\nMostly Oberon\n=============\n\nBy R. S. Doiel, 2020-04-11\n\n**Mostly Oberon** is a series of blog posts documenting my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.\n\n## Overview\n\nOberon is a classical computer language and operating system originated by Professors Niklaus Wirth and Jürg Gutknecht at [ETH](https://en.wikipedia.org/wiki/ETH_Zurich) circa 1987.  It was inspired by their experiences in California at the [Xerox Palo Alto Research Center](https://en.wikipedia.org/wiki/PARC_\\(company\\)).  This series of blog posts are my meandering exploration of Oberon-07 language based on [Project Oberon 2013](http://www.projectoberon.com/).\n\nNOTE: Oberon grew from Wirth's Modula, which grew from Pascal, which grew from his experiences with Algol.\n\n### My Voyage\n\nI am new to both Oberon and the Oberon System.  Oberon language is in the tradition of ALGOL, Pascal, Modula 1 and 2 as well as incorporating ideas from the parent of Object Oriented languages Simula. The Oberon language reminds me of my first programming language [Turbo Pascal](https://en.wikipedia.org/wiki/Turbo_Pascal).  Oberon's language shape is more Pascal than C. For that reason I think it has largely been overlooked.\n\nOberon-07 is Wirth's most recent refinement of the Oberon language.  It is a terse and powerful systems language.  It strikes a different computing path then many popular programming languages used in 2020.  You find its influence along with Simula in more recent popular languages like [Go](https://golang.org).\n\nWhile Wirth conceived of Oberon in the context of a whole system it's use in research and instruction means it is also well suited [POSIX](https://en.wikipedia.org/wiki/POSIX) based systems (e.g. BSD, Linux, macOS).  The difference in programming in Oberon for a POSIX system versus a native Oberon System is primarily in the modules you import. These posts will focus on using Oberon language in a POSIX environment.\n\nNOTE: Oberon was initially a project including the CERES Hardware, Oberon compiler and Oberon operating system for networked workstations.\n\nThe latest Oberon is Prof. Niklaus Wirth and Paul Reeds' Project Oberon 2013. If you want to explore it I suggest using Peter De Wachter's [emulator](https://github.com/pdewacht/oberon-risc-emu). Project Oberon also his links to the updated books and articles in PDF format which are easy to read (or print) on most computing devices.\n\n\n## A starting point\n\nI am starting my exploration with Karl Landström's [OBNC](https://miasap.se/obnc/) compiler. I am focusing on getting comfortable using and writing in the Oberon language.\n\nHere's an example of a simple \"Hello World\" program in Oberon written for a POSIX system. I've named the [source code](HelloWorld.Mod) `HelloWorld.Mod`.\n\nNOTE: In 2020 common POSIX systems include [Linux](https://en.wikipedia.org/wiki/Linux), [BSD](https://en.wikipedia.org/wiki/Berkeley_Software_Distribution) and [macOS](https://en.wikipedia.org/wiki/MacOS).\n\n\n~~~\n\n    MODULE HelloWorld;\n      IMPORT Out;\n    BEGIN\n      Out.String(\"Hello World!\"); Out.Ln;\n    END HelloWorld.\n\n~~~\n\n\nWhile this is longer than a Python \"hello world\" program it is much shorter than I remember writing in Java and about the same number of lines as in C. `BEGIN` and `END` are similar to our opening and closing curly braces in C and the module is the basic unit of source code in Oberon. `IMPORT` includes the module `Out` (modules are similar to a included library in C) for sending values to the console (stdout in POSIX). One thing to note, Oberon language(s) are case sensitive. All language terms are capitalized. This makes it easy to distinguish between source code written in Oberon versus the Oberon language itself.\n\nThe `Out` module includes methods for displaying various data types native\nto Oberon. There is a corresponding `In` for receiving input as well as\nsome additional modules provided with our chosen compiler implementation.\n\nModules in Oberon can include a module wide initialization block. The\n`BEGIN` through `END HelloWorld.` are an initialization block. This is\nsimilar to C or Go's \"main\" function for our POSIX environment.\n\n### OBNC\n\nIf you want to run my \"Hello World\" you need to compile it.  I have found that [OBNC](https://miasap.se/obnc/) compiler runs well on Linux, macOS and [Raspberry Pi](https://www.raspberrypi.org). Karl has also made a precompiled version that runs on Windows available too. It is the Oberon compiler I plan to use in this series of posts.\n\nOBNC compiles Oberon source into C then into machine code for the computer system you are running on. Because it is compiling to C it can function as a [cross compiler](https://en.wikipedia.org/wiki/Cross_compiler). This opens the door to [bare metal programming](https://en.wikipedia.org/wiki/Bare_machine).\n\nIf you're following along please install OBNC on your computer.  Instructions are found at https://maisap.se/obnc. Karl also has excellent documentation and is responsive to questions about his implementation. His contact methods are included on his website.\n\n\n### Running OBNC\n\nOBNC provides a Oberon-07 compiler with some optional modules for working in a POSIX environment.  Compiling our \"Hello World\" is easy from your shell or terminal.\n\n\n~~~\n\n    obnc HelloWorld.Mod\n\n~~~\n\n\nIf all goes well this should produce an executable file named `HelloWorld` (or `HelloWorld.exe` on Windows). You can now run this program with a command like `./HelloWorld` (or `HelloWorld` on Windows).\n\n### Learning more about Oberon\n\nI have faced two challenges in my exploration of Oberon, finding a compiler I was happy with (thank you Karl for OBNC) and sorting out the literature around Oberon language implementations and system versions.\n\nOberon has a rich history though it was not well known in Southern California in 2020. Oberon's history is primarily academic and European. It was commonly used in college level instruction in Europe from it's inception at ETH in the late 80's through the early 2000s. The Oberon System is an Open Source system (predating the term by a decade) and was created in the spirit of other academic systems such as BSD. There are many books (physical books as opposed to ebooks) dating from that era.  They covered the Oberon language and system of their time.  From a historical computing perspective they remain very interesting. But running Oberon on modern 2020 hardware is a little more challenging. Fortunately Prof. Emeritus Wirth and Paul Reed brought things up to date in 2013. I recommend Reed's [www.projectoberon.com](http://www.projectoberon.com) as a good place to start. He includes links to revised versions of the classic Oberon and Oberon System texts written by Wirth et el. Prof. Wirth's [website](https://inf.ethz.ch/personal/wirth/) is still maintained and he features links to most of his major publications. His is the canonical source of information on Oberon.\n\nNOTE: Prof. Wirth's personal website at ETH was available as of 2020-04-11. \n\nI have found the ACM [Digital Library](https://dl.acm.org/) and the ETH [Research Collection](https://www.research-collection.ethz.ch/?locale-attribute=en) very helpful.  While much of the material is now historic it remains useful for both techniques and inspiration.  Today's hardware, even a Raspberry Pi Zero, is more resource rich than the original systems Oberon ran on.\n\nThe online community for Oberon and Oberon System seems mostly centered around a [mail list](https://lists.inf.ethz.ch/mailman/listinfo/oberon) at ETH and net news group [comp.lang.oberon](https://groups.google.com/forum/#!forum/comp.lang.oberon)\n\n\n\n\n\n\n\n\n### Next\n\n+ Next [Modules and Procedures](../12/Mostly-Oberon-Modules.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "byline": "R. S. Doiel",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-04-11",
        "keywords": [
          "Oberon",
          "Wirth",
          "Gutknecht",
          "ETH",
          "Parc",
          "programming",
          "operating systems"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 1,
        "series": "Mostly Oberon",
        "title": "Mostly Oberon"
      },
      "url": "posts/2020/04/11/Mostly-Oberon.json"
    },
    {
      "content": "\n\nOberon Modules and Procedures\n=============================\n\nBy R. S. Doiel, 2020-04-12\n\nThis is the second post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.\n\n## Modules\n\nThe module is a primary code unit of Oberon language. Modules allow you to focus on functional units of code and can be readily composed into larger solutions.\nA module's name should match the filename you are saving it under. A module starts with declaring it's name and ends the declaration with a semicolon\nthe statement separator in Oberon. Our simple \"Hello World\" example \nshows the basic code shape.\n\n\n~~~{.oberon}\n\n    MODULE HelloWorld;\n      IMPORT Out;\n    BEGIN\n      Out.String(\"Hello World!\"); Out.Ln;\n    END HelloWorld.\n\n~~~\n\n\nModules end with a `END` followed by the module's name and a period.\nAny text following the `END` statement is ignored by the compiler. This\nturns out to be very useful as a place to write up ideas about the code\nyou're working on. You can also write any additional special instructions \nthere (e.g. document usage). You can even use it as a scratch pad knowing \nthat the compiler will ignore it.\n\nHere's an example\n\n\n~~~{.oberon}\n\n    MODULE HelloWorld;\n      IMPORT Out;\n    BEGIN\n      Out.String(\"Hello World!\"); Out.Ln;\n    END HelloWorld.\n\n    This program isn't very useful. It has no interactive ability.\n    It'd be nice if it could be more specific about who it was saying\n    hello to.\n\n~~~\n\n\nFor a module to be really useful you want to have the capability\nof including both private and public code. Public code\nallows us to reuse our code in other modules while the private code \nkeeps internal things inside the module safe from colliding with other\nmodules private code. This technique is classically known as \n\"information hiding\" and in computer sciences texts as \"scope\". Lets \ncreate a a more composable module called `SayingHi.Mod`.  In \naddition to display \"Hello World!\" we want a public method \n(procedure in Oberon terminology) that can ask for a name and print \nout a salutation. We will use the `SayingHi.Mod` module along with \na newer version of `HelloWorld.Mod` named `HelloWorld2.Mod`.\n\n\n## Procedures\n\nHow do we write methods in Oberon?  Methods are declared\nusing the keyword `PROCEDURE` followed by their name, a \ndeclaration of any parameters and if the procedure returns a\nvalue (i.e. is a function) it also includes that declaration. \nNext we declare any internal variables needed by the procedure.\nThis is followed by the procedure's body.  The body of the \nprocedure is defined by a `BEGIN` and `END` statement structure. \nThe body contains the steps the procedure needs to execute.\n\nWe'll create a procedure called \"HelloWorld\" in our new module.\nSince we will use this procedure from our new `HelloWorld2.Mod` \nour new \"HelloWorld\" procedure needs to be public.  A public \nprocedure in `SayingHi.Mod` is available for use in our new \n`HelloWorld2.Mod` (or by another module).  Marking a procedure \npublic in Oberon is a little different than in other languages. \nA Module's procedure is public if its name ends with an asterisk. \nBelow is a sketch of our module `SayingHi.Mod` so far.\n\nNOTE: This technique is also used to mark variables, records and constants as public and available to other modules. Public variables are \"read only\" in other modules.\n\n\n~~~{.oberon}\n\n    MODULE SayingHi;\n      IMPORT Out;\n    \n      PROCEDURE HelloWorld*;\n      BEGIN\n        Out.String(\"Hello World!\"); Out.Ln;\n      END HelloWorld;\n    END SayingHi.\n\n~~~\n\n\nThis modules looks allot like `HelloWorld.Mod` with a couple key\ndifferences. Rather than relying on the module's begin and end \nstatements we declare a procedure with its own begin and end statements.\nNotice the procedures end statement includes the procedure name and\nis terminated by semicolon rather than a period.  Like `HelloWorld.Mod`\nwe import the `Out` module to display our greeting.\n\n## Putting it all together\n\nLet's create a new \"Hello World\" module called `HelloWorld2.Mod` and\nuse our `SayingHi` module instead of directly importing `Out`.\n\n\n~~~{.oberon}\n\n    MODULE HelloWorld2;\n      IMPORT SayingHi;\n    BEGIN\n      SayingHi.HelloWorld;\n    END HelloWorld2.\n\n~~~\n\n\nWe can compile our module with OBNC using the command\n\n\n~~~\n\n    obnc HelloWorld2.Mod\n\n~~~\n\n\nWe can run our new \"Hello World\" with the command\n\n\n~~~\n\n    ./HelloWorld2\n\n~~~\n\n\nAt this point we have a way of saying \"Hello World!\" whenever\nwe need in our Oberon programs. But just printing \"Hello World!\"\nto the screen isn't very interactive. It'd be nice if we could\nhave the computer ask our name and then respond with a greeting.\n\nWe'll modify our SayingHi to include a new procedure called \"Greetings\"\nand that procedure needs to ask us our name and then display\nan appropriate greeting. \"Greetings\" will be a public procedure\nmarked by an asterisk like \"HelloWorld\". \n\n\"Greetings\" has three tasks\n\n1. Ask politely for our name\n2. Get the name typed in with our keyboard\n3. Assemble and display a polite greeting\n\nTo keep our \"Greeting\" procedure short we'll split this\nup into some private procedures. These will not be available\noutside `SayingHi.Mod`. Here's a sketch of our improved module.\n\n\n~~~{.oberon}\n\n    MODULE SayingHi;\n      IMPORT In, Out;\n    \n      PROCEDURE HelloWorld*;\n      BEGIN\n        Out.String(\"Hello World!\"); Out.Ln;\n      END HelloWorld;\n    \n      PROCEDURE AskOurName;\n      BEGIN\n        Out.String(\"Excuse me, may I ask your name? \");\n      END AskOurName;\n    \n      PROCEDURE GetName(VAR ourName : ARRAY OF CHAR);\n      BEGIN\n        In.Line(ourName);\n      END GetName;\n    \n      PROCEDURE AssembleGreeting(ourName : ARRAY OF CHAR);\n      BEGIN\n        Out.String(\"Hello \");Out.String(ourName);\n        Out.String (\", very nice to meeting you.\"); Out.Ln;\n      END AssembleGreeting;\n    \n      PROCEDURE Greetings*;\n        VAR ourName : ARRAY 256 OF CHAR;\n      BEGIN\n        AskOurName;\n        GetName(ourName);\n        AssembleGreeting(ourName);\n      END Greetings;\n    END SayingHi.\n\n~~~\n\n\nNow let's add our Greetings procedure to `HelloWorld2.Mod`.\n\n\n~~~{.oberon}\n\n    MODULE HelloWorld2;\n      IMPORT SayingHi;\n    BEGIN\n      SayingHi.HelloWorld;\n      SayingHi.Greetings;\n    END HelloWorld2.\n\n~~~\n\n\nWe compile and run it the same way as before\n\n\n~~~\n\n    obnc HelloWorld2\n    ./HelloWorld2\n\n~~~\n\n\nWhen you run `HelloWorld2` you should now see something like\n(I've answered \"Robert\" and pressed return after the second line.\n\n\n~~~\n\n   Hello World!\n   Excuse me, may I ask your name? Robert\n   Hello Robert, very nice to meeting you.\n\n~~~\n\n\n\n## Reading our code\n\nWhile our revised modules are still short they actually exercise\na number of language features. Let's walk through the code \nblock by block and see what is going.\n\n`HelloWorld2.Mod` is responsible for the general management of\nour program namely say \"Hello World!\" and also for initiating\nand responding with a more personal greeting.  It does this by\nfirst importing our `SayingHi.Mod` module.\n\n\n~~~\n\n    IMPORT SayingHi;\n\n~~~\n\n\n[HelloWorld2.Mod](HelloWorld2.Mod) doesn't have any of its own \nprocedures and like our original [HelloWorld.Mod](HelloWorld.Mod)\nrelies on the module's initialization block to run our two public \nprocedures from `SayingHi`. It calls first `SayingHi.HelloWorld;` \nthen `SayingHi.Greetings'` before existing. Other than using the \n`SayingHi` module it is similar in spirit to our first \n[HelloWorld.Mod](HelloWorld.Mod).\n\nOur second module [SayingHi.Mod](SayingHi.Mod) does the heavy lifting.\nIt contains both public and private procedures.  If you tried to\nuse `GetName` from `SayingHi` in `HelloWorld2.Mod` you would get a\ncompiler error. As far as `HelloWorld2.Mod` is concerned `GetName`\ndoes not exist. This is called information hiding and is an important\ncapability provided by Oberon's Modules system. \n\n### explore `SayingHi` more deeply\n\nIn `SayingHi.Mod` we introduce two important concepts.\n\n1. Public and Private procedures\n2. variables to hold user input\n\n`SayingHi.Mod` imports two module, `In` which is for getting\ntext input from the keyboard, and `Out` which is used for displaying\ntext to standard output.\n\n\n~~~{.oberon}\n\n    IMPORT In, Out;\n\n~~~\n\n\n`In` and `Out` are to modules you will commonly use to either\nreceive input (`In`) from the keyboard or display output (`Out`)\nto the terminal or shell. They provide simple methods for working\nwith variables and constants and built-in Oberon data types. \nThis is a very useful as it lets us focus our procedures\non operating on data rather than the low level steps needed to\ninteract with the operating system and hardware.\n\nNOTE: __basic types__, Oberon has a number of basic types, BYTE holds a byte as a series of bit, CHAR holds a single ASCII character, INTEGER holds a signed integer value, REAL holds a floating point number and BOOLEAN holds a True/False value.\n\nThe first procedure is `HelloWorld` and it's pretty straight forward.\nIt displays a \"Hello World!\" message in our terminal. It uses `Out`.\n`Out.String` to display the \"Hello World!\" and `Out.Ln` to force a new\nline. `Out.String` is responsible for displaying values that are of type\n`ARRAY OF CHAR`. This includes text we provided in double quotes.\n\n\n~~~{.oberon}\n\n    PROCEDURE HelloWorld*;\n    BEGIN\n      Out.String(\"Hello World!\"); Out.Ln;\n    END HelloWorld;\n\n~~~\n\n\nThe notable thing about `HelloWorld*` is its annotation `*`.\nThis asterisk indicates to the compiler that this is\na public procedure and should be made available to other modules.\nProcedures, variables, constants, records (data structures) can be\nmade public with this simple annotation.  If we left off the `*`\nthen we would not be able to use `HelloWorld` procedure from other\nmodule.\n\nOur second procedure is `AskOurName`. It's private because it lacks\nthe `*`. It is invisible to `HelloWorld2.Mod`. It is visible within\n`SayingHi` module and we'll use it later in `Greetings*`. Before\na procedure, variable, constant or record can be used it must be\ndeclared. That is why we most define `AskOurName` before we define\n`Greetings*`. `AskOurName` is in other respects very similar to \n`HelloWorld*`.\n\n\n~~~{.oberon}\n\n    PROCEDURE AskOurName;\n    BEGIN\n      Out.String(\"Excuse me, may I ask your name? \");\n    END AskOurName;\n\n~~~\n\n\nOur third procedure `GetName` is a little more interesting.\nIt demonstrates several features of the Oberon language. Most\nobvious is that it is the first procedure which contains a\nparameter list.\n\n\n~~~{.oberon}\n\n    PROCEDURE GetName(VAR ourName: ARRAY OF CHAR);\n\n~~~\n\n\nThere is allot packed in this single statement in addition\nto putting a name to our procedure. Specifically it uses\na `VAR` in the parameter.  Oberon provides two kinds of parameters\nin declaring procedures. The two are `VAR` and static.  A `VAR` \nparameter means that the procedure is allowed to up date the value \nin the memory location indicated by the name. A static variable \n(a parameter without the `VAR` prefix passes in a read only value. \nThis allows us to distinguish between those procedures and variables\nwhere that can be modified by the procedure and those which\nwill be left the same. Inside of `GetName` we call the \n`In` module using the `Line`. This retrieves a line of text\n(a sequence of keyboard strokes ended with the return key).\n\n\n~~~{.oberon}\n\n    In.Line(ourName);\n\n~~~\n\n\nBecause `ourName` was a variable parameter in `GetName` it\ncan be modified by `In.Line`.\n\nOur next procedure `AssembleGreeting` is private like\n`AskOurName` and `GetName`. Like `HelloWorld*` and `AskOurName`\nit makes use of the `Out` module to display content.\nUnlike `HelloWorld*` it has a parameter but this time\na static one. Notice the missing `VAR`. This indicates that\n`AssembleGreeting` doesn't modify, cannot modify `ourName`.\n\n\n~~~{.oberon}\n\n    PROCEDURE AssembleGreeting(ourName : ARRAY OF CHAR);\n    BEGIN\n      Out.String(\"Hello \");Out.String(ourName);\n      Out.String (\", very nice to meeting you.\"); Out.Ln;\n    END AssembleGreeting;\n\n~~~\n\n\nThe use of `Out.String` is more elaborate then before. Notice how\nwe use trailing spaces to make the output more readable.\n\nOur final procedure is public, `Greetings*`. It does not\nhave any parameters.  Importantly it does include a\nvariable for use inside the procedure called `ourName`. \nThe `VAR` line declares `ourName` as an `ARRAY 256 OF CHAR`. \nThis declaration tells the compiler to allocate memory \nfor storing `ourName` while `Greetings*` is being executed. \nThe declaration tells us three things. First the storage\nis continuous block of memory, that is what `ARRAY` means.\nThe second is the size of this memory block is 256 `CHAR`\nlong and the that we will be storing `CHAR` values in it.\n\nThe memory for `ourName` will be populated when we pass\nthe variable to `GetName` based on what we type at the\nkeyboard. If we type more than 256 ASCII characters they\nwill be ignored. After `GetName` records the typed character\nwe use the memory associated with the `ourName` variable\nwe read that memory to display what we typed in \nthe procedure named `AssembleGreeting`.\n\n\n### Going a little deeper\n\nOberon is a typed language meaning that \nvariables are declared, allocated and checked during compile time\nfor specific characteristics. The one variable we created `ourName`\nin the `Greetings` procedure reserves the space for 256 \n[ASCII](https://en.wikipedia.org/wiki/ASCII) characters. \nIn Oberon we call a single ASCII character a `CHAR`.  Since it\nwould be useful to work with more than one `CHAR` in relationship\nto others Oberon also supports a variable type called `ARRAY`. \nAn `ARRAY` is represented as a block of memory that is allocated\nby the Oberon run time. Because it is allocated ahead of time we\nneed to know its size (i.e. how many `CHAR` are we storing). In\nour case we have declared `ARRAY 256 OF CHAR`. That means we can\nhold names up to 256 ASCII characters. \n\n`Greetings*` does three things and the second thing, `GetName` \nreceives the characters typed at the keyboard.  `GetName` has\na parameter list. In this case the only one parameter is declared\n`VAR ourName : ARRAY OF CHAR`. Notice the similarity and\ndifference between the `VAR` statement in `Greetings` versions\nthe parameter list.  Our `GetName` can accept **any** length of\n`ARRAY OF CHAR` and it **only** can accept an `ARRAY OF CHAR`.\nIf you try to pass another type of variable to `GetName` the\ncompiler will stop with an error message.\n\nWhy is this important?\n\nWe've minimized the memory we've used in our program.  Memory is \ntypically allocated on the stack (a block of memory made available \nby the operating system to the program). We've told the operating \nsystem we need 256 `CHAR` worth of consecutive memory locations \nwhen we allocated room the variable `ourName` in `Greetings`. When \nwe invoke `GetName` Oberon knows to use that same memory location \nfor the value of `ourName` defined in the parameter.  In turn\nwhen `In.String(ourName);` is called the module `In` knows\nto store the name typed on the keyboard in that location of memory.\nWhen `Out.String(outName);` is called the compiler knows to use\nthe same location of memory to send the contents to the display.\nWhen we finally finish the `Greetings*` procedure the memory is \nreleased back to the operating system for re-use by this or\nother programs.\n\n### What we've explored\n\n1. Using a module to break down a simple problem\n2. Using a module's ability to have public and private procedures \n3. Touched on how memory is used in a simple interactive program\n\n\n\n### Next and Previous\n\n+ Next [Basic Types](../18/Mostly-Oberon-Basic-Types.html)\n+ Previous [Mostly Oberon](../11/Mostly-Oberon.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-04-12",
        "keywords": [
          "Oberon",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 2,
        "series": "Mostly Oberon",
        "title": "Oberon Modules and Procedures"
      },
      "url": "posts/2020/04/12/Mostly-Oberon-Modules.json"
    },
    {
      "content": "\n\nOberon Basic Types\n==================\n\n\nBy R. S. Doiel, 2020-04-18\n\nThis is the third post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.\n\n## Simple Types\n\nOberon is a small systems language. It provides a useful but \nlimited umber of basic types. These can be be\nthought of as simple types mapping to specific memory locations\nand more complex types composed of multiple memory locations.\n\nNOTE: __basic types__, INTEGER, REAL, CHAR, ARRAY, RECORD and POINTER TO\n\n### INTEGER\n\nIntegers are easiest to be thought of as whole numbers. They may be\npositive numbers or negative numbers. Declaring an integer\nvariable `i` it would look something like\n\n\n~~~{.oberon}\n\n    VAR i : INTEGER;\n\n~~~\n\n\nSetting `i`'s value to seven would look like\n\n\n~~~{.oberon}\n\n    i := 7;\n\n~~~\n\n\n\n### REAL\n\nReal holds real numbers. Real numbers contain a fractional \ncomponent. We normally notate them with\na decimal value e.g. \"0.01\". Like integers they can also be \npositive or negative.\n\nDeclaring a real number variable `a` would look like\n\n\n~~~{.oberon}\n\n    VAR a : REAL;\n\n~~~\n\n\nSetting the value of `a` to seven and one tenth (7.1) would\nlook like\n\n\n~~~{.oberon}\n\n    a := 7.1;\n\n~~~\n\n\n### CHAR\n\nA CHAR is a single ASCII character. Oberon, unlike more recent\nlanguages like Go or Julia, predates the wide adoption of UTF-8.\nThe character is represented in memory as one 8 bit byte.\nIf you need to work with an extended character set then you need\nto either re-encode the values into ASCII. At this time[^now] there\nis no standard way of handling None ASCII character systems natively.\nIf you need to work directly with an encoding such as UTF-8 you'll\nneed to develop your own modules and procedures for handily their\nencoding, decoding and other operations.\n\nDeclaring a CHAR variable `c` would look like\n\n\n~~~{.oberon}\n\n    VAR c: CHAR;\n\n~~~\n\n\nSetting the value of `c` to capital Z would look like\n\n\n~~~{.oberon}\n\n    c := \"Z\";\n\n~~~\n\n\nNote: Oberon expects double quotes to notate a character.\n\n\n### More complex types\n\nThe simplest types would prove problematic when addressing\nmore complex data representations if Oberon lacked two three built-in\ntypes - ARRAY, RECORD and POINTER TO. \n\n### ARRAY\n\nAn array is a sequence of memory locations which contain a common\ntype.  In Oberon-07 all arrays have to have a known link. This is\nbecause the Oberon compiler is responsible for pre-allocating\nmemory when the program starts to hold the array.  While this\nseems restrictive our next data type, RECORD, lets us move\ninto more dynamic memory structures.  Pre-allocating the array\nsize also has the advantage that we can re-use those locations\neasily in a type safe manner.\n\nDeclaring a variable \"name\" as an array of twelve characters would \nlook like and declaring a variable \"scores\" as an array of ten\nintegers would look like\n\n\n~~~{.oberon}\n\n    VAR \n      name : ARRAY 24 OF CHAR;\n      scores : ARRAY 10 OF INTEGER;\n\n~~~\n\n\nThe length of the array immediately follows the keyword \"ARRAY\" and\nthe \"OF CHAR\" or \"OF INTEGER\" phrases describes the types that can be \ncontained in the array. In the \"OF CHAR\" the type is \"CHAR\" the \n\"OF INTEGER\" is the type \"INTEGER\". \n\nSetting an array value can be done using an index. In this example\nthe zero-th element (first element of the array) is set to the value\n102. \n\n\n~~~{.oberon}\n\n    scores[0] := 102;\n\n~~~\n\n\nIn the case of CHAR arrays the whole array can be set in a simple \nsingle assignment.\n\n\n~~~{.oberon}\n\n    name := \"Ada Lovelace\";\n\n~~~\n\n\nTwo key points of arrays in Oberon are a known length and a single \ntype of data associated with them. Arrays can have more than\none dimension but the cells of the array most contain the same type.\n\nNOTE: __type safety__, Type safe means the compiler or run time verify that the data stored at that location conforms to the program defined, this is helpful in maintaining program correctness.\n\n### RECORD\n\nThe RECORD is Oberon's secret sauce. The record is used to\ncreate new types if data representations. It extend Oberon's basic \ntypes creating structured data representation. In this example we'll \ncreate a record that holds an game's name, a list of three player names \nand a list of three scores. We'll call this record type \n\"TopThreeScoreboard\". \n\n\n~~~{.oberon}\n\n    TYPE\n      TopThreeScoreboard = RECORD\n        gameName : ARRAY 24 OF CHAR;\n        playerNames : ARRAY 3, 24 OF CHAR;\n        scores : ARRAY 3 OF INTEGER\n      END;\n\n~~~\n\n\nNow that we have describe a record of type \"TopThreeScoreboard\" we can\ndeclare it with our \"VAR\" statement.\n\n\n~~~{.oberon}\n\n    VAR\n      scoreboard : TopThreeScoreboard;\n\n~~~\n\n\nSetting the element values in a record uses a dot notation\nand if those elements are themselves. In this case we'll set\nthe game name to \"Basketball\", the three players are\n\"Ada Lovelace\", \"Blaise Pascal\", and \"John McCarthy\", with\nthe scores 102, 101, 100.\n\n\n~~~{.oberon}\n\n   scoreboard.gameName := \"Basketball\";\n   scoreboard.playerNames[0] := \"Ada Lovelace\";\n   scoreboard.scores[0] := 102;\n   scoreboard.playerNames[1] := \"Blaise Pascal\";\n   scoreboard.scores[0] := 101;\n   scoreboard.playerNames[2] := \"John McCarthy\";\n   scoreboard.scores[0] := 100;\n\n~~~\n\n\nRecords are also used to create dynamic memory structures such as lists, trees and maps (see note on \"AD\").  The dynamic nature of records is achieved with\nour next type \"POINTER TO\".\n\nNOTE: __AD__, Prof. Wirth wrote an excellent text on [Algorithms and Data structures](https://inf.ethz.ch/personal/wirth/AD.pdf) available in PDF format.\n### POINTER TO\n\nOberon is a type safe language. To keep things safe in a type\nsafe language you need to place constraints around random\nmemory access. Memory can be thought of a list of locations and\nwe can go to those locations if we know their address. A pointer\nin most languages holds an address. Oberon has pointers but they\nmust point at specific data types. So like array you have to indicate\nthe type of the thing you are pointing at in a declaration. \nE.g. `VAR a : POINTER TO CHAR;` would declare a variable 'a' \nthat points to a memory location that holds a CHAR. The more common \ncase is we use \"POINTER TO\" in records to create dynamic data \nstructures such as linked lists.\n\nHere's a simple data structure representing a dynamic list\nof characters. Let's call it a DString and we will implement\nit using a single link list. The list can be implemented by\ndefining a RECORD type that holds a single character and a pointer\nto the next record. We can then also define a pointer to this type\nof record.  If there is no next character record\nwe assume we're at the end of the string.\n\n\n~~~{.oberon}\n\n    TYPE\n      DStringDesc = RECORD\n        value : CHAR;\n        next : POINTER TO DStringDesc\n      END;\n\n      DString : POINTER TO DStringDesc;\n\n~~~\n\n\nRECORD types are permitted to use recursive definition so our \n\"next\" value is itself a type \"DStringDesc\".  Declaring a \nDString variable is as easy as declaring our scoreboard type variable.\n\n\n~~~{.oberon}\n\n  VAR\n    VAR s : DString;\n\n~~~\n\n\nSetting our DString is a little trickier. This is where\nOberon's procedures come into play. We can pass our variable \"s\"\nof type DString to a procedure to build out our DString from an simple\narray of characters. Note \"s\" is declared as a \"VAR\" parameter\nin our procedure heading. Our `SetDString` will also need to handle\ncreating new elements in our dynamic string. That is what Oberon's\nbuilt-in `NEW()` procedure does. It allocates new memory for our\nlist of records.\n\n\n~~~{.oberon}\n\n    PROCEDURE SetDString(VAR s : DString; buf : ARRAY OF CHAR);\n        VAR i : INTEGER; cur, tmp : DString;\n    BEGIN\n      (* Handle the case where s is NIL *)\n      IF s = NIL THEN\n        NEW(s);\n        s.value := 0X;\n        s.next := NIL;\n      END;\n      cur := s;\n      i := 0;\n      (* check to see if we are at end of string or array *)\n      WHILE (buf[i] # 0X) & (i < LEN(buf)) DO\n        cur.value := buf[i];\n        IF cur.next = NIL THEN\n          NEW(tmp);\n          tmp.value := 0X;\n          tmp.next := NIL;\n          cur.next := tmp;\n        END;\n        (* Advance our current pointer to the next element *)\n        cur := cur.next;\n        i := i + 1;\n      END;\n    END SetDString;\n\n~~~\n\n\nWe can move our string back into a fixed length array of char\nwith a similar procedure.\n\n\n~~~{.oberon}\n\n    PROCEDURE DStringToCharArray(s : DString; VAR buf : ARRAY OF CHAR);\n      VAR cur : DString; i, l : INTEGER;\n    BEGIN\n      l := LEN(buf);\n      i := 0;\n      cur := s;\n      WHILE (i < l) & (cur # NIL) DO\n        buf[i] := cur.value; \n        cur := cur.next;\n        i := i + 1;\n      END;\n      (* Zero out the rest of the string. *)\n      WHILE (i < l) DO\n        buf[i] := 0X;\n        i := i + 1;\n      END;\n    END DStringToCharArray;\n\n~~~\n\n\nAt this stage we have the basics of data organization. Modules\nallow us to group operations and data into cohesive focused units.\nProcedures allow us to define consistent ways of interacting with\nout data, and types singularly and collectively allow us to structure\ndata in a way that is useful to solving problems.\n\n## Putting it all together\n\nHere is a [module demoing our basic type](BasicTypeDemo.Mod). In it\nwe can define procedures to demo our assignments, display their results\nall called from inside the module's initialization block.\n\n\n~~~{.oberon}\n\n    MODULE BasicTypeDemo;\n      IMPORT Out;\n    \n      (* These are our custom data types definitions. *)\n      TYPE\n          TopThreeScoreboard = RECORD\n            gameName : ARRAY 24 OF CHAR;\n            playerNames : ARRAY 3, 24 OF CHAR;\n            scores : ARRAY 3 OF INTEGER\n          END;\n    \n          DStringDesc = RECORD\n            value : CHAR;\n            next : POINTER TO DStringDesc\n          END;\n    \n          DString = POINTER TO DStringDesc;\n    \n      (* Here are our private variables. *)\n      VAR \n        i : INTEGER;\n        a : REAL;\n        c: CHAR;\n        name : ARRAY 24 OF CHAR;\n        scores : ARRAY 10 OF INTEGER;\n        scoreboard : TopThreeScoreboard;\n        s : DString;\n    \n    \n      PROCEDURE SimpleTypes;\n      BEGIN\n        i := 7;\n        a := 7.1;\n        c := \"Z\";\n      END SimpleTypes;\n    \n      PROCEDURE DisplaySimpleTypes;\n      BEGIN\n        Out.String(\" i: \");Out.Int(i, 1);Out.Ln;\n        Out.String(\" a: \");Out.Real(a, 1);Out.Ln;\n        Out.String(\" c: \");Out.Char(c);Out.Ln;\n      END DisplaySimpleTypes;\n    \n    \n      PROCEDURE MoreComplexTypes;\n      BEGIN\n        scores[0] := 102;\n        name := \"Ada Lovelace\";\n        scoreboard.gameName := \"Basketball\";\n        scoreboard.playerNames[0] := \"Ada Lovelace\";\n        scoreboard.scores[0] := 102;\n        scoreboard.playerNames[1] := \"Blaise Pascal\";\n        scoreboard.scores[0] := 101;\n        scoreboard.playerNames[2] := \"John McCarthy\";\n        scoreboard.scores[0] := 100;\n      END MoreComplexTypes;\n    \n      PROCEDURE DisplayMoreComplexTypes;\n        VAR i : INTEGER;\n      BEGIN\n        i := 0;\n        Out.String(\" Game: \");Out.String(scoreboard.gameName);Out.Ln;\n        WHILE i < LEN(scoreboard.playerNames) DO\n          Out.String(\"    player, score: \");\n          Out.String(scoreboard.playerNames[i]);Out.String(\", \");\n          Out.Int(scoreboard.scores[i], 1);\n          Out.Ln;\n          i := i + 1;\n        END;\n      END DisplayMoreComplexTypes;\n    \n      PROCEDURE SetDString(VAR s : DString; buf : ARRAY OF CHAR);\n          VAR i : INTEGER; cur, tmp : DString;\n      BEGIN\n        (* Handle the case where s is NIL *)\n        IF s = NIL THEN\n          NEW(s);\n          s.value := 0X;\n          s.next := NIL;\n        END;\n        cur := s;\n        i := 0;\n        (* check to see if we are at end of string or array *)\n        WHILE (buf[i] # 0X) & (i < LEN(buf)) DO\n          cur.value := buf[i];\n          IF cur.next = NIL THEN\n            NEW(tmp);\n            tmp.value := 0X;\n            tmp.next := NIL;\n            cur.next := tmp;\n          END;\n          cur := cur.next;\n          i := i + 1;\n        END;\n      END SetDString;\n    \n      PROCEDURE DStringToCharArray(s : DString; VAR buf : ARRAY OF CHAR);\n        VAR cur : DString; i, l : INTEGER;\n      BEGIN\n        l := LEN(buf);\n        i := 0;\n        cur := s;\n        WHILE (i < l) & (cur # NIL) DO\n          buf[i] := cur.value; \n          cur := cur.next;\n          i := i + 1;\n        END;\n        (* Zero out the rest of the string. *)\n        WHILE (i < l) DO\n          buf[i] := 0X;\n          i := i + 1;\n        END;\n      END DStringToCharArray;\n    \n    BEGIN\n      SimpleTypes;\n      DisplaySimpleTypes;\n      MoreComplexTypes;\n      DisplayMoreComplexTypes;\n      (* Demo our dynamic string *)\n      Out.String(\"Copy the phrase 'Hello World!' into our dynamic string\");Out.Ln;\n      SetDString(s, \"Hello World!\");\n      Out.String(\"Copy the value of String s into 'name' our array of char\");Out.Ln;\n      DStringToCharArray(s, name);\n      Out.String(\"Display 'name' our array of char: \");Out.String(name);Out.Ln;\n    END BasicTypeDemo.\n\n~~~\n\n\n## Reading through the code\n\nThere are some nuances in Oberon syntax that can creep up on you.\nFirst while most statements end in a semi-colon there are noticeable\nexceptions. Look at the record statements in particular.  The last\nelement of your record before the `END` does not have a semicolon.\nIn that way it is a little like a `RETURN` value in a function\nlike procedure.\n\nIn creating our `DString` data structure the Oberon idiom is to first\ncreate a description record, `DStringDesc` then create a pointer to\nthe descriptive type, i.e. `DString`. This is a very common\nidiom in building out complex data structures. A good place to learn\nabout implementing algorithms and data structures in Oberon-07 is \nProf. Wirth's 2004 edition of [Algorithms and Data Structures](https://inf.ethz.ch/personal/wirth/AD.pdf) which\nis available from his personal website in PDF.\n\n\n### Next and Previous\n\n+ Next [Loops and Conditions](../19/Mostly-Oberon-Loops-and-Conditions.html)\n+ Previous [Modules and Procedures](../12/Mostly-Oberon-Modules.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-04-18",
        "keywords": [
          "Oberon",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 3,
        "series": "Mostly Oberon",
        "title": "Oberon Basic Types"
      },
      "url": "posts/2020/04/18/Mostly-Oberon-Basic-Types.json"
    },
    {
      "content": "\nOberon Loops and Conditions\n===========================\n\nBy R. S. Doiel, 2020-04-19\n\nThis is the four post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.\n\n## Data Flow\n\nOberon is a small systems language and while it is minimalist.\nIt provides you with the necessary primitives to get things done.\nI've touched on code organization, basic types and basic type\nextensions in the previous articles.  I have shown the basic\ncontrol statements but have not talked about them yet.\n\nOberon offers four basic control statements. \n\nIF, ELSIF, ELSE\n: Basic condition test and execution\n\nASSERT\n: A mechanism to trigger a program halt\n\nWHILE DO, ELSIF DO\n: The Loop structure in the language (aside from recursive procedures)\n\nFOR TO, FOR TO BY\n: A counting Loop where incrementing a counter by an integer value (e.g. 1 or by a specified constant).\n\n## IF, ELSIF, ELSE\n\nThe first two provide for conditional statements of the form\nif a condition is true do something. Almost ever computer language\nhas some form of a conditional express and the Oberon IF, ELSIF,\nELSE typical of what you find is more computer languages today.\nBoth ELSIF and ELSE are optional.\n\n```Oberon\n    IF (s = \"Freda\") OR (s = \"Mojo\") THEN\n      Out.String(\"Wowie Zowie! I remember you from ZBS stories.\");Out.Ln;\n    ELSIF (s = \"Bilbo\") OR (s = \"Gandolf\") THEN\n      Out.String(\"Greets, I remember from the Hobbit.\");Out.Ln;\n    ELSE\n      Out.String(\"Glad to meet you \");Out.String(s);Out.Ln;\n    END;\n```\n\n## ASSERT\n\nThe second expression, ASSERT, is a little different. If ASSERT\nevaluates an expression that is FALSE then your program is halted.\nThis is like combining an \"if EXPR is false then system exit\".\n\n```Oberon\n    Out.String(\"Should I continue? Y/n \");\n    In.Line(s);\n    Out.Ln;\n    ASSERT((s = \"Y\") OR (s = \"y\"));\n    (* If you didn't enter Y or y the program will halt *)\n```\n\n\n## WHILE DO, ELSIF DO\n\nOberon-07 also provides two loop structures. These are very \nsimilar to other languages as well. The only expectation is that\na while loop may contain an ELSIF which continues the loop\nexecution until both clauses return FALSE.\n\nThe basic while loop, counting 1 to 10.\n\n```Oberon\n    i := 0;\n    WHILE i < 10 DO\n       i := i + 1;\n       Out.Int(i, 1);Out.String(\" \");\n    END;\n```\n\nA while, elsif loop, counting 1 to 10, then 10 to 100 by 10.\n\n```Oberon\n    i := 0;\n    WHILE i < 10 DO\n       i := i + 1;\n       Out.Int(i, 1); Out.String(\" \");\n    ELSIF i < 100 DO\n       i := i + 10;\n       Out.Int(i, 1);Out.String(\" \");\n    END;\n```\n\n\n## FOR Loops\n\nThe FOR loop in Oberon is very similar to modern FOR loops.\nThe FOR loop increments an integer value with in a range.\nYou the default increments the start value by 1 but if a \nBY clause is included you can control how the increment value\nworks.\n\nRegular for loop, `i` is incremented by 1.\n\n```Oberon\n    FOR i := 1 TO 10 DO\n       Out.Int(i, 1);Out.String(\" \");\n    END;\n```\n\nUsing a BY clause incrementing `i` by 2.\n\n```Oberon\n    FOR i := 0 TO 20 BY 2  DO\n       Out.Int(i, 1);Out.String(\" \");\n    END;\n```\n\n\n## Putting it all together\n\nThe following [module](LoopsAndConditions.Mod) demonstrates\nthe conditional and loop syntax.\n\n```Oberon\n    MODULE LoopsAndConditions;\n      IMPORT In, Out;\n    \n    PROCEDURE IfElsifElseDemo;\n      VAR s : ARRAY 128 OF CHAR;\n    BEGIN\n      Out.String(\"Enter your name: \");\n      In.Line(s);\n      Out.Ln;\n      IF (s = \"Freda\") OR (s = \"Mojo\") THEN\n        Out.String(\"Wowie Zowie! I remember you from ZBS stories.\");Out.Ln;\n      ELSIF (s = \"Bilbo\") OR (s = \"Gandolf\") THEN\n        Out.String(\"Greets, I remember from the Hobbit.\");Out.Ln;\n      ELSE\n        Out.String(\"Glad to meet you \");Out.String(s);Out.Ln;\n      END;\n    END IfElsifElseDemo;\n    \n    PROCEDURE AssertDemo;\n      VAR s : ARRAY 128 OF CHAR;\n    BEGIN\n      Out.String(\"Should I continue? Y/n \");\n      In.Line(s);\n      Out.Ln;\n      ASSERT((s = \"Y\") OR (s = \"y\"));\n    END AssertDemo;\n    \n    PROCEDURE WhileDemo;\n      VAR i : INTEGER;\n    BEGIN\n      Out.String(\"Basic WHILE counting from 1 to 10\");Out.Ln;\n      i := 0;\n      WHILE i < 10 DO\n         i := i + 1;\n         Out.Int(i, 1);Out.String(\" \");\n      END;\n      Out.Ln;\n      Out.String(\"WHILE ELSIF, count 1 to 10 THEN 10 to 100\");Out.Ln;\n      i := 0;\n      WHILE i < 10 DO\n         i := i + 1;\n         Out.Int(i, 1); Out.String(\" \");\n      ELSIF i < 100 DO\n         i := i + 10;\n         Out.Int(i, 1);Out.String(\" \");\n      END;\n      Out.Ln;\n      Out.String(\"Demo of while loop counting one to ten, then by tenths.\");\n    END WhileDemo;\n    \n    PROCEDURE ForDemo;\n      VAR i : INTEGER;\n    BEGIN\n      Out.String(\"Basic FOR LOOP counting from 1 to 10\");Out.Ln;\n      FOR i := 1 TO 10 DO\n         Out.Int(i, 1);Out.String(\" \");\n      END;\n      Out.Ln;\n      Out.String(\"FOR loop counting by twos 1 to 20\");Out.Ln;\n      FOR i := 0 TO 20 BY 2  DO\n         Out.Int(i, 1);Out.String(\" \");\n      END;\n      Out.Ln;\n    END ForDemo;\n    \n    BEGIN\n      IfElsifElseDemo;\n      AssertDemo;\n      WhileDemo;\n      ForDemo;\n    END LoopsAndConditions.\n```\n\n\n### Next and Previous\n\n+ Next [Combining Oberon-07 and C with OBNC](../../05/01/Combining-Oberon-and-C.html)\n+ Previous [Basic Types](../18/Mostly-Oberon-Basic-Types.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-04-19",
        "keywords": [
          "Oberon",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 4,
        "series": "Mostly Oberon",
        "title": "Oberon Loops and Conditions"
      },
      "url": "posts/2020/04/19/Mostly-Oberon-Loops-and-Conditions.json"
    },
    {
      "content": "\n\n# Combining Oberon-07 and C with OBNC\n\nBy R. S. Doiel, 2020-05-01\n\nThis is the fifth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html)\nseries. Mostly Oberon documents my exploration of the Oberon\nLanguage, Oberon System and the various rabbit holes I will\ninevitably fall into.\n\nIn my day job I write allot of code in Go and\norchestration code in Python.  It's nice having\nthe convenience of combining code written one\nlanguage with an another.  You can do the same\nwith [OBNC](https://miasap.se/obnc/).  The OBNC\ncompiler supports inclusion of C code in a\nstraight forward manner. In fact Karl's compiler\nwill generate the C file for you!\n\nIn learning how to combine C code and Oberon-07\nI started by reviewing Karl's [manual page](https://miasap.se/obnc/man/obnc.txt).\nThe bottom part of that manual page describes\nthe steps I will repeat below. The description\nsounds more complicated but when you walk through\nthe steps it turns out to be pretty easy.\n\n## Basic Process\n\nCreating a C extension for use with OBNC is very\nstraight forward.\n\n1. Create a Oberon module with empty exported procedures\n2. Create a Oberon test module that uses your module\n3. Compile your test module with OBNC\n4. Copy the generated module `.c` file to the same directory as your Oberon module source\n5. Edit the skeleton `.c`,  re-compile and test\n\nFive steps may sound complicated but in practice is\nstraight forward.\n\n## Fmt, an example\n\nIn my demonstration of Karl's instructions I will be\ncreating a module named `Fmt` that includes two\nprocedures `Int()` and `Real()` that let you use\na C-style format string to format an INTEGER\nor REAL as an ARRAY OF CHAR. We retain the idiomatic\nway Oberon works with types but allow a little more\nflexibility in how the numbers are converted and\nrendered as strings.\n\n### Step 1\n\nCreate [Fmt.Mod](Fmt.Mod) defining two exported procedures\n`Int*()` and `Real*()`. The procedures body should be\nempty. Karl's practice is to use exported comments to\nexplain the procedures.\n\n\n~~~ {.oberon}\n\n    MODULE Fmt;\n\n    \tPROCEDURE Int*(value : INTEGER; fmt: ARRAY OF CHAR;\n                       VAR dest : ARRAY OF CHAR);\n    \tEND Int;\n\n    \tPROCEDURE Real*(value : REAL; fmt: ARRAY OF CHAR;\n                        VAR dest : ARRAY OF CHAR);\n    \tEND Real;\n\n    BEGIN\n    END Fmt.\n\n~~~\n\n\n### Step 2\n\nCreate a test module, [FmtTest.Mod](FmtTest.Mod), for\n[Fmt.Mod](Fmt.Mod).\n\n\n~~~ {.oberon}\n\n    MODULE FmtTest;\n      IMPORT Out, Fmt;\n\n    PROCEDURE TestInt(): BOOLEAN;\n      VAR\n        fmtString : ARRAY 24 OF CHAR;\n        dest : ARRAY 128 OF CHAR;\n        i : INTEGER;\n    BEGIN\n        i := 42;\n        fmtString := \"%d\";\n        Fmt.Int(i, fmtString, dest);\n        Out.String(dest);Out.Ln;\n        RETURN TRUE\n    END TestInt;\n\n    PROCEDURE TestReal(): BOOLEAN;\n      VAR\n        fmtString : ARRAY 24 OF CHAR;\n        dest : ARRAY 128 OF CHAR;\n        r : REAL;\n    BEGIN\n        r := 3.145;\n        fmtString := \"%d\";\n        Fmt.Real(r, fmtString, dest);\n        Out.String(dest);Out.Ln;\n        RETURN TRUE\n    END TestReal;\n\n    BEGIN\n      ASSERT(TestInt());\n      ASSERT(TestReal());\n      Out.String(\"Success!\");Out.Ln;\n    END FmtTest.\n\n~~~\n\n\n### Step 3\n\nGenerate a new [Fmt.c](Fmt.c) by using the\nOBNC compiler.\n\n\n~~~ {.shell}\n\n    obnc FmtTest.Mod\n    mv .obnc/Fmt.c ./\n\n~~~\n\n\nthe file `.obnc/Fmt.c` is your C template file. Copy it\nto the directory where Fmt.Mod is.\n\n### Step 4\n\nUpdate the skeleton `Fmt.c` with the necessary C code.\nHere's what OBNC generated version.\n\n\n~~~ {.c}\n\n    /*GENERATED BY OBNC 0.16.1*/\n\n    #include \"Fmt.h\"\n    #include <obnc/OBNC.h>\n\n    #define OBERON_SOURCE_FILENAME \"Fmt.Mod\"\n\n    void Fmt__Int_(OBNC_INTEGER value_, const char fmt_[], \n                   OBNC_INTEGER fmt_len, char dest_[], \n                   OBNC_INTEGER dest_len)\n    {\n    }\n\n\n    void Fmt__Real_(OBNC_REAL value_, const char fmt_[],\n                    OBNC_INTEGER fmt_len, char dest_[],\n                    OBNC_INTEGER dest_len)\n    {\n    }\n\n\n    void Fmt__Init(void)\n    {\n    }\n\n~~~\n\n\nHere's the skeleton revised with do what we need to be done.\n\n\n~~~ {.c}\n\n    #include \".obnc/Fmt.h\"\n    #include <obnc/OBNC.h>\n    #include <stdio.h>\n\n    #define OBERON_SOURCE_FILENAME \"Fmt.Mod\"\n\n    void Fmt__Int_(OBNC_INTEGER value_, \n                   const char fmt_[], OBNC_INTEGER fmt_len,\n                   char dest_[], OBNC_INTEGER dest_len)\n    {\n        sprintf(dest_, fmt_, value_);\n    }\n\n\n    void Fmt__Real_(OBNC_REAL value_, const char fmt_[],\n                    OBNC_INTEGER fmt_len, char dest_[],\n                    OBNC_INTEGER dest_len)\n    {\n        sprintf(dest_, fmt_, value_);\n    }\n\n\n    void Fmt__Init(void)\n    {\n    }\n\n~~~\n\n\nNOTE: You need to change the path for the `Fmt.h` file reference.\nI also add the `stdio.h` include so I have access to the C\nfunction I wish to use. Also notice how OBNC the signature\nfor the functions use the `_` character to identify mapped values\nas well as the char arrays being provided with a length parameter.\nIf you are doing more extensive string work you'll want to take\nadvantage of these additional parameters so insure that the\nas strings are terminated properly for Oberon's reuse.\n\n\n### Step 5\n\nRecompile and test.\n\n\n~~~ {.shell}\n\n    obnc FmtTest.Mod\n    ./FmtTest\n\n~~~\n\n\n### Next and Previous\n\n+ Next [Compiling OBNC on macOS](../06/Compiling-OBNC-on-macOS.html)\n+ Previously [Oberon Loops and Conditions](../../04/19/Mostly-Oberon-Loops-and-Conditions.html)\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-05-01",
        "keywords": [
          "Oberon",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 5,
        "series": "Mostly Oberon",
        "title": "Combining Oberon-07 and C with OBNC"
      },
      "url": "posts/2020/05/01/Combining-Oberon-and-C.json"
    },
    {
      "content": "\nCompiling OBNC on macOS \n=======================\n\nBy R. S. Doiel, 2020-05-06\n\nThis is the sixth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.\n\nCompiling OBNC v0.16.1 on macOS (10.13.6) using MacPorts (2.6.2) \nis straight forward if you have the required dependencies and \nenvironment setup up. Below are my notes to get everything working.\n\n## Prerequisites\n\n+ OBNC v0.16.1\n+ SDL v1.2\n+ Boehm-Demers-Weiser GC\n+ A C compiler and linker (OBNC uses this to generate machine specific code)\n\n### SDL 1.2\n\nMacPorts has libsdl 1.2 available as a package called \"libsdl\"\n(not surprisingly). There are other versions of the SDL available\nin ports but this is the one we're using.\n\n\n~~~\n\n   sudo port install libsdl\n\n~~~\n\n\n### The Boehm-Demers-Weiser GC\n\nYou need to install the Boehm-Demers-Weiser GC installed. Using\nMacPorts it is almost as easy as installing under Debian. The\npackage is less obviously named than under Debian. The package\nyou want is \"beohmgc\".\n\n\n~~~\n\n    sudo port install boehmgc\n\n~~~\n\n\nMore info on the GC.\n\n+ [The Boehm-Demers-Weiser GC](https://www.hboehm.info/gc/)\n+ [MacPorts page](https://ports.macports.org/port/boehmgc/summary)\n\n### C compiler and linker\n\nXCode is provides a C compiler and linker. That is what is installed on my\nmachine. It can be a bit of a pain at times with obscure errors, particularly with regards to the linker. Your milleage may very. I know you can\ninstall other C compilers (e.g. Clang) but I haven't tried them yet.\n\n## Setting up my environment\n\nYou need to update your CC variables to find the header and\nshared library files for compilation of obnc with `build`. (I added\nthese to my `.bash_profile`). New Macs ships with zsh and\nyour settings might be in a different location. MacPorts puts \nits libraries under the `/opt/local` directory.\n\n\n~~~\n\n    export C_INCLUDE_PATH=\"/usr/include:/usr/local/include:/opt/local/include\"\n    export LIBRARY_PATH=\"/usr/lib:/usr/local/lib:/opt/local/lib\"\n    export LD_LIBRARY_PATH=\"/usr/lib:/usr/local/lib:/opt/local/lib\"\n\n~~~\n\n\n## OBNC environment variables\n\nThis follows' Karl's docs.  Additionally if you install OBNC extlib so\nyou can do POSIX shell programs you'll need to set your\n`OBNC_IMPORT_PATH` environment.  An example of including a default\ninstall location and local home directory location.\n\n\n~~~\n\n    export OBNC_IMPORT_PATH=\"/usr/local/lib/obnc:$HOME/lib/obnc\"\n\n~~~\n\n\n### Next and Previous\n\n+ Next [Oberon-07 and the file system](../09/Oberon-07-and-the-filesystem.html)\n+ Previous [Combining Oberon-07 and C](../01/Combining-Oberon-and-C.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-05-06",
        "keywords": [
          "Oberon",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 6,
        "series": "Mostly Oberon",
        "title": "Compiling OBNC on macOS"
      },
      "url": "posts/2020/05/06/Compiling-OBNC-on-macOS.json"
    },
    {
      "content": "\n\n# Oberon-07 and the file system\n\nBy R. S. Doiel, 2020-05-09 (updated: 2021-10-29)\n\nThis is the seventh post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.\n\n## Working with files in Oberon-07\n\nIn a POSIX system we often talk of opening files,\nwriting and reading files and close files. The Oberon\nlanguage reflects a more Oberon System point of view.\n\nThe Oberon System generally avoids modality in favor\nof action. Modality is where a context must be set\nbefore a set of actions are possible. The `vi` \ntext editor is a \"modal\" editor. You are in either\nedit (typing) mode or command mode. At the function\nlevel POSIX's `open()`, is also modal. You can \nopen a file for reading, open a file for writing,\nyou can open a file for appending, etc. The Oberon\nlanguage and base modules avoids modality.\n\nThe Oberon System is highly interactive but\nhas a very different idea about code, data and computer\nresources. In POSIX the basic unit of code is a program\nand the basic unit of execution is a program. In Oberon\nthe basic unit of code is the module and the basic unit\nof execution is the procedure.  Modules are brought into \nmemory and persist. As a result it is common in \nthe Oberon System to need to have file representations \nthat can persist across procedure calls. It provides\na set of abstractions that are a little bit like views\nand cursors found in database systems. In taking\nthis approach Oberon language eschews modality at the\nprocedure level. \n\nNOTE: Modules can be explicitly unload otherwise they persist until the computer is turned off\n\n## Key Oberon Concepts\n\nThe following are exported in the `Files` module.\n\nFile\n: is a handle to the representation of a file, a File and Rider form a view into a file.\n\nRider\n: similar to a database cursor, it is the mechanism that lets you navigate in a file\n\nNew\n: Creates a new file (in memory but not on disc).\n\nRegistration\n: Associates a file handle created with New with the file system. A file must be registered to persist in the file system.\n\nOld\n: Opens an existing file for use.\n\nSet\n: Set the position of a rider in a file\n\nPos\n: Gets the position of a rider in a file\n\nClose\n: Writes out unwritten buffers in file to disc, file handle is still value as is the rider.\n\nPurge\n: Sets a file's length to zero.\n\nDelete\n: Unregister the filename with the file system.\n\nIn the Oberon Systems a file can be \"opened\" many\ntimes with only one copy maintained in memory. This allows\nefficient operations across a module's procedures.\nLikewise a file can have one or more Riders associated with\nit. Each rider can move through the file independently operating on\nthe common in memory file. If a file is created with `New` but\nnot registered it can be treated like an in-memory temp file.\nClosing a file writes its buffers but the file remains accessible\nthrough it handle and riders. If a file is not modified it\ndoesn't need to be closed.\n\nIn POSIX we generally want to explicitly close the file when\nwe're through. In the Oberon language we only need to close\na file if we've modified it.\n\nThe behavior of files and riders in Oberon creates interesting\nnuances around deleting files.  The Delete operation can in\nprinciple happen multiple times before the file is deleted on\ndisc.  That is because the file handles and riders may still\nbe operating on it.  To know when a file is finally deleted \nwhen `Delete` procedure is called it includes a results\nparameter. When that value is set to zero by the `Delete`\nprocedure you know your file has been deleted.\n\nThe `Files` module provides a number of methods\nto read and write basic Oberon types. These use the rider\nrather than the file handle. Calling them automatically\nupdates the riders position. The procedures themselves\nmap to what we've seen in the modules `In` and `Out`.  \nThere are a few additional commands for file system house \nkeeping such as `Length`, `GetDate`, `Base`.\nSee the OBNC documentation for the `Files` module for\ndetails <https://miasap.se/obnc/obncdoc/basic/Files.def.html>.\n\nIn the following examples we'll be using the `Files`\nmodule to create, update and delete a file called \n`HelloWorld.txt`.\n\n### Creating a file\n\nThe recipe we want to follow for creating a file is\nNew (creates an empty file in memory), Register\n(associations the filename with the file system), \nSet the rider position, with the rider write our\ncontent and with the file call close because we've\nhave changed the file.\n\nLike our origin `SayingHi` we'll demonstrate this code\nin a new module called `TypingHi.Mod`. Below is\na procedure called `WriteHelloWorld`. It shows how\nto create, write and close the new file called\n\"HelloWorld.txt\".\n\n\n~~~\n\n  PROCEDURE WriteHelloWorld;\n    VAR\n      (* Define a file handle *)\n      f : Files.File;\n      (* Define a file rider *)\n      r : Files.Rider;\n  BEGIN\n    (* Create our file, New returns a file handle *)\n    f := Files.New(\"HelloWorld.txt\");\n    (* Register our file with the file system *)\n    Files.Register(f);\n    (* Set the position of the rider to the beginning *)\n    Files.Set(r, f, 0);\n    (* Use the rider to write out \"Hello World!\" *)\n    Files.WriteString(r, \"Hello World!\");\n    (* Write a end of line *)\n    Files.Write(r, 10);\n    (* Close our modified file *)\n    Files.Close(f);\n  END WriteHelloWorld;\n\n~~~\n\n\n#### Receipt in review\n\n+ New, creates our file\n+ Register, associates the file handle with the file system \n+ Set initializes the rider's position\n+ WriteString, writes our \"Hello World!\" to the file\n+ Close, closes the file, flushing unwritten content to disc\n\n\n### Working with an existing file\n\nIf we're working with an existing file we swap `New` for \na procedure named `Old`. We don't need to register the\nfile because it already exists.  We still need to set\nour rider and we want to read back the string we previously wrote.\nWe don't need to close it because we haven't\nmodified it. To demonstrate a new procedure is added to\nour module called `ReadHelloWorld`.\n\n\n~~~\n\n  PROCEDURE ReadHelloWorld;\n    VAR\n      f : Files.File;\n      r : Files.Rider;\n      (* We need a string to store what we have read *)\n      src : ARRAY 256 OF CHAR;\n  BEGIN\n    (* Create our file, New returns a file handle *)\n    f := Files.Old(\"HelloWorld.txt\");\n    (* Set the position of the rider to the beginning *)\n    Files.Set(r, f, 0);\n    (* Use the rider to write out \"Hello World!\" *)\n    Files.ReadString(r, src);\n    (* Check the value we read, if it is not the name, \n       halt the program with an error *)\n    ASSERT(src = \"Hello World!\");\n  END ReadHelloWorld;\n\n~~~\n\n\n#### Receipt in review\n\n+ Old, gets returns a file handle for an existing file\n+ Set initializes the rider's position\n+ ReadString, read our \"Hello World!\" string from the file\n+ Check the value we read with an ASSERT\n\n### Removing a file\n\nDeleting the file only requires knowing the name of the file.\nLike in `ReadHelloWorld` we'll use the built-in ASSERT\nprocedure to check if the file was successfully removed.\n\n\n~~~\n\n  PROCEDURE DeleteHelloWorld;\n    VAR\n      result : INTEGER;\n  BEGIN\n    (* Delete our file *)\n    Files.Delete(\"HelloWorld.txt\", result);\n    (* Check our result, if not zero then halt program with error *)\n    ASSERT(result = 0);\n  END DeleteHelloWorld;\n\n~~~\n\n\n#### Receipt in review\n\n+ Delete the file setting a result value\n+ Check the value with ASSERT to make sure it worked\n\n## Putting it all together.\n\nHere is the full listing of our module.\n\n\n~~~\n\n    MODULE TypingHi;\n      IMPORT Files;\n    \n      PROCEDURE WriteHelloWorld;\n        VAR\n          (* Define a file handle *)\n          f : Files.File;\n          (* Define a file rider *)\n          r : Files.Rider;\n      BEGIN\n        (* Create our file, New returns a file handle *)\n        f := Files.New(\"HelloWorld.txt\");\n        (* Register our file with the file system *)\n        Files.Register(f);\n        (* Set the position of the rider to the beginning *)\n        Files.Set(r, f, 0);\n        (* Use the rider to write out \"Hello World!\" *)\n        Files.WriteString(r, \"Hello World!\");\n        (* Write a end of line *)\n        Files.Write(r, 10);\n        (* Close our modified file *)\n        Files.Close(f);\n      END WriteHelloWorld;\n    \n      PROCEDURE ReadHelloWorld;\n        VAR\n          f : Files.File;\n          r : Files.Rider;\n          (* We need a string to store what we have read *)\n          src : ARRAY 256 OF CHAR;\n      BEGIN\n        (* Create our file, New returns a file handle *)\n        f := Files.Old(\"HelloWorld.txt\");\n        (* Set the position of the rider to the beginning *)\n        Files.Set(r, f, 0);\n        (* Use the rider to write out \"Hello World!\" *)\n        Files.ReadString(r, src);\n        (* Check the value we read, if it is not the name, \n           halt the program with an error *)\n        ASSERT(src = \"Hello World!\");\n      END ReadHelloWorld;\n    \n      PROCEDURE DeleteHelloWorld;\n        VAR\n          result : INTEGER;\n      BEGIN\n        (* Delete our file *)\n        Files.Delete(\"HelloWorld.txt\", result);\n        (* Check our result, if not zero then halt program with error *)\n        ASSERT(result = 0);\n      END DeleteHelloWorld;\n    \n    BEGIN\n        WriteHelloWorld();\n        ReadHelloWorld();\n        DeleteHelloWorld();\n    END TypingHi.\n\n~~~\n\n## Post Script, 2021-10-29\n\nOn October 29, 2021 I had an email conversation with Jules. It pointed out a problem in this implementation of Hello World.  I had incorrectly coded the end of line with `Files.WriteString(r, 0AX);` this is wrong.  At best it should have been `Files.Write(r, 10);`. There remains an issues with `Files.WriteString(\"Hello World!\");`. The Oakwood module `Files` defines \"WriteString\" to include the trailing NULL character. This will be output in the file. It all depends on how close to the Oakwood specification that your compiler implements the `Files` module.\n\n\n\n### Next and Previous\n\n+ Next [Dynamic Types](../25/Dynamic-types.html)\n+ Previous [Compiling OBNC on macOS](../06/Compiling-OBNC-on-macOS.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-05-09",
        "keywords": [
          "Oberon",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 7,
        "series": "Mostly Oberon",
        "title": "Oberon-07 and the file system",
        "updated": "2021-10-29"
      },
      "url": "posts/2020/05/09/Oberon-07-and-the-filesystem.json"
    },
    {
      "content": "\n\n# Dynamic types\n\nBy R. S. Doiel, 2020-05-25\n\nThis is the eighth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html)\nseries. Mostly Oberon documents my exploration of the Oberon Language, \nOberon System and the various rabbit holes I will inevitably fall into.\n\n## Dynamic Types in Oberon\n\nOberon-07 is a succinct systems language. It provides a minimal\nbut useful set of basic static types. Relying on them addresses \nmany common programming needs. The Oberon compiler ensures \nstatic types are efficiently allocated in memory. One of the \nstrengths of Oberon is this ability to extend the type system. \nThis means when the basic types fall short you can take \nadvantage of Oberon's type  extension features. This includes \ncreating dynamically allocated data structures. In Oberon-07 \ncombining Oberon's `POINTER TO` and `RECORD` types allows us to\ncreate complex and dynamic data structures. \n\n\n## An example, dynamic strings \n\nStrings in Oberon-07 are typical declared as an `ARRAY OF CHAR` \nwith a specific length. If the length of a string is not \nknown a head of time this presents a challenge. One approach is \nto declare a long array but that would allocate allot of memory \nwhich may not get used. Another approach is to create a dynamic\ndata structure. An example is using a linked list of shorter \n`ARRAY OF CHAR`.  The small fixed strings can combine to \nrepresent much larger strings. When one fills up we add \nanother. \n\n### Pointers and records, an Oberon idiom \n\nOur data model is a pointer to a record where the record \ncontains an `ARRAY OF CHAR` and a pointer to the next record. \nA common idiom in Oberon for dynamic types is to declare a \n`POINTER TO` type and declare a `RECORD` type which contains\nthe `POINTER TO` type as an attribute.  If you see this idiom \nyou are looking at some sort of dynamic data structure. The \npointer type is usually named for the dynamic type you want \nwork with and the record type is declared using the same name \nwith a \"Desc\" suffix. In our case `DynamicString` will be the \nname of our `POINTER TO` type and our record type will be \ncalled `DynamicStringDesc` following the convention.  In our \nrecord structure we include a \"value\" to holding a short \nfixed length `ARRAY OF CHAR`  and a \"next\" to holding the \npointer to our next record.\n\nIn our record the value is declared as a static type. We need\nto know how long our \"short\" string should be? I.e. What length\nis our `ARRAY OF CHAR`? It's a question of tuning. If it is too \nshort we spend more time allocating new records, too long and \nwe are wasting memory in each record. A way to make tuning a \nlittle simpler is to use a constant value to describe our array \nlength. Then if we decide our array is too big \nor too small we can adjust the constant knowing that our record \nstructure and the procedures that use that the length \ninformation will continue to work correctly. \n\nLet's take a look at actual code (NOTE: vSize is our constant value). \n\n~~~\n\n    CONST\n      vSize = 128; \n    \n    TYPE\n      DynamicString* = POINTER TO DynamicStringDesc;\n      DynamicStringDesc* = RECORD \n        value : ARRAY vSize OF CHAR; \n        next : DymamicString; \n      END;\n\n~~~\n\nNOTE: Both `DynamicString` and `DynamicStringDesc` are defined \nusing an `*`. These are public and will be available \nto other modules.  Inside our record `DynamicStringDesc` we \nhave two private to our module attributes, `.value` and \n`.next`. They are private so that we can change our \nimplementation in the future without requiring changes in \nmodules that use our dynamic strings. Likewise our constant `vSize`\nis private as that is an internal implementation detail. This\npractice is called information hiding.\n\nNOTE: The asterisk in Oberon decorates procedures, types, variables\nand constants that are \"public\" to other modules.\n\nNOTE: Variables are always exported read only.\n\nNOTE: With information hiding some details of implementation allow us \nto keep a clean division between implementation inside the module and how\nthat implementation might be used. With out information hiding we often\nhave \"leaky\" abstractions that become brittle and hard to maintain and\nrely on.\n\n\n\n## Working with DynamicString\n\nOur type definitions describe to the compiler how to layout our \ndata in memory. The type system in Oberon-07 also ensures that \naccess to that memory is restricted to assignments, operations \nand procedures compatible with that type. To be useful from \nother modules we need a few procedures to help work with\nthis new data type. What follows is a minimal set needed to be \nuseful.\n\n### `New*(VAR str : DynamicString)`\n\n`New` will initialize a DynamicString object setting `.value` to \nan empty string. \n\n\n~~~\n\n  PROCEDURE New*(VAR str : DynamicString);\n  BEGIN NEW(str);\n    str.value := \"\"; \n    str.next := NIL;\n  END New;\n\n~~~\n\n\n### `Set*(VAR str : DynamicString; source : ARRAY OF CHAR)` \n\n`Set` copies an `ARRAY OF CHAR` into an existing DynamicString. \nThis requires that we add and link additional records if the \n`source` is longer than our current dynamic string. Set is a \nbridge procedure between an existing datatype, `ARRAY OF CHAR` \nand our new data type, `DynamicString`.\n\n\n~~~\n\n  PROCEDURE Set*(VAR str : DynamicString; source : ARRAY OF CHAR); \n    VAR cur, next : DynamicString; tmp : ARRAY vSize OF CHAR; \n        i, l : INTEGER;\n  BEGIN cur := str; cur.value := \"\";\n    l := Strings.Length(source);\n    i := 0; \n    WHILE i < l DO\n      Strings.Extract(source, i, i + vSize, tmp);\n      Strings.Append(tmp, cur.value);\n      i := i + Strings.Length(tmp);\n      IF (i < l) THEN\n        IF cur.next = NIL THEN\n          New(next); cur.next := next;\n        END;\n        cur := cur.next;\n      END; \n    END;\n  END Set;\n\n~~~\n\n### `ToCharArray*(str : DynamicString; VAR dest : ARRAY OF CHAR; VAR ok : BOOLEAN)`\n\n`ToCharArray` copies the contents of our dynamic string into an array \nof char setting `ok` to TRUE on success or FALSE if truncated. \nLike `Set*` it is a bridge procedure to let us move data output \nour new dynamic string type.\n\n\n~~~\n\n  PROCEDURE ToCharArray*(str : DynamicString; \n                         VAR dest : ARRAY OF CHAR; \n                         VAR ok : BOOLEAN);\n    VAR cur : DynamicString; i : INTEGER;\n  BEGIN \n    ok := FALSE;\n    cur := str; i := 0;\n    WHILE cur # NIL DO\n      i := i + Strings.Length(cur.value);\n      Strings.Append(cur.value, dest);\n      cur := cur.next;\n    END;\n    ok := (i = Strings.Length(dest));\n  END ToCharArray;\n\n~~~\n\nTwo additional procedures will likely be needed-- `Append` and \n`AppendCharArray`. This first one is trivial, if we want to add \none dynamic string onto another all we need to do is link the \nlast record of the first and point it to a copy of the second string we're appending.\n\n\n### `Append*(extra : DynamicString; VAR dest : DynamicString);`\n\n`Append` adds the `extra` dynamic string to `dest` dynamic string. Our \n\"input\" is `extra` and our output is a modified dynamic string \nnamed `dest`. This parameter order mimics the standard \n`Strings` module's `Append`.\n\nNOTE: Oberon idiom is often input values, modified value and \nresult values. Modified and result values are declared in the parameter\ndefinition using `VAR`.\n\nAlgorithm:\n\n1. Move to the end of `dest` dynamic string\n2. Create a new record at `cur.next`.\n3. Copy `extra.value` info.value `cur.next.value`\n4. Advance `extra` and `cur`, repeating steps 2 to 4 as needed.\n\nImplemented procedure.\n\n~~~\n\n  PROCEDURE Append*(extra: DynamicString; VAR dest : DynamicString);\n    VAR cur : DynamicString;  \n  BEGIN\n    (* Move to the end of the dest DynamicString *)\n    cur := dest;\n    WHILE cur.next # NIL DO cur := cur.next; END;\n    (* Starting initial pointer of `extra` copy its records\n       input new records created in `cur`. *)\n    WHILE extra # NIL DO\n      (* Create a new record *)\n      NEW(cur.next);\n      cur.next.value := \"\";\n      cur.next.next := NIL;\n      (* Copy extra.value into new record *)\n      Strings.Extract(extra.value, 0, vSize, cur.next.value);\n      (* Advance to next record for both cur and extra *)\n      extra := extra.next;\n      cur := cur.next;\n    END;\n  END Append;\n\n~~~\n\nA second procedure for appending an `ARRAY OF CHAR` also \nbecomes trivial. First convert the `ARRAY OF CHAR` to a dynamic \nstring then append it with the previous procedure.\n\n### `AppendCharArray*(src : ARRAY OF CHAR; VAR str : DynamicString);`\n\nThis procedure appends an ARRAY OF CHAR to an existing dynamic string.\n\n~~~\n\n  PROCEDURE AppendCharArray*(extra: ARRAY OF CHAR; VAR dest : DynamicString);\n    VAR extraStr : DynamicString;    \n  BEGIN\n    (* Convert our extra ARRAY OF CHAR into a DynamicString *)\n    New(extraStr); Set(extraStr, extra);\n    (* Now we can append. *)\n    Append(extraStr, dest);\n  END AppendCharArray;\n\n~~~\n\nAt some point we will want to know the length of our dynamic string.\n\n### `Length(str : DynamicString) : INTEGER`\n\nOur `Length` needs to go through our linked list and total up \nthe length of each value. We will use a variable called `cur` \nto point at the current record and add up our total length as \nwe walk through the list.\n\n~~~\n\n  PROCEDURE Length*( source : DynamicString) : INTEGER;\n    VAR cur : DynamicString; total : INTEGER;\n  BEGIN\n    total := 0;\n    cur := source;\n    WHILE cur # NIL DO\n      total := total + Strings.Length(cur.value);\n      cur := cur.next;\n    END; \n    RETURN total\n  END Length;\n\n~~~\n\n## Extending DynamicStrings module\n\nWith these few procedures we have a basic means of working with \ndynamic strings. Moving beyond this we can look at the standard \nOberon `Strings` module for inspiration.  If we use similar \nprocedure signatures we can create a drop in replacement \nfor `Strings` with `DynamicStrings`.\n\nNOTE: Procedure signatures refer to procedures type along \nwith the order and types of parameters. A quick review of the \nprocedure signatures for the standard module [Strings](https://miasap.se/obnc/obncdoc/basic/Strings.def.html) is \nprovided by the [OBNC](https://miasap.se/obnc) compiler docs. \n\nLet's look at recreating `Insert` as a potential guide to\na more fully featured [\"DynamicStrings.Mod\"](DynamicStrings.Mod). \nIn our `Insert` we modify the procedure signature so the \nsource and destinations are dynamic strings.\n\n\n### `Insert(source : DynamicString; pos : INTEGER; VAR dest : DynamicString)`\n\nThe `Insert` procedure inserts a `source` dynamic string at the \nposition provided into our `dest` dynamic string. We are implementing\nthe same signature  for our `DynamicStrings.Insert()` as \n`Strings.Insert()`. Only the parameters for source and destination\nare changed to `DynamicString`.\n\nInternally our procedure for `Insert` is a more complicated than\nthe ones we've written so far. It needs to do all the housing \nkeeping for making sure we add the right content in the correct\nspot.  The general idea is to find the record holding the split \npoint. Split that record into two records. The first retains \nthe characters before the insert position. The second holds the \ncharacters after the insert position and points to next record \nin the dynamic string. Once the split is accomplished it then \nis a matter of linking everything up. The record before the \ninsert position is set to point at the dynamic string to be \ninserted, the inserted dynamic string is set to point at the \nrecord that contained the rest of the characters after the \nsplit.\n\nIt is easy to extract a sub-string from an `ARRAY OF CHAR` \nusing the standard `Strings` module.  We can store the characters\nin the `.value` of the record after the split in a temporary \n`ARRAY OF CHAR`.  The temporary `ARRAY OF CHAR` can be used to \ncreate a new dynamic string record which will be linked to the \nrest of our destination dynamic string. The record which held \nthe characters before the insert position needs to be truncated \nand it needs to be linked to the dynamic string we want to \ninsert. NOTE: This will leave a small amount of unused \nmemory.\n\nNOTE: If conserving memory is critical then re-packing the \ndynamic string could be implemented as another procedure. The \ncost would be complexity and time to shift characters between \nlater records and earlier ones replacing excess NULL values.\n\nWe need to find the record where the split will occur. In the \nrecord to be split we need to calculate a relative \nsplit point. We then can copy the excess characters in that \nsplit record to a new record and truncate the `.value`'s \n`ARRAY OF CHAR` to create our split point. Truncating is easy \nin that we replace the CHAR in the `.values` that are not \nneeded with a NULL character. We can do that with a \nsimple loop. Likewise calculating the relative insertion \nposition can be done by taking the modulo of the `vSize` of \n`.value`.\n\nNOTE: In Oberon stings are terminated with a NULL \ncharacter. A NULL character holds the ASCII value `0X`.\n\nOur algorithm:\n\n1. Set `cur` to point to the start of our destination dynamic string\n2. Move `cur` to the record in the link list where the insertion will take place\n3. Calculate the relative split point in `cur.value`\n4. Copy the characters in `cur.value` from relative split point to end of `.value` into a temporary `ARRAY OF CHAR`\n5. Make a new record, `rest`, using the temporary `ARRAY OF CHAR` and update the value of `.next` to match that of `cur.next`\n6. Truncate the record (cur) at the relative split point\n7. Set `cur.next` to point to our `extra` dynamic string.\n8. Move to the end of extra with `cur`\n9. Set the `cur.next` to point at `rest`\n\nOur procedure:\n\n~~~\n\n  PROCEDURE Insert*(extra : DynamicString; \n                    pos : INTEGER; \n                    VAR dest : DynamicString);\n    VAR cur, rest : DynamicString;\n        tmp : ARRAY vSize OF CHAR;\n        i, splitPos : INTEGER; continue : BOOLEAN;\n  BEGIN\n    (* 1. Set `cur` to the start of our `dest` dynamic string *)\n    cur := dest;\n\n    (* 2. Move to the record which holds the split point *)\n    i := 0;\n    continue := (i < pos);\n    WHILE continue DO\n      i := i + Strings.Length(cur.value);\n      continue := (i < pos);\n      IF continue & (cur.next # NIL) THEN\n        cur := cur.next;\n      ELSE\n        continue := FALSE;\n      END;\n    END;\n\n    (* 3. Copy the characters in `cur.value` from relative\n          split point to end of `.value` into a \n          temporary `ARRAY OF CHAR` *)\n    splitPos := pos MOD vSize;\n    Strings.Extract(cur.value, splitPos,\n                    Strings.Length(cur.value), tmp);\n\n    (* 4. Make a new record, `rest`, using the temporary \n          `ARRAY OF CHAR` and update the value of `.next` to\n          match that of `cur.next` *)\n    New(rest); Set(rest, tmp);\n    rest.next := cur.next;\n\n    (* 5. Truncate `cur.value` at the relative split point *)\n    i := splitPos;\n    WHILE i < LEN(cur.value) DO\n      cur.value[i] := 0X;\n      INC(i);\n    END;\n\n    (* 6. Set `cur.next` to point to our `extra`\n          dynamic string. *)\n    cur.next := extra;\n\n    (* 7. Move to the end of extra with `cur` *)\n    WHILE cur.next # NIL DO cur := cur.next; END;\n\n    (* 8. Set the `cur.next` to point at `rest` *)\n    cur.next := rest;\n  END Insert;\n\n~~~\n\nWhile our `Insert` is the longest procedure so far the steps \nare mostly simple. Additionally we can easily extend this to \nsupport inserting a more traditional `ARRAY OF CHAR` using our\npreviously established design pattern of converting a basic type\ninto our dynamic type before calling the dynamic version of the\nfunction.\n\n~~~\n\n  PROCEDURE InsertCharArray*(source : ARRAY OF CHAR; \n                             pos : INTEGER; \n                             VAR dest : DynamicString);\n    VAR extra : DynamicString;\n  BEGIN\n    New(extra); Set(extra, source);\n    Insert(extra, pos, dest);\n  END InsertCharArray;\n\n~~~\n\n## Where to go next\n\nIt is possible to extend our \"DynamicStrings.Mod\" into a drop \nin replacement for the standard `Strings`.  I've included a \nskeleton of that module as links at the end of this article \nwith stubs for the missing implementations such as `Extract`, \n`Replace`, `Pos`, and `Cap`.  I've also included a \n\"DynamicStringsTest.Mod\" for demonstrating how it works.\n\nThe procedure I suggest is to mirror `Strings` replacing the \nparameters that are `ARRAY OF CHAR` with `DynamicString`. It \nwill be helpful to include some bridging procedures that accept \n`ARRAY OF CHAR` as inputs too. These will use similar names \nwith a suffix of `CharArray`.\n\n## Parameter conventions and order\n\nOberon is highly composable. The trick to creating a drop in \nreplacement module is use the same parameter signatures so \nyou only need to make minor changes like updating the `IMPORT` \nstatement and using a module alias to map the old module to the\nnew one.  The parameter signatures in `Strings` follow a \nconvention you'll see in other Oberon modules. The parameter\norder is based on the \"inputs\", \"modify parameters\", and \n\"output parameters\". Inputs are non-`VAR` parameters. The \nremaining are `VAR` parameters. I think of \"modify parameters\" \nas those objects who reflect side effects. I think of \"output\" \nas values that in other languages would be returned by \nfunctions.  This is only a convention. A variation I've \nread in other Oberon modules is \"object\", \"inputs\", \"outputs\". \n\"object\" and \"outputs\" are `VAR` parameters and \"inputs\" are \nnot. This ordering makes sense when we think of records as \nholding an object. In both cases ordering is a convention \nand not enforced by the language.  Convention and consistency is \nhelpful but readability is the most important.  Oberon is a \nreadable language. It does not reward obfuscation. Readability is \na great virtue in a programming language. When creating your own \nmodules choose readability based on the concepts you want to\nemphasize in the module (e.g. procedural, object oriented).\n\n## The modules so far\n\nYou can read the full source for the module discussed along\nwith a test module in the links that follow.\n\n+ [DynamicStrings.Mod](DynamicStrings.Mod)\n+ [DynamicStringsTest.Mod](DynamicStringsTest.Mod)\n\n\n### Next and Previous \n\n+ Next [Procedures as parameters](../../06/20/Procedures-as-parameters.html)\n+ Previous [Oberon-07 and the file system](../09/Oberon-07-and-the-filesystem.html) \n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-05-25",
        "keywords": [
          "Oberon",
          "programming",
          "type extension",
          "dynamic data"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 8,
        "series": "Mostly Oberon",
        "title": "Dynamic types"
      },
      "url": "posts/2020/05/25/Dynamic-types.json"
    },
    {
      "content": "\n\n# Procedures as parameters\n\nBy R. S. Doiel, 2020-06-20\n\nThis is the ninth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.\nMostly Oberon documents my exploration of the Oberon Language, Oberon System and the \nvarious rabbit holes I will inevitably fall into.\n\nOberon-07 supports the passing of procedures as parameters in a procedure. \nLet's create a module name [Noises.Mod](Noises.Mod) to explore this.\n\nThe key to supporting this is Oberon's type system.  We need to decide what our \ngeneric procedure will look like first. In our case our procedures that will display \nan animal noise will include the name of the animal speaking.  We'll call this type \nof procedure \"Noise\". It'll accept an ARRAY OF CHAR for the name as a parameter \nthen use the standard Out module to display the animal name and noise they make.\n\n\n~~~\n\n    TYPE\n      Noise = PROCEDURE (who : ARRAY OF CHAR);\n\n~~~\n\n\nThe two \"Noise\" procedures will be \"BarkBark\" and \"ChirpChirp\". They will\nimplement the same parameter signature as describe in the \"Noise\" type.\n\n\n~~~\n\n    PROCEDURE BarkBark(who : ARRAY OF CHAR);\n    BEGIN\n      Out.String(who);\n      Out.String(\": Bark, bark\");Out.Ln();\n    END BarkBark;\n\n    PROCEDURE ChirpChirp(who : ARRAY OF CHAR);\n    BEGIN\n      Out.String(who);\n      Out.String(\": Chirp, chirp\");Out.Ln();\n    END ChirpChirp;\n\n~~~\n\n\nWe'll also create a procedure, MakeNoise, that accepts the animal name and\nour \"Noise\" procedure name and it'll call the \"Noise\" type procedure \npassing in the animal name.\n\n\n~~~\n\n    PROCEDURE MakeNoise(name : ARRAY OF CHAR; noise : Noise);\n    BEGIN\n      (* Call noise with the animal name *)\n      noise(name);\n    END MakeNoise;\n\n~~~\n\n\nIf we invoke MakeNoise with our animal name and pass the name of the \nprocedure we want to do the MakeNoise procedure will generate our\nnoise output. Here' is what is looks like all together.\n\n\n~~~\n\n    MODULE Noises;\n      IMPORT Out;\n    \n    TYPE \n      Noise = PROCEDURE(who : ARRAY OF CHAR);\n    \n    PROCEDURE BarkBark(who : ARRAY OF CHAR);\n    BEGIN\n      Out.String(who);\n      Out.String(\": Bark, bark\");Out.Ln();\n    END BarkBark;\n    \n    PROCEDURE ChirpChirp(who : ARRAY OF CHAR);\n    BEGIN\n      Out.String(who);\n      Out.String(\": Chirp, chirp\");Out.Ln();\n    END ChirpChirp;\n    \n    PROCEDURE MakeNoise(name : ARRAY OF CHAR; noise : Noise);\n    BEGIN\n      (* Call noise with the animal name *)\n      noise(name);\n    END MakeNoise;\n    \n    BEGIN\n      MakeNoise(\"Fido\", BarkBark);\n      MakeNoise(\"Tweety\", ChirpChirp);\n      MakeNoise(\"Fido\", ChirpChirp);\n      MakeNoise(\"Tweety\", BarkBark);\n    END Noises.\n\n~~~\n\n\nNote when we pass the procedures we include their name **without** parenthesis.\nOur type definition tells the compiler that the procedure can be a parameter,\nany procedure with the same signature, e.g. `who : ARRAY OF CHAR` as the\nonly parameter will be treated as a \"Noise\" type procedures. \n\n### Next and Previous \n\n+ Next [Procedures in Records](../../07/07/Procedures-in-records.html)\n+ Previous [Dynamic types](../../05/25/Dynamic-types.html) \n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-06-20",
        "keywords": [
          "Oberon",
          "procedures",
          "passing procedures as parameters",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 9,
        "series": "Mostly Oberon",
        "title": "Procedures as parameters"
      },
      "url": "posts/2020/06/20/Procedures-as-parameters.json"
    },
    {
      "content": "\n\n# Procedures in records\n\nBy R. S. Doiel, 2020-07-07\n\nThis is the tenth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.\nMostly Oberon documents my exploration of the Oberon Language, Oberon System and the \nvarious rabbit holes I will inevitably fall into.\n\nIn my last post I looked at how Oberon-07 supports the passing of procedures as parameters in a procedure. In this one I am looking at how we can\ninclude procedures as a part of an Oberon RECORD. \n\nLet's modify our module name [Noises.Mod](Noises.Mod) to explore this.\nCopy \"Noises.Mod\" to \"Creatures.Mod\". Replace the \"MODULE Noises;\" line with\n\"MODULE Creatures;\" and the final \"END Noises.\" statement with \"END Creatures.\".\n\n\n~~~\n\n    MODULE Creatures;\n    \n    (* rest of code here *)\n\n    END Creatures.\n\n~~~\n\n\nThe key to supporting records with procedures as record attributes is once again Oberon's type system.  The type `Noise` we created in the previous post can also be used to declare a record attribute similar to how we use this new type to pass the procedure. In this exploration will create a linked list of \"Creature\" types which include a \"MakeNoise\" attribute.\n\nFirst let's define our \"Creature\" as a type as well as a \n`CreatureList`. Add the following under our `TYPE` \ndefinition in [Creatures.Mod](Creatures.Mod).\n\n\n~~~\n\n    Creature = POINTER TO CreatureDesc;\n    CreatureDesc = RECORD\n                     name : ARRAY 32 OF CHAR;\n                     noises : Noises;\n                   END;\n\n~~~\n\n\nLet's create a new `MakeCreature` procedure that will create\na populate a single `Creature` type record.\n\n\n~~~\n\n    PROCEDURE MakeCreature(name : ARRAY OF CHAR; noise : Noise; VAR creature : Creature);\n    BEGIN\n      NEW(creature);\n      creature.name := name;\n      creature.noise := noise;\n    END MakeCreature;\n\n~~~\n\n\nNow lets modify `MakeNoise` to accept the `Creature` type RECORD\nrather than a name and a noise procedure parameter.\n\n\n~~~\n\n    PROCEDURE MakeNoise(creature : Creator);\n    BEGIN\n      creature.noise(creature.name);\n    END MakeNoise;\n\n~~~\n\n\nHow does this all work?  The two \"Noise\" procedures \n\"BarkBark\" and \"ChirpChirp\" remain as in our original \n\"Noises\" module. But our new `MakeNoise` procedure\nlooks takes a `Creature` record rather than accepting a\nname and procedure as parameters. This makes the code \na little more concise as well as lets you evolve the\ncreature record type using an object oriented approach.\n\nOur revised module should look like this.\n\n\n~~~\n\n    MODULE Noises;\n      IMPORT Out;\n    \n    TYPE \n      Noise = PROCEDURE(who : ARRAY OF CHAR);\n\n      Creature = RECORD\n                   name : ARRAY 32 OF CHAR;\n                   noises : Noises;\n                 END;\n    \n    VAR\n      dog, bird : Creature;\n\n    PROCEDURE BarkBark(who : ARRAY OF CHAR);\n    BEGIN\n      Out.String(who);\n      Out.String(\": Bark, bark\");Out.Ln();\n    END BarkBark;\n    \n    PROCEDURE ChirpChirp(who : ARRAY OF CHAR);\n    BEGIN\n      Out.String(who);\n      Out.String(\": Chirp, chirp\");Out.Ln();\n    END ChirpChirp;\n    \n    PROCEDURE MakeNoise(creature : Creature);\n    BEGIN\n      (* Call noise with the animal name *)\n      creature.noise(creature.name);\n    END MakeNoise;\n\n    PROCEDURE MakeCreature(name : ARRAY OF CHAR; noise : Noise; VAR creature : Creature);\n    BEGIN\n      NEW(creature);\n      creature.name := name;\n      creature.noise := noise;\n    END MakeCreaturel\n    \n    BEGIN\n      MakeCreature(\"Fido\", BarkBark, dog);\n      MakeCreature(\"Tweety\", ChirpChirp, bird);\n      MakeNoise(dog);\n      MakeNoise(bird);\n    END Noises.\n\n~~~\n\n\nWhere to go from here? Think about evolving [Creatures](Creatures.Mod) so\nthat you can create a dynamic set of creatures that mix and match their\nbehaviors. Another idea would be to add a \"MutateCreature\" procedure\nthat would let you change the noise procedure to something new.\n\n\n### Next and Previous \n\n+ Next [Portable Oberon-07](../../08/15/Portable-Oberon-07.html)\n+ Previous [Procedures as parameters](../../06/20/Procedures-as-parameters.html) \n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-07-07",
        "keywords": [
          "Oberon",
          "procedures",
          "record procedures",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 10,
        "series": "Mostly Oberon",
        "title": "Procedures in records"
      },
      "url": "posts/2020/07/07/Procedures-in-records.json"
    },
    {
      "content": "\n\n# Words Matter\n\nBy R. S. Doiel, 2020-07-08\n\nUPDATE (2020-08-15, RSD): When I added a post today I was VERY pleased to \nto see that GitHub now allows me to publish my blog via the \"main\" branch.\nIt's nice to see the change in the words we use.\n\n**Why does software development use the vocabulary of slavery and\nJim Crow to describe our creations?** What we call things matters.\nThis is especially true of the words we use day to day without thinking.\n\n```shell\n    git pull origin master\n```\n\n\"Naming things is a hard problem in computer science.\" That is\na phrase I remember from my student days. We name variables,\nprograms and algorithms. We name architectures. Naming is a choice.\nThe names convey meaning and intent. Names and terms are a human\ncommunication. They matter.\n\n```shell\n    git push origin master\n```\n\n\n## Names can change\n\nI remember the first time I encountered the terminology \"master/slave\"\ndescribing network and database architecture. I remember cringing at\nthe terms. I accepted the terminology because I was a student and\nnaively assumed that those terms were chosen innocently and did not\nmean what they did. I was wrong.\n\nExample MySQL command:\n\n```sql\n    CHANGE MASTER TO MASTER_HOST=host1,\n    MASTER_PORT=3002 FOR CHANNEL 'channel2';\n```\n\nWhen I did not challenge the use of those terms in computer science\nI became complicit in the status quo of systemic racism. I am not happy\nabout that. Not then and not now.  We need inclusive language\nin engineering. We need real diversity to find solutions to\ntoday's challenges.  In software engineering we very much control what\nwe call things. Software is an explicit form of written human\ncommunication. Words count. Words directly impact culture and our\ncommunity.\n\nExample MySQL commands:\n\n```sql\n    START SLAVE;\n    RESET SLAVE;\n    STOP SLAVE;\n```\n\nAs a direct benefactor of white male privilege I find the\nterminology of \"master/slave\" offensive.  I am certain those\nwords caused injury to those who did not benefit from the same white\nmale privilege.  Using \"master/slave\" terminology to describe database,\nreplication, network architectures or as used with version control\nsystems like Git is like polishing statues that glorify slavery.\nIt endorses inhumanity in a subtle and casual way.\n\nWe have a choice about how we communicate and what we communicate\nto convey meaning.  Let's use better words. It is time to change\nsome.\n\n\n## Practice Change\n\n**To change our community's vocabulary we need to thoughtfully choose terms**.\nMy friends and colleagues introduced me to using the term \"main\" as\na better descriptive word than \"master\" in Git repositories. When\nI started making the branch name changes in my Git repositories I ran\nacross a problem at GitHub.\n\n\n## Note the problem\n\nRecently GitHub made it possible to easily change the default branch.\nFor a number of years you could change the published documentation\nbranch from \"gh-pages\" to something you prefer.\nWhat you can't do is change the name of the branch used to publish\npersonal and group websites.  **GitHub explicitly states that the\npublication branch must be named \"master\".** I tested this and\nconfirmed the documentation is accurate as of today (2020-07-08, morning).\n\nWith the recent improvements in GitHub to allow default branches to\nbe named better it seems odd that you still have to publish\npersonal and group pages to \"master\" in order to publish a website. It\nnever made sense to me that person and group pages didn't use the default\nof \"gh-pages\" in the first place.\n\n\n## Taking action\n\nMorning (2020-07-08): I submitted a ticket to GitHub asking to have\nthe option of using another word besides \"master\" for the publication\nbranch in publishing my personal GitHub pages (i.e. this blog).\nUnfortunately the ticket doesn't have a public URL. I don't know how many\nother people have submitted similar requests. If only a few people\nhave requested this recently it will not be changed. Such is the nature\nof systemic problems.\n\nPlease help improve the words and names we use in software.\nI believe it can make a difference in creating a more inclusive and\nequitable community and profession. Raise the issue when it comes to\nyour attention. Silence becomes acceptance with systemic problems.\n\n## A reply\n\nEvening (2020-07-08): I got a reply today from Steve G of customer\nsupport. Not sure if the reply is a bot or human.  It was a nice polite\nreply (I wrote a nice polite ticket).  Steve mentioned that CEO Nat\nFriedman had addressed this on twitter and to follow the GitHub blog\nnews.  Steve said dropping master was a priority for GitHub but there\nis no time line for implementation.\n\nI am not sure how you can develop software with a priority in mind and\nnot have a sense of time it takes to implement it.  I mentioned that in my\nresponse to Steve G.\n\nAfter some Duck Duck searching I found a [BBC article](https://www.bbc.com/news/technology-53050955) dated June 15th, 2020 where Nat made the\nannouncement about Microsoft's GitHub making the change away from \"master\".\nNext week is July 15th. It will be interesting to see how much of an unscheduled priority this change is.\n\nI am skeptical.  If Steve G had indicated that they are actively working\nthe problem and provided a general time range I would be more inclined to\ngive Nat, GitHub and Microsoft the benefit of the doubt.  Given the rest\nof the content I read on Nat's twitter feed I don't think this is a priority\nbeyond press, publicity and stock price.\n\n\n## Where to go from here?\n\nJust as many sites have adopted more gender neutral terms in documentation\npractice we should encourage better descriptive terms for our algorithms,\nand architectures. If you run into terms perpetuating exclusion please\nspeak up.  Most of the web runs on web servers and databases.  Like GitHub\nthose software projects frequently use the terminology of \"master/slave\".\nIt is especially prevalent in documentation about replication. Blindly\nperpetuating the \"master/slave\" terminology to describe distributed\nsoftware and architectures is like polishing a statue to the old\nConfederacy. It can and should be change. We can communicate better\nwithout perpetuating the vocabulary of Jim Crow, segregation, slavery\nand oppression.\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-07-08",
        "keywords": [
          "civil rights",
          "diversity",
          "equality"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "title": "Words Matter"
      },
      "url": "posts/2020/07/08/words-matter.json"
    },
    {
      "content": "\n\n# Portable Oberon-07\n\n## using OBNC modules\n\nThis is the eleventh post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.\nMostly Oberon documents my exploration of the Oberon Language, Oberon System and the\nvarious rabbit holes I will inevitably fall into.\n\n## Working with standard input\n\nBy R. S. Doiel, 2020-08-15 (updated: 2020-09-05)\n\nKarl Landström's [OBNC](https://miasap.se/obnc/), Oberon-07 compiler,\ncomes with an Oberon-2 inspired set of modules\ndescribed in the Oakwood Guidelines as well as\nseveral very useful additions making Oberon-07 suitable for\nwriting programs in a POSIX environment.  We're going to\nexplore three of the Oakwood modules and two of Karl's own additions\nin this post as we create a program called [SlowCat](SlowCat.Mod).\nI am using the term \"portable\" to mean the code can be compiled\nusing OBNC on macOS, Linux, and Raspberry Pi OS and Windows 10\n(i.e. wherever OBNC is available). The Oakwood Guideline modules\nfocus on portability between an Oberon System and other systems.\nI'll leave that discussion along with\n[POW!](http://www.fim.uni-linz.ac.at/pow/pow.htm)\nto the end of this post.\n\n\n### SlowCat\n\nRecently while I was reviewing logs at work using [cat](https://en.wikipedia.org/wiki/Cat_(Unix)), [grep](https://en.wikipedia.org/wiki/Grep)\nand [more](https://en.wikipedia.org/wiki/More_(command)) it\nstruck me that it would have been nice if **cat**\nor **more** came with a time delay so you could use them like a\nteleprompter. This would let you casually watch the file scroll\nby while still being able to read the lines. The program we'll build\nin this post is \"SlowCat\" which accepts a command line parameter\nindicating the delay in seconds between display each line read from\nstandard input.\n\n## Working with Standard Input and Output\n\nThe Oakwood guides for Oberon-2 describe two modules\nparticularly useful for working with standard input and output.\nThey are appropriately called `In` and `Out`. On many Oberon Systems\nthese have been implemented such that your code could run under Unix\nor Oberon System with a simple re-compile.  We've used `Out` in our\nfirst program of this series, \"Hello World\". It provides a means to\nwrite Oberon system base types to standard out.  We've used `In`\na few times too. But `In` is worth diving into a bit more.\n\n### In\n\nThe [In](http://miasap.se/obnc/obncdoc/basic/In.def.html) module provides\na mirror of inputs to those of [Out](http://miasap.se/obnc/obncdoc/basic/Out.def.html). In Karl's implementation we are interested in one procedure\nand module status variable.\n\n+ `In.Line(VAR line: ARRAY OF CHAR)` : Read a sequence of characters from standard input from the current position in the file to the end of line.\n+ `In.Done` : Is a status Boolean variable, if the last call to an procedure in `In` was successful then it is set TRUE, otherwise FALSE (e.g. we're at the end of a file)\n\nWe use Karl's `In.Line()` extension to the standard `In` implementation\nbefore and will do so again as it simplifies our code and keeps things\neasily readable.\n\nThere is one nuance with `In.Done` that is easy to get tripped up on.\n`In.Done` indicates if the last operation was successful.\nSo if you're using `In.Line()` then `In.Done`\nshould be true if reading the line was successful. If you hit the end of\nthe file then `In.Done` should be false.  When you write your loop\nthis can be counter intuitive.  Here is a example of testing `In.Done`\nwith a repeat until loop.\n\n\n~~~\n\n    REPEAT\n      In.Line(text);\n      IF In.Done THEN\n        Out.String(text);Out.Ln();\n      END;\n    UNTIL In.Done = FALSE;\n\n~~~\n\n\nSo when you read this it is easy to think of `In.Done` as you're\ndone reading from standard input but actually we need to check for `FALSE`.\nThe value of `In.Done` was indicating the success of reading our line.\nAn unsuccessful line read, meaning we're at the end of the file, sets\n`In.Done` to false!\n\n### Out\n\nAs mention `Out` provides our output functions. We'll be using\ntwo procedure from `Out`, namely `Out.String()` and `Out.Ln()`.\nWe've seen both before.\n\n### Input\n\n\"SlowCat\" needs to calculate how often to write a line of\ntext to standard output with the `Out` module.  To do that\nI need access to the system's current time.  There isn't an\nOakwood module for time. There is a module called \n`Input` which provides a \"Time\" procedure. As a result\nI need to import `Input` as well as `In` even though\nI am using `In` to manage reading the file I am processing\nwith \"SlowCat\".\n\nA note about Karl's implementation.  `Input` is an Oakwood\nmodule that provides access to three system resources -- \nmouse, keyboard and system time.  Karl \nprovides two versions `Input` and `Input0`, the first is\nintended to be used with the `XYPlane` module for graphical\napplications the second for POSIX shell based application.\nIn the case of \"SlowCat\" I've stuck with `Input` as I am \nonly accessing time I've stuck with `Input` to make my source\ncode more portable if you're using another Oberon compiler.\n\n## Working with Karl's extensions\n\nThis is the part of my code which is not portable\nbetween compiler implementations and with Oberon Systems.\nKarl provides a number of extension module wrapping various\nPOSIX calls.  We are going to use two,\n[extArgs](http://miasap.se/obnc/obncdoc/ext/extArgs.def.html)\nwhich provides access to command line arguments and\n[extConvert](http://miasap.se/obnc/obncdoc/ext/extConvert.def.html)\nwhich provides a means of converting strings to integers.\nIf you are using another Oberon compiler you'll need to \nfind their equivalents and change my code example. I\nuse `extArgs` to access the command line parameters\nincluded in my POSIX shell invocation and I've used\n`extConvert` to convert the string presentation of the\ndelay to an integer value for my delay.\n\n\n## Our Approach\n\nTo create \"SlowCat\" we need four procedures and one\nglobal variable.\n\n`Usage()`\n: display a help text if parameters don't make sense\n\n`ProcessArgs()`\n: to get our delay time from the command line\n\n`Delay(count : INTEGER)`\n: a busy wait procedure\n\n`SlowCat(count : INTEGER)`\n: take standard input and display like a teleprompter\n\n`count`\n: is an integer holding our delay value (seconds of waiting) which is set by ProcessArgs()\n\n### Usage\n\nUsage just wraps helpful text and display it to standard out.\n\n## ProcessArgs()\n\nThis a functional procedure. It uses two of Karl's extension\nmodules. It uses `extArgs` to retrieve the command line parameters\nand `extConvert` the string value retrieved into an integer.\n`ProcessArgs()` returns TRUE if we can successful convert the\ncommand line parameter and set the value of count otherwise return\nFALSE.\n\n## Delay(VAR count : INTEGER)\n\nThis procedure uses `Input0` to fetch the current epoch time\nand counts the number of seconds until we've reached our delay\nvalue. It's a busy loop which isn't ideal but does keep the\nprogram simple.\n\n## SlowCat(VAR count: INTEGER);\n\nThis is the heart of our command line program. It reads\na line of text from standard input, if successful writes it\nto standard out and then waits using delay before repeating\nthis process. The delay is only invoked when a reading a\nline was successful.\n\n## Putting it all together\n\nHere's a \"SlowCat\" program.\n\n\n~~~\n\n    MODULE SlowCat;\n      IMPORT In, Out, Input, Args := extArgs, Convert := extConvert;\n\n    CONST\n      MAXLINE = 1024;\n\n    VAR\n      count: INTEGER;\n\n    PROCEDURE Usage();\n    BEGIN\n      Out.String(\"USAGE:\");Out.Ln();\n      Out.Ln();\n      Out.String(\"SlowCat outputs lines of text delayed by\");Out.Ln();\n      Out.String(\"a number of seconds. It takes one parameter,\");Out.Ln();\n      Out.String(\"an integer, which is the number of seconds to\");Out.Ln();\n      Out.String(\"delay a line of output.\");Out.Ln();\n      Out.String(\"SlowCat works on standard input and output.\");Out.Ln();\n      Out.Ln();\n      Out.String(\"EXAMPLE:\");\n      Out.Ln();\n      Out.String(\"    SlowCat 15 < README.md\");Out.Ln();\n      Out.Ln();\n    END Usage;\n\n    PROCEDURE ProcessArgs() : BOOLEAN;\n      VAR i : INTEGER; ok : BOOLEAN; arg : ARRAY MAXLINE OF CHAR;\n          res : BOOLEAN;\n    BEGIN\n      res := FALSE;\n      IF Args.count = 1 THEN\n        Args.Get(0, arg, i);\n        Convert.StringToInt(arg, i, ok);\n        IF ok THEN\n           (* convert seconds to microseconds of clock *)\n           count := (i * 1000);\n           res := TRUE;\n        END;\n      END;\n      RETURN res\n    END ProcessArgs;\n\n    PROCEDURE Delay*(count : INTEGER);\n      VAR start, current, delay : INTEGER;\n    BEGIN\n       start := Input.Time();\n       REPEAT\n         current := Input.Time();\n         delay := (current - start);\n       UNTIL delay >= count;\n    END Delay;\n\n    PROCEDURE SlowCat(count : INTEGER);\n      VAR text : ARRAY MAXLINE OF CHAR;\n    BEGIN\n      REPEAT\n        In.Line(text);\n        IF In.Done THEN\n          Out.String(text);Out.Ln();\n          (* Delay by count *)\n          Delay(count);\n        END;\n      UNTIL In.Done = FALSE;\n    END SlowCat;\n\n    BEGIN\n      count := 0;\n      IF ProcessArgs() THEN\n        SlowCat(count);\n      ELSE\n        Usage();\n      END;\n    END SlowCat.\n\n~~~\n\n\n## Compiling and trying it out\n\nTo compile our program and try it out reading\nour source code do the following.\n\n\n~~~\n\n    obnc SlowCat.Mod\n    # If successful\n    ./SlowCat 2 < SlowCat.Mod\n\n~~~\n\n\n\n## Oakwood Guidelines and POW!\n\nOberon and Oberon-2 were both used in creating and enhancing the\nOberon System(s) as well as for writing programs on other operating\nsystems (e.g. Apple's Mac and Microsoft Windows).\nImplementing Oberon compilers on non Oberon Systems required clarification\nbeyond the specification. The Oakwood Guidelines were an agreement\nbetween some of the important Oberon-2 compiler implementers which\nattempted to fill in that gap while encouraging portability in\nsource code between operating systems. Portability was desirable\nbecause it allowed programmers (e.g. students) to compile\nand run their Oberon programs with minimal modification in any\nenvironment where an Oakwood compliant compiler was available.\n\nCitation for Oakwood can be found in [Oberon-2 Programming with Windows](https://archive.org/details/oberonprogrammin00mhlb/page/n363/mode/2up?q=Oakwood+Guidlines).\n\n> Kirk B.(ed): The Oakwood Guidelines for Oberon-2 Compiler Developers. Available via FTP from ftp.fim.uni-linz.ac.at, /pub/soft/pow-oberon/oakwood\n\nThe FTP machine doesn't exist any more and does not appear to have been included in JKU's preservation plans. Fortunately the POW! website has been preserved.\n\n[POW!](http://www.fim.uni-linz.ac.at/pow/pow.htm) was a\ndifferent approach. It was a compiler and IDE targeting\nother than Oberon Systems (e.g. Windows and later Java). It was\nintended to be used in a hybrid development environment and to\nfacilitate leveraging non-Oberon resources (e.g. Java classes,\nnative Windows API).  POW project proposed \"Opal\" which was a\nsuper set of modules that went beyond Oakwood. Having skimmed\n\"Oberon-2 Programming with Windows\" some may seem reasonable to\nport to Oberon-07, others less so.\n\nWhy Oakwood and POW? These efforts are of interest to Oberon-07\ndevelopers as a well worn path to write code that is easy to\ncompile on POSIX systems and on systems that are based on the\nmore recent [Project Oberon 2013](http://www.projectoberon.com/).\nIt enhances the opportunity to bring forward well written modules\nfrom prior systems like [A2](https://en.wikibooks.org/wiki/Oberon/A2)\nbut implemented for the next generation of Oberon Systems\nlike [Integrated Oberon](https://github.com/io-core/io).\n\n### Oakwood PDF\n\nFinding a PDF of the original Oakwood guidelines is going to become\ntricky in the future. It was created by Robinson Associates and the\ncopy I've read from 1995 includes a page saying not for distribution.\nWhich sorta makes sense in the era of closed source software\ndevelopment. It is problematic for those of us who want to explore\nhow systems evolved.  The term \"Oakwood Guidelines\" is bandied about\nafter 1993 and several of the modules have had influence on the language\nuse via book publications.  I was able to find a PDF of the 1995\nversion of the guidelines at\n[http://www.math.bas.bg/bantchev/place/oberon/oakwood-guidelines.pdf](http://www.math.bas.bg/bantchev/place/oberon/oakwood-guidelines.pdf).\n\nHere's a typical explanation of Oakwood from \n[http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines](http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines)\nfor a description of Oakwood.\n\n> __The Oakwood Guidelines for the Oberon-2 Compiler Developers /These guidelines have been produced by a group of Oberon-2 compiler developers, including ETH developers, after a meeting at the Oakwood Hotel in Croydon, UK in June 1993__\n\n[http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines](http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines)  \n(an OS/2 developer website) was helpful for providing details about Oakwood.\n\nIt would have been nice if the Oakwood document had made its way\ninto either ETH's or JKU's research libraries.\n\nLeveraging prior art opens doors to the past and future. Karl has\ndone with this with the modules he provides with his OBNC compiler\nproject.\n\n### Next and Previous\n\n+ Next [Oberon to Markdown](../../10/03/Oberon-to-markdown.html)\n+ Previous [Procedures in records](../..//07/07/Procedures-in-records.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-08-15",
        "keywords": [
          "Oberon",
          "portable",
          "stdin"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 11,
        "series": "Mostly Oberon",
        "title": "Portable Oberon-07"
      },
      "url": "posts/2020/08/15/Portable-Oberon-07.json"
    },
    {
      "content": "\n\n# Software Tools, Getting Started\n\n## Overview\n\nThis post is the first in a series revisiting the\nprograms described in the 1981 book by Brian W. Kernighan and\nP. J. Plauger's called [Software Tools in Pascal](https://archive.org/details/softwaretoolsinp00kern).\nThe book is available from the [Open Library](https://openlibrary.org/)\nand physical copies are still (2020) commonly available from used book\nsellers.  The book was an early text on creating portable command\nline programs.  \n\nIn this series I present the K & P (i.e. Software Tools in Pascal)\nprograms re-implemented in Oberon-07. I have testing my implementations\nusing Karl Landström's [OBNC](http://miasap.se/obnc/)\ncompiler and his implementation of the Oakwood Guide's modules\nfor portable Oberon programs. Karl also provides a few additional\nmodules for working in a POSIX environment (e.g. BSD, macOS, Linux,\nWindows 10 with Linux subsystem). I have also tested these\nprograms with Mike Spivey's [Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler) an aside\nfrom the differences file extensions that both compilers use\nthe source code works the same. \n\nNOTE: OBNC compiler is the work of Karl Langström, it is portable across many systems where the C tool chain is available.\n\nNOTE: POSIX defines a standard of compatibility inspired by [UNIX](https://en.wikipedia.org/wiki/Unix), see <https://en.wikipedia.org/wiki/POSIX>\n\n\n## Getting Started.\n\nChapter one in K & P is the first chapter that presents code. It introduces\nsome challenges and constraints creating portable Pascal suitable for use\nacross hardware architectures and operating systems. In 1981 this included\nmainframes, minicomputers as well as the recent evolution of the microcomputer.\nThe programs presented build up from simple to increasingly complex as\nyou move through the book.  They provide example documentation and discuss\ntheir implementation choices. It is well worth reading the book for those\ndiscussions, while specific to the era, mirror the problems program authors\nface today in spite of the wide spread success of the POXIS model, the\nconsolidation of CPU types and improvements made in development tools in\nthe intervening decades.\n\nThrough out K & P you'll see the bones of many POSIX commands we have today.\n\nPrograms from this chapter include:\n\n1. **copyprog**, this is like \"cat\" in a POSIX system\n2. **charcount**, this is like the \"wc\" POSIX command using the \"-c\" option\n3. **linecount**, this is like the \"wc\" POSIX command using the \"-l\" option\n4. **wordcount**, this is like the \"wc\" POSIX command using the \"-w\" option\n5. **detab**, converts tabs to spaces using tab stops every four characters in a line\n\nAll programs in this chapter rely solely on standard input and output.\nToday's reader will notice an absence to common concepts in today's\ncommand line programs.  First is the lack of interaction with command line\nparameters, the second is no example take advantage of environment variables.\nThese operating system features were not always available across\noperating systems of the early 1980s. Finally I'd like to point out a\nreally nice feature included in the book. It is often left out as a topic\nin programming books.  K & P provide example documentation. It's structure\nlike an early UNIX man page. It very clear and short. This is something\nI wish all programming texts at least mentioned. Documentation is important\nto the program author because it clarifies scope of the problem being\ntackled and to the program user so they understand what they are using.\n\n\n### [1.1. File Copying](https://archive.org/details/softwaretoolsinp00kern/page/7/mode/1up)\n\nHere's how K & P describe \"copyprog.pas\" (referred to as \"copy\" in\nthe documentation).\n\n\n~~~\n\nPROGRAM\n\n    copy    copy input to output\n\nUSAGE\n\n    copy\n\nFUNCTION\n\n    copy copies its input to its output unchanged. It is useful for copying\n    from a terminal to a file, from file to file, or even from terminal to\n    terminal. It may be used for displaying the contents of a file, without\n    interpretation or formatting, by copying from the file to terminal.\n\nEXAMPLE\n\n    To echo lines type at your terminal.\n\n    copy\n    hello there, are you listening?\n    **hello there, are you listening?**\n    yes, I am.\n    **yes, I am.**\n    <ENDFILE>\n\n~~~\n\nThe source code for \"copyprog.pas\" is shown on\n[page 9](https://archive.org/details/softwaretoolsinp00kern/page/9/mode/1up)\nof K & P.  First the authors introduce the __copy__ procedure\nthen a complete the section introducing it in context of the complete Pascal\nprogram. After this first example K & P leave implementation of the full\nprogram up to the reader.\n\nThe body of the Pascal program invokes a procedure called\n[copy](https://archive.org/details/softwaretoolsinp00kern/page/8/mode/1up)\nwhich reads from standard input character by character and writes\nto standard output character by character without modification.  Two\nsupporting procedures are introduced, \"getc\" and \"putc\". These are shown\nin the complete program listing on page 9. They are repeatedly used\nthrough out the book. One of the really good aspects of this simple\nprogram is relying on the idea of standard input and output. This makes\n\"copyprog.pas\" a simple filter and template for writing many of the programs\nthat follow. K & P provide a good explanation for this simple approach.\nAlso note K & P's rational for working character by character versus\nline by line.\n\nMy Oberon-07 version takes a similar approach. The module looks remarkably\nsimilar to the Pascal but is shorter because reading and writing characters are\nprovided for by Oberon's standard modules \"In\" and \"Out\".\nI have chosen to use a \"REPEAT/UNTIL\" loop over the \"WHILE\"\nloop used by K & P is the attempt to read from standard input needs to happen\nat least once. Note in my \"REPEAT/UNTIL\" loop's terminating condition.\nThe value of `In.Done` is true on successful read and false\notherwise (e.g. you hit an end of the file). That means our loop must\nterminate on `In.Done # TRUE` rather than `In.Done = TRUE`. This appears\ncounter intuitive unless you keep in mind our loop stops when we having\nnothing more to read, rather than when we can continue to read.\nIt `In.Done` means the read was successful and does not\nmean \"I'm done and can exit now\". Likewise before writing out the character\nwe read, it is good practice to check the `In.Done` value. If `In.Done` is\nTRUE, I know can safely display the character using `Out.Char(c);`.\n\n~~~\n\nMODULE CopyProg;\nIMPORT In, Out;\n\nPROCEDURE copy;\nVAR\n  c : CHAR;\nBEGIN\n  REPEAT\n    In.Char(c);\n    IF In.Done THEN\n        Out.Char(c);\n    END;\n  UNTIL In.Done # TRUE;\nEND copy;\n\nBEGIN\n  copy();\nEND CopyProg.\n\n~~~\n\n#### Limitations\n\nThis program only works with standard input and output. A more generalized\nversion would work with named files.\n\n### [1.2 Counting Characters](https://archive.org/details/softwaretoolsinp00kern/page/13/mode/1up)\n\n~~~\n\nPROGRAM\n\n  charcount count characters in input\n\nUSAGE\n\n  charcount\n\nFUNCTION\n\n  charcount counts the characters in its input and writes the total\n  as a single line of text to the output. Since each line of text is\n  internally delimited by a NEWLINE character, the total count is the\n  number of lines plus the number of characters within each line.\n\nEXAMPLE\n\n  charcount\n  A single line of input.\n  <ENDFILE>\n  24\n\n~~~\n\n[On page 13](https://archive.org/details/softwaretoolsinp00kern/page/13/mode/1up)\nK & P introduces their second program, **charcount**. It is based on a single\nprocedure that reads from standard input and counts up the number of\ncharacters encountered then writes the total number found to standard out\nfollowed by a newline. In the text only the procedure is shown, it is\nassumed you'll write the outer wrapper of the program yourself as\nwas done with the **copyprog** program. My Oberon-07 version is very similar\nto the Pascal. Like in the our first \"CopyProg\" we will make use of the\n\"In\" and \"Out\" modules. Since we will\nneed to write an INTEGER value we'll also use \"Out.Int()\" procedure which\nis very similar to K & P's \"putdec()\". Aside from the counting this is very\nsimple  like our first program.\n\n~~~\n\nMODULE CharCount;\nIMPORT In, Out;\n\nPROCEDURE CharCount;\nVAR\n  nc : INTEGER;\n  c : CHAR;\nBEGIN\n  nc := 0;\n\n  REPEAT\n    In.Char(c);\n    IF In.Done THEN\n      nc := nc + 1;\n    END;\n  UNTIL In.Done # TRUE;\n  Out.Int(nc, 1);\n  Out.Ln();\nEND CharCount;\n\nBEGIN\n  CharCount();\nEND CharCount.\n\n~~~\n\n#### Limitations\n\nThe primary limitation in counting characters is most readers are\ninterested in visible character count. In our implementation\neven non-printed characters are counted. Like our first program\nthis only works on standard input and output. Ideally this should\nbe written so it works on any file including standard input and\noutput. If the reader implements that it could become part of a\npackage on statistical analysis of plain text files.\n\n### [1.3 Counting Lines](https://archive.org/details/softwaretoolsinp00kern/page/14/mode/1up)\n\n~~~\n\nPROGRAM\n\n  linecount count lines in input\n\nUSAGE\n\n  linecount\n\nFUNCTION\n\n  linecount counts the lines in its input and write the total as a\n  line of text to the output.\n\nEXAMPLE\n\n  linecount\n  A single line of input.\n  <ENDFILE>\n  1\n\n~~~\n\n**linecount**, from [page 15](https://archive.org/details/softwaretoolsinp00kern/page/15/mode/1up)\nis very similar to **charcount** except adding a\nconditional count in the loop for processing the file. In\nour Oberon-07 implementation we'll check if the `In.Char(c)`\ncall was successful but we'll add a second condition to see if the\ncharacter read was a NEWLINE. If it was I increment\nour counter variable.\n\n~~~\n\nMODULE LineCount;\nIMPORT In, Out;\n\nPROCEDURE LineCount;\nCONST\n  NEWLINE = 10;\n\nVAR\n  nl : INTEGER;\n  c : CHAR;\nBEGIN\n  nl := 0;\n  REPEAT\n    In.Char(c);\n    IF In.Done & (ORD(c) = NEWLINE) THEN\n      nl := nl + 1;\n    END;\n  UNTIL In.Done # TRUE;\n  Out.Int(nl, 1);\n  Out.Ln();\nEND LineCount;\n\nBEGIN\n  LineCount();\nEND LineCount.\n\n~~~\n\n#### Limitations\n\nThis program assumes that NEWLINE is ASCII value 10. Line delimiters\nvary between operating systems.  If your OS used carriage returns\nwithout a NEWLINE then this program would not count lines correctly.\nThe reader could extend the checking to support carriage returns,\nnew lines, and carriage return with new lines and cover most versions\nof line endings.\n\n\n### [1.4 Counting Words](https://archive.org/details/softwaretoolsinp00kern/page/14/mode/1up)\n\n~~~\n\nPROGRAM\n\n  wordcount count words in input\n\nUSAGE\n\n  wordcount\n\nFUNCTION\n\n  wordcount counts the words in its input and write the total as a\n  line of text to the output. A \"word\" is a maximal sequence of characters\n  not containing a blank or tab or newline.\n\nEXAMPLE\n\n  wordcount\n  A single line of input.\n  <ENDFILE>\n  5\n\nBUGS\n\n  The definition of \"word\" is simplistic.\n\n~~~\n\n[Page 17](https://archive.org/details/softwaretoolsinp00kern/page/17/mode/1up)\nbrings us to the **wordcount** program. Counting words can be\nvery nuanced but here K & P have chosen a simple definition\nwhich most of the time is \"good enough\" for languages like English.\nA word is defined simply as an run of characters separated by\na space, tab or newline characters.  In practice most documents\nwill work with this minimal definition. It also makes the code\nstraight forward.  This is a good example of taking the simple\nroad if you can. It keeps this program short and sweet.\n\nIf you follow along in the K & P book note their rational\nand choices in arriving at there solutions. There solutions\nwill often balance readability and clarity over machine efficiency.\nWhile the code has progressed from \"if then\" to \"if then else if\"\nlogical sequence, the solution's modeled remains\nclear. This means the person reading the source code can easily verify\nif the approach chosen was too simple to meet their needs or it was\n\"good enough\".\n\nMy Oberon-07 implementation is again very simple. Like in previous programs\nI still have an outer check to see if the read worked (i.e. \"In.Done = TRUE\"),\notherwise the conditional logic is the same as the Pascal implementation.\n\n~~~\n\nMODULE WordCount;\nIMPORT In, Out;\n\nPROCEDURE WordCount;\nCONST\n  NEWLINE = 10;\n  BLANK = 32;\n  TAB = 9;\n\nVAR\n  nw : INTEGER;\n  c : CHAR;\n  inword : BOOLEAN;\nBEGIN\n  nw := 0;\n  inword := FALSE;\n  REPEAT\n    In.Char(c);\n    IF In.Done THEN\n      IF ((ORD(c) = BLANK) OR (ORD(c) = NEWLINE) OR (ORD(c) = TAB)) THEN\n        inword := FALSE;\n      ELSIF (inword = FALSE) THEN\n        inword := TRUE;\n        nw := nw + 1;\n      END;\n    END;\n  UNTIL In.Done # TRUE;\n  Out.Int(nw, 1);\n  Out.Ln();\nEND WordCount;\n\nBEGIN\n  WordCount();\nEND WordCount.\n\n~~~\n\n## [1.5 Removing Tabs](https://archive.org/details/softwaretoolsinp00kern/page/20/mode/1up)\n\n~~~\n\nPROGRAM\n\n  detab convert tabs into blanks\n\nUSAGE\n\n  detab\n\nFUNCTION\n\n  detab copies its input to its output, expanding the horizontal\n  tabs to blanks along the way, so that the output is visually\n  the same as the input, but contains no tab characters. Tab stops\n  are assumed to be set every four columns (i.e. 1, 5, 9, ...), so\n  each tab character is replaced by from one to four blanks.\n\nEXAMPLE\n\n  Usaing \"->\" as a visible tab:\n\n  detab\n  ->col 1->2->34->rest\n      col 1   2   34  rest\n\nBUGS\n\n  detab is naive about backspaces, vertical motions, and\n  non-printing characters.\n\n~~~\n\nThe source code for \"detab\" can be found on\n[page 24](https://archive.org/details/softwaretoolsinp00kern/page/24/mode/1up)\nin the last section of chapter 1. **detab** removes\ntabs and replaces them with spaces. Rather than a simple \"tab\"\nreplaced with four spaces **detab** preserves a concept found on\ntypewriters called \"tab stops\". In 1981 typewrites were still widely\nused though word processing software would become common. Supporting the\n\"tab stop\" model means the program works with what office workers would\nexpect from older tools like the typewriter or even the computer's\nteletype machine. I think this shows an important aspect of writing\nprograms. Write the program for people, support existing common concepts\nthey will likely know.\n\nK & P implementation includes separate source files\nfor setting tab stops and checking a tab stop.  The Pascal K & P\nwrote for didn't support separate source files or program modules. Recent Pascal\nversions did support the concept of modularization (e.g. UCSD Pascal). Since\nand significant goal of K & P was portability they needed to come up\nwith a solution that worked on the \"standard\" Pascal compilers available on\nminicomputers and mainframes and not write their solution to a specific\nPascal system like UCSD Pascal (see Appendix, \"IMPLEMENTATION\nPRIMITIVES [page 315](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/315/mode/1up)).\nModularization facilitates code reuse and like information hiding is an\nimport software technique. Unfortunately the preprocessor approach doesn't\nsupport information hiding.\n\nTo facilitate code reuse the K & P book includes a preprocessor as part\nof the Pascal development tools (see [page 71](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/71/mode/1up)\nfor implementation). The preprocessor written\nin Pascal was based on the early versions of the \"C\" preprocessor\nthey had available in the early UNIX systems. Not terribly Pascal\nlike but it worked and allowed the two files to be shared between\nthis program and one in the next chapter.\n\nOberon-07 of course benefits from all of Wirth's language improvements\nthat came after Pascal. Oberon-07 supports modules and as such\nthere is no need for a preprocessor.  Because of Oberon-07's module\nsupport I've implemented the Oberon version using two files\nrather than three. My main program file is \"Detab.Mod\",\nthe supporting library module is \"Tabs.Mod\". \"Tabs\" is where I\ndefine our tab stop data structure as well as the\nprocedures that operating on that data structure.\n\nLet's look at the first part, \"Detab.Mod\". This is the module\nthat forms the program and it features an module level \"BEGIN/END\" block.\nIn that block I call \"Detab();\" which implements the program's functionality.\nI import \"In\", \"Out\" as before but I also import \"Tabs\" which I will show next.\nLike my previous examples I validate the read was successful before proceeding\nwith the logic presented in the original Pascal and deciding\nwhat to write to standard output.\n\n~~~\n\nMODULE Detab;\n  IMPORT In, Out, Tabs;\n\nCONST\n  NEWLINE = 10;\n  TAB = 9;\n  BLANK = 32;\n\nPROCEDURE Detab;\nVAR\n  c : CHAR;\n  col : INTEGER;\n  tabstops : Tabs.TabType;\nBEGIN\n  Tabs.SetTabs(tabstops); (* set initial tab stops *)\n  col := 1;\n  REPEAT\n    In.Char(c);\n    IF In.Done THEN\n      IF (ORD(c) = TAB) THEN\n        REPEAT\n          Out.Char(CHR(BLANK));\n          col := col + 1;\n        UNTIL Tabs.TabPos(col, tabstops);\n      ELSIF (ORD(c) = NEWLINE) THEN\n        Out.Char(c);\n        col := 1;\n      ELSE\n        Out.Char(c);\n        col := col + 1;\n      END;\n    END;\n  UNTIL In.Done # TRUE;\nEND Detab;\n\nBEGIN\n  Detab();\nEND Detab.\n\n~~~\n\nOur second module is \"Tabs.Mod\". It provides the supporting procedures\nand definition of the our \"TabType\" data structure. For us this\nis the first time we write a module which \"exports\" procedures\nand type definitions. If you are new to Oberon, expected constants,\nvariables and procedures names have a trailing \"*\". Otherwise the\nOberon compiler will assume a local use only. This is a very\npowerful information hiding capability and what allows you to\nevolve a modules' internal implementation independently of the\nprograms that rely on it.\n\n~~~\n\nMODULE Tabs;\n\nCONST\n  MAXLINE = 1000; (* or whatever *)\n\nTYPE\n  TabType* = ARRAY MAXLINE OF BOOLEAN;\n\n(* TabPos -- return TRUE if col is a tab stop *)\nPROCEDURE TabPos*(col : INTEGER; VAR tabstops : TabType) : BOOLEAN;\n  VAR res : BOOLEAN;\nBEGIN\n  res := FALSE; (* Initialize our internal default return value *)\n  IF (col >= MAXLINE) THEN\n    res := TRUE;\n  ELSE\n    res := tabstops[col];\n  END;\n  RETURN res\nEND TabPos;\n\n(* SetTabs -- set initial tab stops *)\nPROCEDURE SetTabs*(VAR tabstops: TabType);\nCONST\n  TABSPACE = 4; (* 4 spaces per tab *)\nVAR\n  i : INTEGER;\nBEGIN\n  (* NOTE: Arrays in Oberon start at zero, we want to\n     stop at the last cell *)\n  FOR i := 0 TO (MAXLINE - 1) DO\n    tabstops[i] := ((i MOD TABSPACE) = 0);\n  END;\nEND SetTabs;\n\nEND Tabs.\n\n~~~\n\nNOTE: This module is used by \"Detab.Mod\" and \"Entab.Mod\"\nand provides for common type definitions and code reuse.\nWe exported `TabType`, `TabPos` and `SetTabs`. Everything else\nis private to this module.\n\n## In closing\n\nThis post briefly highlighted ports of the programs\npresented in Chapter 1 of \"Software Tools in Pascal\".\nBelow are links to my source files of the my\nimplementations inspired by the K & P book. Included\nin each Oberon module source after the module definition\nis transcribed text of the program documentation as well\nas transcribed text of the K & P Pascal implementations.\nEach file should compiler without modification using the\nOBNC compiler.  By default the OBNC compiler will use the\nmodule's name as the name of the executable version. I\nI have used mixed case module names, if you prefer lower\ncase executable names use the \"-o\" option with the OBNC\ncompiler.\n\n~~~\n\n    obnc -o copy CopyProg.Mod\n    obnc -o charcount CharCount.Mod\n    obnc -o linecount LineCount.Mod\n    obnc -o wordcount WordCount.Mod\n    obnc -o detab Detab.Mod\n\n~~~\n\nIf you happen to be using The [Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler)\nyou need to rename the files ending in \".Mod\" to \".m\" \nand you can compiler with the following command.\n\n~~~\n    obc -07 -o copyprog CopyProg.m\n    obc -07 -o charcount CharCount.m\n    obc -07 -o linecount LineCount.m\n    obc -07 -o wordcount WordCount.m\n    obc -07 -o detab Tabs.m Detab.m\n~~~\n\nNote the line for compiling \"Detab\" with **obc**, your\nlocal modules need to become before the module calling them.\n\n\n+ [CopyProg](CopyProg.Mod)\n+ [CharCount](CharCount.Mod)\n+ [LineCount](LineCount.Mod)\n+ [WordCount](WordCount.Mod)\n+ [Detab](Detab.Mod)\n    + [Tabs](Tabs.Mod), this one we'll revisit in next installment.\n\n\n# Next\n\n+ [Filters](../../10/31/Filters.html)\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-09-29",
        "keywords": [
          "Oberon",
          "Pascal",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 1,
        "series": "Software Tools",
        "title": "Software Tools, Getting Started"
      },
      "url": "posts/2020/09/29/Software-Tools-1.json"
    },
    {
      "content": "\n\nOberon to Markdown\n==================\n\nThis is the twelfth post in the [Mostly Oberon](https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.\n\nA nice feature of Oberon\n------------------------\n\nOberon source code has a very nice property in that anything\nafter the closing end statement is ignored by the compiler.\nThis makes it a nice place to write documentation, program\nnotes and other ideas.\n\nI've gotten in the habit of writing up program docs and\nnotes there. When I prep to make a web document I used to\ncopy the source file, doing a cut and paste to re-order\nthe module code to the bottom of the document. I'd follow\nthat with adding headers and code fences. Not hard but\ntedious. Of course if I changed the source code I'd also\nhave to do another cut and paste edit. This program,\n`ObnToMd.Mod` automates that process.\n\nProgram Documentation\n---------------------\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPROGRAM\n\n  ObnToMd\n\nFUNCTION\n\n  This is a simple program that reads Oberon modules\n  from standard in and re-renders that to standard output\n  such that it is suitable to process with Pandoc or other\n  text processing system.\n\nEXAMPLE\n\n  Read the source for this program and render a file\n  called \"blog-post.md\". Use Pandoc to render HTML.\n\n    ObnToMd <ObnToMd.Mod > blog-post.md\n    pandoc -s --metadata title=\"Blog Post\" \\\n        blog-post.md >blog-post.html\n\nBUGS\n\n  It uses a naive line analysis to identify the module\n  name and then the end of module statement. Might be\n  tripped up by comments containing the same strings.\n  The temporary file created is called \"o2m.tmp\" and\n  this filename could potentially conflict with another\n  file.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n\nSource code for **ObnToMd.Mod**\n-------------------------------\n\n~~~\n\n(* ObnToMd.Mod - an simple filter process for reading\nan Oberon-07 module source file and rendering a markdown\nfriendly output suitable for piping into Pandoc. The\nfilter reads from standard input and writes to standard\noutput and makes use of a temp file name o2m.tmp which\nit removes after successful rendering.\n\n@Author R. S. Doiel, <rsdoiel@gmail.com>\ncopyright (c) 2020, all rights reserved.\nReleased under the BSD 2-clause license\nSee: https://opensource.org/licenses/BSD-2-Clause\n*)\nMODULE ObnToMd;\n  IMPORT In, Out, Files, Strings;\n\nCONST\n  MAXLENGTH = 1024;\n  LF = CHR(10);\n\nVAR\n  endOfLine : ARRAY 2 OF CHAR;\n\n(*\n * Helper methods\n *)\nPROCEDURE GenTempName(prefix, suffix : ARRAY OF CHAR; VAR name : ARRAY OF CHAR);\nBEGIN\n  name := \"\";\n  Strings.Append(prefix, name);\n  Strings.Append(\".\", name);\n  Strings.Append(suffix, name);\nEND GenTempName;\n\nPROCEDURE GenTempFile(name : ARRAY OF CHAR; VAR r : Files.Rider; VAR f : Files.File);\nBEGIN\n  f := Files.New(name);\n  IF f = NIL THEN\n    Out.String(\"ERROR: can't create \");Out.String(name);Out.Ln();\n    ASSERT(FALSE);\n  END;\n  Files.Register(f);\n  Files.Set(r, f, 0);\nEND GenTempFile;\n\n\nPROCEDURE StartsWith(target, source : ARRAY OF CHAR) : BOOLEAN;\n  VAR res : BOOLEAN;\nBEGIN\n  IF Strings.Pos(target, source, 0) > -1 THEN\n    res := TRUE;\n  ELSE\n    res := FALSE;\n  END;\n  RETURN res\nEND StartsWith;\n\nPROCEDURE ClearString(VAR s : ARRAY OF CHAR);\n  VAR i : INTEGER;\nBEGIN\n  FOR i := 0 TO LEN(s) - 1 DO\n    s[i] := 0X;\n  END;\nEND ClearString;\n\n\nPROCEDURE ProcessModuleDef(VAR r : Files.Rider; VAR modName : ARRAY OF CHAR);\n  VAR\n    line, endStmt : ARRAY MAXLENGTH OF CHAR;\n    start, end : INTEGER;\nBEGIN\n  line := \"\";\n  endStmt := \"\";\n  modName := \"\";\n  (* Find the name of the module and calc the \"END {NAME}.\" statement *)\n  REPEAT\n    ClearString(line);\n    In.Line(line);\n    IF In.Done THEN\n      Files.WriteString(r, line); Files.WriteString(r, endOfLine);\n      (* When `MODULE {NAME};` is encountered extract the module name *)\n      IF StartsWith(\"MODULE \", line) THEN\n        start := 7;\n        end := Strings.Pos(\";\", line, 0);\n        IF (end > -1) & (end > start) THEN\n            Strings.Extract(line, start, end - start, modName);\n            endStmt := \"END \";\n            Strings.Append(modName, endStmt);\n            Strings.Append(\".\", endStmt);\n        END;\n      END;\n    END;\n  UNTIL (In.Done # TRUE) OR (endStmt # \"\");\n\n  (* When `END {NAME}.` is encountered  stop writing tmp file *)\n  REPEAT\n    In.Line(line);\n    IF In.Done THEN\n      Files.WriteString(r, line); Files.WriteString(r, endOfLine);\n    END;\n  UNTIL (In.Done # TRUE) OR StartsWith(endStmt, line);\nEND ProcessModuleDef;\n\nPROCEDURE WriteModuleDef(name : ARRAY OF CHAR; VAR r : Files.Rider; VAR f : Files.File);\n  VAR s : ARRAY MAXLENGTH OF CHAR; res : INTEGER;\nBEGIN\n  Files.Set(r, f, 0);\n  REPEAT\n    Files.ReadString(r, s);\n    IF r.eof # TRUE THEN\n      Out.String(s);\n    END;\n  UNTIL r.eof;\n  Files.Close(f);\n  Files.Delete(name, res);\nEND WriteModuleDef;\n\n\nPROCEDURE OberonToMarkdown();\nVAR\n  tmpName, modName, line : ARRAY MAXLENGTH OF CHAR;\n  f : Files.File;\n  r : Files.Rider;\n  i : INTEGER;\nBEGIN\n  tmpName := \"\"; modName := \"\";  line := \"\";\n  (* Open temp file *)\n  GenTempName(\"o2m\", \"tmp\", tmpName);\n  GenTempFile(tmpName, r, f);\n\n  (* Read the Oberon source from standard input echo the lines tmp file *)\n  ProcessModuleDef(r, modName);\n\n  (* Write remainder of file to standard out *)\n  REPEAT\n    In.Line(line);\n    IF In.Done THEN\n      Out.String(line);Out.Ln();\n    END;\n  UNTIL In.Done # TRUE;\n\n  (* Write two new lines *)\n  Out.Ln(); Out.Ln();\n  (* Write heading `Source code for {NAME}` *)\n  ClearString(line);\n  line := \"Source code for **\";\n  Strings.Append(modName, line);\n  Strings.Append(\".Mod**\", line);\n  Out.String(line); Out.Ln();\n  FOR i := 0 TO Strings.Length(line) - 1 DO\n    Out.String(\"-\");\n  END;\n  Out.Ln();\n  (* Write code fence *)\n  Out.Ln();Out.String(\"~~~\");Out.Ln();\n  (* Reset rider to top of tmp file\n     Write temp file to standard out\n     cleanup demp file *)\n  WriteModuleDef(tmpName, r, f);\n  (* Write code fence *)\n  Out.Ln();Out.String(\"~~~\");Out.Ln();\n  (* Write tailing line and exit procedure *)\n  Out.Ln();\nEND OberonToMarkdown;\n\nBEGIN\n  endOfLine[0] := LF;\n  endOfLine[1] := 0X;\n  OberonToMarkdown();\nEND ObnToMd.\n\n~~~\n\n### Next, Previous\n\n+ Next [Assembling Pages](../../10/19/Assemble-pages.html)\n+ Previous [Portable Oberon-07](../../08/15/Portable-Oberon-07.html)\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-10-03",
        "keywords": [
          "Oberon",
          "portable",
          "markdown",
          "programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 12,
        "series": "Mostly Oberon",
        "title": "Oberon to Markdown"
      },
      "url": "posts/2020/10/03/Oberon-to-markdown.json"
    },
    {
      "content": "\n\nAssembling pages\n================\n\nThis is the thirteenth post in the [Mostly Oberon](https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.\n\nPandoc and JSON\n---------------\n\nI use [Pandoc](https://pandoc.org) to process Markdown documents. I like to keep my\nfront matter in JSON rather than Pandoc's YAML. Fortunately Pandoc\ndoes support working with JSON as a metadata file include. Normally I would\nmanually split the JSON front matter and the rest of the markup into two\nseparate files, then process with Pandoc and other tooling like\n[LunrJS](https://lunrjs.com). [AssemblePage](AssemblePage.Mod) automates this\nprocess.\n\nExample shell usage:\n\n~~~\n\n   AssemblePage MyText.txt \\\n      metadata=document.json \\\n      document=document.md\n   pandoc --from markdown --to html \\\n      --metadata-file document.json \\\n      --standalone \\\n      document.md >MyText.html\n\n~~~\n\nSource code for **AssemblePage.Mod**\n------------------------------------\n\n~~~\n\nMODULE AssemblePage;\n  IMPORT Out, Strings, Files, Args := extArgs;\n\nVAR\n  srcName, metaName, docName : ARRAY 1024 OF CHAR;\n\n(* FrontMatter takes a \"read\" Rider, r, and a \"write\" Rider \"w\".\nIf the first character read by r is an opening curly bracket\n(the start of the front matter) it writes it out with w, until\nit finds a matching closing curly bracket or the file ends. *)\nPROCEDURE FrontMatter*(VAR r : Files.Rider; VAR w : Files.Rider);\n  VAR c : BYTE; cCnt : INTEGER;\nBEGIN\n  (* Scan for opening JSON front matter *)\n  cCnt := 0;\n  REPEAT\n    Files.Read(r, c);\n    IF r.eof = FALSE THEN\n      IF c = ORD(\"{\") THEN\n        cCnt := cCnt + 1;\n      ELSIF c = ORD(\"}\") THEN\n        cCnt := cCnt - 1;\n      END;\n      Files.Write(w, c);\n    END;\n  UNTIL (r.eof = TRUE) OR (cCnt = 0);\n  IF cCnt # 0 THEN\n    Out.String(\"ERROR: mis matched '{' and '}' in front matter\");\n    ASSERT(FALSE);\n  END;\nEND FrontMatter;\n\n(* CopyIO copies the characters from a \"read\" Rider to a \"write\" Rider *)\nPROCEDURE CopyIO*(VAR r : Files.Rider; VAR w: Files.Rider);\n  VAR c : BYTE;\nBEGIN\n  REPEAT\n    Files.Read(r, c);\n    IF r.eof = FALSE THEN\n      Files.Write(w, c);\n    END;\n  UNTIL r.eof = TRUE;\nEND CopyIO;\n\nPROCEDURE ProcessParameters(VAR sName, mName, dName : ARRAY OF CHAR);\n  VAR\n    arg : ARRAY 1024 OF CHAR;\n    i, res : INTEGER;\nBEGIN\n  mName := \"document.json\";\n  dName := \"document.txt\";\n  arg := \"\";\n  FOR i := 0 TO (Args.count - 1) DO\n    Args.Get(i, arg, res);\n    IF Strings.Pos(\"metadata=\", arg, 0) = 0 THEN\n      Strings.Extract(arg, 9, Strings.Length(arg), mName);\n    ELSIF Strings.Pos(\"document=\", arg, 0) = 0 THEN\n      Strings.Extract(arg, 9, Strings.Length(arg), dName);\n    ELSE\n      Strings.Extract(arg, 0, Strings.Length(arg), sName);\n    END;\n  END;\nEND ProcessParameters;\n\nPROCEDURE AssemblePage(srcName, metaName, docName : ARRAY OF CHAR);\nVAR\n  src, meta, doc : Files.File;\n  reader, writer : Files.Rider;\nBEGIN\n  src := Files.Old(srcName);\n  IF src # NIL THEN\n    Files.Set(reader, src, 0);\n    IF metaName # \"\" THEN\n      meta := Files.New(metaName);\n      Files.Register(meta);\n      Files.Set(writer, meta, 0);\n      FrontMatter(reader, writer);\n      Files.Close(meta);\n    END;\n    IF docName # \"\" THEN\n      doc := Files.New(docName);\n      Files.Register(doc);\n      Files.Set(writer, doc, 0);\n      CopyIO(reader, writer);\n      Files.Close(doc);\n    END;\n  ELSE\n    Out.String(\"ERROR: Could not read \");Out.String(srcName);Out.Ln();\n    ASSERT(FALSE);\n  END;\n  Files.Close(src);\nEND AssemblePage;\n\nBEGIN\n  ProcessParameters(srcName, metaName, docName);\n  AssemblePage(srcName, metaName, docName);\nEND AssemblePage.\n\n~~~\n\n### Next, Previous\n\n+ Next [Dates & Clock](../../11/27/Dates-and-Clock.html)\n+ Previous [Oberon To Markdown](../../10/03/Oberon-to-markdown.html)\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-10-19",
        "keywords": [
          "Oberon-07",
          "portable",
          "markdown",
          "pandoc",
          "frontmatter"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 13,
        "series": "Mostly Oberon",
        "title": "Assembling Pages"
      },
      "url": "posts/2020/10/19/Assemble-pages.json"
    },
    {
      "content": "\n\nSoftware Tools, Filters\n=======================\n\nOverview\n--------\n\nThis post is the second in a series revisiting the programs\ndescribed in the 1981 book by Brian W. Kernighan and P. J.\nPlauger's called [Software Tools in Pascal](https://archive.org/details/softwaretoolsinp00kern). The book is available from the\n[Open Library](https://openlibrary.org/) and physical copies\nare still (2020) commonly available from used book sellers.\nThe book was an late 20th century text on creating portable\ncommand line programs using ISO standard Pascal of the era.\n\nIn this chapter K & P focuses on developing the idea of filters.\nFilters are programs which typically process standard input, do\nsome sort of transformation or calculation and write to standard\noutput.  They are intended to work either standalone or in a pipeline\nto solve more complex problems. I like to think of filters as\nsoftware [LEGO](https://en.wikipedia.org/wiki/Lego).\nFilter programs can be \"snapped\" together creating simple shapes\ndata shapes or combined to for complex compositions.\n\nThe programs from this chapter include:\n\n+ **entab**, respecting tabstops, convert strings of spaces to tabs\n+ **overstrike**, this is probably not useful anymore, it would allow \"overstriking\" characters on devices that supported it. From [wikipedia](https://en.wikipedia.org/wiki/Overstrike), \"In typography, overstrike is a method of printing characters that are missing from the printer's character set. The character was created by placing one character on another one — for example, overstriking \"L\" with \"-\" resulted in printing a \"Ł\" (L with stroke) character.\"\n+ **compress**, an early UNIX style compress for plain text files\n+ **expand**, an early UNIX style expand for plain text files, previously run through with **compress**\n+ **echo**, write echo's command line parameters to standard output, introduces working with command line parameters\n+ **translit**, transliterate characters using a simple from/to substitution with a simple notation to describe character sequences and negation. My implementation diverges from K & P\n\nImplementing in Oberon-07\n------------------------\n\nWith the exception of **echo** (used to introduce command line parameter processing) each program increases in complexity.  The last program **translit**is the most complex in this chapter.  It introducing what we a \"domain specific language\" or \"DSL\".  A DSL is a notation allowing us to describe something implicitly rather than explicitly. All the programs except **translit** follow closely the original Pascal translated to Oberon-07.  **translit** book implementation is very much a result of the constraints of Pascal of the early 1980s as well as the minimalist assumption that could be made about the host operating system. I will focus on revising that program in particular bring the code up to current practice as well as offering insights I've learned.\n\n\nThe program **translit** introduces what is called a \"Domain Specific Language\".Domain specific languages or DSL for short are often simple notations to describe how to solve vary narrow problems.  If you've used any of the popular spreadsheet programs where you've entered a formula to compute something you've used a domain specific language.  If you've ever search for text in a document using a regular expression you've used a domain specific language.  By focusing a notation on a small problem space you can often come up with simple ways of expressing or composing programmatic solutions to get a job done.\n\nIn **translit** the notation let's us describe what we want to translate. At the simplest level the **translit** program takes a character and replaces it with another character. What make increases **translit** utility is that it can take a set of characters and replace it with another.  If you want to change all lower cases letters and replace them with uppercase letters. This \"from set\" and \"to set\" are easy to describe as two ranges, \"a\" to \"z\" and \"A\" to \"Z\".  Our domain notation allows us to express this as \"a-z\" and \"A-Z\".  K & P include several of features in there notation including characters to exclude from a translation as well as an \"escape notation\" for describing characters like new lines, tabs, or the characters that describe a range and exclusion (i.e. dash and caret).\n\n\n\n2.1 Putting Tabs Back\n=====================\n\n[Page 31](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/31/mode/1up)\n\nImplementing **entab** in Oberon-07 is straight forward.\nLike my [Detab](Detab.Mod) implementation I am using\na second modules called [Tabs](Tabs.Mod). This removes\nthe need for the `#include` macros used in the K & P version.\nI have used the same loop structure as K & P this time.\nThere is a difference in my `WHILE` loop. I separate the\ncharacter read from the `WHILE` conditional test.  Combining the\ntwo is common in \"C\" and is consistent with the programming style\nother books by Kernighan.  In Oberon-07 doesn't make sense at all.\nOberon's `In.Char()` is not a function returning as in the Pascal\nprimitives implemented for the K & P book or indeed like in the \"C\"\nlanguage. In Oberon's \"In\" module the status of a read operation is\nexposed by `In.Done`. I've chosen to put the next call to\n`In.Char()` at the bottom of my `WHILE` loop because it is clear\nthat it is the last think done before ether iterating again or\nexiting the loop. Other than that the Oberon version looks much\nlike K & P's Pascal.\n\n\nProgram Documentation\n---------------------\n\n[Page 32](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/32/mode/1up)\n\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPROGRAM\n\n  entab\tconvert runs of blanks into tabs\n\nUSAGE\n\n  entab\n\nFUNCTION\n\n  entab copies its input to its output, replacing strings of\n  blanks by tabs so the output is visually the same as the\n  input, but contains fewer characters. Tab stops are assumed\n  to be set every four columns (i.e. 1, 5, 9, ...), so that\n  each sequence of one to four blanks ending on a tab stop\n  is replaced by a tab character\n\nEXAMPLE\n\n  Using -> as visible tab:\n\n    entab\n      col  1   2   34  rest\n    ->col->1->2->34->rest\n\nBUGS\n\n  entab is naive about backspaces, virtical motions, and\n  non-printing characters. entab will convert  a single blank\n  to a tab if it occurs at a tab stop. The entab is not an\n  exact inverse of detab.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSource code for **Entab.Mod**\n-----------------------------\n\n~~~\n\nMODULE Entab;\n  IMPORT In, Out, Tabs;\n\nCONST\n  NEWLINE = 10;\n  TAB = 9;\n  BLANK = 32;\n\nPROCEDURE Entab();\nVAR\n  c : CHAR;\n  col, newcol : INTEGER;\n  tabstops : Tabs.TabType;\nBEGIN\n  Tabs.SetTabs(tabstops);\n  col := 1;\n  REPEAT\n    newcol := col;\n    In.Char(c);\n    IF In.Done THEN (* NOTE: We check that the read was successful! *)\n      WHILE (ORD(c) = BLANK) DO\n        newcol := newcol + 1;\n        IF (Tabs.TabPos(newcol, tabstops)) THEN\n          Out.Char(CHR(TAB));\n          col := newcol;\n        END;\n        (* NOTE: Get the next char, check the loop condition\n           and either iterate or exit the loop *)\n        In.Char(c);\n      END;\n      WHILE (col < newcol) DO\n        Out.Char(CHR(BLANK)); (* output left over blanks *)\n        col := col + 1;\n      END;\n      (* NOTE: Since we may have gotten a new char in the first WHILE\n         we need to check again if the read was successful *)\n      IF In.Done THEN\n        Out.Char(c);\n        IF (ORD(c) = NEWLINE) THEN\n          col := 1;\n        ELSE\n          col := col + 1;\n        END;\n      END;\n    END;\n  UNTIL In.Done # TRUE;\nEND Entab;\n\nBEGIN\n  Entab();\nEND Entab.\n\n~~~\n\n\n\n2.2 Overstrikes\n===============\n\n\n[Page 34](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/34/mode/1up)\n\n\nOverstrike isn't a tool that is useful today but I've included it\nsimply to be follow along the flow of the K & P book. It very much\nreflects an error where teletype like devices where still common and\nprinters printed much like typewriters did. On a 20th century\nmanual type writer you could underline a word or letter by backing\nup the carriage then typing the underscore character. Striking out\na word was accomplished by a similar technique. The mid to late\n20th century computers device retained this mechanism though by\n1980's it was beginning to disappear along with manual typewriters.\nThis program relies on the the nature of ASCII character set and\nreflects some of the non-print character's functionality. I\nfound it did not work on today's terminal emulators reliably. Your\nmileage may very nor do I have a vintage printer to test it on.\n\nOur module follows K & P design almost verbatim. The differences\nare those suggested by differences between Pascal and Oberon-07.\nLike in previous examples we don't need to use an ENDFILE constant\nas we can simply check the value of `In.Done` to determine\nif the last read was successful. This simplifies some of\nthe `IF/ELSE` logic and the termination of the `REPEAT/UNTIL`\nloop.  It makes the `WHILE/DO` loop a little more verbose.\n\nOne thing I would like to point out in the original Pascal of the\nbook is a problem often referred to as the \"dangling else\" problem.\nWhile this is usually discussed in the context of compiler\nimplementation I feel like it is a bigger issue for the person\nreading the source code. It is particularly problematic when you\nhave complex \"IF/ELSE\" sequences that are nested.  This is not\nlimited to the 1980's era Pascal. You see it in other languages\nlike C.  It is a convenience for the person typing the source code\nbut a problem for those who maintain it. We see this ambiguity in\nthe Pascal procedure **overstrike** inside the repeat loop\non [page 35](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/35/mode/1up).\nIt is made worse by the fact that K & P have taken advantage of\nomitting the semi-colons where optional. If you type in this\nprocedure and remove the indication if quickly becomes ambiguous\nabout where on \"IF/ELSE\" begins and the next ends. In Oberon-07 it\nis clear when you have a dangling \"IF\" statement. This vintage\nPascal, not so much.\n\nK & P do mention the dangling \"ELSE\" problem later in the text.\nTheir recommend practice was include the explicit final \"ELSE\"\nat a comment to avoid confusion. But you can see how easy an\nomitting the comment is in the **overstrike** program.\n\nLimitations\n-----------\n\nThis is documented \"BUG\" section describes the limitations\nwell, \"**overstrike** is naive about vertical motions and non-\nprinting characters. It produces one over struck line for each\nsequence of backspaces\". But in addition to that most printing\ndevices these days either have their own drivers or expect to work\nwith a standard like Postscript. This limited the usefulness of\nthis program today though controlling character movement in a\n\"vt100\" emulation using old fashion ASCII control codes is\nstill interesting if only for historical reasons.\n\n\nProgram Documentation\n---------------------\n\n[Page 36](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/36/mode/1up)\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPROGRAM\n\n  overstrike    replace overstrikes by multiple-lines\n\nUSAGE\n\n  overstrike\n\nFUNCTION\n\n  overstrike copies in input to its output, replacing lines\n  containing backspaces by multiple lines that overstrike\n  to print the same as input, but containing no backspaces.\n  It is assumed that the output is to be printed on a device\n  that takes the first character of each line as a carriage\n  control; a blank carriage control causes normal space before\n  print, while a plus sign '+' suppresses space before print\n  and hence causes the remainder of the line to overstrike\n  the previous line.\n\nEXAMPLE\n\n  Using <- as a visible backspace:\n\n    overstrike\n    abc<-<-<-___\n     abc\n    +___\n\nBUGS\n\n  overstrike is naive about vertical motions and non-printing\n  characters. It produces one over struck line for each sequence\n  of backspaces.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nSource code for **Overstrike.Mod**\n----------------------------------\n\n~~~\n\nMODULE Overstrike;\nIMPORT In, Out;\n\nCONST\n  NEWLINE = 10;\n  BLANK = 32;\n  PLUS = 43;\n  BACKSPACE = 8;\n\nPROCEDURE Max(x, y : INTEGER) : INTEGER;\nVAR max : INTEGER;\nBEGIN\n  IF (x > y) THEN\n    max := x\n  ELSE\n    max := y\n  END;\n  RETURN max\nEND Max;\n\nPROCEDURE Overstrike;\nCONST\n  SKIP = BLANK;\n  NOSKIP = PLUS;\nVAR\n  c : CHAR;\n  col, newcol, i : INTEGER;\nBEGIN\n  col := 1;\n  REPEAT\n    newcol := col;\n    In.Char(c);\n    (* NOTE We check In.Done on each loop evalution *)\n    WHILE (In.Done = TRUE) & (ORD(c) = BACKSPACE) DO (* eat the backspaces *)\n      newcol := Max(newcol, 1);\n      In.Char(c);\n    END;\n    (* NOTE: We check In.Done again, since we may have\n       additional reads when eating the backspaces. If\n       the previous while loop has taken us to the end of file.\n       this will be also mean In.Done = FALSE. *)\n    IF In.Done THEN\n      IF (newcol < col) THEN\n        Out.Char(CHR(NEWLINE)); (* start overstrike line *)\n        Out.Char(CHR(NOSKIP));\n        FOR i := 0 TO newcol DO\n          Out.Char(CHR(BLANK));\n        END;\n        col := newcol;\n      ELSIF (col = 1) THEN (* NOTE: In.Done already check for end of file *)\n        Out.Char(CHR(SKIP)); (* normal line *)\n      END;\n      (* NOTE: In.Done already was checked so we are in mid line *)\n      Out.Char(c);    (* normal character *)\n      IF (ORD(c) = NEWLINE) THEN\n        col := 1\n      ELSE\n        col := col + 1\n      END;\n    END;\n  UNTIL In.Done # TRUE;\nEND Overstrike;\n\nBEGIN\n  Overstrike();\nEND Overstrike.\n\n~~~\n\n\n2.3 Text Compression\n====================\n\n[Page 37](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/37/mode/1up)\n\nIn 20th century computing everything is expensive, memory,\npersistent storage computational ability in CPU.  If you were\nprimarily working with text you still worried about running out of\nspace in your storage medium. You see it in the units\nof measurement used in that era such as bytes, kilobytes, hertz and\nkilohertz. To day we talk about megabytes, gigabytes, terabytes and\npetabytes. Plain text files are a tiny size compared to must\ndigital objects today but in the late 20th century\ntheir size in storage was still a concern.  One way to solve this\nproblem was to encode your plain text to use less storage space.\nEarly attempts at file compression took advantage of repetition to\nsave space. Many text documents have repeated characters\nwhether spaces or punctuation or other formatting. This is what\ninspired the K & P implementation of **compress** and **expand**.\nToday we'd use other approaches to save space whether we were\nstoring text or a digital photograph.\n\n\nProgram Documentation\n---------------------\n\n[Page ](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/40/mode/1up)\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPROGRAM\n\n    compress    compress input by encoding repeated characters\n\nUSAGE\n\n    compress\n\nFUNCTION\n\n    compress copies its input to its output, replacing strings\n    of four or more identical characters by a code sequence so\n    that the output generally contains fewer characters than the\n    input. A run of x's is encoded as -nx, where the count n is\n    a character: 'A' calls for a repetition of one x, 'B' a\n    repetition of two x's, and so on. Runs longer than 26 are\n    broken into several shorter ones. Runs of -'s of any length\n    are encoded.\n\nEXAMPLE\n\n    compress\n    Item     Name           Value\n    Item-D Name-I Value\n    1       car             -$7,000.00\n    1-G car-J -A-$7,000.00\n    <ENDFILE>\n\nBUGS\n\n    The implementation assumes 26 legal characters beginning with A.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nSource code for **Compress.Mod**\n--------------------------------\n\n~~~\n\nMODULE Compress;\nIMPORT In, Out;\n\nCONST\n    TILDE = \"~\";\n    WARNING = TILDE;    (* ~ *)\n\n(* Min -- compute minimum of two integers *)\nPROCEDURE Min(x, y : INTEGER) : INTEGER;\nVAR min : INTEGER;\nBEGIN\n    IF (x < y) THEN\n        min := x\n    ELSE\n        min := y\n    END;\n    RETURN min\nEND Min;\n\n(* PutRep -- put out representation of run of n 'c's *)\nPROCEDURE PutRep (n : INTEGER; c : CHAR);\nCONST\n    MAXREP = 26;    (* assuming 'A' .. 'Z' *)\n    THRESH = 4;\nVAR i : INTEGER;\nBEGIN\n    WHILE (n >= THRESH) OR ((c = WARNING) & (n > 0)) DO\n        Out.Char(WARNING);\n        Out.Char(CHR((Min(n, MAXREP) - 1) + ORD(\"A\")));\n        Out.Char(c);\n        n := n - MAXREP;\n    END;\n    FOR i := n TO 1 BY (-1) DO\n        Out.Char(c);\n    END;\nEND PutRep;\n\n(* Compress -- compress standard input *)\nPROCEDURE Compress();\nVAR\n    c, lastc : CHAR;\n    n : INTEGER;\nBEGIN\n    n := 1;\n    In.Char(lastc);\n    WHILE (In.Done = TRUE) DO\n        In.Char(c);\n        IF (In.Done = FALSE) THEN\n            IF (n > 1) OR (lastc = WARNING) THEN\n                PutRep(n, lastc)\n            ELSE\n                Out.Char(lastc);\n            END;\n        ELSIF (c = lastc) THEN\n            n := n + 1\n        ELSIF (n > 1) OR (lastc = WARNING) THEN\n            PutRep(n, lastc);\n            n := 1\n        ELSE\n            Out.Char(lastc);\n        END;\n        lastc := c;\n    END;\nEND Compress;\n\n\nBEGIN\n    Compress();\nEND Compress.\n\n~~~\n\n\n\n2.4 Text Expansion\n==================\n\n[Page 41](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/41/mode/1up)\n\nOur procedures map closely to the original Pascal with a few\nsignificant differences.  As previously I've chosen a\n`REPEAT ... UNTIL` loop structure because we are always attempting\nto read at least once. The `IF THEN ELSIF ELSE` logic is a little\ndifferent. In the K & P version they combine retrieving\na character and testing its value.  This is a style common in\nlanguages like C. As previous mentioned I split the read of the\ncharacter from the test.  Aside from the choices imposed by the\n\"In\" module I also feel that retrieving the value, then testing is\na simpler statement to read. There is little need to worry about a\nside effect when you separate the action from the test. It does\nchange the structure of the inner and outer `IF` statements.\n\n\n\nProgram Documentation\n---------------------\n\n[Page 43](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/43/mode/1up)\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPROGRAM\n\n    expand  expand compressed input\n\nUSAGE\n\n    expand\n\nFUNCTION\n\n    expand copies its input, which has presumably been encoded by\n    compress, to its output, replacing code sequences -nc by the\n    repeated characters they stand for so that the text output\n    exactly matches that which was originally encoded. The\n    occurrence of the warning character - in the input means that\n    which was originally encoded. The occurrence of the warning\n    character - in the input means that the next character is a\n    repetition count; 'A' calls for one instance of the following\n    character, 'B' calls for two, and so on up to 'Z'.\n\nEXAMPLE\n\n    expand\n    Item~D Name~I Value\n    Item    Name        Value\n    1~G car~J ~A~$7,000.00\n    1       car         -$7,000.00\n    <ENDFILE>\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nSource code for **Expand.Mod**\n------------------------------\n\n~~~\nMODULE Expand;\nIMPORT In, Out;\n\nCONST\n    TILDE = \"~\";\n    WARNING = TILDE;    (* ~ *)\n    LetterA = ORD(\"A\");\n    LetterZ = ORD(\"Z\");\n\n(* IsUpper -- true if c is upper case letter *)\nPROCEDURE IsUpper (c : CHAR) : BOOLEAN;\nVAR res : BOOLEAN;\nBEGIN\n    IF (ORD(c) >= LetterA) & (ORD(c) <= LetterZ) THEN\n        res := TRUE;\n    ELSE\n        res := FALSE;\n    END\n    RETURN res\nEND IsUpper;\n\n(* Expand -- uncompress standard input *)\nPROCEDURE Expand();\nVAR\n    c : CHAR;\n    n, i : INTEGER;\nBEGIN\n    REPEAT\n        In.Char(c);\n        IF (c # WARNING) THEN\n            Out.Char(c);\n        ELSE\n            In.Char(c);\n            IF IsUpper(c) THEN\n                n := (ORD(c) - ORD(\"A\")) + 1;\n                In.Char(c);\n                IF (In.Done) THEN\n                    FOR i := n TO 1 BY -1 DO\n                        Out.Char(c);\n                    END;\n                ELSE\n                    Out.Char(WARNING);\n                    Out.Char(CHR((n - 1) + ORD(\"A\")));\n                END;\n            ELSE\n                Out.Char(WARNING);\n                IF In.Done THEN\n                    Out.Char(c);\n                END;\n            END;\n        END;\n    UNTIL In.Done # TRUE;\nEND Expand;\n\nBEGIN\n    Expand();\nEND Expand.\n\n~~~\n\n\n2.5 Command Arguments\n=====================\n\n[Page 44](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/44/mode/1up)\n\n\nProgram Documentation\n---------------------\n\n[Page 45](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/45/mode/1up)\n\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPROGRAM\n\n    echo    echo arguments to standard output\n\nUSAGE\n\n    echo [ argument ... ]\n\nFUNCTION\n\n    echo copies its command line arguments to its output as a line\n    of text with one space\n    between each argument. IF there are no arguments, no output is\n    produced.\n\nEXAMPLE\n\n    To see if your system is alive:\n\n        echo hello world!\n        hello world!\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nSource code for **Echo.Mod**\n----------------------------\n\n~~~\n\nMODULE Echo;\nIMPORT Out, Args := extArgs;\n\nCONST\n    MAXSTR = 1024; (* or whatever *)\n    BLANK = \" \";\n\n(* Echo -- echo command line arguments to output *)\nPROCEDURE Echo();\nVAR\n    i, res : INTEGER;\n    argstr : ARRAY MAXSTR OF CHAR;\nBEGIN\n    i := 0;\n    FOR i := 0 TO (Args.count - 1) DO\n        Args.Get(i, argstr, res);\n        IF (i > 0) THEN\n            Out.Char(BLANK);\n        END;\n        Out.String(argstr);\n    END;\n    IF Args.count > 0 THEN\n        Out.Ln();\n    END;\nEND Echo;\n\nBEGIN\n    Echo();\nEND Echo.\n\n~~~\n\n\n2.6 Character Transliteration\n=============================\n\n[Page 47](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/47/mode/1up)\n\n\n**translit** is the most complicated program so far in the book.\nMost of the translation process from Pascal to Oberon-07 has\nremained similar to the previous examples.\n\nMy implementation of **translit** diverges from the K & P\nimplementation at several points. Much of this is a result of\nOberon evolution beyond Pascal. First Oberon counts arrays from\nzero instead of one so I have opted to use -1 as a value to\nindicate the index of a character in a string was not found.\nEqually I have simplified the logic in `xindex()` to make it clear\nhow I am handling the index lookup described in `index()` of the\nPascal implementation. K & P implemented `makeset()` and `dodash()`.\n`dodash()` particularly looked troublesome. If you came across the\nfunction name `dodash()` without seeing the code comments\n\"doing a dash\" seems a little obscure.  I have chosen to name\nthat process \"Expand Sequence\" for clarity. I have simplified the\ntask of making sets of characters for translation into three cases\nby splitting the test conditions from the actions. First check to\nsee if we have an escape sequence and if so handle it. Second check\nto see if we have an expansion sequence and if so handle it else\nappend the char found to the end of the set being assembled. This\nresulted in `dodash()` being replaced by `IsSequence()` and\n`ExpandSequence()`.  Likewise `esc()` was replaced with `IsEscape()`\nand `ExpandEscape()`. I renamed `addchar()` to `AppendChar()`\nin the \"Chars\" module as that seemed more specific and clearer.\n\nI choose to advance the value used when expanding a set description\nin the loop inside of my `MakeSet()`. I minimized the side effects\nof the expand functions to the target destination.  It is clearer\nwhile in the `MakeSet()` loop to see the relationship of the test\nand transformation and how to advance through the string. This also\nallowed me to use fewer parameters to procedures which tends to\nmake things more readable as well as simpler.\n\nI have included an additional procedure not included in the K & P\nPascal of this program. `Error()` displays a string and halts.\nK & P provide this as part of their Pascal environment. I have\nchosen to embed it here because it is short and trivial.\n\nTranslit suggested the \"Chars\" module because of the repetition in\nprevious programs. In K & P the approach to code reuse is to create\na separate source file and to included via a pre-processor. In\nOberon we have the module concept.\n\nMy [Chars](Chars.Mod) module provides a useful set of test\nprocedures like `IsAlpha(c)`, `IsUpper(c)`, `IsLower()` in addition\nto the `CharInRange()` and `IsAlphaNum()`.  It also includes\n`AppendChar()` which can be used to append a single character value\nto an end of an array of char.\n\n\nProgram Documentation\n---------------------\n\n[Page 56](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/56/mode/1up)\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPROGRAM\n\n    translit    transliterate characters\n\nUSAGE\n\n    translit    [^]src [dest]\n\nFUNCTION\n\n    translit maps its input, on a character by character basis, and\n    writes the translated version to its output.In the simplest case,\n    each character is the argument src is translated to the\n    corresponding character is the argument dest; all other characters\n    are copies as is. Both the src and dest may contain substrings of\n    the form c1 - c2 as shorthand for all the characters in the range\n    c1..c2 and c2 must both be digits, or both be letter of the same\n    case. If dest is absent, all characters represented by src are\n    deleted. Otherwise, if dest is shorter than src, all characters\n    is src that would map to or beyond the last character in\n    dest are mapped to the last character in dest; moreover adjacent\n    instances of such characters in the input are represented in the\n    output by a single instance of the last character in dest. The\n\n        translit 0-9 9\n\n    converts each string of digits to the single digit 9.\n    Finally, if src is precedded by ^, then all but the characters\n    represented by src are taken as the source string; i.e., they are\n    all deleted if dest is absent, or they are all collapsed if the\n    last character in dest is present.\n\nEXAMPLE\n\n    To convert upper case to lower:\n\n        translit A-Z a-z\n\n    To discard punctualtion and isolate words by spaces on each line:\n\n        translit ^a-zA-Z@n \" \"\n        This is a simple-minded test, i.e., a test of translit.\n        This is a simple minded test i e a test of translit\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPascal Source\n-------------\n\n[translit.p, Page 48](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/48/mode/1up)\n\n[makeset.p, Page 52](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/52/mode/2up)\n\n\n[addstr.p, Page 53](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/53/mode/1up)\n\n[dodash.p, Page 53](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/53/mode/1up)\n\n[isalphanum.p, Page 54](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/54/mode/1up)\n\n[esc.p, Page 55](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/55/mode/1up)\n\n\n[length.p, Page 46](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/46/mode/1up)\n\n\nThe impacts of having a richer language than 1980s ISO Pascal and\nevolution in practice suggest a revision in the K & P approach. I\nhave attempted to keep the spirit of their example program while\nreflecting changes in practice that have occurred in the last four\ndecades.\n\n\nSource code for **Translit.Mod**\n--------------------------------\n\n~~~\nMODULE Translit;\nIMPORT In, Out, Args := extArgs, Strings, Chars;\n\nCONST\n    MAXSTR = 1024; (* or whatever *)\n    DASH = Chars.DASH;\n    ENDSTR = Chars.ENDSTR;\n    ESCAPE = \"@\";\n    TAB* = Chars.TAB;\n\n(* Error -- write an error string to standard out and\n   halt program *)\nPROCEDURE Error(s : ARRAY OF CHAR);\nBEGIN\n    Out.String(s);Out.Ln();\n    ASSERT(FALSE);\nEND Error;\n\n(* IsEscape - this procedure looks to see if we have an\nescape sequence at position in variable i *)\nPROCEDURE IsEscape*(src : ARRAY OF CHAR; i : INTEGER) : BOOLEAN;\nVAR res : BOOLEAN; last : INTEGER;\nBEGIN\n  res := FALSE;\n  last := Strings.Length(src) - 1;\n  IF (i < last) & (src[i] = ESCAPE) THEN\n    res := TRUE;\n  END;\n  RETURN res\nEND IsEscape;\n\n(* ExpandEscape - this procedure takes a source array, a\n   position and appends the escaped value to the destintation\n   array.  It returns TRUE on successuss, FALSE otherwise. *)\nPROCEDURE ExpandEscape*(src : ARRAY OF CHAR; i : INTEGER; VAR dest : ARRAY OF CHAR) : BOOLEAN;\nVAR res : BOOLEAN; j : INTEGER;\nBEGIN\n res := FALSE;\n j := i + 1;\n IF j < Strings.Length(src)  THEN\n    res := Chars.AppendChar(src[j], dest)\n END\n RETURN res\nEND ExpandEscape;\n\n(* IsSequence - this procedure looks at position i and checks\n   to see if we have a sequence to expand *)\nPROCEDURE IsSequence*(src : ARRAY OF CHAR; i : INTEGER) : BOOLEAN;\nVAR res : BOOLEAN;\nBEGIN\n  res := Strings.Length(src) - i >= 3;\n  (* Do we have a sequence of alphumeric character\n     DASH alpanumeric character? *)\n  IF res & Chars.IsAlphaNum(src[i]) & (src[i+1] = DASH) &\n            Chars.IsAlphaNum(src[i+2]) THEN\n      res := TRUE;\n  END;\n  RETURN res\nEND IsSequence;\n\n(* ExpandSequence - this procedure expands a sequence x\n   starting at i and append the sequence into the destination\n   string. It returns TRUE on success, FALSE otherwise *)\nPROCEDURE ExpandSequence*(src : ARRAY OF CHAR; i : INTEGER; VAR dest : ARRAY OF CHAR) : BOOLEAN;\nVAR res : BOOLEAN; cur, start, end : INTEGER;\nBEGIN\n  (* Make sure sequence is assending *)\n  res := TRUE;\n  start := ORD(src[i]);\n  end := ORD(src[i+2]);\n  IF start < end THEN\n    FOR cur := start TO end DO\n      IF res THEN\n        res := Chars.AppendChar(CHR(cur), dest);\n      END;\n    END;\n  ELSE\n    res := FALSE;\n  END;\n  RETURN res\nEND ExpandSequence;\n\n\n(* makeset -- make sets based on src expanded into destination *)\nPROCEDURE MakeSet* (src : ARRAY OF CHAR; start : INTEGER; VAR dest : ARRAY OF CHAR) : BOOLEAN;\nVAR i : INTEGER; makeset : BOOLEAN;\nBEGIN\n    i := start;\n    makeset := TRUE;\n    WHILE (makeset = TRUE) & (i < Strings.Length(src)) DO\n        IF IsEscape(src, i) THEN\n            makeset := ExpandEscape(src, i, dest);\n            i := i + 2;\n        ELSIF IsSequence(src, i) THEN\n            makeset := ExpandSequence(src, i, dest);\n            i := i + 3;\n        ELSE\n            makeset := Chars.AppendChar(src[i], dest);\n            i := i + 1;\n        END;\n    END;\n    RETURN makeset\nEND MakeSet;\n\n\n(* Index -- find position of character c in string s *)\nPROCEDURE Index* (VAR s : ARRAY OF CHAR; c : CHAR) : INTEGER;\nVAR\n    i, index : INTEGER;\nBEGIN\n    i := 0;\n    WHILE (s[i] # c) & (s[i] # ENDSTR) DO\n        i := i + 1;\n    END;\n    IF (s[i] = ENDSTR) THEN\n        index := -1; (* Value not found *)\n    ELSE\n        index := i; (* Value found *)\n    END;\n    RETURN index\nEND Index;\n\n(* XIndex -- conditionally invert value found in index *)\nPROCEDURE XIndex* (VAR inset : ARRAY OF CHAR; c : CHAR;\n    allbut : BOOLEAN; lastto : INTEGER) : INTEGER;\nVAR\n    xindex : INTEGER;\nBEGIN\n    (* Uninverted index value *)\n    xindex := Index(inset, c);\n    (* Handle inverted index value *)\n    IF (allbut = TRUE) THEN\n        IF (xindex = -1)  THEN\n            (* Translate as an inverted the response *)\n            xindex := 0; (* lastto - 1; *)\n        ELSE\n            (* Indicate no translate *)\n            xindex := -1;\n        END;\n    END;\n    RETURN xindex\nEND XIndex;\n\n(* Translit -- map characters *)\nPROCEDURE Translit* ();\nCONST\n    NEGATE = Chars.CARET; (* ^ *)\nVAR\n    arg, fromset, toset : ARRAY MAXSTR OF CHAR;\n    c : CHAR;\n    i, lastto : INTEGER;\n    allbut, squash : BOOLEAN;\n    res : INTEGER;\nBEGIN\n    i := 0;\n    lastto := MAXSTR - 1;\n    (* NOTE: We are doing low level of string manimulation. Oberon\n       strings are terminated by 0X, but Oberon compilers do not\n       automatically initialize memory to a specific state. In the\n       OBNC implementation of Oberon-07 assign \"\" to an assignment\n       like `s := \"\";` only writes a 0X to position zero of the\n       array of char. Since we are doing position based character\n       assignment and can easily overwrite a single 0X.  To be safe\n       we want to assign all the positions in the array to 0X so the\n       memory is in a known state.  *)\n    Chars.Clear(arg);\n    Chars.Clear(fromset);\n    Chars.Clear(toset);\n    IF (Args.count = 0) THEN\n        Error(\"usage: translit from to\");\n    END;\n    (* NOTE: I have not used an IF ELSE here because we have\n       additional conditions that lead to complex logic.  The\n       procedure Error() calls ASSERT(FALSE); which in Oberon-07\n       halts the program from further execution *)\n    IF (Args.count > 0) THEN\n        Args.Get(0, arg, res);\n        allbut := (arg[0] = NEGATE);\n        IF (allbut) THEN\n            i := 1;\n        ELSE\n            i := 0;\n        END;\n        IF MakeSet(arg, i, fromset) = FALSE THEN\n            Error(\"from set too long\");\n        END;\n    END;\n    (* NOTE: We have initialized our array of char earlier so we only\n       need to know if we need to update toset to a new value *)\n    Chars.Clear(arg);\n    IF (Args.count = 2) THEN\n        Args.Get(1, arg, res);\n        IF MakeSet(arg, 0, toset) = FALSE THEN\n            Error(\"to set too long\");\n        END;\n    END;\n\n    lastto := Strings.Length(toset);\n    squash := (Strings.Length(fromset) > lastto) OR (allbut);\n    REPEAT\n        In.Char(c);\n        IF In.Done THEN\n            i := XIndex(fromset, c, allbut, lastto);\n            IF (squash) & (i>=lastto) & (lastto>0) THEN (* translate *)\n                Out.Char(toset[lastto]);\n            ELSIF (i >= 0) & (lastto > 0) THEN    (* translate *)\n                Out.Char(toset[i]);\n            ELSIF i = -1 THEN                        (* copy *)\n              (* Do not translate the character *)\n              Out.Char(c);\n              (* NOTE: No else clause needed as not writing out\n\t\t\t     a cut value is deleting *)\n            END;\n        END;\n    UNTIL (In.Done # TRUE);\nEND Translit;\n\nBEGIN\n    Translit();\nEND Translit.\n\n~~~\n\n\n\nIn closing\n==========\n\nIn this chapter we interact with some of the most common features\nof command line programs available on POSIX systems. K & P have given\nus a solid foundation on which to build more complex and ambitious\nprograms. In the following chapters the read will find an\naccelerated level of complexity bit also programs that are\nsignificantly more powerful.\n\nOberon language evolved with the Oberon System which had a very\ndifferent rich text user interface when compared with POSIX.\nFortunately Karl's OBNC comes with a set of modules that make\nOberon-07 friendly for building programs for POSIX operating systems.\nI've taken advantage of his `extArgs` module much in the way\nthat K & P relied on a set of primitive tools to provide a common\nprogramming environment. K & P's version of\n[implementation of primitives](https://archive.org/details/softwaretoolsinp00kern/page/315/mode/1up)\nlisted in their appendix. Karl's OBNC extensions modules are\ndescribed on [website](https://miasap.se/obnc/obncdoc/ext/).\nOther Oberon compilers provide similar modules though implementation\nspecific. A good example is Spivey's [Oxford Oberon-2 Compiler](https://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler).\nK & P chose to target multiple Pascal implementations, I have the\nluxury of targeting one Oberon-07 implementation. That said if you\nadded a pre-processor like K & P did you could also take their approach\nto allow you Oberon-07 code to work across many Oberon compiler\nimplementations. I leave that as an exercise for the reader.\n\nI've chosen to revise some of the code presented in K & P's book. I\nbelieve the K & P implementations still contains wisdom in their\nimplementations. They had different constraints and thus made\ndifferent choices in implementation. Understand the trade offs and\nchallenges to writing portable code capable of running in very\ndivergent set of early 1980's operating systems remains useful today.\n\nCompiling with OBNC:\n\n~~~\n\n    obnc -o entab Entab.Mod\n    obnc -o overstrike Overstrike.Mod\n    obnc -o compress Compress.Mod\n    obnc -o expand Expand.Mod\n    obnc -o echo Echo.Mod\n    obnc -o translit Translit.Mod\n\n~~~\n\n+ [Entab](Entab.Mod)\n    + [Tabs](Tabs.Mod), this one visited this one in last installment.\n+ [Overstrike](Overstrike.Mod)\n+ [Compress](Compress.Mod)\n+ [Expand](Expand.Mod)\n+ [Echo](Echo.Mod)\n+ [Translit](Translit.Mod)\n\t+ [Chars](Chars.Mod)\n\n<!--\nNext and Previous\n-----------------\n\n+ Next: [Files]()\n-->\n\nPrevious\n--------\n\n+ Previous: [Getting Started](../../09/29/Software-Tools-1.html)\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "keywords": [
          "Oberon",
          "Pascal",
          "Programming"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 2,
        "series": "Software Tools",
        "title": "Software Tools, Filters"
      },
      "url": "posts/2020/10/31/Filters.json"
    },
    {
      "content": "\n\nPandoc Partial Templates\n========================\n\nMost people know about [Pandoc](https://pandoc.org/) from its\nfantastic ability to convert various markup formats from one to\nanother. A little less obvious is Pandoc can be a template engine\nfor rendering static websites allowing you full control over the\nrendered content.\n\nThe main Pandoc documentation of the template engine can be found\nin the [User Guide](https://pandoc.org/MANUAL.html#templates).\nThe documentation is complete in terms of describing the template\ncapabilities but lacks a tutorial for using as a replacement for more\nambitious rendering systems like [Jekyll](https://jekyllrb.com/) or\n[Hugo](https://gohugo.io/). Pandoc takes a vary direct approach and\ncan be deceptively simple to implement.\n\nUse your own template\n---------------------\n\nFirst thing in this tutorial is to use our own template with Pandoc\nwhen rendering a single webpage. You use the `–-template` option to\nprovide your a template name. I think of this as the page level template.\nThis template, as I will show later, can then call other partial\ntemplates as needed.\n\nExample, render the [Pandoc-Partials.txt](Pandoc-Partials.txt) file using the\ntemplate named [index1.tmpl](index1.tmpl):\n\n~~~{.shell}\n\n    pandoc --from=markdown --to=html \\\n        --template=index1.tmpl Pandoc-Partials.txt > index1.htm\n\n~~~\n\nThis is a simple template page level template.\n\n~~~{.html-code}\n\n    <!DOCTYPE html>\n    <html>\n    <head>\n    </head>\n    <body>\n    ${body}\n    </body>\n    </html>\n\n~~~\n\nWhen we run our Pandoc command the file called\n[Pandoc-Partials.txt](Pandoc-Partials.txt) is passed into the template as\nthe \"body\" element where it says `${body}`. See this Pandoc \n[User Guide](https://pandoc.org/MANUAL.html#templates) for the basics.\n\nExample 1 rendered: [index1.htm](index1.htm)\n\nVariables and metadata\n----------------------\n\nPandoc's documentation is good at describing the\nways of referencing a variable or using the built-in\ntemplate functions. Where do the variables get their values?\nThe easiest way I've found is to set the variables values in\na JSON metadata file.  While Pandoc can also use the metadata\ndescribed in YAML front matter Pandoc doesn't support some of the\nother common front matter formats.  If you're using another format\nlike JSON or TOML for front matter there are tools which can split\nthe front matter from the rest of the markdown document. For\nthis example I have created the metadata as JSON in a file\ncalled [metadata.json](metadata.json).\n\nExample [metadata.json](metadata.json):\n\n~~~{.json}\n\n    {\n        \"title\": \"Pandoc Partial Examples\",\n        \"nav\": [\n            {\"label\": \"Pandoc-Partials\", \"href\": \"Pandoc-Partials.html\" },\n            {\"label\": \"Version 1\", \"href\": \"index1.htm\" },\n            {\"label\": \"Version 2\", \"href\": \"index2.htm\" },\n            {\"label\": \"Version 3\", \"href\": \"index3.htm\" }\n        ]\n    }\n\n~~~\n\nLet's modify our initial template to include our simple navigation\nand title.\n\nExample [index2.tmpl](index2.tmpl):\n\n~~~{.html-code}\n\n    <!DOCTYPE html>\n    <html>\n    <head>\n      ${if(title)}<title>${title}</title>${endif}\n    </head>\n    <body>\n    <nav>\n    ${for(nav)}<a href=\"${it.href}\">${it.label}</a>${sep}, ${endfor}\n    </nav>\n    <section>\n    ${body}\n    </section>\n    </body>\n    </html>\n\n~~~\n\nWe would include our navigation metadata with a Pandoc command like\n\n~~~{.shell}\n\n    pandoc --from=markdown --to=html \\\n           --template=index2.tmpl \\\n           --metadata-file=metadata.json Pandoc-Partials.txt > index2.htm\n\n~~~\n\nWhen we render this we now should be able to view a web page\nwith simple navigation driven by the JSON file as well as the\nbody content contained in the Pandoc-Partials.txt file.\n\nExample 2 rendered: [index2.htm](index2.htm)\n\nPartials\n--------\n\nSometimes you have more complex documents. Putting this all in\none template can become tedious. Web designers use a term called\n\"partials\". This usually means a template for a \"part\" of a page.\nIn our initial example we can split our navigation into it's own\ntemplate.\n\nImplementing partials\n---------------------\n\nPandoc will look in the current directory for partials as well\nas in a sub directory called \"templates\" of the current direct.\nIn this example I am going to include my partial template for\nnavigation in the current directory along side my\n[index3.tmpl](index3.tmpl).  My navigation template is called\n[nav.tmpl](nav.tmpl).\n\nHere's my partial template:\n\n~~~{.html-code}\n\n    <nav>\n    ${for(nav)}<a href=\"${it.href}\">${it.label}</a>${sep}, ${endfor}\n    </nav>\n\n~~~\n\nHere's my third iteration of our index template, [index3.tmpl](index3.tmpl).\n\n~~~{.html-code}\n\n    <!DOCTYPE html>\n    <html>\n    <head>\n    ${if(title)}<title>${title}</title>${endif}\n    </head>\n    <body>\n    ${if(nav)}\n    ${nav.tmpl()}\n    ${endif}\n    <section>\n    ${body}\n    </section>\n    </body>\n    </html>\n\n~~~\n\nPandoc only requires you to reference the partial by using\nits base name. Many people will name their templates with the\nextension \".html\". I find this problematic as if you're trying\nto list the templates in the directory you can not easily list\nthem separately. I use the \".tmpl\" extension to identify my templates.\nSince I have other documents that share the base name \"nav\" I\nexplicit call my navigation partial using the full filename followed\nby the open and closed parenthesis. I have also chosen to wrap\nthe template in an \"if\" condition. That way if I don't want navigation\non a page I skip defining it in my metadata file.\n\nInside the partial template we inherit the parent metadata object.\nYou can use all the built-in Pandoc template functions and variables\nprovided by Pandoc in your partial templates.\n\nPutting it all together:\n\n~~~{.shell}\n\n    pandoc --from=markdown --to=html \\\n           --template=index3.tmpl \\\n           --metadata-file=metadata.json Pandoc-Partials.txt > index3.htm\n\n~~~\n\nExample 3 rendered: [index3.htm](index3.htm)\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "keywords": [
          "Pandoc",
          "Templates"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 1,
        "series": "Pandoc Techniques",
        "title": "Pandoc Partials"
      },
      "url": "posts/2020/11/09/Pandoc-Partials.json"
    },
    {
      "content": "\n\nPandoc & Metadata \n=================\n\nPandoc supports three ways of providing metadata to its template\nengine. \n\n1. Front matter\n2. Command line optional metadata\n3. A JSON metadata file.\n\nFront matter\n------------\n\nFront matter is a community term that comes from physical world\nof paper books and articles.  It is the information that comes \nbefore the primary content.  This information might be things \nlike title, author, publisher and publication date. These days \nit'll also include things like identifiers like ISSN, ISBN possibly \nnewer identifiers like DOI or ORCID. In the library and programming\ncommunity we refer to this type of structured information as\nmetadata.  Data about the publication or article.\n\nMany publication systems like TeX/LaTeX support provided means of \nincorporating metadata into the document.  When simple markup formats \nlike Markdown, Textile and Asciidoc became popular the practice was \ncontinued by including the metadata in some sort of structured encoding\nat the beginning of the document. The community adopted the term from\nthe print world, \"front matter\". \n\nPandoc provides for several ways of working with metadata and supports\none format of front matter encoding called [YAML](https://yaml.org/). \nOther markup processors support other encoding of front matter. Two\npopular alternatives of encoding are [TOML](https://toml.io/en/) and \n[JSON](https://json.org).  If you use one of the alternative encoding\nfor your front matter then you'll need to split the front matter\nout of your document before processing with Pandoc[^1].  \n\n[^1]: The [MkPage Project](https://caltechlibrary.github.io/mkpage/) provides a tool called [frontmatter](https://caltechlibrary.github.io/mkpage/docs/frontmatter/) that can be easy or your can easily roll your own in Python or other favorite language.\n\n\nIf you provide YAML formatted front matter Pandoc will pass this\nmetadata on and make it available to it's template engine and the\ntemplates you create to render content with Pandoc. See the Pandoc\nUser Guide section [YAML metadata blocks](https://pandoc.org/MANUAL.html#extension-yaml_metadata_block) for more details. If you've used another\nencoding of front matter then the metadata file approach is probably\nthe ticket.\n\nMetadata passed by command line\n-------------------------------\n\nIf you only have a metadata elements you would like to\nmake available to the template (e.g. title, pub date) you\ncan easily add them using the `--metadata` command line option.\nThis is documented in the Pandoc User Guide under the heading\n[Reader Options](https://pandoc.org/MANUAL.html). Here's a simple\nexample where we have a title, \"U. S. Constitution\" and a\npublication date of \"September 28, 1787\".\n\n~~~{.shell}\n    pandoc --metadata \\\n        title=\"U. S. Constitution\" \\\n        pubdate=\"September 28, 1787\" \\\n        --from markdown --to html --template doc1.tmpl \\\n        constitution.txt\n~~~\n\nThe template now has two additional values available as metadata\nin addition to `body`, namely `pubdate` and `title`. Here's an\nexample template [doc1.tmpl](doc1.tmpl).\n\n~~~\n\n   <!DOCTYPE html>\n   <html>\n   <head>\n       <title>${title}</title>\n   </head>\n   <body>\n      <h1>${title}</h1>\n      <h2>${pubdate}</h2>\n      <p>\n      ${body}\n      <p>\n   </body>\n   </html>\n\n~~~\n\nMore complex metadata is better suited to creating a JSON document\nwith the structure you need to render your template.\n\n\nMetadata file\n-------------\n\nMetadata files can be included with the option `--metadata-file`. This\nlike the `--metadata` option are discussed in the Pandoc User Guide under\nthe [Read Options(https://pandoc.org/MANUAL.html) heading.  The JSON \ndocument should contain an Object where each attribute corresponds to\nthe variable you wish to referenced in template.  Pandoc's template\nengine support both single values but also objects and arrays. In this\nway you can structure the elements you wish to include even elements\nwhich are iterative (e.g. a list of links or topics). Below is a\nJSON data structure that includes the page title as well as links\nfor the navigation.  The nav attribute holds a list of objects \nwith attributes of href and label containing data that will be used\nto render a list of anchor elements in the template.\n\n\n~~~{.json}\n\n    {\n        \"title\": \"U. S. Constitution\",\n        \"pubdate\": \"September 28, 1787\",\n        \"nav\": [\n            {\"label\": \"Pandoc Metadata\", \"href\": \"Pandoc-Metadata.html\" },\n            {\"label\": \"Magnacarta\", \"href\": \"magnacarta.html\" },\n            {\"label\": \"Declaration of Independence\", \"href\": \"independence.html\" },\n            {\"label\": \"U. S. Constitution\", \"href\": \"constitution.html\"}\n        ]\n    }\n\n~~~\n\nHere's a revised template to include the navigation,\nsee [doc2.tmpl](doc2.tmpl).\n\n~~~\n\n   <!DOCTYPE html>\n   <html>\n   <head>\n       <title>${title}</title>\n   </head>\n   <body>\n      <nav>\n      ${for(nav)}<a href=\"${nav.href}\">${nav.label}</a>${sep}, ${endfor}\n      </nav>\n      <h1>${title}</h1>\n      ${if(pubdate)}<h2>${pubdate}</h2>${endif}\n      <p>\n      ${body}\n      <p>\n   </body>\n   </html>\n\n~~~\n\n\nCombining Techniques\n--------------------\n\nIt is worth noting that these approaches can be mixed and matched.\nIn the following example I use the same [metadata.json](metadata.json)\nfile which has title and pubdate attributes but override them\nusing the command line `--metadata` option. In this way I can use that \nfile along with [doc2.tmpl](doc2.tmpl) and render each \nTo render the constitution page from a Markdown version of the \nU. S. Constitution you could use the following Pandoc command:\n\n~~~{.shell}\n\n\tpandoc --from markdown --to html --template doc2.tmpl \\\n        --metadata-file metadata.json \\\n        --metadata title=\"Magna Carta\" \\\n\t\t--metadata pubdate=\"1215\" \\\n\t\t-o magnacarta.html \\\n\t\tmagnacarta.txt\n\n\tpandoc --from markdown --to html --template doc2.tmpl \\\n        --metadata-file metadata.json \\\n        --metadata title=\"The Declaration of Indepenence\" \\\n\t\t--metadata pubdate=\"July 4, 1776\" \\\n        -o independence.html \\\n        independence.txt\n\n\tpandoc --from markdown --to html --template doc2.tmpl \\\n        --metadata-file metadata.json \\\n        --metadata title=\"U. S. Constitution\" \\\n\t\t--metadata pubdate=\"September 28, 1787\" \\\n        -o constitution.html \\\n        constitution.txt\n\n~~~\n\nSee [Magna Carta](magnacarta.html), [The Declaration of Independence](independence.html), [U. S. Constitution](constitution.html)\n\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "keywords": [
          "Pandoc",
          "Metadata",
          "Templates"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 2,
        "series": "Pandoc Techniques",
        "title": "Pandoc & Metadata"
      },
      "url": "posts/2020/11/11/Pandoc-Metadata.json"
    },
    {
      "content": "\n\nChars\n=====\n\nThis module provides common character oriented tests.\n\nInRange\n: Check to see if a character, c, is in an inclusive range from a lower to upper character.\n\nIsUpper\n: Check to see if a character is upper case\n\nIsLower\n: Check to see if a character is lower case\n\nIsAlpha\n: Check to see if a character is alphabetic, i.e. in the range of \"a\" to \"z\"\nor \"A\" to \"Z\".\n\nIsDigit\n: Check to see if a character is a digit, i.e. in range of \"0\" to \"9\"\n\nIsAlphaNum\n: Check to see if a character is alpha or a digit\n\nIsSpace\n: Check to see if a character is a space, tab, carriage return or line feed\n\nAppendChar\n: Append a single char to the end of an ARRAY OF CHAR adjusting the terminating null character and return TRUE on success or FALSE otherwise.\n\nAppendChars\n: Append an ARRAY OF CHAR to another the destination ARRAY OF CHAR.\n\nEqual\n: Compares two ARRAY OF CHAR and returns TRUE if they match, FALSE otherwise\n\nClear\n: Sets all cells in an ARRAY OF CHAR to 0X.\n\nTrimSpace\n: Trim the leading and trailing space characters from an ARRAY OF CHAR\n\nTrimLeftSpace\n: Trim the leading space characters from an ARRAY OF CHAR\n\nTrimRightSpace\n: Trim the trailing space characters from an ARRAY OF CHAR\n\nStartsWith\n: Checks to see if a prefix ARRAY OF CHAR matches a target ARRAY OF CHAR return TRUE if found, FALSE otherwise\n\nEndsWith\n: Checks to see if a suffix ARRAY OF CHAR matches a target ARRAY OF CHAR return TRUE if found, FALSE otherwise\n\nTrimPrefix\n: Trim a prefix ARRAY OF CHAR from a target ARRAY OF CHAR\n\nTrimSuffix\n: Trim a suffix ARRAY OF CHAR from a target ARRAY OF CHAR\n\n\n\n\nSource code for **Chars.Mod**\n-----------------------------\n\n~~~\nMODULE Chars;\n\nIMPORT Strings;\n\nCONST\n    MAXSTR* = 1024; (* or whatever *)\n    (* byte constants *)\n    LF* = 10;\n    CR* = 13;\n    (* Character constants *)\n    ENDSTR* = 0X;\n    NEWLINE* = 10X;\n    TAB* = 9X;\n    SPACE* = \" \";\n    DASH* = \"-\";\n    CARET* = \"^\";\n    TILDE* = \"~\";\n    QUOTE* = CHR(34);\n\n(* InRange -- given a character to check and an inclusive range of\ncharacters in the ASCII character set. Compare the ordinal values\nfor inclusively. Return TRUE if in range FALSE otherwise. *)\nPROCEDURE InRange* (c, lower, upper : CHAR) : BOOLEAN;\nVAR inrange : BOOLEAN;\nBEGIN\n  IF (ORD(c) >= ORD(lower)) & (ORD(c) <= ORD(upper)) THEN\n    inrange := TRUE;\n  ELSE\n    inrange := FALSE;\n  END;\n  RETURN inrange\nEND InRange;\n\n(* IsUpper return true if the character is an upper case letter *)\nPROCEDURE IsUpper*(c : CHAR) : BOOLEAN;\nVAR isupper : BOOLEAN;\nBEGIN\n    IF InRange(c, \"A\", \"Z\") THEN\n        isupper := TRUE;\n    ELSE\n        isupper := FALSE;\n    END\n    RETURN isupper\nEND IsUpper;\n\n\n(* IsLower return true if the character is a lower case letter *)\nPROCEDURE IsLower*(c : CHAR) : BOOLEAN;\nVAR islower : BOOLEAN;\nBEGIN\n    IF InRange(c, \"a\", \"a\") THEN\n        islower := TRUE;\n    ELSE\n        islower := FALSE;\n    END\n    RETURN islower\nEND IsLower;\n\n(* IsDigit return true if the character in the range of \"0\" to \"9\" *)\nPROCEDURE IsDigit*(c : CHAR) : BOOLEAN;\nVAR isdigit : BOOLEAN;\nBEGIN\n    IF InRange(c, \"0\", \"9\") THEN\n        isdigit := TRUE;\n    ELSE\n        isdigit := FALSE;\n    END;\n    RETURN isdigit\nEND IsDigit;\n\n(* IsAlpha return true is character is either upper or lower case letter *)\nPROCEDURE IsAlpha*(c : CHAR) : BOOLEAN;\nVAR isalpha : BOOLEAN;\nBEGIN\n    IF IsUpper(c) OR IsLower(c) THEN\n        isalpha := TRUE;\n    ELSE\n        isalpha := FALSE;\n    END;\n    RETURN isalpha\nEND IsAlpha;\n\n(* IsAlphaNum return true is IsAlpha or IsDigit *)\nPROCEDURE IsAlphaNum* (c : CHAR) : BOOLEAN;\nVAR isalphanum : BOOLEAN;\nBEGIN\n    IF IsAlpha(c) OR IsDigit(c) THEN\n        isalphanum := TRUE;\n    ELSE\n        isalphanum := FALSE;\n    END;\n    RETURN isalphanum\nEND IsAlphaNum;\n\n(* IsSpace returns TRUE if the char is a space, tab, carriage return or line feed *)\nPROCEDURE IsSpace*(c : CHAR) : BOOLEAN;\nVAR isSpace : BOOLEAN;\nBEGIN\n\tisSpace := FALSE;\n\tIF (c = SPACE) OR (c = TAB) OR (ORD(c) = CR) OR (ORD(c) = LF) THEN\n    \tisSpace := TRUE;\n\tEND;\n\tRETURN isSpace\nEND IsSpace;\n\n(* AppendChar - this copies the char and appends it to\n   the destination. Returns FALSE if append fails. *)\nPROCEDURE AppendChar*(c : CHAR; VAR dest : ARRAY OF CHAR) : BOOLEAN;\nVAR res : BOOLEAN; l : INTEGER;\nBEGIN\n  l := Strings.Length(dest);\n  (* NOTE: we need to account for a trailing 0X to end\n     the string. *)\n  IF l < (LEN(dest) - 1) THEN\n    dest[l] := c;\n    dest[l + 1] := 0X;\n    res := TRUE;\n  ELSE\n    res := FALSE;\n  END;\n  RETURN res\nEND AppendChar;\n\n(* AppendChars - copy the contents of src ARRAY OF CHAR to end of\n   dest ARRAY OF CHAR *)\nPROCEDURE AppendChars*(src : ARRAY OF CHAR; VAR dest : ARRAY OF CHAR);\nVAR i, j : INTEGER;\nBEGIN\n  i := 0;\n  WHILE (i < LEN(dest)) & (dest[i] # 0X) DO\n    i := i + 1;\n  END;\n  j := 0;\n  WHILE (i < LEN(dest)) & (j < Strings.Length(src)) DO\n    dest[i] := src[j];\n    i := i + 1;\n    j := j + 1;\n  END;\n  WHILE i < LEN(dest) DO\n    dest[i] := 0X;\n    i := i + 1;\n  END;\nEND AppendChars;\n\n(* Equal - compares two ARRAY OF CHAR and returns TRUE\nif the characters match up to the end of string, FALSE otherwise. *)\nPROCEDURE Equal*(a : ARRAY OF CHAR; b : ARRAY OF CHAR) : BOOLEAN;\nVAR isSame : BOOLEAN; i : INTEGER;\nBEGIN\n  isSame := (Strings.Length(a) = Strings.Length(b));\n  i := 0;\n  WHILE isSame & (i < Strings.Length(a)) DO\n    IF a[i] # b[i] THEN\n      isSame := FALSE;\n    END;\n    i := i + 1;\n  END;\n  RETURN isSame\nEND Equal;\n\n\n(* StartsWith - check to see of a prefix starts an ARRAY OF CHAR *)\nPROCEDURE StartsWith*(prefix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR) : BOOLEAN;\nVAR startsWith : BOOLEAN; i: INTEGER;\nBEGIN\n    startsWith := FALSE;\n    IF Strings.Length(prefix) <= Strings.Length(src) THEN\n        startsWith := TRUE;\n        i := 0;\n        WHILE (i < Strings.Length(prefix)) & startsWith DO\n            IF prefix[i] # src[i] THEN\n                startsWith := FALSE;\n            END;\n            i := i + 1;\n        END;\n    END;    \n    RETURN startsWith\nEND StartsWith;\n\n(* EndsWith - check to see of a prefix starts an ARRAY OF CHAR *)\nPROCEDURE EndsWith*(suffix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR) : BOOLEAN;\nVAR endsWith : BOOLEAN; i, j : INTEGER;\nBEGIN\n    endsWith := FALSE;\n    IF Strings.Length(suffix) <= Strings.Length(src) THEN\n        endsWith := TRUE;\n        i := 0;\n        j := Strings.Length(src) - Strings.Length(suffix);\n        WHILE (i < Strings.Length(suffix)) & endsWith DO\n            IF suffix[i] # src[j] THEN\n                endsWith := FALSE;\n            END;\n            i := i + 1;\n            j := j + 1;\n        END;\n    END;\n    RETURN endsWith\nEND EndsWith;\n\n\n(* Clear - resets all cells of an ARRAY OF CHAR to 0X *)\nPROCEDURE Clear*(VAR a : ARRAY OF CHAR);\nVAR i : INTEGER;\nBEGIN\n  FOR i := 0 TO (LEN(a) - 1) DO\n    a[i] := 0X;\n  END;\nEND Clear;\n\n(* Shift returns the first character of an ARRAY OF CHAR and shifts the\nremaining elements left appending an extra 0X if necessary *)\nPROCEDURE Shift*(VAR src : ARRAY OF CHAR) : CHAR;\nVAR i, last : INTEGER; c : CHAR;\nBEGIN\n    i := 0;\n    c := src[i];\n    Strings.Delete(src, 0, 1);\n    last := Strings.Length(src) - 1;\n    FOR i := last TO (LEN(src) - 1) DO\n        src[i] := 0X;\n    END;\n    RETURN c\nEND Shift;\n\n(* Pop returns the last non-OX element of an ARRAY OF CHAR replacing\n   it with an OX *)\nPROCEDURE Pop*(VAR src : ARRAY OF CHAR) : CHAR;\nVAR i, last : INTEGER; c : CHAR;\nBEGIN\n\t(* Move to the last non-0X cell *)\n\ti := 0;\n\tlast := LEN(src);\n\tWHILE (i < last) & (src[i] # 0X) DO\n\t   i := i + 1;\n\tEND;\n\tIF i > 0 THEN\n\t\ti := i - 1;\n\tELSE\n\t\ti := 0;\n\tEND;\n\tc := src[i];\n\tWHILE (i < last) DO\n\t\tsrc[i] := 0X;\n\t\ti := i + 1;\n\tEND;\n\tRETURN c\nEND Pop;\n\n(* TrimLeftSpace - remove leading spaces from an ARRAY OF CHAR *)\nPROCEDURE TrimLeftSpace*(VAR src : ARRAY OF CHAR);\nVAR i : INTEGER;\nBEGIN\n    (* find the first non-space or end of the string *)\n    i := 0;\n    WHILE (i < LEN(src)) & IsSpace(src[i]) DO\n        i := i + 1;\n    END;\n    (* Trims the beginning of the string *)\n    IF i > 0 THEN\n        Strings.Delete(src, 0, i);\n    END;\nEND TrimLeftSpace;\n\n(* TrimRightSpace - remove the trailing spaces from an ARRAY OF CHAR *)\nPROCEDURE TrimRightSpace*(VAR src : ARRAY OF CHAR);\nVAR i, l : INTEGER; \nBEGIN\n    (* Find the first 0X, end of string *)\n\tl := Strings.Length(src);\n\ti := l - 1;\n\t(* Find the start of the trailing space sequence *)\n\tWHILE (i > 0) & IsSpace(src[i]) DO\n\t\ti := i - 1;\n\tEND;\n\t(* Delete the trailing spaces *)\n\tStrings.Delete(src, i + 1, l - i);\nEND TrimRightSpace;\n\n(* TrimSpace - remove leading and trailing space CHARS from an ARRAY OF CHAR *)\nPROCEDURE TrimSpace*(VAR src : ARRAY OF CHAR);\nBEGIN\n\tTrimLeftSpace(src);\n\tTrimRightSpace(src);    \nEND TrimSpace;    \n    \n\n(* TrimPrefix - remove a prefix ARRAY OF CHAR from a target ARRAY OF CHAR *)\nPROCEDURE TrimPrefix*(prefix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR);\nVAR l : INTEGER;\nBEGIN\n    IF StartsWith(prefix, src) THEN\n         l := Strings.Length(prefix);\n         Strings.Delete(src, 0, l);\n    END;\nEND TrimPrefix;\n\n(* TrimSuffix - remove a suffix ARRAY OF CHAR from a target ARRAY OF CHAR *)\nPROCEDURE TrimSuffix*(suffix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR);\nVAR i, l : INTEGER;\nBEGIN\n\tIF EndsWith(suffix, src) THEN\n\t\tl := Strings.Length(src) - 1;\n\t\tFOR i := ((l - Strings.Length(suffix)) + 1) TO l DO\n\t\t\tsrc[i] := 0X;\n\t\tEND;\n\tEND;\nEND TrimSuffix;\n\n\nEND Chars.\n\n~~~\n\n\n",
      "data": {
        "title": "Chars"
      },
      "url": "posts/2020/11/27/Chars.json"
    },
    {
      "content": "\nClock\n=====\n\nThis is a C time library wrapper for getting system time\nto support Dates.Mod. The procedures are read only as\nsetting time is non-standard on many Unix-like systems[^1].\nThe two procedures follow the A2 style procedure signatures\nadjusted for Oberon-07.\n\n\n[^1]: Eric Raymond discusses time functions, http://www.catb.org/esr/time-programming/\n\n\n\nSource code for **Clock.Mod**\n-----------------------------\n\n~~~\nMODULE Clock;\n\nPROCEDURE GetRtcTime*(VAR second, minute, hour, day, month, year : INTEGER);\nBEGIN\nEND GetRtcTime;\n\nPROCEDURE Get*(VAR time, date : INTEGER);\nBEGIN\nEND Get;\n\nEND Clock.\n\n~~~\n\n\nThe C Source generated by OBNC then modified for this module.\n\n~~~\n/*GENERATED BY OBNC 0.16.1*/\n\n#include \".obnc/Clock.h\"\n#include <obnc/OBNC.h>\n#include <time.h>\n\n#define OBERON_SOURCE_FILENAME \"Clock.Mod\"\n\nvoid Clock__GetRtcTime_(OBNC_INTEGER *second_, OBNC_INTEGER *minute_, OBNC_INTEGER *hour_, OBNC_INTEGER *day_, OBNC_INTEGER *month_, OBNC_INTEGER *year_)\n{\n    time_t now;\n    struct tm *time_info;\n    now = time(NULL);\n    time_info = localtime(&now);\n    *second_ = time_info->tm_sec;\n    *minute_ = time_info->tm_min;\n    *hour_ = time_info->tm_hour;\n    *day_ = time_info->tm_mday;\n    *month_ = time_info->tm_mon;\n    *year_ = (time_info->tm_year) + 1900;\n}\n\n\nvoid Clock__Get_(OBNC_INTEGER *time_, OBNC_INTEGER *date_)\n{\n\tOBNC_INTEGER second_, minute_, hour_, day_, month_, year_;\n\n\tClock__GetRtcTime_(&second_, &minute_, &hour_, &day_, &month_, &year_);\n\t(*time_) = ((hour_ * 4096) + (minute_ * 64)) + second_;\n\t(*date_) = ((year_ * 512) + (month_ * 32)) + day_;\n}\n\n\nvoid Clock__Init(void)\n{\n}\n~~~\n",
      "data": {
        "title": "Clock"
      },
      "url": "posts/2020/11/27/Clock.json"
    },
    {
      "content": "\n\nDates and Clock\n===============\n\nBy R. S. Doiel, 2020-11-27\n\nThe [Oakwood](http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines)\nguidelines specified a common set of modules for Oberon-2 for writing\nprograms outside of an Oberon System. A missing module from the Oakwood\nguidelines is modules for working with dates and the system clock.\nFortunately the A2 Oberon System[^1] provides a template for that\nfunctionality. In this article I am exploring implementing the\n[Dates](Dates.Mod) and [Clock](Clock.Mod) modules for Oberon-07. I\nalso plan to go beyond the A2 implementations and provide additional\nfunctionality such as parsing procedures and the ability to work with\neither the date or time related attributes separately in the\n`Dates.DateTime` record.\n\n[^1]: A2 information can be found in the [Oberon wikibook](https://en.wikibooks.org/wiki/Oberon#In_A2)\n\nDivergences\n-----------\n\nOne of the noticeable differences between Oberon-07 and Active Oberon\nis the types that functional procedures can return. We cannot return\nan Object in Oberon-07. This is not much of a handicap as we have\nvariable procedure parameters.  Likewise Active Oberon provides\na large variety of integer number types. In Oberon-07 we have only\nINTEGER. Where I've create new procedures I've used the Oberon idiom\nof read only input parameters followed by variable parameters with\nside effects and finally parameters for the target record or values\nto be updated.\n\n\nSimilarities\n------------\n\nIn spite of the divergence I have split the module into two.\nThe [Dates](Dates.html) module is the one you would include in your\nprogram, it provides a DateTime record type which holds the integer\nvalues for year, month, day, hour, minute and second. It provides the\nmeans of parsing a date or time string, comparison, difference and addition\nof dates.  The second module [Clock](Clock.html) provides a mechanism\nto retrieve the real time clock value from the host system and map the\nC based time object into our own DateTime record.  Clock is specific to\nOBNC method of interfacing to the C standard libraries of the host system.\nIf you were to use a different Oberon compiled such as the Oxford\nOberon Compiler you would need to re-implement Clock. Dates itself\nshould be system independent and work with Oberon-07 compilers generally.\n\nClock\n-----\n\nThe Clock module is built from a skeleton in Oberon-07 describing the\nsignatures of the procedure and an implementation in [C](Clock.c) that\nis built using the technique for discussed in my post\n[Combining Oberon-07 and C with OBNC](../../05/01/Combining-Oberon-and-C.html). In that article I outline Karl's three step process to create a\nmodule that will be an interface to C code.  In Step one I create\nthe Oberon module. Normally I'd leave all procedures empty and\ndevelop them in C. In this specific case I went ahead and wrote\nthe procedure called `Get` in Oberon and left the procedure `GetRtcTime`\nblank. This allowed OBNC to generate the C code for `Get` saving\nme some time and create the skeleton for `GetRtcTime` which does\nthe work interfacing with the system clock via C library calls.\n\nThe interface Oberon module looked like this:\n\n~~~{.oberon}\n\nMODULE Clock;\n\nPROCEDURE GetRtcTime*(VAR second, minute, hour, day, month, year : INTEGER);\nBEGIN\nEND GetRtcTime;\n\nPROCEDURE Get*(VAR time, date : INTEGER);\nVAR\n    second, minute, hour, day, month, year : INTEGER;\nBEGIN\n\tGetRtcTime(second, minute, hour, day, month, year);\n\ttime = ((hour * 4096) + (minute * 64)) + second;\n\tdate = ((year * 512) + (month * 32)) + day;\nEND Get;\n\nEND Clock.\n\n~~~\n\nI wrote the `Get` procedure code in Oberon-07 is the OBNC\ncompiler will render the Oberon as C during the\ncompilation process. I save myself writing some C code\nin by leveraging OBNC.\n\n\nStep two was to write [ClockTest.Mod](ClockTest.Mod) in Oberon-07.\n\n~~~{.oberon}\n\nMODULE ClockTest;\n\nIMPORT Tests, Chars, Clock; (* , Out; *)\n\nCONST\n    MAXSTR = Chars.MAXSTR;\n\nVAR\n    title : ARRAY MAXSTR OF CHAR;\n    success, errors : INTEGER;\n\nPROCEDURE TestGetRtcTime() : BOOLEAN;\nVAR second, minute, hour, day, month, year : INTEGER; \n    test, expected, result: BOOLEAN;\nBEGIN\n    test := TRUE;\n    second := 0; minute := 0; hour := 0;\n    day := 0; month := 0; year := 0;\n    expected := TRUE;\n    Clock.GetRtcTime(second, minute, hour, day, month, year);\n\n\n    result := (year > 1900);\n    Tests.ExpectedBool(expected, result, \n          \"year should be greater than 1900\", test);\n    result := (month >= 0) & (month <= 11);\n    Tests.ExpectedBool(expected, result,\n          \"month should be [0, 11]\", test);\n    result := (day >= 1) & (day <= 31);\n    Tests.ExpectedBool(expected, result,\n          \"day should be non-zero\", test);\n\n    result := (hour >= 0) & (hour <= 23);\n    Tests.ExpectedBool(expected, result,\n          \"hour should be [0, 23]\", test);\n    result := (minute >= 0) & (minute <= 59);\n    Tests.ExpectedBool(expected, result, \n          \"minute should be [0, 59]\", test);\n    result := (second >= 0) & (second <= 60);\n    Tests.ExpectedBool(expected, result,\n          \"second year should be [0,60]\", test);\n    RETURN test\nEND TestGetRtcTime;\n\nPROCEDURE TestGet() : BOOLEAN;\nVAR time, date : INTEGER; \n    test, expected, result : BOOLEAN;\nBEGIN\n    test := TRUE;\n    time := 0;\n    date := 0;\n    Clock.Get(time, date);\n    expected := TRUE;\n    result := (time > 0);\n    Tests.ExpectedBool(expected, result,\n        \"time should not be zero\", test);\n    result := (date > 0);\n    Tests.ExpectedBool(expected, result,\n        \"date should not be zero\", test);\n\n    RETURN test\nEND TestGet;\n\nBEGIN\n    Chars.Set(\"Clock module test\", title);\n    success := 0; errors := 0;\n    Tests.Test(TestGetRtcTime, success, errors);\n    Tests.Test(TestGet, success, errors);\n    Tests.Summarize(title, success, errors);\nEND ClockTest.\n\n~~~\n\nClockTest is a simple test module for [Clock.Mod](Clock.Mod).\nIt also serves the role when compiled with OBNC to create the\ntemplate C code for [Clock.c](Clock.c). Here's the steps we\ntake to generate `Clock.c` with OBNC:\n\n~~~{.shell}\n\nobnc ClockTest.Mod\nmv .obnc/Clock.c ./\nvi Clock.c\n\n~~~\n\nAfter compiling `.obnc/Clock.c` I then moved `.obnc/Clock.c`\nto my working directory. Filled in the C version of\n`GetRtcTime` function and modified my [Clock.Mod](Clock.Mod)\nto contain my empty procedure.\n\nThe finally version of Clock.c looks like (note how we need to\ninclude \"Clock.h\" in the head of the our C source file).\n\n~~~{.c}\n\n/*GENERATED BY OBNC 0.16.1*/\n\n#include \".obnc/Clock.h\"\n#include <obnc/OBNC.h>\n#include <time.h>\n\n#define OBERON_SOURCE_FILENAME \"Clock.Mod\"\n\nvoid Clock__GetRtcTime_(OBNC_INTEGER *second_, OBNC_INTEGER *minute_,\n     OBNC_INTEGER *hour_, OBNC_INTEGER *day_,\n     OBNC_INTEGER *month_, OBNC_INTEGER *year_)\n{\n    time_t now;\n    struct tm *time_info;\n    now = time(NULL);\n    time_info = localtime(&now);\n    *second_ = time_info->tm_sec;\n    *minute_ = time_info->tm_min;\n    *hour_ = time_info->tm_hour;\n    *day_ = time_info->tm_mday;\n    *month_ = time_info->tm_mon;\n    *year_ = (time_info->tm_year) + 1900;\n}\n\n\nvoid Clock__Get_(OBNC_INTEGER *time_, OBNC_INTEGER *date_)\n{\n\tOBNC_INTEGER second_, minute_, hour_, day_, month_, year_;\n\n\tClock__GetRtcTime_(&second_, &minute_, \n                       &hour_, &day_, &month_, &year_);\n\t(*time_) = ((hour_ * 4096) + (minute_ * 64)) + second_;\n\t(*date_) = ((year_ * 512) + (month_ * 32)) + day_;\n}\n\n\nvoid Clock__Init(void)\n{\n}\n\n~~~\n\nThe final version of Clock.Mod looks like\n\n~~~{.oberon}\n\nMODULE Clock;\n\nPROCEDURE GetRtcTime*(VAR second, minute, \n                      hour, day, month, year : INTEGER);\nBEGIN\nEND GetRtcTime;\n\nPROCEDURE Get*(VAR time, date : INTEGER);\nBEGIN\nEND Get;\n\nEND Clock.\n\n~~~\n\n\nStep three was to re-compile `ClockTest.Mod` and run the tests.\n\n~~~{.shell}\n\n    obnc ClockTest.Mod\n    ./ClockTest\n\n~~~\n\nDates\n-----\n\nThe dates module provides a rich variety of\nprocedures for working with dates. This includes parsing\ndate strings into `DateTime` records, testing strings for\nsupported date formats, setting dates or time in a `DateTime`\nrecord as well as comparison, difference and addition\n(both addition and subtraction) of dates. Tests for the Dates\nmodule is implemented in [DatesTest.Mod](DatesTest.Mod).\n\n~~~{.oberon}\n\nMODULE Dates;\nIMPORT Chars, Strings, Clock, Convert := extConvert;\n\nCONST\n    MAXSTR = Chars.MAXSTR;\n    SHORTSTR = Chars.SHORTSTR;\n\n    YYYYMMDD* = 1; (* YYYY-MM-DD format *)\n    MMDDYYYY* = 2; (* MM/DD/YYYY format *)\n    YYYYMMDDHHMMSS* = 3; (* YYYY-MM-DD HH:MM:SS format *)\n\nTYPE\n    DateTime* = RECORD\n        year*, month*, day*, hour*, minute*, second* : INTEGER\n    END;\n\nVAR\n    (* Month names, January = 0, December = 11 *)\n    Months*: ARRAY 23 OF ARRAY 10 OF CHAR;\n    (* Days of week, Monday = 0, Sunday = 6 *)\n    Days*: ARRAY 7 OF ARRAY 10 OF CHAR;\n    DaysInMonth: ARRAY 12 OF INTEGER;\n\n\n(* Set -- initialize a date record year, month and day values *)\nPROCEDURE Set*(year, month, day, hour, minute, second : INTEGER; \n               VAR dt: DateTime);\nBEGIN\n    dt.year := year;\n    dt.month := month;\n    dt.day := day;\n    dt.hour := hour;\n    dt.minute := minute;\n    dt.second := second;\nEND Set;\n\n(* SetDate -- set a Date record's year, month and day attributes *)\nPROCEDURE SetDate*(year, month, day : INTEGER; VAR dt: DateTime);\nBEGIN\n    dt.year := year;\n    dt.month := month;\n    dt.day := day;\nEND SetDate;\n\n(* SetTime -- set a Date record's hour, minute, second attributes *)\nPROCEDURE SetTime*(hour, minute, second : INTEGER; VAR dt: DateTime);\nBEGIN\n    dt.hour := hour;\n    dt.minute := minute;\n    dt.second := second;\nEND SetTime;\n\n(* Copy -- copy the values from one date record to another *)\nPROCEDURE Copy*(src : DateTime; VAR dest : DateTime);\nBEGIN\n    dest.year := src.year;\n    dest.month := src.month;\n    dest.day := src.day;\n    dest.hour := src.hour;\n    dest.minute := src.minute;\n    dest.second := src.second;\nEND Copy;\n\n(* ToChars -- converts a date record into an array of chars using\nthe format constant. Formats supported are YYYY-MM-DD HH:MM:SS\nor MM/DD/YYYY HH:MM:SS. *)\nPROCEDURE ToChars*(dt: DateTime; fmt : INTEGER;\n                   VAR src : ARRAY OF CHAR);\nVAR ok : BOOLEAN;\nBEGIN\n    Chars.Clear(src);\n    IF fmt = YYYYMMDD THEN\n        Chars.AppendInt(dt.year, 4, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.month, 2, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.day, 2, \"0\", src);\n    ELSIF fmt = MMDDYYYY THEN\n        Chars.AppendInt(dt.month, 2, \"0\", src);\n        ok := Chars.AppendChar(\"/\", src);\n        Chars.AppendInt(dt.day, 2, \"0\", src);\n        ok := Chars.AppendChar(\"/\", src);\n        Chars.AppendInt(dt.year, 4, \"0\", src);\n    ELSIF fmt = YYYYMMDDHHMMSS THEN\n        Chars.AppendInt(dt.year, 4, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.month, 2, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.day, 2, \"0\", src);\n        ok := Chars.AppendChar(\" \", src);\n        Chars.AppendInt(dt.hour, 2, \"0\", src);\n        ok := Chars.AppendChar(\":\", src);\n        Chars.AppendInt(dt.minute, 2, \"0\", src);\n        ok := Chars.AppendChar(\":\", src);\n        Chars.AppendInt(dt.second, 2, \"0\", src);\n    END;\nEND ToChars;\n\n(*\n * Date and Time functions very much inspired by A2 but\n * adapted for use in Oberon-07 and OBNC compiler.\n *)\n\n(* LeapYear -- returns TRUE if 'year' is a leap year *)\nPROCEDURE LeapYear*(year: INTEGER): BOOLEAN;\nBEGIN\n\tRETURN (year > 0) & (year MOD 4 = 0) & \n           (~(year MOD 100 = 0) OR (year MOD 400 = 0))\nEND LeapYear;\n\n(* NumOfDays -- number of days, returns the number of \ndays in that month *)\nPROCEDURE NumOfDays*(year, month: INTEGER): INTEGER;\nVAR result : INTEGER;\nBEGIN\n    result := 0;\n\tDEC(month);\n\tIF ((month >= 0) & (month < 12)) THEN\n\t    IF (month = 1) & LeapYear(year) THEN\n            result := DaysInMonth[1]+1;\n\t    ELSE\n            result := DaysInMonth[month];\n\t    END;\n    END;\n    RETURN result\nEND NumOfDays;\n\n(* IsValid -- checks if the attributes set in a \nDateTime record are valid *)\nPROCEDURE IsValid*(dt: DateTime): BOOLEAN;\nBEGIN\n\tRETURN ((dt.year > 0) & (dt.month > 0) &\n           (dt.month <= 12) & (dt.day > 0) &\n           (dt.day <= NumOfDays(dt.year, dt.month)) &\n           (dt.hour >= 0) & (dt.hour < 24) & (dt.minute >= 0) &\n           (dt.minute < 60) & (dt.second >= 0) & (dt.second < 60))\nEND IsValid;\n\n(* IsValidDate -- checks to see if a datetime record \nhas valid day, month and year attributes *)\nPROCEDURE IsValidDate*(dt: DateTime) : BOOLEAN;\nBEGIN\n\tRETURN (dt.year > 0) & (dt.month > 0) &\n           (dt.month <= 12) & (dt.day > 0) &\n           (dt.day <= NumOfDays(dt.year, dt.month))\nEND IsValidDate;\n\n(* IsValidTime -- checks if the hour, minute, second\nattributes set in a DateTime record are valid *)\nPROCEDURE IsValidTime*(dt: DateTime): BOOLEAN;\nBEGIN\n\tRETURN (dt.hour >= 0) & (dt.hour < 24) &\n           (dt.minute >= 0) & (dt.minute < 60) &\n           (dt.second >= 0) & (dt.second < 60)\nEND IsValidTime;\n\n\n(* OberonToDateTime -- convert an Oberon date/time \nto a DateTime structure *)\nPROCEDURE OberonToDateTime*(Date, Time: INTEGER; \n                            VAR dt : DateTime);\nBEGIN\n\tdt.second := Time MOD 64; Time := Time DIV 64;\n\tdt.minute := Time MOD 64; Time := Time DIV 64;\n\tdt.hour := Time MOD 24;\n\tdt.day := Date MOD 32; Date := Date DIV 32;\n\tdt.month := (Date MOD 16) + 1; Date := Date DIV 16;\n\tdt.year := Date;\nEND OberonToDateTime;\n\n(* DateTimeToOberon -- convert a DateTime structure\nto an Oberon date/time *)\nPROCEDURE DateTimeToOberon*(dt: DateTime;\n                            VAR date, time: INTEGER);\nBEGIN\n\tIF IsValid(dt) THEN\n\tdate := (dt.year)*512 + dt.month*32 + dt.day;\n\ttime := dt.hour*4096 + dt.minute*64 + dt.second\n    ELSE\n        date := 0;\n        time := 0;\n    END;\nEND DateTimeToOberon;\n\n(* Now -- returns the current date and time as a\nDateTime record. *)\nPROCEDURE Now*(VAR dt: DateTime);\nVAR d, t: INTEGER;\nBEGIN\n\tClock.Get(t, d);\n\tOberonToDateTime(d, t, dt);\nEND Now;\n\n\n(* WeekDate -- returns the ISO 8601 year number,\nweek number & week day (Monday=1, ....Sunday=7)\nAlgorithm is by Rick McCarty, \nhttp://personal.ecu.edu/mccartyr/ISOwdALG.txt\n*)\nPROCEDURE WeekDate*(dt: DateTime; \n                    VAR year, week, weekday: INTEGER);\nVAR doy, i, yy, c, g, jan1: INTEGER; leap: BOOLEAN;\nBEGIN\n\tIF IsValid(dt) THEN\n\t\tleap := LeapYear(dt.year);\n\t\tdoy := dt.day; i := 0;\n\t\tWHILE (i < (dt.month - 1)) DO\n            doy := doy + DaysInMonth[i];\n            INC(i);\n        END;\n\t\tIF leap & (dt.month > 2) THEN\n            INC(doy);\n        END;\n\t\tyy := (dt.year - 1) MOD 100;\n        c := (dt.year - 1) - yy;\n        g := (yy + yy) DIV 4;\n\t\tjan1 := 1 + (((((c DIV 100) MOD 4) * 5) + g) MOD 7);\n\n\t\tweekday := 1 + (((doy + (jan1 - 1)) - 1) MOD 7);\n        (* does doy fall in year-1 ? *)\n\t\tIF (doy <= (8 - jan1)) & (jan1 > 4) THEN \n\t\t\tyear := dt.year - 1;\n\t\t\tIF (jan1 = 5) OR ((jan1 = 6) & LeapYear(year)) THEN\n                week := 53;\n\t\t\tELSE\n                week := 52;\n\t\t\tEND;\n\t\tELSE\n\t\t\tIF leap THEN\n                i := 366;\n            ELSE\n                i := 365;\n            END;\n\t\t\tIF ((i - doy) < (4 - weekday)) THEN\n\t\t\t\tyear := dt.year + 1;\n\t\t\t\tweek := 1;\n\t\t\tELSE\n\t\t\t\tyear := dt.year;\n\t\t\t\ti := doy + (7-weekday) + (jan1-1);\n\t\t\t\tweek := i DIV 7;\n\t\t\t\tIF (jan1 > 4) THEN\n                    DEC(week);\n                END;\n\t\t\tEND;\n\t\tEND;\n\tELSE\n\t\tyear := -1; week := -1; weekday := -1;\n\tEND;\nEND WeekDate;\n\n(* Equal -- compare to date records to see if they \nare equal values *)\nPROCEDURE Equal*(t1, t2: DateTime) : BOOLEAN;\nBEGIN\n\tRETURN ((t1.second = t2.second) &\n            (t1.minute = t2.minute) & (t1.hour = t2.hour) &\n            (t1.day = t2.day) & (t1.month = t2.month) &\n            (t1.year = t2.year))\nEND Equal;\n\n(* compare -- used in Compare only for comparing\nspecific values, returning an appropriate -1, 0, 1 *)\nPROCEDURE compare(t1, t2 : INTEGER) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tIF (t1 < t2) THEN\n        result := -1;\n\tELSIF (t1 > t2) THEN\n        result := 1;\n\tELSE\n        result := 0;\n\tEND;\n\tRETURN result\nEND compare;\n\n(* Compare -- returns -1 if (t1 < t2), \n0 if (t1 = t2) or 1 if (t1 >  t2) *)\nPROCEDURE Compare*(t1, t2: DateTime) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tresult := compare(t1.year, t2.year);\n\tIF (result = 0) THEN\n\t\tresult := compare(t1.month, t2.month);\n\t\tIF (result = 0) THEN\n\t\t\tresult := compare(t1.day, t2.day);\n\t\t\tIF (result = 0) THEN\n\t\t\t\tresult := compare(t1.hour, t2.hour);\n\t\t\t\tIF (result = 0) THEN\n\t\t\t\t\tresult := compare(t1.minute, t2.minute);\n\t\t\t\t\tIF (result = 0) THEN\n\t\t\t\t\t\tresult := compare(t1.second, t2.second);\n\t\t\t\t\tEND;\n\t\t\t\tEND;\n\t\t\tEND;\n\t\tEND;\n\tEND;\n\tRETURN result\nEND Compare;\n\n(* CompareDate -- compare day, month and year\nvalues only *)\nPROCEDURE CompareDate*(t1, t2: DateTime) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tresult := compare(t1.year, t2.year);\n\tIF (result = 0) THEN\n\t\tresult := compare(t1.month, t2.month);\n\t\tIF (result = 0) THEN\n\t\t\tresult := compare(t1.day, t2.day);\n\t\tEND;\n\tEND;\n\tRETURN result\nEND CompareDate;\n\n(* CompareTime -- compare second, minute and\nhour values only *)\nPROCEDURE CompareTime*(t1, t2: DateTime) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tresult := compare(t1.hour, t2.hour);\n\tIF (result = 0) THEN\n\t\tresult := compare(t1.minute, t2.minute);\n\t\tIF (result = 0) THEN\n\t\t\tresult := compare(t1.second, t2.second);\n\t\tEND;\n\tEND;\n\tRETURN result\nEND CompareTime;\n\n\n\n(* TimeDifferences -- returns the absolute time\ndifference between\nt1 and t2.\n\nNote that leap seconds are not counted,\nsee http://www.eecis.udel.edu/~mills/leap.html *)\nPROCEDURE TimeDifference*(t1, t2: DateTime;\n              VAR days, hours, minutes, seconds : INTEGER);\nCONST \n    SecondsPerMinute = 60; \n    SecondsPerHour = 3600; \n    SecondsPerDay = 86400;\nVAR start, end: DateTime; year, month, second : INTEGER;\nBEGIN\n\tIF (Compare(t1, t2) = -1) THEN\n        start := t1;\n        end := t2;\n    ELSE\n        start := t2;\n        end := t1;\n    END;\n\tIF (start.year = end.year) & (start.month = end.month) &\n       (start.day = end.day) THEN\n\t\tsecond := end.second - start.second + \n                  ((end.minute - start.minute) * SecondsPerMinute) +\n                  ((end.hour - start.hour) * SecondsPerHour);\n\t\tdays := 0;\n        hours := 0;\n        minutes := 0;\n\tELSE\n\t\t(* use start date/time as reference point *)\n\t\t(* seconds until end of the start.day *)\n\t\tsecond := (SecondsPerDay - start.second) -\n                  (start.minute * SecondsPerMinute) -\n                  (start.hour * SecondsPerHour);\n\t\tIF (start.year = end.year) &\n           (start.month = end.month) THEN\n\t\t\t(* days between start.day and end.day *)\n\t\t\tdays := (end.day - start.day) - 1;\n\t\tELSE\n\t\t\t(* days until start.month ends excluding start.day *)\n\t\t\tdays := NumOfDays(start.year, start.month) - start.day;\n\t\t\tIF (start.year = end.year) THEN\n\t\t\t\t(* months between start.month and end.month *)\n\t\t\t\tFOR month := start.month + 1 TO end.month - 1 DO\n\t\t\t\t\tdays := days + NumOfDays(start.year, month);\n\t\t\t\tEND;\n\t\t\tELSE\n\t\t\t\t(* days until start.year ends (excluding start.month) *)\n\t\t\t\tFOR month := start.month + 1 TO 12 DO\n\t\t\t\t\tdays := days + NumOfDays(start.year, month);\n\t\t\t\tEND;\n                (* days between start.years and end.year *)\n\t\t\t\tFOR year := start.year + 1 TO end.year - 1 DO\n\t\t\t\t\tIF LeapYear(year) THEN days := days + 366; \n                    ELSE days := days + 365; END;\n\t\t\t\tEND;\n                (* days until we reach end.month in end.year *)\n\t\t\t\tFOR month := 1 TO end.month - 1 DO\n\t\t\t\t\tdays := days + NumOfDays(end.year, month);\n\t\t\t\tEND;\n\t\t\tEND;\n\t\t\t(* days in end.month until reaching end.day excluding end.day *)\n\t\t\tdays := (days + end.day) - 1;\n\t\tEND;\n\t\t(* seconds in end.day *)\n\t\tsecond := second + end.second +\n                  (end.minute * SecondsPerMinute) +\n                  (end.hour * SecondsPerHour);\n\tEND;\n\tdays := days + (second DIV SecondsPerDay); \n    second := (second MOD SecondsPerDay);\n\thours := (second DIV SecondsPerHour); \n    second := (second MOD SecondsPerHour);\n\tminutes := (second DIV SecondsPerMinute);\n    second := (second MOD SecondsPerMinute);\n\tseconds := second;\nEND TimeDifference;\n\n(* AddYear -- Add/Subtract a number of years to/from date *)\nPROCEDURE AddYears*(VAR dt: DateTime; years : INTEGER);\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.year := dt.year + years;\n\tASSERT(IsValid(dt));\nEND AddYears;\n\n(* AddMonths -- Add/Subtract a number of months to/from date.\nThis will adjust date.year if necessary *)\nPROCEDURE AddMonths*(VAR dt: DateTime; months : INTEGER);\nVAR years : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tyears := months DIV 12;\n\tdt.month := dt.month + (months MOD 12);\n\tIF (dt.month > 12) THEN\n\t\tdt.month := dt.month - 12;\n\t\tINC(years);\n\tELSIF (dt.month < 1) THEN\n\t\tdt.month := dt.month + 12;\n\t\tDEC(years);\n\tEND;\n\tIF (years # 0) THEN AddYears(dt, years); END;\n\tASSERT(IsValid(dt));\nEND AddMonths;\n\n(* AddDays --  Add/Subtract a number of days to/from date.\nThis will adjust date.month and date.year if necessary *)\nPROCEDURE AddDays*(VAR dt: DateTime; days : INTEGER);\nVAR nofDaysLeft : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tIF (days > 0) THEN\n\t\tWHILE (days > 0) DO\n\t\t\tnofDaysLeft := NumOfDays(dt.year, dt.month) - dt.day;\n\t\t\tIF (days > nofDaysLeft) THEN\n\t\t\t\tdt.day := 1;\n\t\t\t\tAddMonths(dt, 1);\n                (* -1 because we consume the first day \n                    of the next month *)\n\t\t\t\tdays := days - nofDaysLeft - 1;\n\t\t\tELSE\n\t\t\t\tdt.day := dt.day + days;\n\t\t\t\tdays := 0;\n\t\t\tEND;\n\t\tEND;\n\tELSIF (days < 0) THEN\n\t\tdays := -days;\n\t\tWHILE (days > 0) DO\n\t\t\tnofDaysLeft := dt.day - 1;\n\t\t\tIF (days > nofDaysLeft) THEN\n                (* otherwise, dt could become an invalid \n                   date if the previous month has less \n                   days than dt.day *)\n\t\t\t\tdt.day := 1; \n\t\t\t\tAddMonths(dt, -1);\n\t\t\t\tdt.day := NumOfDays(dt.year, dt.month);\n                (* -1 because we consume the last day \n                   of the previous month *)\n\t\t\t\tdays := days - nofDaysLeft - 1;\n\t\t\tELSE\n\t\t\t\tdt.day := dt.day - days;\n\t\t\t\tdays := 0;\n\t\t\tEND;\n\t\tEND;\n\tEND;\n\tASSERT(IsValid(dt));\nEND AddDays;\n\n(* AddHours -- Add/Subtract a number of hours to/from date.\nThis will adjust date.day, date.month and date.year if necessary *)\nPROCEDURE AddHours*(VAR dt: DateTime; hours : INTEGER);\nVAR days : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.hour := dt.hour + hours;\n\tdays := dt.hour DIV 24;\n\tdt.hour := dt.hour MOD 24;\n\tIF (dt.hour < 0) THEN\n\t\tdt.hour := dt.hour + 24;\n\t\tDEC(days);\n\tEND;\n\tIF (days # 0) THEN AddDays(dt, days); END;\n\tASSERT(IsValid(dt));\nEND AddHours;\n\n(* AddMinutes -- Add/Subtract a number of minutes to/from date.\nThis will adjust date.hour, date.day, date.month and date.year\nif necessary *)\nPROCEDURE AddMinutes*(VAR dt: DateTime; minutes : INTEGER);\nVAR hours : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.minute := dt.minute + minutes;\n\thours := dt.minute DIV 60;\n\tdt.minute := dt.minute MOD 60;\n\tIF (dt.minute < 0) THEN\n\t\tdt.minute := dt.minute + 60;\n\t\tDEC(hours);\n\tEND;\n\tIF (hours # 0) THEN AddHours(dt, hours); END;\n\tASSERT(IsValid(dt));\nEND AddMinutes;\n\n(* AddSeconds -- Add/Subtract a number of seconds to/from date.\nThis will adjust date.minute, date.hour, date.day, date.month and\ndate.year if necessary *)\nPROCEDURE AddSeconds*(VAR dt: DateTime; seconds : INTEGER);\nVAR minutes : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.second := dt.second + seconds;\n\tminutes := dt.second DIV 60;\n\tdt.second := dt.second MOD 60;\n\tIF (dt.second < 0) THEN\n\t\tdt.second := dt.second + 60;\n\t\tDEC(minutes);\n\tEND;\n\tIF (minutes # 0) THEN AddMinutes(dt, minutes); END;\n\tASSERT(IsValid(dt));\nEND AddSeconds;\n\n\n(* IsDateString -- return TRUE if the ARRAY OF CHAR is 10 characters\nlong and is either in the form of YYYY-MM-DD or MM/DD/YYYY where\nY, M and D are digits.\nNOTE: is DOES NOT check the ranges of the digits. *)\nPROCEDURE IsDateString*(inline : ARRAY OF CHAR) : BOOLEAN;\nVAR\n    test : BOOLEAN; i, pos : INTEGER;\n    src : ARRAY MAXSTR OF CHAR;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.TrimSpace(src);\n    test := FALSE;\n    IF Strings.Length(src) = 10 THEN\n        pos := Strings.Pos(\"-\", src, 0);\n        IF pos > 0 THEN\n            IF (src[4] = \"-\") & (src[7] = \"-\") THEN\n                test := TRUE;\n                FOR i := 0 TO 9 DO\n                    IF (i # 4) & (i # 7) THEN\n                       IF Chars.IsDigit(src[i]) = FALSE THEN\n                           test := FALSE;\n                       END;\n                    END;\n                END;\n            ELSE\n                test := FALSE;\n            END;\n        END;\n        pos := Strings.Pos(\"/\", src, 0);\n        IF pos > 0 THEN\n            IF (src[2] = \"/\") & (src[5] = \"/\") THEN\n                test := TRUE;\n                FOR i := 0 TO 9 DO\n                    IF (i # 2) & (i # 5) THEN\n                        IF Chars.IsDigit(src[i]) = FALSE THEN\n                            test := FALSE;\n                        END;\n                    END;\n                END;\n            ELSE\n                test := FALSE;\n            END;\n        END;\n    END;\n    RETURN test\nEND IsDateString;\n\n(* IsTimeString -- return TRUE if the ARRAY OF CHAR has 4 to 8\ncharacters in the form of H:MM, HH:MM, HH:MM:SS where H, M and S\nare digits. *)\nPROCEDURE IsTimeString*(inline : ARRAY OF CHAR) : BOOLEAN;\nVAR\n    test : BOOLEAN;\n    l : INTEGER;\n    src : ARRAY MAXSTR OF CHAR;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.TrimSpace(src);\n    (* remove any trailing am/pm suffixes *)\n    IF Chars.EndsWith(\"m\", src) THEN\n        IF Chars.EndsWith(\"am\", src) THEN\n            Chars.TrimSuffix(\"am\", src);\n        ELSE\n            Chars.TrimSuffix(\"pm\", src);\n        END;\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"M\", src) THEN\n        Chars.TrimSuffix(\"AM\", src);\n        Chars.TrimSuffix(\"PM\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"p\", src) THEN\n        Chars.TrimSuffix(\"p\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"P\", src) THEN\n        Chars.TrimSuffix(\"P\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"a\", src) THEN\n        Chars.TrimSuffix(\"a\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"A\", src) THEN\n        Chars.TrimSuffix(\"A\", src);\n        Chars.TrimSpace(src);\n    END;\n    Strings.Extract(src, 0, 8, src);\n    test := FALSE;\n    l := Strings.Length(src);\n    IF (l = 4) THEN\n        IF Chars.IsDigit(src[0]) & (src[1] = \":\") &\n            Chars.IsDigit(src[2]) & Chars.IsDigit(src[3]) THEN\n            test := TRUE;\n        ELSE\n            test := FALSE;\n        END;\n    ELSIF (l = 5) THEN\n        IF Chars.IsDigit(src[0]) & Chars.IsDigit(src[1]) &\n            (src[2] = \":\") &\n            Chars.IsDigit(src[3]) & Chars.IsDigit(src[4]) THEN\n            test := TRUE;\n        ELSE\n            test := FALSE;\n        END;\n    ELSIF (l = 8) THEN\n        IF Chars.IsDigit(src[0]) & Chars.IsDigit(src[1]) &\n            (src[2] = \":\") &\n            Chars.IsDigit(src[3]) & Chars.IsDigit(src[4]) &\n            (src[5] = \":\") &\n            Chars.IsDigit(src[6]) & Chars.IsDigit(src[7]) THEN\n            test := TRUE;\n        ELSE\n            test := FALSE;\n        END;\n    ELSE\n        test := FALSE;\n    END;\n    RETURN test\nEND IsTimeString;\n\n(* ParseDate -- parses a date string in YYYY-MM-DD or\nMM/DD/YYYY format. *)\nPROCEDURE ParseDate*(inline : ARRAY OF CHAR;\n                     VAR year, month, day : INTEGER) : BOOLEAN;\nVAR src, tmp : ARRAY MAXSTR OF CHAR; ok, b : BOOLEAN;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.Clear(tmp);\n    ok := FALSE;\n\tIF IsDateString(src) THEN\n        (* LIMITATION: Need to allow for more than 4 digit years! *)\n        IF (src[2] = \"/\") & (src[5] = \"/\") THEN\n            ok := TRUE;\n            Strings.Extract(src, 0, 2, tmp);\n            Convert.StringToInt(tmp, month, b);\n            ok := ok & b;\n            Strings.Extract(src, 4, 2, tmp);\n            Convert.StringToInt(tmp, day, b);\n            ok := ok & b;\n            Strings.Extract(src, 6, 4, tmp);\n            Convert.StringToInt(tmp, year, b);\n            ok := ok & b;\n        ELSIF (src[4] = \"-\") & (src[7] = \"-\") THEN\n            ok := TRUE;\n            Strings.Extract(src, 0, 4, tmp);\n            Convert.StringToInt(tmp, year, b);\n            ok := ok & b;\n            Strings.Extract(src, 5, 2, tmp);\n            Convert.StringToInt(tmp, month, b);\n            ok := ok & b;\n            Strings.Extract(src, 8, 2, tmp);\n            Convert.StringToInt(tmp, day, b);\n            ok := ok & b;\n        ELSE\n            ok := FALSE;\n        END;\n    END;\n    RETURN ok\nEND ParseDate;\n\n(* ParseTime -- procedure for parsing time strings into hour,\nminute, second. Returns TRUE on successful parse, FALSE otherwise *)\nPROCEDURE ParseTime*(inline : ARRAY OF CHAR;\n                     VAR hour, minute, second : INTEGER) : BOOLEAN;\nVAR src, tmp : ARRAY MAXSTR OF CHAR;\n    ok : BOOLEAN; cur, pos, l : INTEGER;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.Clear(tmp);\n\tIF IsTimeString(src) THEN\n        ok := TRUE;\n        cur := 0; pos := 0;\n        pos := Strings.Pos(\":\", src, cur);\n        IF pos > 0 THEN\n        (* Get Hour *)\n            Strings.Extract(src, cur, pos - cur, tmp);\n            Convert.StringToInt(tmp, hour, ok);\n            IF ok THEN\n                (* Get Minute *)\n                cur := pos + 1;\n                Strings.Extract(src, cur, 2, tmp);\n                Convert.StringToInt(tmp, minute, ok);\n                IF ok THEN\n                    (* Get second, optional, default to zero *)\n                    pos := Strings.Pos(\":\", src, cur);\n                    IF pos > 0 THEN\n                        cur := pos + 1;\n                        Strings.Extract(src, cur, 2, tmp);\n                        Convert.StringToInt(tmp, second, ok);\n                        cur := cur + 2;\n                    ELSE\n                        second := 0;\n                    END;\n                    (* Get AM/PM, optional, adjust hour if PM *)\n                    l := Strings.Length(src);\n                    WHILE (cur < l) & Chars.IsSpace(src[cur]) DO\n                        cur := cur + 1;\n                    END;\n                    Strings.Extract(src, cur, 2, tmp);\n                    Chars.TrimSpace(tmp);\n                    IF Chars.Equal(tmp, \"PM\") OR Chars.Equal(tmp, \"pm\") THEN\n                        hour := hour + 12;\n                    END;\n                ELSE\n                    ok := FALSE;\n                END;\n            END;\n        ELSE\n            ok := FALSE;\n        END;\n    ELSE\n        ok := FALSE;\n    END;\n    IF ok THEN\n        ok := ((hour >= 0) & (hour <= 23)) &\n            ((minute >= 0) & (minute <= 59)) &\n                ((second >= 0) & (second <= 59));\n    END;\n    RETURN ok\nEND ParseTime;\n\n\n(* Parse accepts a date array of chars in either dates, times\nor dates and times separate by spaces. Date formats supported\ninclude YYYY-MM-DD, MM/DD/YYYY. Time formats include\nH:MM, HH:MM, H:MM:SS, HH:MM:SS with 'a', 'am', 'p', 'pm'\nsuffixes.  Dates and times can also be accepted as JSON\nexpressions with the individual time compontents are specified\nas attributes, e.g. {\"year\": 1998, \"month\": 12, \"day\": 10,\n\"hour\": 11, \"minute\": 4, \"second\": 3}.\nParse returns TRUE on successful parse, FALSE otherwise.\n\nBUG: Assumes a 4 digit year.\n*)\nPROCEDURE Parse*(inline : ARRAY OF CHAR; VAR dt: DateTime) : BOOLEAN;\nVAR src, ds, ts, tmp : ARRAY SHORTSTR OF CHAR; ok, okDate, okTime : BOOLEAN;\n    pos, year, month, day, hour, minute, second : INTEGER;\nBEGIN\n    dt.year := 0;\n    dt.month := 0;\n    dt.day := 0;\n    dt.hour := 0;\n    dt.minute := 0;\n    dt.second := 0;\n    Chars.Clear(tmp);\n    Chars.Set(inline, src);\n    Chars.TrimSpace(src);\n    (* Split into Date and Time components *)\n    pos := Strings.Pos(\" \", src, 0);\n    IF pos >= 0 THEN\n        Strings.Extract(src, 0, pos, ds);\n        pos := pos + 1;\n        Strings.Extract(src, pos, Strings.Length(src) - pos, ts);\n    ELSE\n        Chars.Set(src, ds);\n        Chars.Set(src, ts);\n    END;\n    ok := FALSE;\n    IF IsDateString(ds) THEN\n        ok := TRUE;\n        okDate := ParseDate(ds, year, month, day);\n        SetDate(year, month, day, dt);\n        ok := ok & okDate;\n    END;\n    IF IsTimeString(ts) THEN\n        ok := ok OR okDate;\n        okTime := ParseTime(ts, hour, minute, second);\n        SetTime(hour, minute, second, dt);\n        ok := ok & okTime;\n    END;\n    RETURN ok\nEND Parse;\n\nBEGIN\n    Chars.Set(\"January\", Months[0]);\n    Chars.Set(\"February\", Months[1]);\n    Chars.Set(\"March\", Months[2]);\n    Chars.Set(\"April\", Months[3]);\n    Chars.Set(\"May\", Months[4]);\n    Chars.Set(\"June\", Months[5]);\n    Chars.Set(\"July\", Months[6]);\n    Chars.Set(\"August\", Months[7]);\n    Chars.Set(\"September\", Months[8]);\n    Chars.Set(\"October\", Months[9]);\n    Chars.Set(\"November\", Months[10]);\n    Chars.Set(\"December\", Months[11]);\n\n    Chars.Set(\"Sunday\", Days[0]);\n    Chars.Set(\"Monday\", Days[1]);\n    Chars.Set(\"Tuesday\", Days[2]);\n    Chars.Set(\"Wednesday\", Days[3]);\n    Chars.Set(\"Thursday\", Days[4]);\n    Chars.Set(\"Friday\", Days[5]);\n    Chars.Set(\"Saturday\", Days[6]);\n\n    DaysInMonth[0] := 31; (* January *)\n    DaysInMonth[1] := 28; (* February *)\n    DaysInMonth[2] := 31; (* March *)\n    DaysInMonth[3] := 30; (* April *)\n    DaysInMonth[4] := 31; (* May *)\n    DaysInMonth[5] := 30; (* June *)\n    DaysInMonth[6] := 31; (* July *)\n    DaysInMonth[7] := 31; (* August *)\n    DaysInMonth[8] := 30; (* September *)\n    DaysInMonth[9] := 31; (* October *)\n    DaysInMonth[10] := 30; (* November *)\n    DaysInMonth[11] := 31; (* December *)\n\nEND Dates.\n\n~~~\n\nPostscript: In this article I included a reference to the module\n**[Chars](Chars.html)**. This is a non-standard module I wrote\nfor Oberon-07. Here is a link to [Chars](Chars.Mod). RSD, 2021-05-06\n\n### Next, Previous\n\n+ Next [Beyond Oakwood, Modules and Aliases](/blog/2021/05/16/Beyond-Oakwood-Modules-and-Aliases.html)\n+ Previous [Assemble Pages](/blog/2020/10/19/Assemble-pages.html)\n\n",
      "data": {
        "author": "rsdoiel@gmail.com (R. S. Doiel)",
        "copyright": "copyright (c) 2020, R. S. Doiel",
        "date": "2020-11-27",
        "keywords": [
          "Oberon-07",
          "C-shared",
          "obnc"
        ],
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "number": 14,
        "series": "Mostly Oberon",
        "title": "Dates & Clock"
      },
      "url": "posts/2020/11/27/Dates-and-Clock.json"
    },
    {
      "content": "\nDates\n=====\n\nThis module provides minimal date time records and procedures\nfor working with dates in YYYY-MM-DD and MM/DD/YYYY format and\ntimes in H:MM, HH:MM and HH:MM:SS formats.\n\n\nSet\n: Set a DateTime record providing year, month, day, hour, minute and second as integers and the DateTime record to be populated.\n\nSetDate\n: Set the date portion of a DateTime record, leaves the hour, minute and second attributes unmodified. \n\nSetTime\n: Set the time portion of a DateTime record, leaves the year, month, date attributes unmodified.\n\nCopy\n: Copy the attributes of one DateTime record into another DateTime record\n\nToChars\n: Given a DateTime record and a format constant, render the DateTime record to an array of CHAR.\n\nLeapYear\n: Given a DateTime record check to see if it is a  leap year.\n\nNumOfDays\n: Given a year and monoth return the number of days in the month.\n\nIsValid\n: Check to see if the all attributes in a DateTime record are valid.\n\nOberonToDateTime\n: Convert oberon date and time integer values into a DateTime record\n\nDateTimeToOberon\n: Convert a DateTime record into Oberon date and time integer values.\n\nNow\n: Set a DateTime record's attributes to the current time, depends of on the implementation of Clock.Mod.\n\nWeekDate\n: Given a DateTime record calculates the year, week and weekday as integers values.\n\nEqual\n: Checks to DateTime records to see if they have equivalent attribute values.\n\nCompare\n: Compare two DateTime records, if t1 < t2 then return -1, if t1 = t2 return 0 else if t1 > t2 return 1.\n\nCompareDate\n: Compare the year, month, day attributes of two DateTime records following the approach used in Compare.\n\nCompareTime\n: Compare the hour, minute, second attributes of two DateTime records following the approach used in Compare.\n\nTimeDifference\n: Take the differ of two DateTime records setting the difference in integer values for days, hours, minutes and seconds.\n\nAddYears\n: Add years to a DateTime record. Years is a positive or negative integer.\n\nAddMonths\n: Add months to a DateTime record. Months is either a positive or negative integer. Months will propogate to year in the DataTime record.\n\nAddDays\n: Add days to a DateTime record. Days can be either a positive or negative integer.  Days will propogate to month and year attributes of the DateTime record.\n\nAddHours\n: Add hours to a DateTime record. Hours can be either a positive or negative integer.  Hours will propogate to day, month and year attributes of the DateTime record.\n\nAddMinutes\n: Add minutes to a DateTime record. Minutes can be either a positive or negative integer. Minutes will propogate to hour, day, month and year attributes of the DateTime record.\n\nAddSeconds\n: Add seconds to a DateTime record. Seconds can be either a positive or negatice integer.  Seconds will propogate to minute, hour, day, month, year attributes of the DateTime record.\n\nIsValidDate\n: IsValidDate checks the day, month, year attributes of a DateTime record and validates the values. Returns TRUE if everthing is ok, FALSE otherwise.\n\nIsValidTime\n: IsValidTime checks the hour, minute, second attributes of a DateTime record and validates the values. Returns TRUE if everthing is ok, FALSE otherwise.\n\nIsDateString\n: Checks to see if an ARRAY OF CHAR is a parsiable date string (e.g. in 2020-11-26 or 11/26/2020). Returns TRUE if the string is parsable, FALSE otherwise. NOTE: It does NOT check to see if the day, month or year values are valid. It only checks the format of the string.\n\nIsTimeString\n: Checks to see if an ARRAY OF CHAR is a parsible time string (e.g. 3:32, 14:55, 09:19:22). NOTE: It only checks the format and does not check the hour, minute and second values.\n\nParseDate\n: Parse an ARRAY OF CHAR setting the values if year, month and day. Return TRUE on successful parse and FALSE otherwise.\n\nParseTime\n: Parse an ARRAY OF CHAR setting the values of hour, minute and second. Return TRUE on succesful parse and FALSE otherwise.\n\nParse\n: Parse an ARRAY OF CHAR setting the attributes of a DateTime record. Return TURE on success, FALSE otherwise.\n\nLimitations\n-----------\n\nDates are presumed to be in the YYYY-DD-MM or MM/DD/YYYY formats.\nDoes not handle dates with spelled out months or weekdays.\n\nTime portion of the date object doesn't include time zone.\nThis will need to be rectified at some point.\n\n\n\nSource code for **Dates.Mod**\n-----------------------------\n\n~~~\n(* Dates -- this module was inspired by the A2's Dates module, adapted\n   for Oberon-07 and a POSIX system. It provides an assortment of procedures\n   for working with a simple datetime record.\n\nCopyright (C) 2020 R. S. Doiel\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as\npublished by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n\n@Author R. S. Doiel, <rsdoiel@gmail.com>\ncopyright (c) 2020, all rights reserved.\nThis software is released under the GNU AGPL\nSee http://www.gnu.org/licenses/agpl-3.0.html\n*)\nMODULE Dates;\nIMPORT Chars, Strings, Clock, Convert := extConvert; (*, Out; **)\n\nCONST\n    MAXSTR = Chars.MAXSTR;\n    SHORTSTR = Chars.SHORTSTR;\n\n    YYYYMMDD* = 1; (* YYYY-MM-DD format *)\n    MMDDYYYY* = 2; (* MM/DD/YYYY format *)\n    YYYYMMDDHHMMSS* = 3; (* YYYY-MM-DD HH:MM:SS format *)\n\nTYPE\n    DateTime* = RECORD\n        year*, month*, day*, hour*, minute*, second* : INTEGER\n    END;\n\nVAR\n    (* Month names, January = 0, December = 11 *)\n    Months*: ARRAY 23 OF ARRAY 10 OF CHAR; \n    (* Days of week, Monday = 0, Sunday = 6 *)\n    Days*: ARRAY 7 OF ARRAY 10 OF CHAR;\n    DaysInMonth: ARRAY 12 OF INTEGER;\n\n\n(* Set -- initialize a date record year, month and day values *)\nPROCEDURE Set*(year, month, day, hour, minute, second : INTEGER; VAR dt: DateTime);\nBEGIN\n    dt.year := year;\n    dt.month := month;\n    dt.day := day;\n    dt.hour := hour;\n    dt.minute := minute;\n    dt.second := second;\nEND Set;\n\n(* SetDate -- set a Date record's year, month and day attributes *)\nPROCEDURE SetDate*(year, month, day : INTEGER; VAR dt: DateTime);\nBEGIN\n    dt.year := year;\n    dt.month := month;\n    dt.day := day;\nEND SetDate;\n\n(* SetTime -- set a Date record's hour, minute, second attributes *)\nPROCEDURE SetTime*(hour, minute, second : INTEGER; VAR dt: DateTime);\nBEGIN\n    dt.hour := hour;\n    dt.minute := minute;\n    dt.second := second;\nEND SetTime;\n\n(* Copy -- copy the values from one date record to another *)\nPROCEDURE Copy*(src : DateTime; VAR dest : DateTime);\nBEGIN\n    dest.year := src.year;\n    dest.month := src.month;\n    dest.day := src.day;\n    dest.hour := src.hour;\n    dest.minute := src.minute;\n    dest.second := src.second;\nEND Copy;\n\n(* ToChars -- converts a date record into an array of chars using\nthe format constant. Formats supported are YYYY-MM-DD HH:MM:SS\nor MM/DD/YYYY HH:MM:SS. *)\nPROCEDURE ToChars*(dt: DateTime; fmt : INTEGER; VAR src : ARRAY OF CHAR);\nVAR ok : BOOLEAN;\nBEGIN\n    Chars.Clear(src);\n    IF fmt = YYYYMMDD THEN\n        Chars.AppendInt(dt.year, 4, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.month, 2, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.day, 2, \"0\", src);\n    ELSIF fmt = MMDDYYYY THEN\n        Chars.AppendInt(dt.month, 2, \"0\", src);\n        ok := Chars.AppendChar(\"/\", src);\n        Chars.AppendInt(dt.day, 2, \"0\", src);\n        ok := Chars.AppendChar(\"/\", src);\n        Chars.AppendInt(dt.year, 4, \"0\", src);\n    ELSIF fmt = YYYYMMDDHHMMSS THEN\n        Chars.AppendInt(dt.year, 4, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.month, 2, \"0\", src);\n        ok := Chars.AppendChar(\"-\", src);\n        Chars.AppendInt(dt.day, 2, \"0\", src);\n        ok := Chars.AppendChar(\" \", src);\n        Chars.AppendInt(dt.hour, 2, \"0\", src);\n        ok := Chars.AppendChar(\":\", src);\n        Chars.AppendInt(dt.minute, 2, \"0\", src);\n        ok := Chars.AppendChar(\":\", src);\n        Chars.AppendInt(dt.second, 2, \"0\", src);\n    END;\nEND ToChars;\n\n(* \n * Date and Time functions very much inspired by A2 but\n * adapted for use in Oberon-07 and OBNC compiler.\n *)\n\n(* LeapYear -- returns TRUE if 'year' is a leap year *)\nPROCEDURE LeapYear*(year: INTEGER): BOOLEAN;\nBEGIN\n\tRETURN (year > 0) & (year MOD 4 = 0) & (~(year MOD 100 = 0) OR (year MOD 400 = 0))\nEND LeapYear;\n\n(* NumOfDays -- number of days, returns the number of days in that month *)\nPROCEDURE NumOfDays*(year, month: INTEGER): INTEGER;\nVAR result : INTEGER;\nBEGIN\n    result := 0;\n\tDEC(month);\n\tIF ((month >= 0) & (month < 12)) THEN\n\t    IF (month = 1) & LeapYear(year) THEN \n            result := DaysInMonth[1]+1;\n\t    ELSE \n            result := DaysInMonth[month];\n\t    END;\n    END;\n    RETURN result\nEND NumOfDays;\n\n(* IsValid -- checks if the attributes set in a DateTime record are valid *)\nPROCEDURE IsValid*(dt: DateTime): BOOLEAN;\nBEGIN\n\tRETURN ((dt.year > 0) & (dt.month > 0) & (dt.month <= 12) & (dt.day > 0) & (dt.day <= NumOfDays(dt.year, dt.month)) & (dt.hour >= 0) & (dt.hour < 24) & (dt.minute >= 0) & (dt.minute < 60) & (dt.second >= 0) & (dt.second < 60))\nEND IsValid;\n\n(* IsValidDate -- checks to see if a datetime record has valid day, month and year\nattributes *)\nPROCEDURE IsValidDate*(dt: DateTime) : BOOLEAN;\nBEGIN\n\tRETURN (dt.year > 0) & (dt.month > 0) & (dt.month <= 12) & (dt.day > 0) & (dt.day <= NumOfDays(dt.year, dt.month))\nEND IsValidDate;\n\n(* IsValidTime -- checks if the hour, minute, second attributes set in a DateTime record are valid *)\nPROCEDURE IsValidTime*(dt: DateTime): BOOLEAN;\nBEGIN\n\tRETURN (dt.hour >= 0) & (dt.hour < 24) & (dt.minute >= 0) & (dt.minute < 60) & (dt.second >= 0) & (dt.second < 60)\nEND IsValidTime;\n\n\n(* OberonToDateTime -- convert an Oberon date/time to a DateTime \nstructure *)\nPROCEDURE OberonToDateTime*(Date, Time: INTEGER; VAR dt : DateTime);\nBEGIN\n\tdt.second := Time MOD 64; Time := Time DIV 64;\n\tdt.minute := Time MOD 64; Time := Time DIV 64;\n\tdt.hour := Time MOD 24;\n\tdt.day := Date MOD 32; Date := Date DIV 32;\n\tdt.month := (Date MOD 16) + 1; Date := Date DIV 16;\n\tdt.year := Date;\nEND OberonToDateTime;\n\n(* DateTimeToOberon -- convert a DateTime structure to an Oberon \ndate/time *)\nPROCEDURE DateTimeToOberon*(dt: DateTime; VAR date, time: INTEGER);\nBEGIN\n\tIF IsValid(dt) THEN\n\tdate := (dt.year)*512 + dt.month*32 + dt.day;\n\ttime := dt.hour*4096 + dt.minute*64 + dt.second\n    ELSE\n        date := 0;\n        time := 0;\n    END;\nEND DateTimeToOberon;\n\n(* Now -- returns the current date and time as a DateTime record. *)\nPROCEDURE Now*(VAR dt: DateTime);\nVAR d, t: INTEGER;\nBEGIN\n\tClock.Get(t, d);\n\tOberonToDateTime(d, t, dt);\nEND Now;\n\n\n(* WeekDate -- returns the ISO 8601 year number, week number &\nweek day (Monday=1, ....Sunday=7) \nAlgorithm is by Rick McCarty, http://personal.ecu.edu/mccartyr/ISOwdALG.txt\n*)\nPROCEDURE WeekDate*(dt: DateTime; VAR year, week, weekday: INTEGER);\nVAR doy, i, yy, c, g, jan1: INTEGER; leap: BOOLEAN;\nBEGIN\n\tIF IsValid(dt) THEN\n\t\tleap := LeapYear(dt.year);\n\t\tdoy := dt.day; i := 0;\n\t\tWHILE (i < (dt.month - 1)) DO \n            doy := doy + DaysInMonth[i];\n            INC(i);\n        END;\n\t\tIF leap & (dt.month > 2) THEN \n            INC(doy);\n        END;\n\t\tyy := (dt.year - 1) MOD 100; \n        c := (dt.year - 1) - yy; \n        g := (yy + yy) DIV 4;\n\t\tjan1 := 1 + (((((c DIV 100) MOD 4) * 5) + g) MOD 7);\n\n\t\tweekday := 1 + (((doy + (jan1 - 1)) - 1) MOD 7);\n\n\t\tIF (doy <= (8 - jan1)) & (jan1 > 4) THEN (* falls in year-1 ? *)\n\t\t\tyear := dt.year - 1;\n\t\t\tIF (jan1 = 5) OR ((jan1 = 6) & LeapYear(year)) THEN \n                week := 53;\n\t\t\tELSE \n                week := 52;\n\t\t\tEND;\n\t\tELSE\n\t\t\tIF leap THEN \n                i := 366;\n            ELSE \n                i := 365;\n            END;\n\t\t\tIF ((i - doy) < (4 - weekday)) THEN\n\t\t\t\tyear := dt.year + 1;\n\t\t\t\tweek := 1;\n\t\t\tELSE\n\t\t\t\tyear := dt.year;\n\t\t\t\ti := doy + (7-weekday) + (jan1-1);\n\t\t\t\tweek := i DIV 7;\n\t\t\t\tIF (jan1 > 4) THEN \n                    DEC(week);\n                END;\n\t\t\tEND;\n\t\tEND;\n\tELSE\n\t\tyear := -1; week := -1; weekday := -1;\n\tEND;\nEND WeekDate;\n\n(* Equal -- compare to date records to see if they are equal values *)\nPROCEDURE Equal*(t1, t2: DateTime) : BOOLEAN;\nBEGIN\n\tRETURN ((t1.second = t2.second) & (t1.minute = t2.minute) & (t1.hour = t2.hour) & (t1.day = t2.day) & (t1.month = t2.month) & (t1.year = t2.year))\nEND Equal;\n\n(* compare -- used in Compare only for comparing specific values,\n    returning an appropriate -1, 0, 1 *)\nPROCEDURE compare(t1, t2 : INTEGER) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tIF (t1 < t2) THEN \n        result := -1;\n\tELSIF (t1 > t2) THEN \n        result := 1;\n\tELSE \n        result := 0;\n\tEND;\n\tRETURN result\nEND compare;\n\n(* Compare -- returns -1 if (t1 < t2), 0 if (t1 = t2) or 1 if (t1 >  t2) *)\nPROCEDURE Compare*(t1, t2: DateTime) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tresult := compare(t1.year, t2.year);\n\tIF (result = 0) THEN\n\t\tresult := compare(t1.month, t2.month);\n\t\tIF (result = 0) THEN\n\t\t\tresult := compare(t1.day, t2.day);\n\t\t\tIF (result = 0) THEN\n\t\t\t\tresult := compare(t1.hour, t2.hour);\n\t\t\t\tIF (result = 0) THEN\n\t\t\t\t\tresult := compare(t1.minute, t2.minute);\n\t\t\t\t\tIF (result = 0) THEN\n\t\t\t\t\t\tresult := compare(t1.second, t2.second);\n\t\t\t\t\tEND;\n\t\t\t\tEND;\n\t\t\tEND;\n\t\tEND;\n\tEND;\n\tRETURN result\nEND Compare;\n\n(* CompareDate -- compare day, month and year values only *)\nPROCEDURE CompareDate*(t1, t2: DateTime) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tresult := compare(t1.year, t2.year);\n\tIF (result = 0) THEN\n\t\tresult := compare(t1.month, t2.month);\n\t\tIF (result = 0) THEN\n\t\t\tresult := compare(t1.day, t2.day);\n\t\tEND;\n\tEND;\n\tRETURN result\nEND CompareDate;\n\n(* CompareTime -- compare second, minute and hour values only *)\nPROCEDURE CompareTime*(t1, t2: DateTime) : INTEGER;\nVAR result : INTEGER;\nBEGIN\n\tresult := compare(t1.hour, t2.hour);\n\tIF (result = 0) THEN\n\t\tresult := compare(t1.minute, t2.minute);\n\t\tIF (result = 0) THEN\n\t\t\tresult := compare(t1.second, t2.second);\n\t\tEND;\n\tEND;\n\tRETURN result\nEND CompareTime;\n\n\n\n(* TimeDifferences -- returns the absolute time difference between \nt1 and t2.\n\nNote that leap seconds are not counted, \nsee http://www.eecis.udel.edu/~mills/leap.html *)\nPROCEDURE TimeDifference*(t1, t2: DateTime; VAR days, hours, minutes, seconds : INTEGER);\nCONST SecondsPerMinute = 60; SecondsPerHour = 3600; SecondsPerDay = 86400;\nVAR start, end: DateTime; year, month, second : INTEGER;\nBEGIN\n\tIF (Compare(t1, t2) = -1) THEN \n        start := t1; \n        end := t2; \n    ELSE \n        start := t2; \n        end := t1; \n    END;\n\tIF (start.year = end.year) & (start.month = end.month) & (start.day = end.day) THEN\n\t\tsecond := end.second - start.second + ((end.minute - start.minute) * SecondsPerMinute) + ((end.hour - start.hour) * SecondsPerHour);\n\t\tdays := 0;\n        hours := 0;\n        minutes := 0;\n\tELSE\n\t\t(* use start date/time as reference point *)\n\t\t(* seconds until end of the start.day *)\n\t\tsecond := (SecondsPerDay - start.second) - (start.minute * SecondsPerMinute) - (start.hour * SecondsPerHour);\n\t\tIF (start.year = end.year) & (start.month = end.month) THEN\n\t\t\t(* days between start.day and end.day *)\n\t\t\tdays := (end.day - start.day) - 1;\n\t\tELSE\n\t\t\t(* days until start.month ends excluding start.day *)\n\t\t\tdays := NumOfDays(start.year, start.month) - start.day;\n\t\t\tIF (start.year = end.year) THEN\n\t\t\t\t(* months between start.month and end.month *)\n\t\t\t\tFOR month := start.month + 1 TO end.month - 1 DO\n\t\t\t\t\tdays := days + NumOfDays(start.year, month);\n\t\t\t\tEND;\n\t\t\tELSE\n\t\t\t\t(* days until start.year ends (excluding start.month) *)\n\t\t\t\tFOR month := start.month + 1 TO 12 DO\n\t\t\t\t\tdays := days + NumOfDays(start.year, month);\n\t\t\t\tEND;\n\t\t\t\tFOR year := start.year + 1 TO end.year - 1 DO (* days between start.years and end.year *)\n\t\t\t\t\tIF LeapYear(year) THEN days := days + 366; ELSE days := days + 365; END;\n\t\t\t\tEND;\n\t\t\t\tFOR month := 1 TO end.month - 1 DO (* days until we reach end.month in end.year *)\n\t\t\t\t\tdays := days + NumOfDays(end.year, month);\n\t\t\t\tEND;\n\t\t\tEND;\n\t\t\t(* days in end.month until reaching end.day excluding end.day *)\n\t\t\tdays := (days + end.day) - 1;\n\t\tEND;\n\t\t(* seconds in end.day *)\n\t\tsecond := second + end.second + (end.minute * SecondsPerMinute) + (end.hour * SecondsPerHour);\n\tEND;\n\tdays := days + (second DIV SecondsPerDay); second := (second MOD SecondsPerDay);\n\thours := (second DIV SecondsPerHour); second := (second MOD SecondsPerHour);\n\tminutes := (second DIV SecondsPerMinute); second := (second MOD SecondsPerMinute);\n\tseconds := second;\nEND TimeDifference;\n\n(* AddYear -- Add/Subtract a number of years to/from date *)\nPROCEDURE AddYears*(VAR dt: DateTime; years : INTEGER);\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.year := dt.year + years;\n\tASSERT(IsValid(dt));\nEND AddYears;\n\n(* AddMonths -- Add/Subtract a number of months to/from date.\nThis will adjust date.year if necessary *)\nPROCEDURE AddMonths*(VAR dt: DateTime; months : INTEGER);\nVAR years : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tyears := months DIV 12;\n\tdt.month := dt.month + (months MOD 12);\n\tIF (dt.month > 12) THEN\n\t\tdt.month := dt.month - 12;\n\t\tINC(years);\n\tELSIF (dt.month < 1) THEN\n\t\tdt.month := dt.month + 12;\n\t\tDEC(years);\n\tEND;\n\tIF (years # 0) THEN AddYears(dt, years); END;\n\tASSERT(IsValid(dt));\nEND AddMonths;\n\n(* AddDays --  Add/Subtract a number of days to/from date.\nThis will adjust date.month and date.year if necessary *)\nPROCEDURE AddDays*(VAR dt: DateTime; days : INTEGER);\nVAR nofDaysLeft : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tIF (days > 0) THEN\n\t\tWHILE (days > 0) DO\n\t\t\tnofDaysLeft := NumOfDays(dt.year, dt.month) - dt.day;\n\t\t\tIF (days > nofDaysLeft) THEN\n\t\t\t\tdt.day := 1;\n\t\t\t\tAddMonths(dt, 1);\n\t\t\t\tdays := days - nofDaysLeft - 1; (* -1 because we consume the first day of the next month *)\n\t\t\tELSE\n\t\t\t\tdt.day := dt.day + days;\n\t\t\t\tdays := 0;\n\t\t\tEND;\n\t\tEND;\n\tELSIF (days < 0) THEN\n\t\tdays := -days;\n\t\tWHILE (days > 0) DO\n\t\t\tnofDaysLeft := dt.day - 1;\n\t\t\tIF (days > nofDaysLeft) THEN\n\t\t\t\tdt.day := 1; (* otherwise, dt could become an invalid date if the previous month has less days than dt.day *)\n\t\t\t\tAddMonths(dt, -1);\n\t\t\t\tdt.day := NumOfDays(dt.year, dt.month);\n\t\t\t\tdays := days - nofDaysLeft - 1; (* -1 because we consume the last day of the previous month *)\n\t\t\tELSE\n\t\t\t\tdt.day := dt.day - days;\n\t\t\t\tdays := 0;\n\t\t\tEND;\n\t\tEND;\n\tEND;\n\tASSERT(IsValid(dt));\nEND AddDays;\n\n(* AddHours -- Add/Subtract a number of hours to/from date.\nThis will adjust date.day, date.month and date.year if necessary *)\nPROCEDURE AddHours*(VAR dt: DateTime; hours : INTEGER);\nVAR days : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.hour := dt.hour + hours;\n\tdays := dt.hour DIV 24;\n\tdt.hour := dt.hour MOD 24;\n\tIF (dt.hour < 0) THEN\n\t\tdt.hour := dt.hour + 24;\n\t\tDEC(days);\n\tEND;\n\tIF (days # 0) THEN AddDays(dt, days); END;\n\tASSERT(IsValid(dt));\nEND AddHours;\n\n(* AddMinutes -- Add/Subtract a number of minutes to/from date.\nThis will adjust date.hour, date.day, date.month and date.year \nif necessary *)\nPROCEDURE AddMinutes*(VAR dt: DateTime; minutes : INTEGER);\nVAR hours : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.minute := dt.minute + minutes;\n\thours := dt.minute DIV 60;\n\tdt.minute := dt.minute MOD 60;\n\tIF (dt.minute < 0) THEN\n\t\tdt.minute := dt.minute + 60;\n\t\tDEC(hours);\n\tEND;\n\tIF (hours # 0) THEN AddHours(dt, hours); END;\n\tASSERT(IsValid(dt));\nEND AddMinutes;\n\n(* AddSeconds -- Add/Subtract a number of seconds to/from date.\nThis will adjust date.minute, date.hour, date.day, date.month and\ndate.year if necessary *)\nPROCEDURE AddSeconds*(VAR dt: DateTime; seconds : INTEGER);\nVAR minutes : INTEGER;\nBEGIN\n\tASSERT(IsValid(dt));\n\tdt.second := dt.second + seconds;\n\tminutes := dt.second DIV 60;\n\tdt.second := dt.second MOD 60;\n\tIF (dt.second < 0) THEN\n\t\tdt.second := dt.second + 60;\n\t\tDEC(minutes);\n\tEND;\n\tIF (minutes # 0) THEN AddMinutes(dt, minutes); END;\n\tASSERT(IsValid(dt));\nEND AddSeconds;\n\n\n(* IsDateString -- return TRUE if the ARRAY OF CHAR is 10 characters\nlong and is either in the form of YYYY-MM-DD or MM/DD/YYYY where\nY, M and D are digits. \nNOTE: is DOES NOT check the ranges of the digits. *)\nPROCEDURE IsDateString*(inline : ARRAY OF CHAR) : BOOLEAN;\nVAR \n    test : BOOLEAN; i, pos : INTEGER;\n    src : ARRAY MAXSTR OF CHAR;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.TrimSpace(src);\n    test := FALSE;\n    IF Strings.Length(src) = 10 THEN\n        pos := Strings.Pos(\"-\", src, 0);\n        IF pos > 0 THEN\n            IF (src[4] = \"-\") & (src[7] = \"-\") THEN\n                test := TRUE;\n                FOR i := 0 TO 9 DO\n                    IF (i # 4) & (i # 7) THEN\n                       IF Chars.IsDigit(src[i]) = FALSE THEN\n                           test := FALSE;\n                       END;\n                    END;\n                END;\n            ELSE\n                test := FALSE;\n            END;\n        END;\n        pos := Strings.Pos(\"/\", src, 0);\n        IF pos > 0 THEN\n            IF (src[2] = \"/\") & (src[5] = \"/\") THEN\n                test := TRUE;\n                FOR i := 0 TO 9 DO\n                    IF (i # 2) & (i # 5) THEN\n                        IF Chars.IsDigit(src[i]) = FALSE THEN\n                            test := FALSE;\n                        END;\n                    END;\n                END;\n            ELSE\n                test := FALSE;\n            END;\n        END;\n    END;\n    RETURN test\nEND IsDateString;\n\n(* IsTimeString -- return TRUE if the ARRAY OF CHAR has 4 to 8 \ncharacters in the form of H:MM, HH:MM, HH:MM:SS where H, M and S\nare digits. *)\nPROCEDURE IsTimeString*(inline : ARRAY OF CHAR) : BOOLEAN;\nVAR \n    test : BOOLEAN; \n    l : INTEGER;\n    src : ARRAY MAXSTR OF CHAR;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.TrimSpace(src);\n    (* remove any trailing am/pm suffixes *)\n    IF Chars.EndsWith(\"m\", src) THEN\n        IF Chars.EndsWith(\"am\", src) THEN\n            Chars.TrimSuffix(\"am\", src);\n        ELSE\n            Chars.TrimSuffix(\"pm\", src);\n        END;\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"M\", src) THEN\n        Chars.TrimSuffix(\"AM\", src);\n        Chars.TrimSuffix(\"PM\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"p\", src) THEN\n        Chars.TrimSuffix(\"p\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"P\", src) THEN\n        Chars.TrimSuffix(\"P\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"a\", src) THEN\n        Chars.TrimSuffix(\"a\", src);\n        Chars.TrimSpace(src);\n    ELSIF Chars.EndsWith(\"A\", src) THEN\n        Chars.TrimSuffix(\"A\", src);\n        Chars.TrimSpace(src);\n    END;\n    Strings.Extract(src, 0, 8, src);\n    test := FALSE;\n    l := Strings.Length(src);\n    IF (l = 4) THEN\n        IF Chars.IsDigit(src[0]) & (src[1] = \":\") & \n            Chars.IsDigit(src[2]) & Chars.IsDigit(src[3]) THEN\n            test := TRUE;\n        ELSE\n            test := FALSE;\n        END;\n    ELSIF (l = 5) THEN\n        IF Chars.IsDigit(src[0]) & Chars.IsDigit(src[1]) &\n            (src[2] = \":\") & \n            Chars.IsDigit(src[3]) & Chars.IsDigit(src[4]) THEN\n            test := TRUE;\n        ELSE\n            test := FALSE;\n        END;\n    ELSIF (l = 8) THEN\n        IF Chars.IsDigit(src[0]) & Chars.IsDigit(src[1]) &\n            (src[2] = \":\") & \n            Chars.IsDigit(src[3]) & Chars.IsDigit(src[4]) & \n            (src[5] = \":\") & \n            Chars.IsDigit(src[6]) & Chars.IsDigit(src[7]) THEN\n            test := TRUE;\n        ELSE\n            test := FALSE;\n        END;\n    ELSE\n        test := FALSE;\n    END;\n    RETURN test\nEND IsTimeString;\n\n(* ParseDate -- parses a date string in YYYY-MM-DD or\nMM/DD/YYYY format. *)\nPROCEDURE ParseDate*(inline : ARRAY OF CHAR; VAR year, month, day : INTEGER) : BOOLEAN;\nVAR src, tmp : ARRAY MAXSTR OF CHAR; ok, b : BOOLEAN;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.Clear(tmp);\n    ok := FALSE;\n\tIF IsDateString(src) THEN\n        (* FIXME: Need to allow for more than 4 digit years! *)\n        IF (src[2] = \"/\") & (src[5] = \"/\") THEN\n            ok := TRUE;\n            Strings.Extract(src, 0, 2, tmp);\n            Convert.StringToInt(tmp, month, b);\n            ok := ok & b;\n            Strings.Extract(src, 4, 2, tmp);\n            Convert.StringToInt(tmp, day, b);\n            ok := ok & b;\n            Strings.Extract(src, 6, 4, tmp);\n            Convert.StringToInt(tmp, year, b);\n            ok := ok & b;\n        ELSIF (src[4] = \"-\") & (src[7] = \"-\") THEN\n            ok := TRUE;\n            Strings.Extract(src, 0, 4, tmp);\n            Convert.StringToInt(tmp, year, b);\n            ok := ok & b;\n            Strings.Extract(src, 5, 2, tmp);\n            Convert.StringToInt(tmp, month, b);\n            ok := ok & b;\n            Strings.Extract(src, 8, 2, tmp);\n            Convert.StringToInt(tmp, day, b);\n            ok := ok & b;\n        ELSE\n            ok := FALSE;\n        END;\n    END;\n    RETURN ok\nEND ParseDate;\n\n(* ParseTime -- procedure for parsing time strings into hour,\nminute, second. Returns TRUE on successful parse, FALSE otherwise *)\nPROCEDURE ParseTime*(inline : ARRAY OF CHAR; VAR hour, minute, second : INTEGER) : BOOLEAN;\nVAR src, tmp : ARRAY MAXSTR OF CHAR; ok : BOOLEAN; cur, pos, l : INTEGER;\nBEGIN\n    Chars.Set(inline, src);\n    Chars.Clear(tmp);\n\tIF IsTimeString(src) THEN\n        ok := TRUE;\n        cur := 0; pos := 0;\n        pos := Strings.Pos(\":\", src, cur);\n        IF pos > 0 THEN\n        (* Get Hour *)\n            Strings.Extract(src, cur, pos - cur, tmp);\n            Convert.StringToInt(tmp, hour, ok);\n            IF ok THEN\n                (* Get Minute *)\n                cur := pos + 1;\n                Strings.Extract(src, cur, 2, tmp);\n                Convert.StringToInt(tmp, minute, ok);\n                IF ok THEN\n                    (* Get second, optional, default to zero *)\n                    pos := Strings.Pos(\":\", src, cur);\n                    IF pos > 0 THEN\n                        cur := pos + 1;\n                        Strings.Extract(src, cur, 2, tmp);\n                        Convert.StringToInt(tmp, second, ok);\n                        cur := cur + 2;\n                    ELSE\n                        second := 0;\n                    END;\n                    (* Get AM/PM, optional, adjust hour if PM *)\n                    l := Strings.Length(src);\n                    WHILE (cur < l) & Chars.IsSpace(src[cur]) DO\n                        cur := cur + 1;\n                    END;\n                    Strings.Extract(src, cur, 2, tmp);\n                    Chars.TrimSpace(tmp);\n                    IF Chars.Equal(tmp, \"PM\") OR Chars.Equal(tmp, \"pm\") THEN\n                        hour := hour + 12;\n                    END;\n                ELSE\n                    ok := FALSE;\n                END;\n            END;\n        ELSE\n            ok := FALSE;\n        END;\n    ELSE\n        ok := FALSE;\n    END;\n    IF ok THEN\n        ok := ((hour >= 0) & (hour <= 23)) &\n            ((minute >= 0) & (minute <= 59)) &\n                ((second >= 0) & (second <= 59));\n    END;\n    RETURN ok\nEND ParseTime;\n\n\n(* Parse accepts a date array of chars in either dates, times\nor dates and times separate by spaces. Date formats supported\ninclude YYYY-MM-DD, MM/DD/YYYY. Time formats include\nH:MM, HH:MM, H:MM:SS, HH:MM:SS with 'a', 'am', 'p', 'pm' \nsuffixes.  Dates and times can also be accepted as JSON \nexpressions with the individual time compontents are specified \nas attributes, e.g. `{\"year\": 1998, \"month\": 12, \"day\": 10,\n\"hour\": 11, \"minute\": 4, \"second\": 3}.\nParse returns TRUE on successful parse, FALSE otherwise.\n\nBUG: Assumes a 4 digit year.\n*) \nPROCEDURE Parse*(inline : ARRAY OF CHAR; VAR dt: DateTime) : BOOLEAN;\nVAR src, ds, ts, tmp : ARRAY SHORTSTR OF CHAR; ok, okDate, okTime : BOOLEAN; \n    pos, year, month, day, hour, minute, second : INTEGER;\nBEGIN\n    dt.year := 0;\n    dt.month := 0;\n    dt.day := 0;\n    dt.hour := 0;\n    dt.minute := 0;\n    dt.second := 0;\n    Chars.Clear(tmp);\n    Chars.Set(inline, src);\n    Chars.TrimSpace(src);\n    (* Split into Date and Time components *)\n    pos := Strings.Pos(\" \", src, 0);\n    IF pos >= 0 THEN\n        Strings.Extract(src, 0, pos, ds);\n        pos := pos + 1;\n        Strings.Extract(src, pos, Strings.Length(src) - pos, ts);\n    ELSE\n        Chars.Set(src, ds);\n        Chars.Set(src, ts);\n    END;\n    ok := FALSE;\n    IF IsDateString(ds) THEN\n        ok := TRUE;\n        okDate := ParseDate(ds, year, month, day);\n        SetDate(year, month, day, dt);\n        ok := ok & okDate;\n    END;\n    IF IsTimeString(ts) THEN\n        ok := ok OR okDate;\n        okTime := ParseTime(ts, hour, minute, second);\n        SetTime(hour, minute, second, dt);\n        ok := ok & okTime;\n    END;\n    RETURN ok\nEND Parse;\n\nBEGIN\n    Chars.Set(\"January\", Months[0]);\n    Chars.Set(\"February\", Months[1]);\n    Chars.Set(\"March\", Months[2]);\n    Chars.Set(\"April\", Months[3]);\n    Chars.Set(\"May\", Months[4]);\n    Chars.Set(\"June\", Months[5]);\n    Chars.Set(\"July\", Months[6]);\n    Chars.Set(\"August\", Months[7]);\n    Chars.Set(\"September\", Months[8]);\n    Chars.Set(\"October\", Months[9]);\n    Chars.Set(\"November\", Months[10]);\n    Chars.Set(\"December\", Months[11]);\n\n    Chars.Set(\"Sunday\", Days[0]);\n    Chars.Set(\"Monday\", Days[1]);\n    Chars.Set(\"Tuesday\", Days[2]);\n    Chars.Set(\"Wednesday\", Days[3]);\n    Chars.Set(\"Thursday\", Days[4]);\n    Chars.Set(\"Friday\", Days[5]);\n    Chars.Set(\"Saturday\", Days[6]);\n\n    DaysInMonth[0] := 31; (* January *)\n    DaysInMonth[1] := 28; (* February *)\n    DaysInMonth[2] := 31; (* March *)\n    DaysInMonth[3] := 30; (* April *)\n    DaysInMonth[4] := 31; (* May *)\n    DaysInMonth[5] := 30; (* June *)\n    DaysInMonth[6] := 31; (* July *)\n    DaysInMonth[7] := 31; (* August *)\n    DaysInMonth[8] := 30; (* September *)\n    DaysInMonth[9] := 31; (* October *)\n    DaysInMonth[10] := 30; (* November *)\n    DaysInMonth[11] := 31; (* December *)\n\nEND Dates.\n\n~~~\n\n\n",
      "data": {
        "title": "Dates"
      },
      "url": "posts/2020/11/27/Dates.json"
    }
  ]
}