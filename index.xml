<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:source="https://source.scripting.com/">
  <atom:link href="https://rsdoiel.github.io/pages.xml" rel="self" type="application/rss+xml" />
  <channel>
    <title>An Antenna Website</title>
    <description>
      This is the default websites created by the antenna init action.
    </description>
    <link>https://rsdoiel.github.io/pages.xml</link>
    <lastBuildDate>01 Jan 26 14:29 -0800</lastBuildDate>
    <generator>antenna/0.0.20</generator>
    <docs>https://cyber.harvard.edu/rss/rss.html</docs>
    <item>
      <title>Reflection on a few decades</title>
      <description>
        <![CDATA[A long post reflecting on things I've been thinking about in 2025 as we start
        2026.]]>
      </description>
      <source:markdown># Reflection on a few decades

By R. S. Doiel, 2025-12-31

I have been thinking about my career recently. It extends into the prior century. I have also been thinking allot about the Internet and the evolution of the Web. I see a pretty consistant thread in my career. I write software that transforms content and information from one system into another. Sometimes it&#39;s structured data and sometimes it&#39;s unstructured text. The thread became clearer with the arrival of the World Wide Web in the early 1990s. While the systems used over that time have grown complex I think they can be made far simpler. __I think they can be better implemented for writers. They can be writer centric.__ Finding a path to something simple has preoccupied my nonworking hours for most of 2025.

## A lesson from my past

An early set of programs I wrote for the Web were written in a new at the time language called Perl. My University had a &#34;mail form handler&#34; service that would let you create a web form and have the results sent to your email. I wrote a simple mail filter that would extract email&#39;s web form content and append the results to a CSV file. I had another Perl script that would read in the CSV file and generate a set of HTML calendar pages in the directory that the Unix host used for personal websites. It worked pretty good. It was so trivial I didn&#39;t keep a copy of it or port it to a new language when I stopped writing software in Perl all those years ago.

This little calendar project lead to career opportunities. I didn&#39;t find that out to much later. A colleague mentioned I was considered a hacker (in the good sense of the word) by some of the system admins because I had done that. It was just one of many experiments I&#39;ve done over my years working with computers.

That personal project taught me several things. I&#39;ve kept them in mind over the years.

1. Composible systems are powerful, Unix combined with the Web is composible
2. Structured data aids in composibility (EMail and CSV are only two examples)
3. Systems get used in unexpected ways.

No 3. has proved important. The web can be viewed from both the audience of one and the audience of the general public. You can also split the authoring system from the presentation system. You can divvy those up between machines. It all depends on the project scope, rates of data changes combined with scale of the specific audiences.

When I first started maintaining and building web publishing system I almost always leveraged a database management system too.  This was particularly true when [LAMP](https://en.wikipedia.org/wiki/LAMP_(software_bundle)) emerged as the common platform.

Databases were convenient because structured data is easy to work. The unstructured document can be worked into structure using indirection. A path can point to the unstructured thing. Links are a form of path. A link provides a structured reference to the unstructured object. Structure allows us to use a database to provide locking behaviors and avoid write collisions. In turn that made it easier to write the web programs.

Eventually the bespoke web publishing systems became content management systems. Bespoke gave way to more standardized software like [Movable Type](https://en.wikipedia.org/wiki/Movable_Type), [Zope](https://en.wikipedia.org/wiki/Zope), [WordPress](https://en.wikipedia.org/wiki/WordPress) and [Drupal](https://en.wikipedia.org/wiki/Drupal). Those arrive in the second Internet age, the age of commercialization.

## Commercialization era

When the Internet and Web went from an academic and research platform to a commercial platform many things changed. The Internet and Web came to be consumed by monetization, business models and greed. Those are why we have a surveillance economy today.

With these new drivers new demands came to the forefront. One big side effect was the Internet grew very large very quickly. By the time the first [iPhone](https://en.wikipedia.org/wiki/IPhone) was announced the Web was already experiencing growing pains. One of the big issues was spikes in Web traffic.

In the early days we assumed a few dozen concurrent connections. By 2000 you needed to handle much more. Buy 2005 you worried about tens of thousands of concurrent connections. The problem was even large enough to be named. It was the &#34;10K problem&#34;. It was talking about by web people in the same way the old aerospace engineers used to talk about the sound barrier.

Money was on the line so people tried various solutions. Some were hardware approaches, some content approaches others were changing how we implemented web services generally. Eventually the general purpose web services like NginX and Apache2 easily handled 10K concurrent requests. Then the worry was 100K, 1000K, etc. It is one of the reasons that distributed systems came to dominate the commercial web.

What hasn&#39;t been acknowledged since the concurrency problems is that we&#39;re may be approaching peak human use. Most people on the planet experience the web today either in embedded systems (appliances) or their phones. Most of the growth in traffic I&#39;ve seen at work for more than a decade isn&#39;t human users. The growth in traffic is driven by bots. Bots are software systems. Early on bots  tended to be dominated by web crawlers. Bots that indexed web content, surveyed the content or mirrored content. The bots were largely useful. Since 2024 the problem has been AI bot swarms. These bots were used to harvest web content to train large language models. In practice they were a use to deny services to competing bots and humans. 

Scaling up to bots leads to more bots. Maybe we should be thinking about scaling down instead of up.

### Observations on site construction

Since at least 2005 there has been a gathering renaissance of static websites. There has been many drivers. Static sites often are easier to scale for high volume traffic. They are easier to secure against attacks and fix if defaced. Often they are much cheaper to run and maintain.

Several innovations helped this happen.

1. Template systems are easy to build
2. Light weight markup adoption, like Markdown, provides a source for generating HTML
3. Cheap to rent static storage system became commercially available (S3)
4. The web browser went from a green screen surrogate to a full on rich client. They work even on phones.

Markdown has become a very common markup expression well suited to generating HTML.  Support is easy to integrate into commercial content systems like Atlassian&#39;s Confluence and GitHub&#39;s pages. Commercial systems like GitHub offer favorable hosting options for static website. Finally, in spite of the term, a static site can be fully interactive without requiring additional resources from the web server for many use cases.

### Observations on computer systems

In 2025 commercially available computer systems follow variations of the Von Neuman computer model. A CPU, Memory (RAM) and storage. That model also assumes inputs and outputs that are distinct (people forget this). The scaling problem observed in the commercial era often presumes the input and outputs are used at the same rate for data consumed by us humans or even software. It is important to challenge our assumptions another rates of data changes.

My observation is the rates of change, both in the software implementation and in data, are important in guiding how you approach the lessons of scaling. Curation requires more dynamic responses while consumption is simply a current state problem.

Because of the needs of the monetized Web the there has been an out sized concern with scaling up while assuming dynamic systems are the base line. This is only true when you are creating monoliths like the walled gardens most people used on the Web today. You can scale writing (inputs) separately form reading (outputs) and that leads to better scaling. Unfortunately most developers and many engineers fail to see that distinction. We keep building complex monolithic systems even when you compose the monoliths from distributed systems.

Scaling up appears to be the goal, a single direction of scaling. This is a false lesson.Scale goes up and down. That can be an advantage.

## Are we entering a new web era yet?

Starting before &#34;Web 2.0&#34; there has been cyclic hype about the next web or the web&#39;s replacement.  Most of the time this is driven by some sort of technological change being commercialized. There is a good share of doom and gloom along side Utopian thinking in these hype cycles. At some point we will arrive at a new era but I suspect it may not be driven purely by technical changes.

### My bet, Simplification 

Why is simplification both desirable and possible in leading to an a new era on the Internet and Web? The Internet and Web were built as distributed systems. The specific technologies are largely intact even though commercial interests don&#39;t align with distributed systems. A lesson from that late 20th century and early 21st Web are unregulated markets, services, tend to collapse towards monopolies. Centralization inevitably becomes a profit advantage. Yet centralization makes them brittle. Eventually decentralization is re-embraced as Web habits wvolve. Given  the underpinning technology of the Internet and Web remains distributable I don&#39;t think reinvention is a requirement to usher in the next era. Easier paths exist.

The next web isn&#39;t likely to be based on block chain, bitcoin or large language models either. Why? None of these really enhance communication. The Internet and the Web are communication systems. Computers unlike their calculator predecessor are communication devices too. That telephone in your pocket is a computer after all. Making it easier to communicate is likely to be the source of the next evolution. How do we make it easier to communicate?

Market consolidation does not lead to innovation no matter how many startups are bought by conglomerates. I think people are where the next web happens. It&#39;s happens at a human scale of a few people at a time. If there is enough critical mass of people doing the &#34;new thing&#34; then that&#39;s when the new web will arrive. It&#39;ll have roots in the old web, may even be technologically indistinguishable but it&#39;ll be different because people choose to use it differently and think about it differently.

I think the next web needs to be more personal, more authentic. Less about algorithms. To make that happen it will start as a countervailing force to the current onslaught of centralization and control. It will likely happen in spite of centralization and control.

There are elements I see as being factors in reasserting decentralization. There has been a computer revolution happening that has largely gone unnamed and unnoticed by mainstream media and society. That revolution is in inexpensive single board computers. Some of it has been dismissed as fad in the form of the DIY maker movement. Before that it was buried under the commercial hype of the Internet of Things. Single board computers are real. I&#39;ve been using them for years. I&#39;m typing this post on one now. They are web friendly.

### then and now

My first computer cost me about $1800.00 US dollars. My first computer was a clone 286 machine. I ran DOS and Minix on it. I had to go out buy the parts and assemble it myself. This was back in the late 1980s. A mini computer that could host a website in the early 1990s range between $50,000.00 to $250,000.00 US. The cost of a house at that time. Prices came down and eventually personal computers could run websites directly. The price of a web server settled down to about I paid for my  first personal computer.

Fast forward to now. For $1800.00 US today I can easily assemble four Raspberry Pi 500+ workstations, a 16 port network switch and Ethernet cables to connect them. I can grow out that network with web servers running about $150 to $200.00 US per server depending on the storage I buy. A single Raspberry Pi 5 with solid state disk storage is enough to host a website with concurrent users in the thousands. What that 1990&#39;s era mini computer did for $50,000.00 US. That&#39;s an important change.

Here&#39;s how I came up with my $1800.00 budget. A sixteen port Ethernet switch from Netgear runs $100.00 US when I checked Best Buy today. A Raspberry Pi 500+ runs at $200.00 US plus tax and shipping. A Pi Monitor runs $100.00 US. Add in cables and such you&#39;ll looking at about $350.00 per work station. That&#39;s how I know I can build a four workstation network with room to expand for a $1800.00 budget. I don&#39;t even need to assemble the computers I just need to connect them up and power them on.  Each machine can be either a web host, workstation or both. My choice.  Expanding that system is also low coast. You can build useful web servers for less than the price of the Raspberry 500+ and monitor.

My little four workstation network is enough to hardware to support a neighborhood paper. Adding a few more machines and you can provide hardware for a small town paper. You just need the physical space to house them and of course the writers and graphics people to use them.

Unlike the small town newspapers when I grew up there are no printing presses to purchase, rent or maintain. There&#39;s just several computers. The computers I listed last decades. This suggests something to me. Maybe the hardware we have is good enough for the next web. Maybe hardware isn&#39;t the issue it once was.

The cost factor in the next generation of the web doesn&#39;t require huge expenditures of capital. It doesn&#39;t even require big data centers in spite of what the Big Companies will swear are required. 

This suggestions we can scale down. We can scale by going small. Heck, any &#34;smart&#34; appliance is likely to have a web server embedded in it and they don&#39;t put in any excess compute capability into appliances. The Internet of things has been enabled by the cheapness of some single board computers. In the meantime the foundations of the Internet and Web remain intact and those little machines fit it just fine.

The trick is to making the web really distributed is to allow it be easily editable. I think we do that by embracing the little single board computers like the Raspberry Pi. We embrace individuals owning the hardware, software and the content they produce. Maybe we don&#39;t need to rent to host our presence. What if we didn&#39;t need to pay a toll to use the Internet either?

The small set of network computers I described run the same protocols as the Internet and Web. If I connect them up to the switch I have a local private network version of the Internet and Web. What would happen if I connected up with my next neighbors&#39; network? 

### Moving beyond commercial Internet Service Providers

When the Internet was commercialized it was done so using the justification that Government couldn&#39;t afford giving the public access to the Government created resource. The claim was only private business could do that. Not surprising considering the political wisdom of the 1980s and 1990s. That wisdom suggested all problems as business problems and markets solve business problems.

In practice that approach has been a failure. Try to use a cell phone in rural America today, chances are it&#39;ll fail. The Internet for most Americans is experienced through their cells phones. Heck, in many urban areas cell service remains really problematic. It is common to get poor cell service through out most of Los Angeles county. Some of that is geography (Mountains) but much is also the lack of investment by business as territory was divided up among the decreasing number of Internet Service Providers.

In response to markets and government failures and the continue push to put everything online and people have started to pull together and find ways to cope. An interesting approach has been community owned internet service providers. If you&#39;re a town with less than 1000 people you&#39;re unlikely to have many business offering service at affordable rates. The best you might due is satellite connections like Starlink. Satellite access is slow because of transmission distance. A fiber connection is so much faster. This is a mater of physics, radio signals travel slower than the speed of light. For fixed locations like farms, homes and small businesses fiber gives you more reliable connections.

What has started to happen in some rural areas is communities lay their own fiber between houses or network cell towers. Then they connect these to a location that also has an Internet connection with significant bandwidth. If a railroad runs through their town then that might be used as a connection point (Rail systems have used fiber optics for switch control since the last century, they sometimes sell their excess capacity). Similarly a large antenna array receiver  can cary more data to and from the satellite then a home system like Starlink can. By pooling the communications together this becomes an affordable option.

Rural community cooperatives can get by with minimal staffing and little in the way of physical space. They function like other small business in terms of budget but since they are nonprofit they don&#39;t need to enhance the cost charged to members nor do they need to pay a competitor off to avoid competition.

A policy of encouraging lots of little cooperatives is technically doable. It is how the Internet was designed. Like public streets and highways communities can choose to provide board band access that connect to the Internet. Organizations like [Institute for Local Self-Reliance](https://ilsr.org/) show examples of this approach. 

### What else is needed?

The act of producing a website needs to be simplified too. Ideally it&#39;d be a turn key system I can run on my Raspberry Pi sized computers. That turns out to be incredibly easy when you think of the content system as being single user.

I&#39;ve notice about a decade and a half ago people seemed to rediscover &#34;static&#34; web. This appeared to be driven by a reaction to the walled gardens, to privacy costs and the desire for local control. The static web is a bit of a misnomer. It refers to how the content is hosted not how the content behaves. A static website in 2025 can be highly interactive. 

During the first dot com era (approx. 1995 through 2000) &#34;dynamic&#34; websites rose in popularity. While the web browser was becoming a rich client allot dynamism happened server side. By 2004 and 2005 the rich features provided by web browser were being leverage in &#34;web properties&#34; like GMail and Google Maps. At that point the static approach become a misnomer. The web browser can run whole applications.  You can approach site generation like just like you did in the original web and still have a dynamic experience. By 2010 this was well understood. By 2020 it was taken for granted. I don&#39;t think the public realizes that the web they experience on their phones is produced in some part by static websites. All those &#34;apps&#34; that run on your phone that use webview are simply websites and services. Many statically hosted.

When JavaScript and CSS became predominantly supported features in web browsers they allowed the client, the web browser, to be independently dynamic. In terms of how we call things it made it much harder to explain that a &#34;static&#34; website doesn&#39;t imply it can&#39;t be interactive. In 2000 providing search for a web site required a hosted service. Today you can run the search service inside the browser itself. Indexes are updated when you generate the static website. The indexes are partitioned and retrieged by the browser as needed. Unless the website is Wikipedia size you can just provide it browser side. The term static site remains but it&#39;s not limited to the approaches used by the retro or nostalgic web.

The way the walled gardens are built and scaled ultimately comes out of the tried and true lessons going go back to the 2000s (if not earlier). That tells us something. We need more of a new vision than inventing technology to move forward. 

What happens when we separate the acts of writing, the acts of content curation and the website delivery?  We have an opportunity to radically scale down after decades of scaling up. In the process I think we can discover a more human and simpler approach to a sharing the human presence online.

## The Open Web, the Web or a web?

Many weeks ago I read a short post by Dave Winer talking about the terms Indie Web, Open Web and the Web. I haven&#39;t put my finger on where I read that but I think he was on to something. The Web didn&#39;t go away. It is still here. It&#39;s just the walled gardens have been successful in dominating marketing and news cycles. There is a perception, that many people assumed the destination **must be** a walled garden because of the [network effect](https://en.wikipedia.org/wiki/Network_effect). In 2025 I have come to the belief that &#34;network effect&#34; is over hyped, maybe just hype and marketing.

I can call anyone from my telephone regardless of what phone company they use. I&#39;m not limited by the big players like AT&#38;T or Verizon. That is true because the phone systems use common protocols to route calls and data. The Web is like that too. We need to start thinking in terms of the common protocols that work. We have allot of them. I would go a step further and look at simpler protocols and see how far we can push those. I don&#39;t think the answer is the social graph or Activity Pub pub. Those are marketing tools not communication tools. I don&#39;t be ATProto from BlueSky is the solution either. The solution that does and has worked for me is HTTP and RSS. It&#39;s worked for me since before RSS 1.0 was formalized. It hasn&#39;t stopped working since even though the marketing gravity has moved elsewhere. 

What has failed to fully materialize is content systems oriented to both inbound and outbound RSS. WordPress comes close and I see that as part of Dave Winer&#39;s excitement about WordPress as the API for writing for the web. But I don&#39;t think content systems necessarily need to be web based. I certainly prefer using my text editors for writing than typing into a text box of a website even if it does implement a WYSIWYG editor option.

A step in the right direction is to empower the people who create content. It is to empower the writers. The software should allow our writing to be easily integrated into a website. Where that website winds up being hosted shouldn&#39;t be determined by that software.Especially if it is a walled garden like Substack and Medium. 

### Scaling down

The scaling down of the web is an under explored in topic. I think it is worth exploring. I suspect it&#39;ll be my continued focus in 2026.

I want a simple piece of software the manages the content and renders the website. It let&#39;s me compose the site from pages, posts and feeds. I want it to be light weight and specifically to be able to run on my Raspberry Pi 500+. Preferably be possible to run on a Raspberry Pi Zero 2W if I attach a screen and keyboard. I&#39;m looking for a small core feature set. I think the system can be perfectly functional as a single user system. That of itself can simplify the whole process.

You might think a single user system can&#39;t be collaborative. Two existing pieces of tech solve that constraint. First is feeds. RSS 2.0 feeds let you move Markdown between content systems easily. That markdown can be repackaged and republished to on anther site. The second is distributed version control systems, Git being the current most common one. I work on my copy, you work on your and we merge the differences. That&#39;s collaboration too. We do this in software development all the time and it amazes me that this feature is rarely available to writers unless the using some web service to do so.

In terms of feeds I would choose to read RSS, Atom and JSON feeds as a minimum but produce just RSS 2.0 feeds. Why? Because a lesson we learned from the development of the Internet has been be liberal in what you accept but strict in what you produce. RSS 2.0 is extensible. It has a track record of successful extension. Just look at Podcasting, it&#39;s enabled because of enclosures supported pointing at non-text documents.  Similarly the practice of supporting Dave Winer&#39;s proposed source element means we can ship Markdown content right there in the RSS item. RSS can be used as a safe content transport platform for rendering on someone else website. All that is necessary is to exclude embedded HTML and use Markdown for the content. On the end render Markdown to get back standard HTML. It&#39;s just a matter of thinking differently how we use this venerable RSS format.

Here&#39;s an example of applying RSS in practice.  The walled garden news sites feature content written and hosted elsewhere. Google News and the not quite dead year Yahoo News are good examples of this. Microsoft Windows even takes this approach in its screen saver notification display. How do they get the news content into their aggregated news pages?  Why they read feeds then format the output as they need
Does this actually require a big company to pull off this feat? No. Does it require that you read the news in a web browser? No.

Googles doesn&#39;t hire writers to write the news stories they re-purpose the content provided by other people and organizations. The only thing that might be novel is that a filter is applied to show you think you want to click on. It&#39;s the same approach as search engine results. RSS feeds are even used as data sources of what to add to their search engine indexes. We can take advantage of that too. 

For several years I&#39;ve run a personal news [aggregation sites](/antenna). It followed close to one thousand different feeds. It takes a few minutes to harvest and a minute or so to rendering out as HTML pages. Then another minute to publish via GitHub pages. I run the harvest once a day and I can read at my leisure. I run a copy on local home network that I update once ever couple hours. I just don&#39;t bother with Yahoo and Google News anymore. I even include feeds from social networks like Mastodon, BlueSky  because they produce RSS feeds too. In theory I could follow something in Meta&#39;s threads this way if I wanted.

The computer I use to aggregate is a Raspberry Pi 3B+. The SD card is small, maybe 32 Gigabytes. Smaller than the one in my phone. I think it has about a Gig of RAM so it&#39;s not a big computer. It&#39;s all that I need as a web publishing platform. Scale small. I think that 3B+ costs less than $50.00 when I bought it and that included a case an power supply too. 

You don&#39;t need Google, Meta, Microsoft, Apple or Amazon. They are not necessary for the web or a web presence. At most what they sell is a convenience. I find myself increasingly questioning the cost of that convenience as time moves forward.

What I dream of is a feed oriented content management system I run on my inexpensive single board computer. One that I can chose to copy to the public Internet and Web or keep private on my network. I&#39;ve spent much of 2025 thinking through this problem havw been exploring how turn key I can make it.

## What I&#39;ve been up to

In my spare time I&#39;ve been reorganizing my personal websites and the tooling behind them. By August 2025 these efforts had resulted in a project I call [Antenna App](https://github.com/rsdoiel/antennaApp). The first step was to wrangle how I built my personal news site, [Antenna](https://rsdoiel.github.io/antenna),l. It is hosted via GitHub pages. I wanted to turn that into a turn key system. It was fortuitous that I started aggregation. It made the resulting content management system both simple and flexible. The initial versions of Antenna App provided for harvesting feeds and then publishing them as HTML pages in a static site directory. An approach similar to my calendar app all those years ago. Antenna App was inspired by the simplicity of Newsboat and it&#39;s use of SQLite3 databases. After working out ab aggregation approach I added support for simple blogging. This left me with a feed oriented core. I can aggregate, I can render posts and output the combined feeds as well. What was missing was support general case web pages so I added that.

By November 2025 I was generate most of my personal website, a club website and my aggregated news site using Antenna App. At this stage it was a command line program.

Command line programs are nice, they tended to be focused. They have a problem, you need to learn the command line syntax and that can put people off. I want people to realize that web publishing can be as easy and typing up a blog post. In December I started working on turning that command line tool into an interactive tool. Getting that completed will happen in 2026. I think it will get me closer to what I think is needed to get the Web out of its current walled garden rut.

## What I hope is next

The popularity of static site generators missed an opportunity. There are lots of systems that will take Markdown content and render a website with it. It&#39;s what caused the explosion of web publishing on GitHub before Microsoft purchased them. It continues on GitHub to this day. The problem is the Git is complicated and Microsoft is in the process of turning GitHub and VS Code into another walled Garden with AI gatekeepers. That sucks.

What I haven&#39;t seen is a turn key content management system that is designed to run on your own computer. Especially the really good single board computers like the Raspberry Pi.  Technically you can run a local copy of [WordPress](https://wordpress.com) on a Raspberry Pi but it is a hassle as well as a bit of a resource pig. I think we can do better than that.

I think there needs to be single user content systems that can read and write feeds. Reading feeds gives us the ability to aggregate. Writing feeds allows us to syndicate. I think inbound RSS is a reasonable means of replacing the centralized walled gardens we call social media.

[Andy Sylvester](https://andysylvester.com/2025/04/19/my-thoughts-on-inbound-rss/) wrote a thoughtful rebuttal to Dave Winer&#39;s request for inbound RSS support in content systems. I happen to disagree with Andy but he made a good point. Writers haven&#39;t insisted on in bound RSS support. I think that is in large part because they don&#39;t insist on good content management systems either. Most writers I&#39;ve met deal with content systems as a price of publishing their writing. What if writers didn&#39;t have to pay that price anymore? Would they write more?  

To appreciate in bound RSS I think you need to have a feed oriented content management system to start with. You need to acknowledge writers read as much or more than they write. I also think you need to keep you content management system separate from your text editor (otherwise just wrangle EMACS). Text editors are like typewriters in the mid 20th century. Writers had their preferences.

I&#39;ve used many editors over years on many different operating systems. The ones I reach for today tend to be [Gnome TextEdit](https://apps.gnome.org/TextEditor/) and [aretext](https://aretext.org). Ask the next writer you run into I would guess those two editors will not be their first choices!

That&#39;s one of the reasons I don&#39;t run WordPress on my Raspberry Pi and simply mirror the results as a static site. I don&#39;t like the textarea editors for doing any writing of length. But that doesn&#39;t means I wouldn&#39;t want to use a local content management system. I want a content management system to manage my writing. That system should take  care of seamlessly generating HTML, RSS and other files for a functional the website. I want a content management system that doesn&#39;t require me to learn yet another template system or language.

Inbound RSS allows me to integrate Andy&#39;s and Dave&#39;s content in a novel way. If Andy were to subscribe to my feeds he could reply to me and I&#39;d pick it up from his feed. I wouldn&#39;t have a rely on social media platforms like BlueSky to be aware of his thoughtful comments. Similarly Dave could reply to me on his platform and I&#39;d still pick it up. That is possible when we realize that the scale of one person, me, also can aggregate and publish web content in a simple fashion on a tiny computer like a Raspberry Pi!

I don&#39;t think I have the ultimate answer to kick starting the next web era. I think it is to coming. I think it arrives by doing lots of different software experiments by a diverse group of people with diverse backgrounds. What I can do to help is write what I would like to see as a writer.

Here&#39;s my wish list (much of it is getting built into Antenna App as my time permits).

1. I want to write with the editors of my current choice or the ones I pick in the future
2. I want to use the markup I think is appropriate, that should automatically be converted to HTML on site generation. I like 
  a. Markdown
  b. CommonMark
  c. Open Screen Play format (nice for dialogues and transcripts)
3. I shouldn&#39;t have to worry about document metadata when I am writing, much of it should be automatically generated on import. I should be able to add metadata as I choose
4. I want to support posts, pages and content blocks.
5. The extra bits that make a website really useful should be automatically generated
  a. sitemaps
  b. RSS feeds
  c. OPML lists
  d. Open Search documents
6. The system should be single user and run on a small computer like a Raspberry Pi without bogging down
7. The resulting website should be trivial to copy to a public service, e.g. via Git, scp or dragging and dropping files


These are features I&#39;ve desired for a long time. They have been pretty consistent since at least 2015 for me.  I&#39;ve written numerous systems that have some of these characteristics. In 2025 I&#39;ve started to pull them together in Antenna App. I&#39;d like to see other software developers do something similar. I am very certain there are others with better visions, more refined design skills than me. I am happy to share what I am writing and welcome others to use it but I also think other eyes and other takes on the feature set benefit everyone.

My current setup for producing this website is as follows.

1. [aretext](https://aretext.org) and [Gnome TextEdit](https://apps.gnome.org/TextEditor/) are my current preferred editors
2. [Antenna App](https://rsdoiel.github.io/antennaApp) is my preferred feed oriented content management system
3. I publish via GitHub pages

Where I would like to be at the end of 2026? I&#39;d like to move away from GitHub and once again host my own site on a system I run. Currently looking at options from [Mythic Beasts](https://www.mythic-beasts.com/).

I run the draft version of my websites on my local home network. This gives me a space where I can test and review things if needed. I can also update them more frequently then I currently am comfortable with GitHub.

A cool idea I would like to complete in 2026 is to make Antenna App really useful for people aside from myself. I also want to include a recipe where you can create a Raspberry Pi Zero 2W web writer appliance that will let you carry your blog publish systems in your pocket or key chain. Doing that could let us comment in the real world, on the go, without resorting to the cloud then webwrite. We can defer publishing to the next time we have access to the public Internet.

I have hope for 2026. In spite of the challenges and hardships of 2025, I think we can build the future for the rest of us, and for all of us if we choose.</source:markdown>
    </item>    <item>
      <title>Reflection on a few decades</title>
      <link>https://rsdoiel.github.io/blog/2026/01/01/reflections_2025.html</link>
      <description>
        <![CDATA[A long post reflecting on things I've been thinking about in 2025 as we start
        2026.]]>
      </description>
      <source:markdown>---
author: R. S. Doiel
dateCreated: &#34;2025-12-31&#34;
dateModified: &#34;2026-01-01&#34;
datePublished: &#34;2026-01-01&#34;
description: |
    A long post reflecting on things I&#39;ve been thinking about in 2025 as we start
    2026.
keywords:
    - world wide web
postPath: blog/2026/01/01/reflections_2025.md
title: Reflection on a few decades

---


# Reflection on a few decades

By R. S. Doiel, 2025-12-31

I have been thinking about my career recently. It extends into the prior century. I have also been thinking allot about the Internet and the evolution of the Web. I see a pretty consistant thread in my career. I write software that transforms content and information from one system into another. Sometimes it&#39;s structured data and sometimes it&#39;s unstructured text. The thread became clearer with the arrival of the World Wide Web in the early 1990s. While the systems used over that time have grown complex I think they can be made far simpler. __I think they can be better implemented for writers. They can be writer centric.__ Finding a path to something simple has preoccupied my nonworking hours for much of 2025.

## A lesson from my past

An early set of programs I wrote for the Web were written in a language called Perl. My University had a &#34;mail form handler&#34; service. That service would take the a web form submission and sent the results to your email account. I wrote a simple mail filter that would extract email&#39;s web form content and append the results to a CSV file. I had another Perl script that would read in the CSV file and generate a set of HTML calendar pages in the directory that the used for a personal website. It worked pretty good. It was so trivial I didn&#39;t keep a copy of it or port it to a new language when I stopped writing software in Perl all those years ago.

This little calendar project lead to career opportunities. I didn&#39;t find that out to much later. A colleague mentioned I was considered a hacker (in the good sense of the word) by some of the system admins because I had done that. It was just one of many experiments I&#39;ve done over my years working with computers and the Web.

That personal project taught me several things. I&#39;ve kept them in mind over the years.

1. Composible systems are powerful, Unix combined with the Web is composible
2. Structured data aids in composibility (EMail and CSV are only two examples)
3. Systems get used in unexpected ways

No 3. has proved important. The web can be viewed from both the audience of one and the audience of the general public. You can also split the authoring system from the presentation system. You can divvy those up between machines. It all depends on the project scope, rates of data change all combined with the constraints of scale.

When I first started maintaining and building web publishing system I almost always leveraged a database management system too.  This was particularly true when [LAMP](https://en.wikipedia.org/wiki/LAMP_(software_bundle)) emerged as the common platform. Today that isn&#39;t the case. I avoid relying on database management systems unless there is a clear requirement for them. Most of the time there isn&#39;t.

Databases were convenient because they make working with structured data easy. Database records, even NoSQL database records have a structure. The unstructured document can be worked into structure using indirection. A path can point to the unstructured thing. Links are a form of path. A link provides a structured reference to the unstructured object. You often can simply encapsulate the unstructured content as an element in the structured record. This happens in RSS feeds. Structure allowed us to use a databases to enforce  record locking behaviors. That was needed to avoid write collisions. Originally databases made it easier to write web applications. That isn&#39;t necessarily true today.

Eventually the bespoke web publishing systems became content management systems. Bespoke gave way to more standardized software like [Movable Type](https://en.wikipedia.org/wiki/Movable_Type), [Zope](https://en.wikipedia.org/wiki/Zope), [WordPress](https://en.wikipedia.org/wiki/WordPress) and [Drupal](https://en.wikipedia.org/wiki/Drupal). Those arrived in what I think of as the second Internet age, the age of commercialization.

## The Commercialization Era

When the Internet and Web went from an academic and research platform to a commercial platform many things changed. The Internet and Web came to be consumed by monetization, business models and greed. Those enabled the surveillance economy we have today.

With these new drivers new demands came to the forefront. One big side effect was the Internet grew very large very quickly. By the time the first [iPhone](https://en.wikipedia.org/wiki/IPhone) was announced the Web was already experiencing growing pains. One of the big issues was spikes in Web traffic.

In the early days we assumed a few dozen concurrent connections. By 2000 you needed to handle much more. Buy 2005 you worried about tens of thousands of concurrent connections. The problem was even large enough to be named. It was the &#34;10K problem&#34;. It was talked about by web people in the same way the old aerospace engineers used to talk about the sound barrier.

Money was on the line so people tried various solutions. Some were hardware approaches, some content approaches others were changing how we implemented web services generally. Eventually the general purpose web services like NginX and Apache2 easily handled 10K concurrent requests. The next worry point was the worry was 100K, 1000K, etc. That came to be called &#34;google scale&#34;. Probably should have been called global scale. Global scale is one of the reasons that distributed systems came to dominate the commercial web. That is interms of practical implementation, the use of the web though was using those distributed services and monolithic systems. Monoliths are eaiser to monetize. Monoliths and scaling up are only one possible direction to push.

What hasn&#39;t been acknowledged since the 10K problem era is that we&#39;re may be approaching __peak human use__. Think about it. The web has become global since 2000. Most people on the planet experience the web today either in embedded systems (appliances) or their phones. Cell phone saturation has been a reality for manufacturers for a decade in many countries. I suspect the continued rapid growth in traffic isn&#39;t direct human interactions anymore. It&#39;s software systems.

From my own work experiences the growth in traffic to our websites (I work in at a research library) has been bots. It is not human users. Not by a long shot. Bots are software systems. Early on bots tended to be useful. They were dominated by web crawlers. Web crawlers either mirrored sites (e.g. for preservation) or gathered what was needed to create search indexes. While they could be unruley norms and expectations developed making it possible to respond to the bots without delaying responces to real humans. The bots were largely useful. Since 2024 the problem has been AI bot swarms. These bots were initially used to harvest web content to train large language models. In practice they were a use to deny service engines. The continued goal appears to be deny access to website to competing bots and humans. The big monoliths have to protect investments in capturing attention after all.

Scaling up has lead to more bots. Maybe we should be thinking about scaling down instead of up. Maybe we reach the global by focusing on the local.

### Observations on website construction

Since at least 2005 there has been a gathering renaissance of static websites. There has been many drivers. Static sites often are easier to scale for high volume traffic. They are easier to secure against attacks and fix if defaced. Often they are much cheaper to run and maintain.

Several innovations helped this happen.

1. Template systems are easy to build, I&#39;ve seen lots of these come and go
2. Light weight markup adoption, like Markdown, provides a source for generating HTML that is easy for writers
3. Cheap to rent static storage systems became commercially available (S3)
4. The web browser went from a green screen surrogate to a full on rich client. Web browsers even work on phones now

### Observations on computer systems

In 2025 commercially available computer systems still use a Von Neuman computer architecture, a CPU, Memory (RAM) and storage. That model also assumes inputs and outputs that are distinct (people forget this). The scaling problem observed in the commercial era often presumes the input (writes) and outputs (reads) are used at the same frequency. In practice the rates of reads and writes diverage. In data consumed, read, more often than it is written. That&#39;s true for us humans or as well as for software. It is why it is important when designing web systems to consider the rates of data changes just as we make choices constrainted around scale. It is really important to challenge our assumptions another rates of data changes and if that is constant or not.

My experience is that rates of change also change. That is true in both in the software implementation and in data. It is important in guiding how we approach the lessons of scaling. Curation requires more dynamic responses while stability for consumption is simply a reflection of current state problem. Over time the last current state tends to stablize for many communication use cases.

The needs of the monetized Web have delivered an out sized concern with scaling up. This in part has been a result of over reliance on dynamic content web systems. This is particularly true if the goal is to create and encourage the monoliths like the walled gardens most people use on the Web today. You can scale writing separately form reading. In fact that leads to more options in scaling. Unfortunately most developers and many engineers fail to see that distinction. We keep building complex monolithic systems even when we compose the monoliths from distributed systems. 

Scaling up appears to be the goal, a single direction of scaling. I believe this is a false lesson. You can scale up as well as down. Scaling down can be an advantage.

## Are we entering a new web era yet?

Long before &#34;Web 2.0&#34; there was already a cyclic hype about the next web or the web&#39;s replacement.  Most of the time this is driven by some sort of technological change, buzz or approach. It tended towards peak hype as it was monitized. There is a good share of doom and gloom along side Utopian thinking in these hype cycles. The hype cycle doesn&#39;t actually match the actual changes most of the time. It isn&#39;t even when the impacts are always felt. At some point we will arrive we do enter a new era. The current status quo is very problematic, how do we get to the new one?

### My bet, Simplification 

Why simplification? Why is this both desirable and possible? How does that lead to a new era of the Internet and Web? The Internet and Web were built as distributed systems. Those specific technologies are largely intact. Commercial interests may not align with distributed systems but globally a peer to peer structure scales. So that is how we&#39;ve implemented the global Web. On the other hand a lesson from the late 20th century Web is that unregulated markets, unregulated services, tend to collapse towards monopolies. Sometimes they just collapse and cease to exist. Centralization as some advantages if the goal is wealth accomulation. Yet centralization makes them brittle and precarious. In 1990 there were not regular global head lines of Internet outages. In the 2020s this has become common. Eventually decentralization embraced again as our Web habits evolve. Given the underpinning technology of the Internet and Web remains distributable I don&#39;t think reinvention is a requirement to usher in the next era. The easier path already exists.

The next web isn&#39;t likely to be based on block chain, bitcoin or large language models either. Why? None of these really enhance communication. The Internet and the Web are communication systems. Computers unlike their calculator predecessor are communication devices. That telephone in your pocket is a computer after all. Making it easier to communicate is likely to be the source of the next web evolution. How do we make it easier to communicate? We make it simpler.

Market consolidation does not lead to innovation no matter how many startups are bought by conglomerates. I think people are where the next web happens. It&#39;s happens at a human scale of a few people at a time. If there is enough critical mass of people doing the &#34;new thing&#34; then that&#39;s when the new web will arrive. It&#39;ll have roots in the old web, may even be technologically indistinguishable but it&#39;ll be different because people choose to use it differently and think about it differently.

I think the next web needs to be more personal, more authentic, less about algorithms. It needs to be more about human choices. General automation by autonous agents is not necessarily needed. I think the new Web happens as a a countervailing force to the current onslaught of centralization and control. It think the new Web will happen in spite of centralization and control. It will not require W3C blessing because it doesn&#39;t require new specifications and aggreements. It requires new individual human interactions.

There are elements I see as being factors in reasserting decentralization. There has been a computer revolution happening that has largely gone unnamed and unnoticed by mainstream media and society. That revolution is in inexpensive single board computers. Some of it has been dismissed as fad in the form of the DIY maker movement. Before that it was buried under the commercial hype of the Internet of Things. Single board computers are real. I&#39;ve been using them for years. I&#39;m typing this post on one now. They are web friendly.

### then and now

My first computer cost me about \$1800.00 US dollars. My first computer was a clone 286 machine. I ran DOS and Minix on it. I had to go out buy the parts and assemble it myself. This was back in the late 1980s. A mini computer that could host a website in the early 1990s range between \$50,000.00 to \$250,000.00 US. The cost of a house at that time. Prices came down and eventually personal computers could run websites directly. The price of a web server settled down to about I paid for my  first personal computer.

Fast forward to now. For \$1800.00 US dollars I can easily assemble four Raspberry Pi 500+ workstations, a 16 port network switch and Ethernet cables to connect them. I can grow that network with web servers running about \$150 to \$200.00 US per server depending on the storage I buy. A single Raspberry Pi 5 with solid state disk storage is enough to host a website with concurrent users in the thousands. What that 1990&#39;s era mini computer did for \$50,000.00 US can be done for a \$200.00 web appliance today. That&#39;s an important change.

Here&#39;s how I came up with my \$1800.00 budget. A sixteen port Ethernet switch from Netgear runs \$100.00 US when I checked Best Buy today. A Raspberry Pi 500+ runs at \$200.00 US plus tax and shipping. A Pi Monitor runs \$100.00 US. Add in cables and such you&#39;ll looking at about \$350.00 per work station. That&#39;s how I know I can build a four workstation network with room to expand for a \$1800.00 budget. I don&#39;t even need to assemble the computers I just need to connect them up and power them on. Each machine can be either a web host, workstation or both. My choice.  Expanding that system is also low coast. You can build useful web servers for less than the price of the Raspberry 500+ and monitor.

My little four workstation network is enough to hardware to support a neighborhood paper. Adding a few more machines and you can provide hardware for a small town paper. You just need the physical space to house them and of course the writers and graphics people to use them.

Unlike the small town newspapers when I grew up there are no printing presses to purchase, rent or maintain. There&#39;s just several computers. The computers I listed last decades. This suggests something to me. Maybe the hardware we have is good enough for the next web. Maybe hardware isn&#39;t the issue it once was.

The cost factor in the next generation of the web doesn&#39;t require huge expenditures of capital. It doesn&#39;t even require big data centers in spite of what the Big Companies will swear are required. 

It suggests to me that we can scale down. We scale globally by going small. Heck, your &#34;smart&#34; appliance is likely has an embedded web server and it&#39;s doesn&#39;t pack a huge ammound of memory or a fast clock cycle. Appliance manufactures don&#39;t put in any excess compute capability into appliances. That cuts into profits. The Internet of things, the &#34;smart&#34; appliaces, have been enabled by the cheap single board computers. In the meantime the foundations of the Internet and Web remain intact and those little machines fit it just fine.

The trick is to making the web really distributed is to allow it be both affordable and easily editable. I am skeptical of centralization as the means of achieving that. I think we get there by embracing the little single board computers like the Raspberry Pi. We embrace individuals owning the hardware, software and the content they produce. We embrace individual control so that collectively we have a share medium. I think the model where we rent a connection, we rent our presense is a broken one. What happens if we don&#39;t need to pay a toll to use the Internet? What happens when it is as convient as the public sidewalk?

The small set of network computers I described run the same protocols as the Internet and Web. If I connect them up to the switch I have a local private version of the Internet and Web. Let&#39;s call that an internet (common noun) and a web (common noun). What would happens when I connect my web to my neighbors network? What happens when they connect theirs to another neighbor? When does that arrive at the proper nouns, Internet and Web? Maybe the next web is out there in the small networks interconnected.

### Moving beyond commercial Internet Service Providers

When the Internet was commercialized it was done so using the justification that Government couldn&#39;t afford giving the public access to the tax payer funded Government created resource. The claim was only private business could do that. Not surprising considering the political wisdom of the 1980s and 1990s. That wisdom suggested all problems as business problems and markets solve business problems. Many dubious assumptions by a whole bunch of smart people.

In practice that approach has been a failure. Try to use a cell phone in rural America today, chances are it&#39;ll fail. The Internet for most Americans is experienced through their cells phones. Heck, in many urban areas cell service remains really problematic. It is common to get poor cell service through out most of Los Angeles county. Some of that is geography (Mountains) but much is also the lack of investment by business as territory was divided up among the decreasing number of Internet Service Providers.

In response to market failures and the continue push to put everything online and people have started to pull together to find ways to cope. An interesting approach has been community owned nonprofit internet service providers. If you&#39;re a town with less than 1000 people you&#39;re unlikely to have many business offering service at affordable rates. The best you might due is satellite connections like Starlink. Satellite access is slow because of transmission distance. A fiber connection is so much faster. This is a mater of physics, radio signals travel slower than the speed of light. For fixed locations like farms, homes and small businesses fiber gives you more reliable connections.

What has started to happen in some rural areas is communities lay their own fiber between houses or network cell towers. Then they connect these to a location that also has an Internet connection with significant bandwidth. If a railroad runs through their town then that might be used as a connection point (Rail systems have used fiber optics for switch control since the last century, they sometimes sell their excess capacity). Similarly a large antenna array can cary more data to and from the satellite then a home system like Starlink can. By pooling the communications together this becomes an affordable option. A public nonprofit service can do this.

Rural community cooperatives can get by with minimal staffing and little in the way of physical space. They function like other small business in terms of budget but since they are nonprofit they don&#39;t need to enhance the cost charged to members nor do they need to pay a competitor off to avoid competition.

A policy of encouraging lots of little cooperatives is technically doable. It is how the Internet was designed. Like public streets and highways communities can choose to provide board band access that connect to the Internet. Organizations like [Institute for Local Self-Reliance](https://ilsr.org/) show examples of this approach. 

### What else is needed?

The act of producing a website needs to be simplified too. Ideally it&#39;d be a turn key system I can run on my Raspberry Pi sized computers. That turns out to be incredibly easy when you think of the content system as being single user.

I&#39;ve notice about a decade and a half ago people seemed to rediscover &#34;static&#34; web. This appeared to be driven by a reaction to the walled gardens, to privacy costs and the desire for local control. The static web is a bit of a misnomer. It refers to how the content is hosted not how the content behaves. A static website in 2025 can be highly interactive. 

During the first dot com era (approx. 1995 through 2000) &#34;dynamic&#34; websites rose in popularity. While the web browser was becoming a rich client allot dynamism happened server side. By 2004 and 2005 the rich features provided by web browser were being leverage in &#34;web properties&#34; like GMail and Google Maps. At that point the static approach become a misnomer. The web browser can run whole applications.  You can approach site generation like just like you did in the original web and still have a dynamic experience. By 2010 this was well understood. By 2020 it was taken for granted. I don&#39;t think the public realizes that the web they experience on their phones is produced in some part by static websites. All those &#34;apps&#34; that run on your phone that use webview are simply websites and services. Many statically hosted.

When JavaScript and CSS became widely supported features in web browsers they allowed the client, the web browser, to be independently dynamic. In terms of how we call things it made it much harder to explain that a &#34;static&#34; website doesn&#39;t imply it can&#39;t be interactive. In 2000 providing search for a web site required a hosted service. Today you can run the search service inside the browser itself. Indexes are updated when you generate the static website. The indexes are partitioned and retrieged by the browser as needed. Unless the website is Wikipedia size you can just provide it browser side. The term static site remains but it&#39;s not limited to the approaches used by the retro or nostalgic web.

The way the walled gardens are built and scaled ultimately comes out of the tried and true lessons going go back to the 2000s (if not earlier). That tells us something. We need more of a new vision than inventing technology to move forward. 

What happens when we separate the acts of writing, the acts of content curation and the website delivery?  We have an opportunity to radically scale down after decades of scaling up. In the process I think we can discover a more human and simpler approach to a sharing the human presence online.

## The Open Web, the Web or a web?

Many weeks ago I read a short post by Dave Winer talking about the terms Indie Web, Open Web and the Web. I wish I saved the link. I think he was on to something. The Web has not gone away. It is still here. The walled gardens have been very successful in dominating marketing and news cycles. Many people assumed the destination **must be** a walled garden because of the [network effect](https://en.wikipedia.org/wiki/Network_effect). Maybe that is only a general perception and not a fact. In 2025 I have come to the believe that &#34;network effect&#34; is over hyped, maybe just effective marketing.

I can call anyone from my telephone regardless of what phone company they use. I&#39;m not limited by the big players like AT&#38;T or Verizon. That is true because the phone systems use common protocols to route calls and data. The Web is like that too. We need to start thinking in terms of the common protocols that work. We have allot of them. I would go a step further and look at simpler protocols and see how far we can push those. I don&#39;t think the answer is the social graph or Activity Pub pub. Those are marketing tools not communication tools. I don&#39;t believe it is ATProto from BlueSky is the solution either. The solution that does and has worked for me is HTTP and RSS. It&#39;s worked for me since RSS was formalized. It hasn&#39;t stopped working since even though the marketing gravity has moved elsewhere. 

What has failed to fully materialize is content systems oriented to both inbound and outbound RSS. WordPress comes close and I see that as part of Dave Winer&#39;s excitement about WordPress as the API for writing for the web. But I don&#39;t think content systems necessarily need to be web based. I certainly prefer using my text editors for writing than typing into a text box of a website even if it does implement a WYSIWYG editor option.

A step in the right direction is to empower the people who create content. It is to empower the writers. The software should allow our writing to be easily integrated into a website. Where that website winds up being hosted shouldn&#39;t be determined by that software.Especially if it is a walled garden like Substack and Medium. 

### Scaling down

The scaling down of the web is an under explored in topic. I think it is worth exploring. I suspect it&#39;ll be my continued focus in 2026.

I want a simple piece of software the manages the content and renders the website. It should allow me to easily compose the site from pages, posts, text blocks and feeds. I want it to be light weight and specifically I need it to work well on my Raspberry Pi 500+. Preferably it should work equally well from a Raspberry Pi Zero 2W. I&#39;m looking for a small core feature set that is easily explained, easy to think about and gets the job done without allot of fuss. I think the system can be perfectly functional as a single user system. Focusing on a single user system allows us to simplify the whole process.

Communication is a social act, doesn&#39;t that mean we need multi-user collaberative system?  A single user system can be collaborative. Two pieces of tech enable the single user system to be used collaberatively. The first that comes to mind are RSS 2.0 feeds. These let you move Markdown between content systems that support inbound RSS easily. That transported Markdown can be repackaged and republished on another site. The second is distributed version control systems. Git happens to be the current popular one. If I work on my copy, you work on yours we collaberate when merge the differences between our to copies. We do this in software development all the time. It amazes me that this feature is rarely available to writers outside the web.

In terms of inbound feeds I would choose to read RSS, Atom and JSON feeds as a minimum but produce only RSS 2.0 feeds. Why? Because a lesson learned from the development of the Internet was adoption is enhanced by being liberal in what you accept for inputs but strict in what you produce as an output. RSS 2.0 is extensible. It has a track record of successful extension. Just look at Podcasting. Podcasting was possible because RSS 2.0 had support for enclosures and the point at the non-XML, non-text content. That&#39;s can be an audio file, video, images, you name it.  Similarly the practice of supporting Dave Winer&#39;s source element means we can ship Markdown content right there in the RSS item. I don&#39;t have to send it as an enclosure. The RSS can be used as a safely transport Markdown content. Markdown, stripped of any embedded HTML, can be safely rendering by anysome else. On the receiver end render Markdown to get back standard HTML. It&#39;s just a matter of thinking differently how we use this venerable RSS format.

Here&#39;s an example of applying RSS in practice.  The walled garden news sites feature content written and hosted elsewhere. Google News and the not quite dead yet Yahoo News are good examples of this. Microsoft Windows even takes this approach in its screen saver notification display. How do they get the news content into their aggregated news pages?  Why they read feeds, RSS, Atom and JSON feeds. The take the feeds, normalize them and then output HTML as needed. Does this approach require a big company to this pull off? No. Does it require that you read the news in a web browser? No. For years I read news in my terminal using [Newsboat](https://newsboat.org). It&#39;s not a web browser.

Googles doesn&#39;t hire writers to write the news stories they re-purpose the content provided by other people and organizations. The only thing that might be novel is that a filter the aggregate of feeds such they you are highly likely to click a link in the feed results they display. It&#39;s the same approach as search engine results in terms of monitization. We can take advantage of that too. 

For several years I&#39;ve run a personal news aggregation site, [Antenna](/antenna). Currently that site is generated from close to one thousand seperate feeds. It takes a few minutes to harvest, a minute or so to rendering as HTML and another minute to publish to the website via GitHub pages. I run the harvest once a day and I can read at my leisure. I also run a copy on local home network. My local network version updates every hour or so. I can remember the last time I bothered with Google, Yahoo or Window&#39;s news. When many friends left Twitter I started following them on Mastodon because Mastodon produces RSS feeds. When people showed up at BlueSky I followed them too because BlueSky supports RSS as well. In theory I could follow something in Meta&#39;s threads this way if I wanted, I just don&#39;t know anyone there.

The computer I use to aggregate feeds as well as stage my website is a Raspberry Pi 3B+. It runs headless (without a monitor and keyboard attached). The storage is an SD card is small. With case it is the size of a bar of soap. I think it has about a Gig of RAM, a few CPU cores.  It&#39;s not a big computer. It&#39;s all that I need as a web publishing platform. Scale small. I think that 3B+ costs less than $50.00 when I bought it and that included a case and power supply too. 

You don&#39;t need Google, Meta, Microsoft, Apple or Amazon. They are not necessary for the web or a web presence. At most what they sell is a convenience. I find myself increasingly questioning the cost of that convenience. Running scp on a schedule is probably easier than dealing with Git, branches and versions.

What I dream of is a feed oriented content management system I run on my inexpensive single board computer. One that I can chose to copy to the public Internet and Web or keep private on my network. I&#39;ve spent much of 2025 thinking through this problem have been exploring how to make it a turn key system I can share with others.

## What I&#39;ve been up to

In my spare time I&#39;ve been reorganizing my personal websites and the tooling behind them. By August 2025 these efforts had resulted in a project I call [Antenna App](https://github.com/rsdoiel/antennaApp). The first step was to wrangle how I built my personal news site, [Antenna](https://rsdoiel.github.io/antenna). Currently it is hosted via GitHub pages. It was fortuitous that I started aggregation. It made the resulting content management system both simple and flexible. The initial versions of Antenna App provided for harvesting feeds and then publishing them as HTML pages in a static site directory. An approach similar to my calendar app all those years ago. Antenna App was inspired by the simplicity of Newsboat and it&#39;s use of SQLite3 databases for content collections. After working out my aggregation approach I added support for simple blogging. Posts become items in an RSS feed, they just require an extra step to render as individual HTML pages. This left me with a feed oriented core. I can aggregate, I can render posts and output the combined feeds as well as well a generate lists of feeds shared as [OPML](https://en.wikipedia.org/wiki/OPML) files. What was missing was support general case web pages so I added that this past Fall. By November 2025 I was generating my personal website, a club website and my aggregated news site using Antenna App.

Command line programs are nice, they tended to be focused. There is a problem encountered when sharing them. You need to learn the command line syntax and that can put people off. I want people to realize that web publishing can be as easy and typing up a blog post. In December I started working on transforming the Antenna App command line tool into an interactive tool. Getting that completed will happen in 2026. I think it will get me closer to what I think is needed to get the Web out of its current walled garden rut.


## What I hope is next

The rising popularity of static site generators has given rise to Markdown becoming widely known and well documented. It has also come with some missed opportunities. There are lots of systems that will take Markdown content and render a website with it. It&#39;s triggered an exposion of web publishing on GitHub before Microsoft purchased them. It continues as a common use case on GitHub to this day. The problem is the Git is complicated and Microsoft is in the process of turning GitHub and VS Code into another walled Garden with AI gatekeepers. That sucks. I think we can move beyond the GitHub use case. I think its time to move beyond complex rendering setups and scripts.

What I think is missing are simple, turn key, feed oriented content management systems that are is designed to run on your own computer. I think that could be a &#34;killer app&#34;, especially if they run on low coast single board computers like the Raspberry Pi. Technically I can run a local copy of [WordPress](https://wordpress.com) on a Raspberry Pi. In practice WordPress is a hassle as well as a resource hog. Why run a multi-user system if it is only going to be used by single person? Running WordPress means running and maintain PHP, a web server and MySQL database. That isn&#39;t really necessary for managing content at the single user level. I think we can do better than that. I think we can create something far simpler than that.

I think there needs to be single user content systems that can read and write feeds. Reading feeds gives us the ability to aggregate. Writing feeds allows us to syndicate. I think inbound RSS is a reasonable means to recreate some of the useful features offerred by the centralized walled gardens we call social media. If I can take an item from an inbound feed, repost it or write a post quoting it, I&#39;ve got the seed needed to be social. That&#39;s the missing bit of the RSS ecosystem. I don&#39;t require a Twitter or Facebook systems at all. I don&#39;t require Substack or Medium. I just need to host my content, publish to public web and be clever about getting the word out. I think the concept of an inbound RSS feed is useful and worth exploring. Not everyone aggrees though.

[Andy Sylvester](https://andysylvester.com/2025/04/19/my-thoughts-on-inbound-rss/) wrote a thoughtful rebuttal to Dave Winer&#39;s request for inbound RSS support in content systems like BlueSky, Threads and Mastodon. I happen to disagree with Andy but he made a very good point. Writers haven&#39;t insisted on inbound RSS support. It is not on their radar at all. I think that due to the fact that writers haven&#39;t insisited on decent content management systems either. They&#39;ve lived with cut and paste for so long they just assume that&#39;s the way it has to be. Most writers I&#39;ve met deal with content systems as a price of publishing their writing. These systems function at the minimum &#34;good enough&#34; standard. What if writers didn&#39;t have to pay that price anymore? Would they write more?  

To appreciate inbound RSS I people need to see a system that supports inbound RSS working. I think you need to have a feed oriented content management system to start with. Why create such a system? Acknowledge that writers are also readers. Most rights I&#39;ve met read as much or more than they write. Feeds are a way of pulling your current reading into one place. A feed oriented content management system provides for both reading as well as a mechanism to publishing your writing on a website.

Unlike the Web base content systems I there are advantages in keeping your content management system separate from your text editor. If you like them combined I recommended EMACS and it&#39;s wiki software. Text editors are like typewriters in the mid 20th century. Writers have their preferences. What the content management system&#39;s role is taking the manuscript and turning it into a publication. On that web that might be a post one sentence long without a title or it might be a whole novel or screenplay. Either the role of the content system is to transform the simple text files into HTML web pages.

I&#39;ve used many editors over years on many different operating systems. The ones I reach for today tend to be [Gnome TextEdit](https://apps.gnome.org/TextEditor/) and [aretext](https://aretext.org). Ask the next writer you run into I would guess those two editors will not be their first choices!

That&#39;s one of the reasons I don&#39;t run WordPress on my Raspberry Pi and simply mirror the results as a static site. I don&#39;t like the textarea editors for doing any writing of length. But that doesn&#39;t means I wouldn&#39;t want to use a local content management system. I want a content management system to manage my writing. That system should take  care of seamlessly generating HTML, RSS and other files for a functional the website. I want a content management system that doesn&#39;t require me to learn yet another template system or language.

Inbound RSS allows me to integrate Andy&#39;s and Dave&#39;s content in a novel way. If Andy were to subscribe to my feeds he could reply to me and I&#39;d pick it up from his feed. I wouldn&#39;t have a rely on social media platforms like BlueSky to be aware of his thoughtful comments. Similarly Dave could reply to me on his platform and I&#39;d still pick it up. That is possible when we realize that the scale of one person, me, also can aggregate and publish web content in a simple fashion on a tiny computer like a Raspberry Pi!

I don&#39;t think I have the ultimate answer to kick starting the next web era. I think it is to coming. I think it arrives by doing lots of different software experiments by a diverse group of people with diverse backgrounds. What I can do to help is write what I would like to see as a writer.

Here&#39;s my wish list (much of it is getting built into Antenna App as my time permits).

1. I want to write with the editors of my current choice or the ones I pick in the future
2. I want to use the markup I think is appropriate, that should automatically be converted to HTML on site generation. I like 
  a. Markdown
  b. CommonMark
  c. Open Screen Play format (nice for dialogues and transcripts)
3. I shouldn&#39;t have to worry about document metadata when I am writing, much of it should be automatically generated on import. I should be able to add metadata as I choose
4. I want to support posts, pages and content blocks.
5. The extra bits that make a website really useful should be automatically generated
  a. sitemaps
  b. RSS feeds
  c. OPML lists
  d. Open Search documents
6. The system should be single user and run on a small computer like a Raspberry Pi without bogging down
7. The resulting website should be trivial to copy to a public service, e.g. via Git, scp or dragging and dropping files


These are features I&#39;ve desired for a long time. They have been pretty consistent since at least 2015 for me.  I&#39;ve written numerous systems that have some of these characteristics. In 2025 I&#39;ve started to pull them together in Antenna App. I&#39;d like to see other software developers do something similar. I am very certain there are others with better visions, more refined design skills than me. I am happy to share what I am writing and welcome others to use it but I also think other eyes and other takes on the feature set benefit everyone.

My current setup for producing this website is as follows.

1. [aretext](https://aretext.org) and [Gnome TextEdit](https://apps.gnome.org/TextEditor/) are my current preferred editors
2. [Antenna App](https://rsdoiel.github.io/antennaApp) is my preferred feed oriented content management system
3. I publish via GitHub pages

Where I would like to be at the end of 2026? I&#39;d like to move away from GitHub and once again host my own site on a system I run. Currently looking at options from [Mythic Beasts](https://www.mythic-beasts.com/).

I run the draft version of my websites on my local home network. This gives me a space where I can test and review things if needed. I can also update them more frequently then I currently am comfortable with GitHub.

A cool idea I would like to complete in 2026 is to make Antenna App really useful for people aside from myself. I also want to include a recipe where you can create a Raspberry Pi Zero 2W web writer appliance that will let you carry your blog publish systems in your pocket or key chain. Doing that could let us comment in the real world, on the go, without resorting to the cloud then webwrite. We can defer publishing to the next time we have access to the public Internet.

I have hope for 2026. In spite of the challenges and hardships of 2025, I think we can build the future for the rest of us, and for all of us if we choose.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2026/01/01/reflections_2025.md" length="50056" type="text/markdown" />
    </item>    <item>
      <title>Python Setup on Raspberry Pi OS 6</title>
      <link>https://rsdoiel.github.io/blog/2025/12/19/python_setup_pi_os_6.html</link>
      <description>
        <![CDATA[A set of quick notes for wrangling Python to work nicely on Raspberry Pi OS 6]]>
      </description>
      <source:markdown># Python Setup on Raspberry Pi OS 6

By R. S. Doiel, 2025-12-19

I like the python programming language but I don&#39;t like to program in it as much as I used to. The trouble is the challenge of versions and packages. At work we&#39;re shifting to using [uv](https://docs.astral.sh/uv/getting-started/installation/) to manage the &#34;python environment&#34; (a symptom of how python has become less than fun). It works well on the machines and operating systems I use for work. My personal computing platform of choice these days is a Raspberry Pi 500 though. Raspberry Pi OS is based on Debian but when it comes to Python things can get persnickety pretty quick. The best way to install uv I&#39;ve found isn&#39;t via methods suggested at the beginning of the linked install pages. For me I have found installing via Rust&#39;s cargo command the most reliable.

```shell
cargo install --locked uv
```

Once installed you can update it with the following

```shell
uv self update
```

Since I don&#39;t setup a new Pi frequently I tend to forget the simplicity of this approach and waste an hour trying the other suggested ways. This post is just a note to myself so I remember that!</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/12/19/python_setup_pi_os_6.md" length="1494" type="text/markdown" />
    </item>
  </channel>
</rss>
