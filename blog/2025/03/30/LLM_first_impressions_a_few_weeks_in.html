<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="language" content="EN">
  <title>LLM_first_impressions_a_few_weeks_in</title>

  <link rel="stylesheet" type="text/css"  href="/css/site.css" media="screen" />
  <link rel="stylesheet" type="text/css"  href="/css/tables.css" media="screen" />
  <link title="RSS feed for rsdoiel's blog" rel="alternate" type="application/rss+xml" href="https://rsdoiel.github.io/rss.xml" />
  
  <link rel="alternative" type="application/markdown" href="/blog/2025/03/30/LLM_first_impressions_a_few_weeks_in.md">
  <link rel="search" type="application/opensearchdescription+xml"
        title="Robert's Rambling Search Engine"
        href="search.osdx">
</head>
<body>
<nav>
<ul>
<li><a href="/">R. S. Doiel</a></li>
<li><a href="/about.html">About</a></li>
<li><a href="/blog/">Blog</a></li>
<li><a href="/cv.html">CV</a></li>
<li><a href="https://github.com/rsdoiel">GitHub</a></li>
<li><a href="/library-terminology.html">Library Jargon</a></li>
<li><a href="/presentations.html">Presentations</a></li>
<li><a href="/projects.html">Projects</a></li>
<li><a href="/resume.html">Resume</a></li>
<li><a href="/search.html">Search</a></li>
<li><a href="/series/">Series</a></li>
</ul>
</nav>

<section>
  <article>
<h1 id="llm-first-impressions-a-few-weeks-in">LLM first impressions a
few weeks in</h1>
<p>By R.S. Doiel, 2025-03-30</p>
<p>Writing code with an LLM is far cry from what the hype cycle
promises. It requires care. An attention to detail. It imposes
significant compute resources. Those impose significant amount of
electricity consumption. Even cloud hosted LLM are slow beyond the first
few iterations. If you want to find a happy medium between describing
what want and how you want it done you need to commit to a non trivial
amount of effort. Depending on your level of experience it may be faster
to limit code generation to specific parts of a project. In early 2025
it maybe faster to code simple projects yourself.</p>
<p>When I compare working with an LLM like Gemma, Llama, Phi, Mistral,
Chat-GPT to traditional <a
href="https://en.wikipedia.org/wiki/Rapid_application_development"
title="Rapid Application Development">RAD</a> the shiny of the current
“AI” hype is diminished. RAD tools often are easier operate, been around
so long we forget about them and use significantly less compute
resources<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>. RAD tools are venerable in 2025.
The computational overhead as well as the complexity of running an
integrated LLM environment that supports <a
href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation"
title="Retrieval Augmented Generation">RAG</a>, <a
href="https://en.wikipedia.org/wiki/Software_agent"
title="software agent explained">agency</a> and <a
href="https://www.forbes.com/councils/forbestechcouncil/2025/03/27/your-essential-primer-on-large-language-model-agent-tools/"
title="A Forbes article on tool use with large language models">tools</a>
is much more than writing a simple code generator<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. A
popular one I tried is <a href="https://mysty.app">Msty.app</a>. It
unfortunately is restricted to the x86 platform. So much for energy
efficiency. How do we get more bang for the energy consumed? Does the
LLM enhanced coding environment push things really forward? Are LLM yet
another tool in the toolbox of our profession?</p>
<p>What are the areas I’ve found helpful use LLM?</p>
<p>My hands and wrists aren’t the same as when I started in my career. I
am hoping that an LLM can reduce my typing and reduce the risk of <a
href="https://en.wikipedia.org/wiki/Repetitive_strain_injury"
title="repetitive strain injury">RSI</a>. I am also hoping that I only
need to do detailed reading after waiting for code to be generated
(i.e. I’m taking advantage of the slowness of the LLM eval cycle). My
hope is for lowered eye strain. Is that worth burning down rain forests?
No it is not. Am I convinced that either benefit is true? No. I’m hoping
this aspect of LLM usage get studied. I’m interested in tools that are a
good ergonomic benefit for developers.</p>
<p>Without using an LLM my normal development approach is to write out a
summary of what I want to do in a project. I clarify the modules I want
and the operations I need to perform. Then I code tests before coding
the modules. For many people I think this approach appears upside down.
When I started as a programmer I wrote code first to some general notion
of what I thought the client wanted. I’ve found this inverted approach
yields some documentation. Writing documentation first clarifies my
software architecture and how I will organize the code needed. Writing
tests first means I don’t write more code than needed. The result is
software I can look at a year or more later and still understand.</p>
<p>The first step in my current approach is well suited to adopting an
LLM and using code generation. The LLM I’ve tried are decent at writing
minimal test code. Tooling can automate some of the loop of generating
code that can actually run and pass tests. The process is not fast. It
reminds me of the slow compilers of my youth. It works OK for the most
part. You can speed things up with faster hardware. Are we reinventing
Workstations with “AI computers”? Today you can rent LLM services and
hardware in the cloud.</p>
<p>An intriguing relationship with LLM or “prompt programming” that
isn’t talked about much is the <a
href="https://en.wikipedia.org/wiki/Literate_programming">Literate
programming</a> <a
href="https://en.wikipedia.org/wiki/Donald_Knuth">Knuth</a> advocated.
The LLM I’ve tried have used a chat interface. An documented and editing
approach may be better suited. Using a chat approach the responses are
usually presented in Markdown with embedded code blocks. This is
analogous to a simplified version of what Knuth created using TeX with
embedded code blocks. With the LLM your prompts trigger a narrative of
code and function. That feels familiar to me. The challenges of working
with an LLM is much like the challenges of literate programming. An LLM
might encourage us to think more about what we’re trying to do and
perhaps more objectively vet the resulting code without engaging our
egos.</p>
<p>A significant advantage of the LLM generated code is a byproduct of
large training sets. The generated code tends to look average. It tends
to avoid obfuscation. It mostly is reasonably reasonable. If the
computing resources were significantly lower I’d feel more comfortable
with this approach over the long run. The current state needs radical
simplification. The LLM development environment is overly complex. That
complexity only benefits consolidation in the cloud where we are rented
the tools of our trade. LLM development environment’s complexity and
energy consumption weight heavily on me as I explore this approach. It
is too early to tell if it should be a regular tool in the developers’
toolbox of sustainable software practices.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Delphi’s RAD tooling and Lazarus’ RAD tooling can run on
desktop computers from a decade or more ago. The hardware vendors want
to sell us “AI computers” and “CPU” because of the increase compute
demands of running large LLM software. You can rent GPU as a service
because the costs in terms of hardware and energy consumption from Big
Tech’s LLM obsession.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Code generation goes back to the at least to 1950 or
1960. Code generation was well established practice before the Go
community picked it up over a decade ago to deal with the routine boiler
plate. I routinely create code templates and run it through Pandoc.
These I can even use on a Raspberry Pi without it breaking a sweat.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  </article>
</section>

<footer>
<p>copyright © 2016 - 2025 R. S. Doiel<br /> <a
href="/rssfeed.html">RSS</a> feeds and website built with <a
href="https://rsdoiel.github.io/pttk">pttk</a>, Bash, Make and <a
href="https://pandoc.org">Pandoc</a>.</p>
</footer>
<!-- START: PrettyFi from https://github.com/google/code-prettify -->
<script>
/* We want to add the class "prettyprint" to all the pre elements */
var pre_list = document.querySelectorAll("pre");

pre_list.forEach(function(elem) {
    elem.classList.add("prettyprint");
    elem.classList.add("linenums");/**/
    elem.classList.add("json"); /**/
});
</script>
<style>
li.L0, li.L1, li.L2, li.L3, li.L4, li.L5, li.L6, li.L7, li.L8, li.L9
{
    color: #555;
    list-style-type: decimal;
}
</style>
<link rel="stylesheet" type="text/css" href="/css/prettify.css">
<script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_
prettify.js"></script>
<!--  END: PrettyFi from https://github.com/google/code-prettify -->
<script type="module">
    await import('/pagefind/pagefind-highlight.js');
    new PagefindHighlight({ highlightParam: "highlight" });
</script>
</body>
</html>
