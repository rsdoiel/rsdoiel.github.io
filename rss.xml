<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <atom:link href="https://rsdoiel.github.io/rss.xml" rel="self" type="application/rss+xml"/>
        <title>R. S. Doiel Blog</title>
        <link>https://rsdoiel.github.io</link>
        <description>Robert&#39;s ramblings and wonderigs</description>
<<<<<<< HEAD
        <pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate>
        <lastBuildDate>Sun, 25 Feb 2024 00:00:00 +0000</lastBuildDate>
        <generator>pttk rss 0.0.12</generator>
        <item>
            <title>A Text Oriented Web</title>
            <link>https://rsdoiel.github.io/blog/2024/02/25/text_oriented_web.html</link>
            <description>The web is a busy place. There seems to be a gestalt resonant at the moment on the web that can be summarized by two phrases, &#34;back to basics&#34; and &#34;simplification&#34;. It is not the first time I&#39;ve seen this nor is it likely the last. This blog post describes a thought experiment about a simplification with minimal invention and focus on feature elimination. It&#39;s a way to think about the web status quo a little differently. My intention is to explore the implications of a more text centered web experience that could coexist as a subset of today&#39;s web.&#xA;&#xA;I think the following could form the &#34;good stuff&#34; in a Crockford[^1] sense of pairing things down to the essential.&#xA;&#xA;- the transport layer should remain HTTP but be limited to a few methods (GET, POST and HEAD) and the common header elements (e.g. length, content-type come to mind)&#xA;- The trio of HTML, CSS and JavaScript is really complex, swap this out for Markdown augmented with YAML (Markdown and YAML already have a synergy in Markdown processors like Pandoc)&#xA;- A Web form is expressed using GHYITS[^2], it is delimited in the Markdown document by the familiar &#34;`^---$`&#34; block element, web form content would be encoded as YAML in the body of the POST using the content type &#34;`application/x-yaml`&#34;.&#xA;- Content would be served using the `text/markdown; charset: utf-8` content type already commonly used to identify Markdown content distinct from `plain/text`&#xA;&#xA;I need a nice name for describing the arrangement of Markdown+YAML over HTTP arrangement. Is the descriptive acronym for &#34;text oriented web&#34;, i.e. &#34;tow&#34;, enough? Does it already have a meaning in software or the web? Would the protocol be &#34;tow://&#34;? I really need something a bit more clever and catchy if this is going to proceed beyond a thought experiment.&#xA;&#xA;[^1]: Douglas Crockford &#34;discovered&#34; JSON, see &lt;https://en.wikipedia.org/wiki/Douglas_Crockford&gt; ...</description>
            <pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2024/02/25/text_oriented_web.html</guid>
=======
        <pubDate>Fri, 23 Feb 2024 00:00:00 +0000</pubDate>
        <lastBuildDate>Fri, 23 Feb 2024 00:00:00 +0000</lastBuildDate>
        <generator>pttk rss 0.0.14</generator>
        <item>
            <title>Two missing features from HTML5, an enhanced form.enctype and a list input type</title>
            <link>https://rsdoiel.github.io/blog/2024/02/23/enhanced_form_handling.html</link>
            <description>I wish the form element supported a `application/json` encoding type and there was such a thing as a `list-input` element.&#xA;&#xA;I&#39;ve been thinking about how we can get back to basic HTML documents and move away from JavaScript required to render richer web forms. When web forms arrived on scene in the early 1990s they included a few basic input types. Over the years a few have been added but by and large the data model has remained relatively flat. The exception being the select element with `multiple` attribute set. I believe we are being limited by the original choice of urlencoding web forms and then resort to JavaScript to address it&#39;s limitations.&#xA;&#xA;What does the encoding of a web form actually look like?  The web generally encodes the form using urlencoding. It presents a stream of key value pairs where the keys are the form&#39;s input names and the values are the value of the input element. With a multi-select element the browser simply repeats the key and adds the next value in the selection list to that key.  In Go you can describe this simple data structure as a `map[string][]string`[^1]. Most of the time a key points to a single element array of string but sometimes it can have multiple elements using that key and then the array expands to accommodate. Most of the time we don&#39;t think about this as web developers. The library provided with your programming language decodes the form into a more programmer friendly representation. But still I believe this simple urlencoding has held us back. Let me illustrate the problem through a series of simple form examples.&#xA;&#xA;[^1]: In English this could be described as &#34;a map using a string to point at a list of strings&#34; with &#34;string&#34; being a sequence of letters or characters.&#xA;&#xA;Here&#39;s an example of a simple form with a multi select box. It is asking for your choice of ice cream flavors. ...</description>
            <pubDate>Fri, 23 Feb 2024 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2024/02/23/enhanced_form_handling.html</guid>
>>>>>>> bc24538769755ec2ac60c2ac986708a3696be145
        </item>
        <item>
            <title>Installing pgloader from source</title>
            <link>https://rsdoiel.github.io/blog/2024/02/01/installing-pgloader-from-source.html</link>
            <description>I&#39;m working on macOS at the moment but I don&#39;t use Home Brew so the instructions to install pgloader are problematic for me. Except I know pgloader is a Lisp program and once upon a time I had three different Lisps running on a previous Mac.  So what follows is my modified instructions for bringing pgloader up on my current Mac Mini running macOS Sonoma 14.3 with Xcode already installed.&#xA;&#xA;pgloader is written in common list but the instructions at https://pgloader.readthedocs.io/en/latest/install.html specifically mention compiling with [SBCL](https://sbcl.org) which is one of the Lisps I&#39;ve used in the past. But SBCL isn&#39;t (yet) installed on my machine and SBCL is usually compiled using SBCL but can be compiled using other common lists.  Enter [ECL](https://ecl.common-lisp.dev/), aka Embedded Common-Lisp. ECL compiles via a C compiler including the funky setup that macOS has. This means the prep for my machine should look something like&#xA;&#xA;1. Compile then install ECL&#xA;2. Use ECL to compile SBCL&#xA;3. Install SBCL&#xA;4. Now that we have a working SBCL, follow the instructions to compile pgloader and install&#xA;&#xA;NOTE: pgloader requires some specific configuration of SBCL when SBCL is compiled&#xA;&#xA;This recipe is straight forward. ...</description>
            <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2024/02/01/installing-pgloader-from-source.html</guid>
        </item>
        <item>
            <title>vis for vi and fun</title>
            <link>https://rsdoiel.github.io/blog/2024/01/31/vis-for-vi-and-fun.html</link>
            <description>By R. S. Doiel, 2024-01-31 (updated: 2024-02-02)&#xA;&#xA;I&#39;ve been looking for a `vi` editor that my fingers would be happy with. I learned `vi` when I first encountered Unix in University (1980s). I was a transfer student so didn&#39;t get the &#34;introduction to Unix and Emacs&#34; lecture. Everyone used Emacs to edit programs but Emacs to me was not intuitive. I recall having a heck of a time figuring out how to exit the editor! I knew I needed to learn an editor and Unix fast to do my school work. I head to my college bookstore and found two spiral bound books [Unix in a Nutshell](https://openlibrary.org/works/OL8724416W?edition=key%3A/books/OL24392296M) and &#34;Vi/Ed in a Nutshell&#34;. They helped remedy my ignorance. I spent the afternoon getting comfortable with Unix and learning the basics in Vi. It became my go to text editor. Somewhere along the line `nvi` came along I used that. Eventually `vim` replaced `nvi` as the default &#34;vi&#34; for most Linux system and adapted again.  I like one featured about `vim` over `nvi`. `vim` does syntax highlighting. I routinely get frustrate with `vim` (my old muscle memory throws me into the help systems, very annoying) so I tend to bounce between `nvi` and `vim` depending on how my eyes feel and frustration level.&#xA;&#xA;Recently I stumbled on `vis`. I find it a  very interesting `vi` implementation. Like `vim` it mostly conforms to the classic mappings of a modal editor built on top of `ed`. But `vis` has some nice twists. First it doesn&#39;t try to be a monolithic systems like Emacs or `vim`. Rather then used an application specific scripting language (e.g. Emacs-lisp, vim-script) it uses Lua 5.2 as its configuration language. For me starting up `vis` feels like starting up `nvi`. It is quick and responsive where my typical `vim` setup feels allot like Visual Studio Code in that it&#39;s loading a whole bunch of things I don&#39;t use.&#xA;&#xA;Had `vis` just had syntax highlighting I don&#39;t know if I was would switched from `vim`. `neovim` is a better vim but I don&#39;t use it regularly and don&#39;t go out of my way to install it.  `vis` has one compelling feature that pushed me over the edge. One I didn&#39;t expect. `vis` supports [structured regular expressions](http://doc.cat-v.org/bell_labs/structural_regexps/se.pdf &#34;PDF paper explain structured regular expression by Rob Pike&#34;). This is the command language found in Plan 9 editors like [sam](http://sam.cat-v.org/) and [Acme](http://acme.cat-v.org/). The approach to regexp is oriented around streams of characters rather than lines of characters. It does this by supporting the concept of multiple cursors and operating on selections (note the plural) in parallel. This allows a higher degree of transformation, feels like a stream oriented AWK but with simpler syntax for the things you do all the time. It was easiest enough to learn that my finger quickly adapted to it. It does mean that in command mode my search and replace is different than what I used to type. E.g. changing CRLF to LF&#xA;&#xA;```&#xA;:1,$x/\r/ c//&#xA;``` ...</description>
            <pubDate>Wed, 31 Jan 2024 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2024/01/31/vis-for-vi-and-fun.html</guid>
        </item>
        <item>
            <title>Updated recipe, compiling PostgREST 12.0.2 (M1)</title>
            <link>https://rsdoiel.github.io/blog/2024/01/04/updated-recipe-compiling-postgrest_v12.0.2.html</link>
            <description>These are my updated &#34;quick notes&#34; for compiling PostgREST v12.0.2 on a M1 Mac Mini using the current recommended&#xA;versions of ghc, cabal and stack supplied [GHCup](https://www.haskell.org/ghcup).  When I recently tried to use&#xA;my previous [quick recipe](/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.md) I was disappointed it failed with errors like&#xA;&#xA;~~~&#xA;Resolving dependencies...&#xA;Error: cabal: Could not resolve dependencies:&#xA;[__0] trying: postgrest-9.0.1 (user goal)&#xA;[__1] next goal: optparse-applicative (dependency of postgrest)&#xA;[__1] rejecting: optparse-applicative-0.18.1.0 (conflict: postgrest =&gt;&#xA;optparse-applicative&gt;=0.13 &amp;&amp; &lt;0.17)&#xA;[__1] skipping: optparse-applicative-0.17.1.0, optparse-applicative-0.17.0.0&#xA;(has the same characteristics that caused the previous version to fail:&#xA;excluded by constraint &#39;&gt;=0.13 &amp;&amp; &lt;0.17&#39; from &#39;postgrest&#39;)&#xA;[__1] trying: optparse-applicative-0.16.1.0&#xA;[__2] next goal: directory (dependency of postgrest)&#xA;[__2] rejecting: directory-1.3.7.1/installed-1.3.7.1 (conflict: postgrest =&gt;&#xA;base&gt;=4.9 &amp;&amp; &lt;4.16, directory =&gt; base==4.17.2.1/installed-4.17.2.1)&#xA;[__2] trying: directory-1.3.8.2&#xA;[__3] next goal: base (dependency of postgrest)&#xA;[__3] rejecting: base-4.17.2.1/installed-4.17.2.1 (conflict: postgrest =&gt;&#xA;base&gt;=4.9 &amp;&amp; &lt;4.16)&#xA;&#xA;...&#xA;&#xA;~~~&#xA;&#xA;So this type of output means GHC and Cabal are not finding the versions of things they need&#xA;to compile PostgREST. I then tried picking ghc 9.2.8 since the default.nix file indicated&#xA;a minimum of ghc 9.2.4.  The `ghcup tui` makes it easy to grab a listed version then set it&#xA;as the active one. ...</description>
            <pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2024/01/04/updated-recipe-compiling-postgrest_v12.0.2.html</guid>
        </item>
        <item>
            <title>Finding Bluesky RSS feeds</title>
            <link>https://rsdoiel.github.io/blog/2023/12/23/finding-blue-sky-rss-feeds.html</link>
            <description>With the update to [1.60](https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i) of Bluesky we can now follow people on Bluesky via RSS feeds. This makes things much more convienient for me. &#xA;The RSS feed is visible via the HTML markup on a person&#39;s profile page (which are now public). E.g. My Bluesky profile page is&#xA;at &lt;https://bsky.app/profile/rsdoiel.bsky.social&gt; and if you look at that pages HTML markup you&#39;ll see a link element in the head&#xA;&#xA;```html&#xA; &lt;link rel=&#34;alternate&#34; type=&#34;application/rss+xml&#34; href=&#34;/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss&#34;&gt;&#xA;```&#xA;&#xA;That&#39;s the RSS feed. So now if you want to follow you can expand the URL to&#xA;&#xA;```&#xA;https://bsky.app/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss&#xA;```&#xA;&#xA;And use if via your feed reader. This is a sweat feature. It allows me to move my reading from visiting the website&#xA;to getting updates via my feed reader. ...</description>
            <pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/12/23/finding-blue-sky-rss-feeds.html</guid>
        </item>
        <item>
            <title>RSS and my web experience</title>
            <link>https://rsdoiel.github.io/blog/2023/12/07/rss-and-my-web-experience.html</link>
            <description>RSS is alive and kicking and Bluesky should support it too. Explore my recipe for reading web news.</description>
            <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/12/07/rss-and-my-web-experience.html</guid>
        </item>
        <item>
            <title>Postgres Quick Notes, take two</title>
            <link>https://rsdoiel.github.io/blog/2023/11/17/PostgreSQL-Quick-Notes.html</link>
            <description>A collection of quick notes for setting and Postgres for development.</description>
            <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/11/17/PostgreSQL-Quick-Notes.html</guid>
        </item>
        <item>
            <title>Building A to Z list pages in Pandoc</title>
            <link>https://rsdoiel.github.io/blog/2023/10/18/A-to-Z-lists.html</link>
            <description>Pandoc offers a very good template system. It avoids elaborate features in favor of a few simple ways to bring content into the page.  It knows how to use data specified in “front matter” (a YAML header to a Markdown document) as well as how to merge in JSON or YAML from a metadata file.  One use case that is common in libraries and archives that less obvious of how to handle is building A to Z lists or year/date oriented listings where you have a set of navigation links at the top of the page followed by a set of H2 headers with UL lists between them.  In JSON the typical data presentation would look something like&#xA;&#xA;```json&#xA;{&#xA;  &#34;a_to_z&#34;: [ &#34;A&#34;, &#34;B&#34;],&#xA;  &#34;content&#34;: {&#xA;    &#34;A&#34;: [&#xA;      &#34;After a beautiful day&#34;,&#xA;      &#34;Afterlife&#34;&#xA;    ],&#xA;    &#34;B&#34;: [&#xA;      &#34;Better day after&#34;,&#xA;      &#34;Better Life&#34;&#xA;    ]&#xA;  }&#xA;}&#xA;```&#xA;&#xA;The trouble is that while YAML’s outer dictionary (key/value map) works fine in Pandoc templates there is no way for the the for loop to handle maps of maps like we have above.  Pandoc templates really want to iterate over arrays of objects . That’s nice thing! It gives us more ways to transform the data to provide more flexibility in our template implementation. Here’s how I would restructure the previous JSON to make it easy to process via Pandoc’s template engine.  Note how I’ve taken our simple array of letters and turned them into an object with an “href” and “label” attribute. Similarly I’ve enhanced the “content” objects.&#xA;&#xA;```json&#xA;{&#xA;  &#34;a_to_z&#34;: [ {&#34;href&#34;: &#34;A&#34;, &#34;label&#34;: &#34;A&#34;}, {&#34;href&#34;: &#34;B&#34;, &#34;label&#34;: &#34;B&#34;} ],&#xA;  &#34;content&#34;: [&#xA;      {&#34;letter&#34;: &#34;A&#34;, &#34;title&#34;: &#34;After a beautiful day&#34;, &#34;id&#34;: &#34;after-a-beautiful-day&#34;},&#xA;      {&#34;title&#34;: &#34;Afterlife&#34;, &#34;id&#34;: &#34;afterlife&#34;},&#xA;      {&#34;letter&#34;: &#34;B&#34;, &#34;title&#34;: &#34;Better day after&#34;, &#34;id&#34;: &#34;better-day-after&#34;},&#xA;      {&#34;title&#34;: &#34;Better Life&#34;, &#34;id&#34;: &#34;better-life&#34;}&#xA;  ]&#xA;}&#xA;```&#xA;&#xA;Then the template can be structure something like ...</description>
            <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/10/18/A-to-Z-lists.html</guid>
        </item>
        <item>
            <title>Skimmer</title>
            <link>https://rsdoiel.github.io/blog/2023/10/06/concept.html</link>
            <description>I have a problem. I like to read my feeds in newsboat but I can&#39;t seem to get it working on a few machines I use.&#xA;I miss having access to read feeds. Additionally there are times I would like to read my feeds in the same way&#xA;I read twtxt feeds using `yarnc timeline | less -R`. Just get a list of all items in reverse chronological order.&#xA;&#xA;I am not interested in reinventing newsboat, it does a really good job, but I do want an option where newsboat isn&#39;t&#xA;available or is not not convenient to use.  This lead me to think about an experiment I am calling skimmer&#xA;. Something that works with RSS, Atom and jsonfeeds in the same way I use `yarnc timeline | less -R`.  &#xA;I&#39;m also inspired by Dave Winer&#39;s a river of news site and his outline tooling. But in this case I don&#39;t want&#xA;an output style output, just a simple list of items in reverse chronological order. I&#39;m thinking of a more&#xA;ephemeral experience in reading.&#xA;&#xA;This has left me with some questions.&#xA;&#xA;- How simple is would it be to write skimmer?&#xA;- How much effort would be required to maintain it?&#xA;- Could this tool incorporate support for other feed types, e.g. twtxt, Gopher, Gemini?&#xA;&#xA;There is a Go package called [gofeed](https://github.com/mmcdole/gofeed). The README describes it&#xA;as a &#34;universal&#34; feed reading parser. That seems like a good starting point and picking a very narrowly&#xA;focus task seems like a way to keep the experiment simple to implement. ...</description>
            <pubDate>Fri, 06 Oct 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/10/06/concept.html</guid>
        </item>
        <item>
            <title>Quick recipe, compiling Pandoc (M1)</title>
            <link>https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-Pandoc-M1.html</link>
            <description>These are my quick notes for compiling Pandoc on a M1 Mac Mini. I use a similar recipe for building Pandoc on Linux (NOTE: the challenges with libiconv and Mac Ports&#39; libiconv below if you get a build error).&#xA;&#xA;1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)&#xA;    a. Use the curl example command to install it&#xA;    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)&#xA;2. Make sure GHCup is pointing at the &#34;recommended&#34; versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)&#xA;3. Clone &lt;https://github.com/jgm/pandoc&gt; to your local machine&#xA;4. Check out the version you want to build (e.g. 3.1.4)&#xA;5. Run the &#34;usual&#34; Haskell build sequence with cabal per Pandoc&#39;s installation documentation for building from source&#xA;    a. `cabal clean`&#xA;    b. `cabal update`&#xA;    c. `cabal install pandoc-cli`&#xA;&#xA;Here&#39;s an example of the shell commands I run (I&#39;m assuming you&#39;re installing GHCup for the first time).&#xA;&#xA;~~~&#xA;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh&#xA;source $HOME/.gchup/env&#xA;ghcup tui&#xA;mkdir -p src/github.com/jgm/pandoc&#xA;cd src/github.com/jgm/pandoc&#xA;git clone git@github.com:jgm/pandoc&#xA;cd pandoc&#xA;git checkout 3.1.4&#xA;cabal clean&#xA;cabal update&#xA;cabal install pandoc-cli&#xA;~~~&#xA;&#xA;This will install Pandoc in your `$HOME/.cabal/bin` directory. Make sure&#xA;it is in your path (it should be if you&#39;ve sourced the GHCup environment after you installed GHCup). ...</description>
            <pubDate>Wed, 05 Jul 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-Pandoc-M1.html</guid>
        </item>
        <item>
            <title>Quick recipe, compiling PostgREST (M1)</title>
            <link>https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.html</link>
            <description>These are my quick notes for compiling PostgREST on a M1 Mac Mini. I use a similar recipe for building PostgREST on Linux.&#xA;&#xA;1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)&#xA;    a. Use the curl example command to install it&#xA;    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)&#xA;2. Make sure GHCup is pointing at the &#34;recommended&#34; versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)&#xA;3. Clone &lt;https://github.com/PostgREST/postgrest&gt; to your local machine&#xA;4. Check out the version you want to build (e.g. v11.1.0)&#xA;5. Run the &#34;usual&#34; Haskell build sequence with cabal&#xA;    a. `cabal clean`&#xA;    b. `cabal update`&#xA;    c. `cabal build`&#xA;    d. `cabal install`&#xA;&#xA;Here&#39;s an example of the shell commands I run (I&#39;m assuming you&#39;re installing GHCup for the first time).&#xA;&#xA;~~~&#xA;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh&#xA;source $HOME/.gchup/env&#xA;ghcup tui&#xA;mkdir -p src/github.com/PostgREST&#xA;cd src/github.com/PostgREST&#xA;git clone git@github.com:PostgREST/postgrest&#xA;cd postgrest&#xA;cabal clean&#xA;cabal update&#xA;cabal build&#xA;cabal install&#xA;~~~&#xA;&#xA;This will install PostgREST in your `$HOME/.cabal/bin` directory. Make sure&#xA;it is in your path (it should be if you&#39;ve sourced the GHCup environment after you installed GHCup). ...</description>
            <pubDate>Wed, 05 Jul 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.html</guid>
        </item>
        <item>
            <title>gsettings command</title>
            <link>https://rsdoiel.github.io/blog/2023/05/20/gsettings-commands.html</link>
            <description>One of the things I find annoying about Ubuntu Desktop defaults is that when I open a new application it opens in the upper left corner. I then drag it to the center screen and start working. It&#39;s amazing how a small inconvenience can grind on you over time.  When I&#39;ve search the net for changing this behavior the usual suggestions are &#34;install gnome-tweaks&#34;. This seems ham-handed. I think continue searching and eventually find the command below. So I am making a note of the command here in my blog so I can find it latter.&#xA;&#xA;~~~&#xA;gsettings set org.gnome.mutter center-new-window true&#xA;~~~&#xA;&#xA; ...</description>
            <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/05/20/gsettings-commands.html</guid>
        </item>
        <item>
            <title>First Personal Search Engine Prototype</title>
            <link>https://rsdoiel.github.io/blog/2023/03/10/first-prototype-pse.html</link>
            <description>I&#39;ve implemented a first prototype of my personal search engine which&#xA;I will abbreviate as &#34;pse&#34; from here on out. I implemented it using &#xA;three [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) scripts&#xA;relying on [sqlite3](https://sqlite.org), [wget](https://en.wikipedia.org/wiki/Wget) and [PageFind](https://pagefind.app) to do the heavy lifting.&#xA;&#xA;Both Firefox and newsboat store useful information in sqlite databases.  Firefox&#39;s `moz_places.sqlite` holds both all the URLs visited as well as those that are associated with bookmarks (i.e. the SQLite database `moz_bookmarks.sqlite`).  I had about 2000 bookmarks, less than I thought with many being stale from link rot. Stale page URLs really slow down the harvest process because of the need for wget to wait on various timeouts (e.g. DNS, server response, download times).  The &#34;history&#34; URLs would make an interesting collection to spider but you&#39;d probably want to have an exclude list (e.g. there&#39;s no point in saving queries to search engines, web mail, shopping sites). Exploring that will wait for another prototype.&#xA;&#xA;The `cache.db` associated with Newsboat provided a rich resource of content and much fewer stale links (not surprising because I maintain that URL list more much activity then reviewing my bookmarks).  Between the two I had 16,000 pages. I used SQLite 3 to query the url values from the various DB into sorting for unique URLs into a single text file one URL per line.&#xA;&#xA;The next thing after creating a list of pages I wanted to search was to download them into a directory using wget.  Wget has many options, I choose to enable timestamping, create a protocol directory and then a domain and path directory for each item. This has the advantage of being able to transform the Path into a URL later.&#xA;&#xA;Once the content was harvested I then used PageFind to index the all the harvested content. Since I started using PageFind originally the tool has gained an option called `--serve` which provides a localhost web service on port 1414.  All I needed to do was add an index.html file to the directory where I harvested the content and saved the PageFind indexes. Then I used PageFind to again to provide a localhost based personal search engine. ...</description>
            <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/03/10/first-prototype-pse.html</guid>
        </item>
        <item>
            <title>Prototyping a personal search engine</title>
            <link>https://rsdoiel.github.io/blog/2023/03/07/prototyping-a-personal-search-engine.html</link>
            <description>&gt; Do we really need a search engine to index the &#34;whole web&#34;? Maybe a curated subset is better.&#xA;&#xA;Alex Schreoder&#39;s post [A Vision for Search](https://alexschroeder.ch/wiki/2023-03-07_A_vision_for_search) prompted me to write up an idea I call a &#34;personal search engine&#34;.   I&#39;ve been thinking about a &#34;a personal search engine&#34; for years, maybe a decade.&#xA;&#xA;With the current state of brokenness in commercial search engines, especially with the implosion of the commercial social media platforms, we have an opportunity to re-think search on a more personal level.&#xA;&#xA;The tooling around static site generation where a personal search is an extension of your own website suggests a path out of the quagmire of commercial search engines.  Can techniques I use for my own site search, be extended into a personal search engine?&#xA;&#xA;Search engines happened pretty early on in the web. If my memory is correct they showed up with the arrival of support for [CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface &#34;Common Gateway Interface&#34;) in early web server software. Remembering back through the decades I see a pattern. ...</description>
            <pubDate>Tue, 07 Mar 2023 00:00:00 +0000</pubDate>
            <guid>https://rsdoiel.github.io/blog/2023/03/07/prototyping-a-personal-search-engine.html</guid>
        </item>
    </channel>
</rss>
