<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:source="https://source.scripting.com/">
  <atom:link href="https://rsdoiel.github.io/archive.xml" rel="self" type="application/rss+xml" />
  <channel>
    <title>An Antenna Website</title>
    <description>
      This is the default websites created by the antenna init action.
    </description>
    <link>https://rsdoiel.github.io/archive.xml</link>
    <lastBuildDate>01 Jan 26 08:47 -0800</lastBuildDate>
    <generator>antenna/0.0.20</generator>
    <docs>https://cyber.harvard.edu/rss/rss.html</docs>
    <item>
      <title>Reflection on a few decades</title>
      <link>https://rsdoiel.github.io/blog/2026/01/01/reflections_2025.html</link>
      <description>
        <![CDATA[A long post reflecting on things I've been thinking about in 2025 as we start
        2026.]]>
      </description>
      <source:markdown># Reflection on a few decades

By R. S. Doiel, 2025-12-31

I have been thinking about my career recently. It extends into the prior century. I have also been thinking allot about the Internet and the evolution of the Web. I see a pretty consistant thread in my career. I write software that transforms content and information from one system into another. Sometimes it&#39;s structured data and sometimes it&#39;s unstructured text. The thread became clearer with the arrival of the World Wide Web in the early 1990s. While the systems used over that time have grown complex I think they can be made far simpler. __I think they can be better implemented for writers. They can be writer centric.__. Finding a path to something simple has preoccupied by nonworking hours for most of 2025.

## A lesson from my past

An early set of programs I wrote for the Web were written in a new at the time language called Perl. My University had a &#34;mail form handler&#34; service that would let you create a web form and have the results sent to your email. I wrote a simple mail filter that would extract email&#39;s web form content and append the results to a CSV file. I had another Perl script that would read in the CSV file and generate a set of HTML calendar pages in the directory that the Unix host used for personal websites. It worked pretty good. It was so trivial I didn&#39;t keep a copy of it or port it to a new language when I stopped writing software in Perl all those years ago.

This little calendar project lead to career opportunities. I didn&#39;t find that out to much later. A colleague mentioned I was considered a hacker (in the good sense of the word) by some of the system admins because I had done that. It was just one of many experiments I&#39;ve done over my years working with computers.

That personal project taught me several things. I&#39;ve kept them in mind over the years.

1. Composible systems are powerful, Unix combined with the Web is composible
2. Structured data aids in composibility (EMail and CSV are only two examples)
3. Systems get used in unexpected ways.

No 3. has proved important. The web can be viewed from both the audience of one and the audience of the general public. You can also split the authoring system from the presentation system. You can divvy those up between machines. It all depends on the project scope, rates of data changes combined with scale of the specific audiences.

When I first started maintaining and building web publishing system I almost always leveraged a database management system too.  This was particularly true when [LAMP](https://en.wikipedia.org/wiki/LAMP_(software_bundle)) emerged as the common platform.

Databases were convenient because structured data is easy to work. The unstructured document can be work with through indirection. A file path points the the unstructured thing. Now you have structured reference to the unstructured object. That allows you to use a database to provide locking behaviors and avoid write collisions. In turn that made it easier to write the web programs.

Eventually the bespoke web publishing systems became content management systems. Bespoke gave way to more standardized software like [Movable Type](https://en.wikipedia.org/wiki/Movable_Type), [Zope](https://en.wikipedia.org/wiki/Zope), [WordPress](https://en.wikipedia.org/wiki/WordPress) and [Drupal](https://en.wikipedia.org/wiki/Drupal). Those arrive in the second Internet age, the age of commercialization.

## Commercialization era

When the Internet and Web went from an academic and research platform to a commercial platform many things changed. The Internet and Web came to be consumed by monetization, business models and greed. Those are why we have a surveillance economy today.

With these new drivers new demands came to the forefront. One big side effect was the Internet grew very large very quickly. By the time the first [iPhone](https://en.wikipedia.org/wiki/IPhone) was announced the Web was already experiencing growing pains. One of the big issues was spikes in Web traffic.

In the early days we assumed a few dozen concurrent connections. By 2000 you needed to handle much more. Buy 2005 you worried about tens of thousands of concurrent connections. The problem was even large enough to be named. It was the &#34;10K problem&#34;. It was talking about by we people like the old aerospace engineers used to talk about the sound barrier. Money was on the line so people tried various solutions. Some were hardware approaches, some content approaches others were changing how we implemented web services generally. Eventually the general purpose web services like NginX and Apache2 easily handled 10K concurrent requests. Then the worry was 100K, 1000K, etc. It is one of the reasons that distributed systems came to dominate the commercial web.

What hasn&#39;t been acknowledged since the concurrency problems is that we&#39;ve probably may be approaching peak human use. Most people on the planet experience the web today either in embedded systems (appliances) and their phones. Most of the growth I&#39;ve seen at work for more than a decade isn&#39;t human users but software systems. Early on the software systems tended to be dominated by &#34;bots&#34; that indexed web content or surveyed the content. Since 2024 the problem has be AI bot swarms, buts used to harvest web content to train large language models or deny services to competing bots. Maybe we should be thinking about scaling down instead of up.

### Observations on site construction

Since at least 2005 there has been a gathering renaissance in static websites. There has been many drivers. Static sites easier to scale for high volume traffic. They are easier to secure against attacks and fix when defaced. Often they are much cheaper to run and maintain.

Several innovations helped this happen.

1. Template systems are easy to build
2. Light weight markup adoption, like Markdown, provides a source for generating HTML
3. Cheap static storage system became commercially available (S3)
4. The web browser went from a green screen surrogate to a full on rich client even on phones

Markdown has become a very common markup expression well suited to generating HTML.  Support is easy to integrate into commercial content systems like Atlassian&#39;s Confluence and GitHub&#39;s pages. Commercial systems like GitHub offer favorable hosting options for static website. Finally, in spite of the term, a static site can be fully interactive without requiring additional resources from the web server for many use cases.

### Observations on computer systems

In 2025 commercially available computer systems follow variations of the Von Neuman computer model. A CPU, Memory (RAM) and storage. That model also assumes inputs and outputs that are distinct (people forget this). The scaling problem observed in the commercial era often presumes the input and outputs are used at the same rate for data consumed by us humans or even software.

My observation is the rates of change, both in the software implementation and in data, are important in guiding how you approach the lessons of scaling. Curation requires more dynamic responses while consuming is much more common.

Because of the needs of the monetized Web the there has been an out sized concern with scaling up, assuming dynamic systems are the base line. This is only true when you are creating monoliths like the walled gardens most people used on the Web today. You can scale writing (inputs) separately form reading (outputs) and that leads to better scaling. Unfortunately most developers and many engineers fail to see that and as a result we keep building complex monolithic systems even when you compose the systems from distributed systems.
Scaling up as the single direction of scaling is a false lesson.

## Are we entering a new web era yet?

Since &#34;Web 2.0&#34; there is cyclic hype about the next web or the web&#39;s replacement.  Most of the time this is driven by some sort of technological change being commercialized. There is a good share of doom and gloom along side Utopian thinking in these hype cycles. At some point we will arrive at a new era but I suspect it may not be driven purely by technical changes.

### My bet is on Simplification 

Why is simplification both desirable and possible in leading to an a new era on the Internet and Web? The Internet and Web was built as distributed systems. The specific technologies are largely intact even though commercial interests don&#39;t align with distributed systems. The lesson from that late 20th century and early 21st is unregulated markets tends to collapse towards monopolies. Centralization inevitably becomes a profit advantage. Yet centralization makes them brittle. Eventually decentralization will be re-embraced. Given  the underpinning technology of the Internet and Web remains distributable I don&#39;t think reinvention is a requirement. Easier paths exist.

The next web isn&#39;t likely to be based on block chain, bitcoin or large language models either. Why? None of these really enhance communication. The Internet and the Web are communication systems. Computers unlike their calculator predecessor are communication devices too. That telephone in your pocket is a computer after all. Making it easier to communicate is likely to be the source of the next evolution. How do we make it easier to communicate?

Market consolidation also does not lead to innovation no matter how many startups are bought by conglomerates. I think people are where the next web happens. It&#39;s happens at a human scale of a few people at a time. If there is enough critical mass of people doing the &#34;new thing&#34; then that&#39;s when the new web will arrive. It&#39;ll have roots in the old web, may even be technologically indistinguishable but it&#39;ll be different because people choose to use it differently and think about it differently.

I think the next web needs to be more personal, more authentic. Less about algorithms. To make that happen it will start as a countervailing force to the current onslaught of centralization and control. It will likely happen in spite of centralization and control.

There are elements I see as being factors in reasserting decentralization. There has been a computer revolution happening that has largely gone unnamed and unnoticed by mainstream media and society. That revolution is in inexpensive single board computers. Some of it has been dismissed as fad in the form of the DIY maker movement. Before that it was buried under the commercial hype of the Internet of Things. Single board computers are real. I&#39;ve been using them for years. I&#39;m typing this post on one now. 

### then and now

My first computer cost me about $1800.00 US dollars. My first computer was a clone 286 machine. I ran DOS and Minix on it. I had to go out buy the parts and assemble it myself. This was back in the 1980s. A mini computer that could host a website in the early 1990s range between $50,000.00 to $200,000.00 US. The cost of a house at that time. Prices came down and eventually personal computers could run websites directly. The price of a web server settled down not much more than my first personal computer.

Fast forward to now. For $1800.00 US today I can easily assemble four Raspberry Pi 500+ workstations, a 16 port network switch and Ethernet cables to witch to support them. I can grow out that network with web servers running about $150 to $200.00 US per server depending on the storage I buy. A single Raspberry Pi 5 with solid state disk storage is enough to host a website with concurrent users in the thousands. What that 1990&#39;s era mini computer did for $50,000.00 US.

Here&#39;s how I came up with my $1800.00 budget. A sixteen port Ethernet switch from Netgear runs $100.00 US when I checked Best Buy today. A Raspberry Pi 500+ runs at $200.00 US plus tax and shipping. A Pi Monitors run at $100.00 US. Add in cables and such you&#39;ll looking at about $350.00 per work station. That&#39;s how I know I can build a four workstation network with room to expand for a $1800.00 budget. I don&#39;t even need to assemble the computers I just need to connect them up and power them on.  Each machine can be either a web host, workstation or both. My choice.  Expanding that system is also low coast. You can build useful web servers for less than the price of the Raspberry 500+ with monitors.

My little four workstation network is enough to staff hardware to support a neighborhood paper. Adding a few more machines and you can provide hardware for a small town paper. You just need the physical space to house them and of course the writers and graphics people. 

Unlike the small town newspapers when I grew up there are no printing presses to purchase, rent or maintain. There&#39;s just several computers. The computers I listed last decades. This suggests something to me. Maybe the hardware we have is good enough for the next web. Maybe hardware isn&#39;t the issue it once was.

The cost factor in the next generation of the web doesn&#39;t require huge expenditures of capital. It doesn&#39;t even require big data centers in spite of what the Big Companies will swear are required. This suggestions we can scale down. We can scale by going small. Heck, any &#34;smart&#34; appliance is likely to have a web server embedded in it and they don&#39;t put in any excess compute capability into appliances. The Internet of things has been enabled by the cheapness of some single board computers. In the meantime the foundations of the Internet and Web remain intact and those little machines fit it just fine.

The trick to making the web really distributed is to allow it be easily editable. I think we do that by embracing the little single board computers like the Raspberry Pi. We embrace individuals owning the hardware and software use to produce the content. Maybe even host it. It is even possible to turn content management systems into appliances.

The small set of network computers I described run the same protocols as the Internet and Web. If I connect them up to the switch I have a local private network version of the Internet and Web.

### moving beyond commercial Internet Service Providers

When the Internet was commercialized it was done so using the justification that Government couldn&#39;t afford giving the public access to the Government created resource. Only private business could do that. Not surprising considering the political wisdom of the 1980s and 1990s. That wisdom suggested all problems as business problems and markets solve business problems.

In practice that approach has been a failure. Try to use the a cell phone in rural America today, chances are it&#39;ll fail. The Internet for most Americans is experienced through their cells phones. Heck, in many urban areas cell service remains really problematic. It is common to get poor cell service through out most of Los Angeles county. Some of that is geography (Mountains) but much is also the lack of investment by business as territory was divided up among the decreasing number of Internet Service Providers.

In response to market and government failures and the continue push to put everything online and people have started to pull together and find ways to cope. An interesting approach as been the with community owned internet service providers. If you&#39;re a town with less than 1000 people you&#39;re unlikely to have many business offering service at affordable rates. The best you might due is satellite connections like Starlink. Satellite access is slow because of transmission distance. A fiber connection is so much faster. This is a mater of physics, radio signals travel slower than the speed of light. 

What has started to happen in some rural areas is communities lay their own fiber between houses or network cell towers. Then they connect these to a location that also has an Internet connection with significant bandwidth. If a railroad runs through their town then that might be used as a connection point (Rail systems have used fiber optics for switch control sense the last century, they sometimes sell their excess capacity for other organizations or businesses). Similarly a large antenna receiver and transmitter array can care more data to and from the satellite then a home system like Starlink can. By pooling the communications together this becomes an affordable option.

Rural community cooperatives can get by with minimal staffing and little in the way of physical space. The function like a small business in terms of budget but since they are nonprofit they don&#39;t need to enhance the cost charged to members nor do they need to pay a competitor off to avoid competition.

A policy of encouraging lots of little cooperatives is technically doable. It how the Internet was designed. Like public streets and highways communities can choose to provide board band access that connect to the Internet. Organizations like [Institute for Local Self-Reliance](https://ilsr.org/) are show examples of this approach. 

### What else is needed?

The act of producing a website needs to be simplified too. Ideally it&#39;d be a turn key system I can run on my Raspberry Pi sized computers. That turns out to be incredibly easy when you think of the content system as being single user.

I&#39;ve notice about a decade and a half ago people seemed to rediscover &#34;static&#34; web. This appeared to be driven by a reaction to the walled gardens, to privacy costs and the desire for local control. The static web is a bit of a misnomer. It refers to how the content is hosted not how the content behaves. A static website in 2025 can be highly interactive. 

During the first dot com era (approx. 1995 through 2000) &#34;dynamic&#34; websites rose in popularity. While the web browser was becoming a rich client allot dynamism happened server side. By 2004 and 2005 the rich features provided by web browser were being leverage in &#34;web properties&#34; like GMail and Google Maps. At that point the static approach become a misnomer. The web browser can run whole applications.  You can approach site generation like just like you did in the original web and still have a dynamic experience. By 2010 this was well understood. By 2020 it was taken for granted. I don&#39;t think the public realizes that the web they experience on their phones is produced in some part by static websites. All those &#34;apps&#34; that run on your phone that use webview are simply websites and services. In web view they provide an excellent place to spy on you since web view generally does not prevent data exfiltration hosted by the wrapping application code. On the customer side it also allows for easier updates so there&#39;s that too.

When JavaScript and CSS became predominant features in web browsers they allowed the client, the web browser, to be independently dynamic. In terms of how we call things it made it much harder to explain a &#34;static&#34; website doesn&#39;t imply it can&#39;t be interactive. Today you can a number of effective browser side search engine options that work perfect for website search without resorting to hosted services or forcing you to a dynamic website system. The term static site remains though but it&#39;s just not limited to the approaches used by retro or nostalgic websites.

The way the walled gardens are built and scaled ultimately comes out of the tried and true lessons going go back to the 2000s (if not earlier). That tells us something. We need more of a new vision than inventing technology to move forward. 

What happens when we separate the acts of writing, the acts of content curation and the website delivery?  We have an opportunity to radically scale down have many decades of scaling up. In the process I think we can discover a more human and simpler approach to a sharing the human presence online.

## The Open Web, the Web or a web?

Many weeks ago I read a short post by Dave Winer talking about the terms Indie Web, Open Web and the Web. I haven&#39;t put my finger on where I read that but I think he was on to something. The Web didn&#39;t go away. It still there. It&#39;s just the walled gardens have been successful in dominating marketing and news cycles. There is a perception, that many people assumed the destination **must be** a walled garden because of the [network effect](https://en.wikipedia.org/wiki/Network_effect). In 2025 I have come to the belief that &#34;network effect&#34; is over hyped, maybe just hype and marketing.

I can call anyone from my telephone regardless of what phone company they use. I&#39;m not limited by the big players like AT&#38;T or Verizon. That is true because the phone systems use common protocols to route calls and data. The Web is like that too. We need to start thinking in terms of the common protocols that work. We have allot of them. I would go a step further and look at simpler protocols and see how far we can push those. I don&#39;t think the answer is the social graph or Activity Pub pub. Those are marketing tools not communication tools. I don&#39;t be ATProto from Bluesky is the solution either. The solution that does and has worked for me is HTTP and RSS files. It&#39;s worked for me since before RSS one was codified! It hasn&#39;t stopped working sense even though the marketing gravity has moved elsewhere. 

What has failed to fully materialize is content systems oriented to both inbound and outbound feeds. WordPress comes close and I see that as part of Dave Winer&#39;s excitement about WordPress as the API for writing for the web. But I don&#39;t think content systems necessarily need to be web based. I certainly prefer using my text editors for writing then typing into a text box of a website even if it does implement a WYSIWYG editor option.

A step in the right direction is to empower the people who create content. It is to empower the writers. The software show allow our writing to be easily integrated into a web site. Where that web site winds up being hosted shouldn&#39;t be determined by the software, especially if it is a walled garden like Substack and Medium. 

### Scaling down

The scaling down of the web is an under explored in web space. It&#39;s worth exploring.

I want a simple piece of software the manages the content and renders the website. It let&#39;s me compose the site from pages, posts and feeds. I want it to be light weight and specifically to be able to run on my Raspberry Pi 500+. Preferably be possible to run on a Raspberry Pi Zero 2W if I attach a screen and keyboard. I&#39;m looking for a small core feature set. I think the system can be perfectly functional as a single user system. That of itself can simplify the whole process.

You might think a single user system can&#39;t be collaborative. Two existing pieces of tech solve that. First is feeds. Feeds let you move Markdown between content systems easily. That markdown can be repackaged and republished to on anther site. The second is distributed version control systems, Git being the current most common one. I work on my copy, you work on your and we merge the differences. We do this in software development all the time and it amazes me that this feature is rarely available to writers unless the using some web service to do so.

In terms of feeds I would choose to read RSS, Atom and JSON feeds as a minimum but produce just RSS 2.0 feeds. Why? Because a lesson we learned from the development of the Internet has been be liberal in what you accept but strict in what you produce. RSS 2.0 is extensible. It has a track record of successful extension. Just look at Podcasting, it&#39;s enabled because of enclosures supported pointing at non-text documents.  Similarly the practice of supporting Dave Winer&#39;s proposed source element means we can ship Markdown content right there in the RSS item. RSS becomes a safe content transport platform for rendering on someone else website. All that is necessary is to exclude embedded HTML and use the remaining Markdown rendering as standard HTML. It&#39;s just a matter of thinking differently how we use this venerable format.

Here&#39;s an example of applying RSS in practice.  The walled garden news sites feature content written and hosted elsewhere. Google News and the not quite dead year Yahoo News are good examples of this. How do they get the news content into their aggregated news pages?  Why they read feeds then format the output on their site.  Does this actually require a big company to pull off this feat? No. Does it require that you read the news in a web browser? No.

Googles doesn&#39;t write writers to write the news stories they re-purpose the content provided by other people and organization. The only thing that might be novel is that a filter is applied to show you think you want to click on. It&#39;s the same approach as search engine results. RSS feeds are even used as data sources of what to add to their search engine indexes. 

For several years I&#39;ve run a personal news aggregation site. The follow close to one thousand different feeds. It takes a few minutes to harvest and a minute or so to rendering out as HTML pages. Then another minute to publish via GitHub pages. I run the harvest once a day and I can read at my leisure. I run a copy on local home network that I update once ever couple hours. I just don&#39;t bother with Yahoo and Google News anymore. I even include feeds from social networks like Mastodon, Bluesky because they produce RSS feeds too. In theory I could follow something in Meta&#39;s threads this way too.

The computer I use to aggregate is a Raspberry Pi 3B+. The SD card is small, maybe 32 Gigabytes. Smaller than the one in my phone. I think it has about a Gig of RAM so it&#39;s not a big computer. It&#39;s all that I need as a web publishing platform. Scale small. I think that 3B+ costs less than $50.00 when I bought it and that included a case an power supply too. 

You&#39;d need Google, Meta, Microsoft, Apple or Amazon. They are not necessary for the web or a web presence. At most what they sell is a convenience. I find myself increasingly questioning the cost of that convenience as time moves forward.

What I dream of is a feed oriented content management system I run on my inexpensive single board computer. One that I can chose to copy to the public Internet and Web or keep private to my computer. I&#39;ve spent much of 2025 thinking through this problem has exploring how turn key I can get.

## What I&#39;ve been up to

In my spare time I&#39;ve been reorganizing my personal websites and the tooling behind them. By August 2025 these efforts had resulted in a project I called, [Antenna App](https://github.com/rsdoiel/antennaApp). The first step was to take wrangle how I guilt my personal news site, [Antenna](https://rsdoiel.github.io/antenna), hosted via GitHub into a turn key system. It was fortuitous that I started here as it made the resulting content management system both simple and flexible. The initial versions of Antenna App provided for harvesting feeds and then publishing them as HTML pages in a static site directory. This in turn was inspired by the simplicity of Newsboat and it&#39;s use of SQLite3 databases. Then I added options for supporting simple blogging. This left me with a feed oriented core. I can aggregate and can render posts and output the combined feeds as well. What was missing was support general case web pages so I added that.

By November 2025 I was generate most of my personal website, a club website and my aggregated news site using Antenna App. At this stage it was a command line program.  Command line programs are nice, they tended to be focused. They have a problem, you need to learn the command line syntax and that can put people off. I want people to realize that web publishing can be as easy and typing up a blog post in an editor of your choice. In December I started working on turning that command line tool into an interactive tool. That I think will get me closure to what I think is needed to get the Web out of its current walled garden rut.

## What I hope is next

The popularity of static site generators a missed an opportunity. There are lots of system that will take Markdown content and render a website with it. It&#39;s what caused the explosion of web publishing on GitHub before Microsoft purchased them. It continues on GitHub to this days.  The problem is the Git is complicated and Microsoft is in the process of turning GitHub and VS Code into another walled Garden with AI gatekeepers. That sucks.

What I haven&#39;t seen is a turn key content management system that is designed to run on your own computer. Especially the really good single board computers like the Raspberry Pi.  Technically you can run a local copy of [WordPress](https://wordpress.com) on a Raspberry Pi but it is a hassle as well as a bit of a resource pig. 


I think there needs to be single user content systems that can read and write feeds. Reading feeds gives us the ability to aggregate. Writing feeds allows us to syndicate. [Andy Sylvester](https://andysylvester.com/2025/04/19/my-thoughts-on-inbound-rss/) wrote a thoughtful rebuttal to Dave Winer&#39;s request for inbound RSS support in content systems. I happen to disagree with Andy but he made a good point, writers haven&#39;t insisted on this. I think that is in large part because they don&#39;t insist on good content management systems either, rather they deal with them as a price of writing. What if writers didn&#39;t have to pay that price anymore? Would they write more?  

To appreciate in bound RSS I think you need to have a feed oriented content management system to start with. I also think you need to keep you content management system separate from your text editor (otherwise just wrangle EMACS). I&#39;ve used many editors over years on many different operating systems. The ones I reach for today tend to be [Gnome TextEdit](https://apps.gnome.org/TextEditor/) and [aretext](https://aretext.org). Ask the next writer you run into I would guess those two editors will not be their first choices!

That&#39;s one of the reasons I don&#39;t run WordPress on my Raspberry Pi and simply mirror the resulting site to a static location. I don&#39;t like the textarea editors for doing any writing of length. But that doesn&#39;t means I wouldn&#39;t want to use a local content management system that took care of seamlessly automate the website generation process without making learn yet another template system or language.

Inbound RSS allows me to integrate Andy&#39;s content in a novel way. If Andy were to subscribe to my feeds he could reply to me and I&#39;d pick it up from his feed. I wouldn&#39;t have a rely on social media platforms like BlueSky to be aware of his thoughtful comments. Similarly Dave could reply to me on his platform and I&#39;d still pick it up. That what what is possible when we realize that the scale of one person, me, I can aggregate and publish web content in a simple fashion for a tiny computer like a Raspberry Pi!

I&#39;d like to point out I don&#39;t think I have the ultimate answer to kick starting the next web era I think is to come. I think doing that will lots of different software experiments by a diverse group of people with diverse backgrounds. What I can do is write what I would like to see as a writer.

Here&#39;s my wish list (much of it is getting built into Antenna App as time permits).

1. I want to write with the my editors of choice
2. I want to use the markup I think is appropriate, that should automatically be converted to HTML on site generation 
  a. Markdown
  b. CommonMark
  c. Open Screen Play format (nice for dialogues and transcripts)
3. I shouldn&#39;t have to worry about document metadata when I am writing, much of it should be automatically generated on import. I should be able to add metadata as I choose
4. I want to support both posts and pages
5. The extra bits that make a website really useful should be automatically generated
  a. sitemaps
  b. RSS feeds
  c. OPML lists
  d. Open Search documents
6. The system should be single user and run on a small computer like a Raspberry Pi without bogging down
7. The resulting website should be trivial to copy to a public service, e.g. via Git, scp or dragging and dropping files


These are features I&#39;ve desired for a long time. They have been pretty consistent since at least 2015 for me.  I&#39;ve written numerous systems that have some of characteristics. In 2025 I&#39;ve started to pull them together in Antenna App. I&#39;d like to see other software developers do something similar. I am very certain there are others with better visions, more refined design skills than me. I am happy to share what I am writing and welcome others to use it but I also think other eyes and other takes on the feature set benefit everyone.

My current setup for producing this website is as follows.

1. [aretext](https://aretext.org) and [Gnome TextEdit](https://apps.gnome.org/TextEditor/) are my current preferred editors
2. [Antenna App](https://rsdoiel.github.io/antennaApp) is my preferred feed oriented content management system
3. I publish via GitHub pages

Where I would like to be at the end of 2026? I&#39;d like to move away from GitHub and once again host my own site on a system I run. Currently looking at options from [Mythic Beasts](https://www.mythic-beasts.com/).

I run the draft version of my websites on my local home network. This gives me a space where I can test and review things if needed. I can also update them more frequently then I currently am comfortable with GitHub.

A cool idea I would do complete in 2026 is to make Antenna App really useful for people aside from myself. I also want to include a recipe where you can create a Raspberry Pi Zero 2W web writer appliance that will let you carry your blog publish systems in your pocket or key chain. Do that could let us comment in the real world, on the go, without resorting to the cloud then we can publish to the public when we choose to connect to the public internet.

I have hope for 2026. In spite of the challenges and hardships of 2025, I think we can build the future for the rest of us, for all of us if we choose.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2026/01/01/reflections_2025.md" length="35849" type="text/markdown" />
    </item>    <item>
      <title>Python Setup on Raspberry Pi OS 6</title>
      <link>https://rsdoiel.github.io/blog/2025/12/19/python_setup_pi_os_6.html</link>
      <description>
        <![CDATA[A set of quick notes for wrangling Python to work nicely on Raspberry Pi OS 6]]>
      </description>
      <source:markdown># Python Setup on Raspberry Pi OS 6

By R. S. Doiel, 2025-12-19

I like the python programming language but I don&#39;t like to program in it as much as I used to. The trouble is the challenge of versions and packages. At work we&#39;re shifting to using [uv](https://docs.astral.sh/uv/getting-started/installation/) to manage the &#34;python environment&#34; (a symptom of how python has become less than fun). It works well on the machines and operating systems I use for work. My personal computing platform of choice these days is a Raspberry Pi 500 though. Raspberry Pi OS is based on Debian but when it comes to Python things can get persnickety pretty quick. The best way to install uv I&#39;ve found isn&#39;t via methods suggested at the beginning of the linked install pages. For me I have found installing via Rust&#39;s cargo command the most reliable.

```shell
cargo install --locked uv
```

Once installed you can update it with the following

```shell
uv self update
```

Since I don&#39;t setup a new Pi frequently I tend to forget the simplicity of this approach and waste an hour trying the other suggested ways. This post is just a note to myself so I remember that!</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/12/19/python_setup_pi_os_6.md" length="1494" type="text/markdown" />
    </item>    <item>
      <title>Using text fragments</title>
      <link>https://rsdoiel.github.io/blog/2025/12/03/using_text_fragments.html</link>
      <description>
        <![CDATA[An example of using Antenna App's new reply action
        to transform a text fragment URL into a post. Also
        highlight a good point that Dave Winer made on his blog.]]>
      </description>
      <source:markdown># Using text fragments

By R. S. Doiel, 2025-12-03


&#62; We&#39;ve forgotten how important links are

([scripting.com](http://scripting.com/2025/12/01.html#a144337:~:text=We%27ve%20forgotten%20how%20important%20links%20are), accessed 2025-12-03)

[Dave](https://scripting.com)&#39;s right. We can&#39;t forget the humble link. His one line post nudged me to add a feature to [Antenna App](https://github.com/rsdoiel/antennaApp/releases &#34;see v0.0.17&#34; ). The feature is implemented as a new &#34;action&#34; called &#34;reply&#34;. Here&#39;s how I started this post you&#39;re reading.

~~~shell
antenna reply http://scripting.com/2025/12/01.html#a144337:~:text=We%27ve%20forgotten%20how%20important%20links%20are
~~~

That generate a first draft of this post that looked like this.

~~~markdown

&#62; We&#39;ve forgotten how important links are

[cited](http://scripting.com/2025/12/01.html#a144337:~:text=We%27ve%20forgotten%20how%20important%20links%20are) 2025-12-03

~~~

It was enough to start things off but the &#34;cited&#34; isn&#39;t a useful label IMO. I wound up doing to copy editing. In the next release it&#39;ll be more like how this post started.

The reply feature is intended to be minimal. Just enough to avoid a blank page and minimizing the amount of select, copy, paste and formatting I need to do to get going.

The current implementation takes the text fragment URL and turns it into Markdown. The Markdown consists of a block quote holding the reference to what would be highlighted if you clicked the link below it. 

The code that parses the text fragment is naive. It only supports the &#34;copy link to highlight&#34; current generated in Desktop Firefox and Chrome. The specification has several ways of indicating the text to &#34;highlight&#34;. You can provide the whole text to highlight (this tops out at about a sentence in Firefox). You can provide starting phrase of the text you want to highlight. You can provide start and end phrases bracketing the text to be highlight. I have only implemented the first case. I haven&#39;t found a web browser that implements the phrase parts approach yet.

Since the reply action writes to standard out I pipe the result to a draft of the post I am creating. Long run I way want to add explicit filename support, or to allow a quoted text fragment to be appended to an existing post.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/12/03/using_text_fragments.md" length="2713" type="text/markdown" />
    </item>    <item>
      <title>Upgrade my Raspberry Pi 500+ to Trixie</title>
      <link>https://rsdoiel.github.io/blog/2025/11/28/Upgrade-Pi-500-plus.html</link>
      <description>
        <![CDATA[A brief step by step to upgrading from Raspberry Pi OS 5 to Raspberry Pi OS 6
        for my Raspberry Pi 500+.]]>
      </description>
      <source:markdown># Upgrading my Raspberry Pi 500+ to Trixie

By R. S. Doiel, 2025-11-28

I has a Raspberry Pi 500+ running Raspberry Pi OS 5 (bookworm). It has been a really fun computer for my hobby projects. Raspberry Pi OS 6 is now out (Trixie). The official recommendation to update the OS is to back everything, then re-image the drive. For the Raspberry Pi 500+ the image is on the NVME drive. I don&#39;t have access to boot from Ethernet for the 500+ so how to proceed? One approach is install Pi OS on an SD Card, boot from it to re-image the NVME drive.

Here&#39;s the multi-step process.

1. Using a SD Card and Raspberry Pi Imager, create a boot-able SD card
2. Reboot selecting the SD Card as the boot drive
3. Test Trixie via the SD Card
4. If the tests work out then install Trixie on the NVME drive and reboot

This is pretty straight forward and reminds me a bit of the old days with DOS and CP/M.  The question is step 2. 

I&#39;ve have seen the new boot menu flash by when I boot up the 500+ plus but haven&#39;t payed close attention to it. It came with the original OS install on the Pi 500+.  The boot menu provides an option to select a boot drive. This makes it much easier than the old days when you had to edit files to change the boot drive. 

My missing bit of information was how to interrupt the regular boot sequence to make the selection. The 500+ boots pretty quickly so it was hard to read the text that flashes by. The answer is **to pause the boot process and enter the boot menu start from a cold boot. Power up and press the space bar**. This will present you with some choices. Select the number for the drive and proceed.

## Actual steps

1. Backup home directory on Raspberry Pi 500+ (home was one NVME drive)
2. Confirm firmware is up to date, following the instruction on Raspberry Pi website
2. Confirm I have the latest Raspberry Pi Imager
2. Using Raspberry Pi Imager image an SD Card with latest Raspberry Pi OS 6
3. Power down Raspberry Pi 500+
4. Power up Raspberry Pi 500+ pressing space bar during boot process
5. Select boot from SD Card with fresh OS
6. Setup Raspberry Pi on SD Card, updating to the latest Raspberry Pi Imager
7. Using the Pi Imager on the SD Card, install a fresh image on the NVME drive (NOTE: This erases the whole drive!!!)
8. Then finished, power down the 500+
9. Remove the SD Card from the 500+
10. Boot 500+ from the NVME, complete the usual setup process
11. Restore my home directory backup, install latest Go, Deno, OBNC, etc.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/11/28/Upgrade-Pi-500-plus.md" length="2849" type="text/markdown" />
    </item>    <item>
      <title>Text fragment links as social expression</title>
      <link>https://rsdoiel.github.io/blog/2025/11/17/text_fragment_links_a_social_expression.html</link>
      <description>
        <![CDATA[An exploration of picking up text fragment links in a Markdown document as a means of generating "comment on" feeds.]]>
      </description>
      <source:markdown># Text fragment links as social expression

By R. S. Doiel, 2025-11-17

Some of the attraction of the walled gardens like Instagram, X, BlueSky and Mastodon was the ease at which you can &#34;re-post&#34;, &#34;reply&#34; or &#34;quote&#34;. Automatic linking provides a smooth process. Two or more decades ago this was innovation, today your web browser supports linking via text fragments.

How do I get a text fragment link? Select a phrase or block of text in a web page. Right click to pull up the context menu. Pick the menu item &#34;copy link to highlight&#34;. This will create the text fragment link on your computer&#39;s clipboard. Where should you paste the link?

Pasting the link into a social media site like BlueSky will focus the conversation there. You can also use the link to direct the conversation to your own site. That&#39;s helpful. I think the text fragment link can be more useful than that.

A text fragment link could be a corner stone used to assert a web of our own. The trick is to fully realize the convenience while maximizing the content expressed in the URL. I think it has a place both in the HTML page but also as an integral part of our RSS feeds.

## text fragment link as a content unit

When you choose, &#34;copy link to highlight&#34;, you&#39;re composing a URL. It is a URL with more then a location. It is a URL that holds textual content. The text fragment link&#39;s syntax means you can target it for additional processing without needing to evaluate every other link that might be in the web page[^1]. That begs the question, what does this type of link enable?

[^1]: A text fragment link is implemented in the query string portion of the URL and that query potion starts with &#34;`#:~:`&#34; sequence. What follows is instructions of highlighted text to be referenced. 

Below is an example of a text fragment link that points to another of my blog posts.  It&#39;s pretty hard to read because the important bits are hidden by URL encoding.

~~~markdown
https://rsdoiel.github.io/blog/2025/11/13/urls_for_text_fragments.html#:~:text=While%20the%20URL%20syntax%20for%20text%20fragments%20is%20verbose%20getting%20a%20link%20to%20one%20is%20pretty%20easy%20with%20a%20desktop%20web%20browser%2E%20Here%27s%20the%20steps%20I%20use%20with%20desktop%20Firefox
~~~

Unpacking it you can find the link to the blog post plus the content that had been selected. If you have a url decoder handy you can break it down into its parts. Example, look at the part of that long, horrible url and look closer at the part after &#34;`#:~:text=`&#34;. That&#39;s a description of the quote. You just need to URL decode the spaces and punctuation to recover the text the tells the web browser what to highlight when the web page is opened. Here&#39;s the decoded text.

~~~
&#34;While the URL syntax for text fragments is verbose getting a link to one is pretty easy with a desktop web browser. Here&#39;s the steps I use with desktop Firefox.&#34;
~~~

That quote and link can form the basis of a post, a reply or a comment. It is enough to provide some context to what I want to highlight or discuss at the original URL. That URL can point anywhere on the web, not just some walled garden. Even better I can include that information as an item in an RSS feed. Syndication offers the opportunity for further discussion.

Because of the unique syntax of that type of URL the link can be easily extracted from both HTML documents and Markdown documents. That means I can turn these text fragments into a dedicated feed of things I&#39;ve commented on.

## text fragment links populating a &#34;commented on&#34; feed

On my site I include a recent posts feed. Many blogs and news sites include a &#34;comments&#34; feed too.  The problem with the &#34;comments&#34; feed is you&#39;re handing off editorial control to someone else. This usually leads to an investment in moderation in order to avoid the inevitable arrival of spam and trolls. An alternative approach is to produce a &#34;commented on&#34; feed. I like this approach.

A &#34;commented on&#34; feed comes with advantages over an old fashioned comment system and feed.

- you own your comment, it is on your own site
- your comments are not subjected to someone elses terms of service
- your comments remain easy to reference by linking or feed
- someone else can choose to follow the things you&#39;ve commented on
- if two or more people mutually subscribe to each other&#39;s &#34;commented on&#34; feeds a dialog can occur

Since both reader and writer have to opt in to the conversation I believe you can have higher quality engagement.

## How do you create a &#34;commented on&#34; feed item?

A single text fragment URL contains the location and the content to be highlighted.  What I do with the text fragment depends on my intent. A re-post would just include the highlighted text along with the link to its source. A reply or comment would be formed by adding more. Here&#39;s an example of a reply style post expressed in Markdown.

~~~markdown

&#62; &#34;While the URL syntax for text fragments is verbose getting a link
&#62; to one is pretty easy with a desktop web browser. Here&#39;s the steps
&#62; I use with desktop Firefox.&#34;

[This looks promising](https://rsdoiel.github.io/blog/2025/11/13/urls_for_text_fragments.html#:~:text=While%20the%20URL%20syntax%20for%20text%20fragments%20is%20verbose%20getting%20a%20link%20to%20one%20is%20pretty%20easy%20with%20a%20desktop%20web%20browser%2E%20Here%27s%20the%20steps%20I%20use%20with%20desktop%20Firefox)

~~~

This simple structure can easily be automated. You could create a text fragment handler in any editor that supports macros or plugins.

The point I am trying to drive home is the text fragment itself provides sufficient content to populate a post or an RSS item element. Add additional text and you have something more meaningful. A text fragment can anchor a discussion. The discussion proceeds by exchanging feed items. A feed reader or aggregator can provide conversation threading by following the trail of text fragment links.

If you use a feed oriented content system the social aspect of posts and replies becomes visible through the in bound and out bound RSS feeds. The social context sort of reveals itself naturally. You don&#39;t need a complex social graph represented by activity pub or ATProto. You use trusty old web pages and RSS feeds. The technology does not require change. We simply change how we use it.

## next steps

Today, I can manually create a post using a text fragment link. It would be better if there was some light weight automation around it. Several approaches come to mind.

- a simple tool that accepts a post filename and the text fragment link, it would then format a simple Markdown document and open it in my favorite editor for further editing
- a Markdown processor could collect text fragment links and generate a feed for that purpose
- a text editor plug-in or macro could transform a text fragment link for using as a post

It is an interesting opportunity. Little needs to be created to test the concept. Simple automation can improve the writing experience when handling text fragment links. The technical cost are limited while the practical content benefits could be large. Time for me to add this to my to do list for [antennaApp](https://github.com/rsdoiel/antennaApp).</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/11/17/text_fragment_links_a_social_expression.md" length="7639" type="text/markdown" />
    </item>    <item>
      <title>URLS for text fragments</title>
      <link>https://rsdoiel.github.io/blog/2025/11/13/urls_for_text_fragments.html</link>
      <description>
        <![CDATA[A quick overview of text fragments expressed as URLs.]]>
      </description>
      <source:markdown># URLS for text fragments

By R. S. Doiel, 2025-11-13

A web innovation that I missed was the wide spread support for text fragments expressed as a URL. This is extremely helpful for both citation and quoting sections of a web page in a blog post. The URL syntax is now widely supported by ever green browsers (e.g. Firefox, Safari and Chrome). You can find a nice explanation at [Text fragments](https://developer.mozilla.org/en-US/docs/Web/URI/Reference/Fragment/Text_fragments) on the [MDN](https://developer.mozilla.org/en-US/docs/Web/URI/Reference/Fragment/Text_fragments) website.

The syntax is a little funky. Here&#39;s an example of selecting the second paragraph in my recent blog post [Half-life of Frameworks](https://rsdoiel.github.io/blog/2025/11/07/half-life-of-frameworks.html),

&#60;https://rsdoiel.github.io/blog/2025/11/07/half-life-of-frameworks.html#:~:text=The%20way,choice&#62;

The sequence `#:~:` indicates it is a link to a text fragment. The `text=The%20way,choice` essentially tells the browser to jump to the text block that starts with &#34;The%20way&#34; and ends with &#34;choice&#34;. Using this URL in Firefox will bring up the web page and highlight the text fragment centering it in the view.

It is important to realize that web pages with JavaScript can break this feature. I&#39;ve noticed this on sites like Substack and BlueSky. The link may bring you to the right page but will not behave as expected. Here&#39;s an example of a text fragment link that should work but doesn&#39;t

&#60;https://sarahkendzior.substack.com/notes#:~:text=I%20was%20suspended%20from%20BlueSky%20for%20defending%20the%20honor%20of%20Johnny%20Cash%2E&#62;

This should link to Sarah Kendzoir&#39;s note entry about being banned from BlueSky for her support of rebutting a WSJ article on Johnny Cash. Instead if you want to link to that entry you must resort to a link exposed with the  tiny [&#34;dot dot dot&#34; menu](https://substack.com/@sarahkendzior/note/c-176537092). 

It is not a browser bug. It&#39;s a choice of those who render pages via JavaScript. I get their commercial reasons. One more reason not to link to commercial websites and other walled gardens that break web standards.

## How do you easily generate text fragment link?

While the URL syntax for text fragments is verbose getting a link to one is pretty easy with a desktop web browser.
Here&#39;s the steps I use with desktop Firefox. 

1. navigate to the web page
2. select the text I want a link to
3. using the context menu (e.g. right click with my mouse)
4. click on &#34;Copy Link to Highlight&#34;

I now have a text fragment link saved to my web browser&#39;s clipboard (i.e. the copy buffer). I can now paste it into my Markdown document. These steps work on other popular desktop browsers (e.g. Safari, Orion and Chrome).

On mobile it&#39;s trickier. There isn&#39;t really a context menu like their is on the desktop. I haven&#39;t found a menu that will take a text selection from the web browser and include a &#34;Copy Link to Highlight&#34; option for sharing. That&#39;s shame. While I don&#39;t write blog posts on my phone I do take notes. Maybe mobile OS will catch up to that functionality in the future. I&#39;m not holding by breath.

## text fragment URL possibilities

As commercial social web platforms continue to devolve into muck. The open web can take advantage of features text fragment URLs. This could be an advantage in demonstrating the fluidity of the open web. In my experiments I&#39;ve found it easy to take advantage of text fragment links in Markdown. Their just another link.  If you&#39;re building Markdown processors you could auto-quote the text when you encounter a text fragment link. That&#39;s save some cutting and pasting in the writing process. You could even derive feeds from Markdown documents that include links expressing text fragments.  I&#39;m hoping to have a chance to experiment with these features in my [antennaApp](https://rsdoiel.github.io/antennaApp) project.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/11/13/urls_for_text_fragments.md" length="4226" type="text/markdown" />
    </item>    <item>
      <title>Half-life of Frameworks</title>
      <link>https://rsdoiel.github.io/blog/2025/11/07/half-life-of-frameworks.html</link>
      <description>
        <![CDATA[A discussion of the problems of adopting third part frameworks in programming projects.]]>
      </description>
      <source:markdown># Half-life of Frameworks

By R. S. Doiel, 2025-11-07

[&#34;A Kind of Farewell to the Web&#34;](https://webdirections.org/blog/a-kind-of-farewell-to-the-web/) is an interesting read on front-end development. It&#39;s thoughtful and like other similar blog posts I think important.

Posts like this ask an important question, &#34;What happens when a framework designed to solve a problem is now the problem?&#34;

Software libraries and frameworks often emerge by taking something challenging or tedious and provide a solution that is convenient. If successful, they acquire a specific success problem. The become both a technical solution and a form of cultural identity. This often starts of innocently enough, perhaps a job posting with the framework as a desired skill set. Once the framework is tied to the job though institutionalization sets in. The frameworks is fused into the organizational structure and as a technical solution.  Initially this might seem like a good thing.  Unix I think is a good example of a collection of operating system approaches and libraries that vastly simplified it&#39;s original requirement as a multi user text processing platform. Fast forward to today, and the operating system on your phone, tablet or personal computer to your home thermostat or automobile is running a multi user system. That multi user solution also is, by its nature, vulnerable to attack. Who is controlling your device(s) today is an open question. Is it you, the vendor, a service, an organization like a government or something more nefarious? The success of Unix also created a new set of problems. How do we keep multi user systems safe?  This success issue isn&#39;t unique to operating systems. It holds true for choices of programming languages and the libraries, frameworks and packages associated with them. In my forty years of practice I&#39;ve found it helpful to avoid the cultural linkages when providing software solutions. It&#39;s easy to learn a new library, it&#39;s difficult to change people&#39;s identity, culture and habits. 

The way I avoid it is to first consider the problem I&#39;m trying to solve. Looking at the computing devices, operating system and software languages I have at hand. Account for who I want to collaborate with and their preferences for devices, operating systems and programming languages. Once I know the platform and language take a good look at the standard set of libraries, packages or modules.  If the problem can be **simplified enough** using the standard solutions then they&#39;re usually the right libraries, packages or models to work with.  This has proven true in my experience even when a new shiny package pops on the scene. The new shiny often proves to be but a siren&#39;s song.  That raises the question of why is the standard often the right choice?  

Programming languages, like human languages generally, evolve overtime. Standard libraries tend to evolve with the needs of the language. This results in a relationship of idioms. Idioms make it easier to quickly understand the proposed solution represented in the source code of your program. The idiom turns out to be extremely helpful in communicating with other developers. It is even helpful communicating with your future self.

An often over looked aspect of programming language usage is that it has two audiences. The first audience is people. I think it is the most important. The second is the device and operating system where the program is executed. While the second audience is essential to solving the problem, the first audience is essential to evaluating the solution, implementing the solution and sustaining the correctness of the solution over time. It&#39;s also what us humans interact with. Idioms are important in all human languages included those invented to represent how a computer or computation should proceed. I am not suggesting you never use a non-standard framework. I am suggesting **you must know the cost when choosing a third party framework**.

Another important thing to remember is that choosing a framework doesn&#39;t happen once.  It happens each time you revisit the code. Written programs are like written prose. Almost always the text can be improved. As languages evolve they will pickup features that the communities have already widely adopted in third party frameworks. That is because successful framework address deficits in the language implementation.

jQuery is a good example of a framework addressing browser JavaScript twenty years ago. The jQuery library adopted the CSS selector as a means of identifying elements in the DOM. That single feature made working with the DOM much easier. It also unified and encouraged a closer look at the role of CSS in relationship to the browser experience. JavaScript and CSS came into symbiosis. That feature even became compelling compared to other competing frameworks. The CSS selector syntax eventually became a part of the JavaScript standard DOM implementation, example `let h1Elements = document.querySelectorAll(&#34;h1&#34;);`.  You don&#39;t need jQuery or other framework to work with selectors and the DOM. Today most of the problems jQuery addressed are better solved using plain old JavaScript or CSS.

Recent popular frameworks are also not aging well. Look at the current state of React. jQuery&#39;s success arc is predictive of React&#39;s success arc. Is data binding really an issue today? Web Components are widely supported an address the UI binding issues. Even if you chose React with its limitations it will block your progress towards a better UI. That&#39;s the focus of the article sighted at the start of the post. React has reached problem state. Like jQuery, it&#39;ll still be listed on job requirements or in promotion justifications. It is often recommended when you use a large language model to generate JavaScript and CSS source code. React is beyond it&#39;s half-life.

Today lots of people know React. But does anyone remember [MooTools](https://en.wikipedia.org/wiki/MooTools), [Prototype](https://en.wikipedia.org/wiki/Prototype_JavaScript_Framework) or [YUI](https://en.wikipedia.org/wiki/YUI_Library)?  Anyone remember [Jaxer](https://en.wikipedia.org/wiki/Aptana#Aptana_Jaxer) from Aptana? They were the React of their era. The rise and fall of successful third party frameworks is a cautionary tale.  

By sticking with or returning to the language&#39;s standard modules you accrue an under valued advantage. Languages and the standard libraries tend to remain compatible over long periods of time. Languages grow dormant less frequently than frameworks. The features themselves often see performance improvements or enhancements. A framework beyond its half-life will always struggle to keep up. React, like so many before it, is that that point now.

To be clear, I&#39;m not dogmatically say don&#39;t use a third party framework. I am saying non-standard frameworks have a half-life. You need to evaluate the costs with each 3rd Party framework your project requires. Once you use one you have immediately accrued technical debt. The bill will come due. That the bill comes due far sooner than you&#39;d expect. Always ask can your project afford pay it? Ask yourself how much will it cost to move your source code towards the standard libraries?</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/11/07/half-life-of-frameworks.md" length="7608" type="text/markdown" />
    </item>    <item>
      <title>Is Social Media a Misnomer</title>
      <link>https://rsdoiel.github.io/blog/2025/11/02/is_social_media_a_misnomer.html</link>
      <description>
        <![CDATA[Thoughts on the nature of attention markets versus communication systems]]>
      </description>
      <source:markdown># Is Social Media a Misnomer?

By R. S. Doiel, 2025-11-02

Walled gardens are not normally social. Their walled to keep people inside separated from those outside. The largest social media services on the web are arguably walled gardens.

The largest services run by the likes of Meta, Alphabet, X and ByteDance rely algorithms to determine what a client sees next.  Those algorithms apparent purpose isn&#39;t to facilitate communication but to capture attention. Communication between two or more people could be social but the attention orientation of the algorithms changes the relationship to one of broadcast. 

The practices that have evolved around these systems of likes, dislikes, and re-posting do not serve communication as much as they serve the corporation operating the application. Is shouting a tool of better communication? Is it a symptom of a breakdown in communication? Isn&#39;t this the core purpose of capturing attention in opposition to actual communication?

The web can be a communications medium and can support actual social connection. Intent and how the software is used determine the outcome. The web can be a platform to host conversations. It is not, nor should it be, the only means of social contact. It is not, nor should it be, the only way we communicate.

Maybe this is why I&#39;ve gravitated away from social media platforms. Communication isn&#39;t happening there when the platform reverts to the broadcast model. The broadcast model thrives on the game of attention capture. Being captured is a lonely experience even when we are with other captives.

We&#39;ve been at this choice point before. In the age of physical newspapers a paper&#39;s issue didn&#39;t grow after publication. The amount of attention it could capture was limited. There might be a new issue published but a single issue could only capture your attention for the time it took to read the parts you were interested in. In fact most people didn&#39;t read the whole paper. Most people read the sections they were interested in. The paper was designed as a broadcast medium to make that easy. Example, in North America the Sunday funnies were there for the children to read. Papers were used for antisocial purposes too. They came to be used to encourage wars, enraging people to action, as a platform for propaganda. In my region of North America the Hearst corporation was famous for this. Enough monetary value was acquired by William Randolph Hearst he built a castle on the West Coast of California. In that era the concept of &#34;broadcasting&#34; was was already understood.

Even with the introduction of radio and television, the broadcast initially wasn&#39;t twenty for hours a day seven days a week. As a result there were time periods where assumptions were made about the audiences watching and &#34;content programming&#34; were directed in that way. The content programmer&#39;s goal was to gain audience in the form of viewers and listeners. The lessons learned in the old media were applied again in the new media. Example, &#34;if it bleeds it leads&#34;. The buying and selling of attention was perfected further. 

Each time it became cheaper to &#34;publish&#34; we&#39;ve had an opportunity to choose how the medium evolves. We keep choosing attention consumption and we reap the results of that. If we accept monetization we collectively choose a broadcast platform. Does it need to remain that way? I think we are at a new choice point.

A recent innovation is the financialization of large language models. They are structured as centralized broadcast platforms just like before. They has been promoted under the banner of &#34;AI&#34; or &#34;Artificial Intelligence&#34;. There is a cost to this. Like before the interest of the people are being swept under the carpet in the quest of monetization interests of the platform owner. Our walled garden social media is an expression of that choice. We feel its impact. Few are asking the serious questions. Some ask important question only to have that leveraged to capture more attention. Real conversations are needed.

Large language models are expensive. The models have literal costs in terms of physical space to house the computes, network, and cooling systems that form data centers. Those data centers consume energy that could be used by homes, villages, towns and cities. Is this an acceptable use of resources? The financialized of language models have a human cost too. They are extremely effective attention consumption machines. That already is being abused (examples encouraging suicide, aiding crime and propaganda). To used Cory Doctorow&#39;s terminology, AI has hit the enshittification point. The bubble will burst. There will be a price paid collectively as a result. We have choices to make. How do we minimize the cost individually that we pay collectively as a result of the folly of a few people chasing yet more profit.

When communications technology becomes financially interesting and then becomes financialized it moves from socially useful to the broadcast model.  The broadcasts model is dominated by the requirement selling our attention. It does not have the requirement of facilitating actual communication. Corporate interests are to financially benefit those controlling the corporation and to absolve them of responsibility for negative out comes. Broadcast medium need another countervailing force aside from financialization to remain a collective good. As we enter the second quarter of the twenty first century we have choices to make. If we don&#39;t make them explicitly they will be made for us. Is the communication, the act of connecting people to people, more important then making money? Can broadcast be used as a force of good when we rely only on financialization to implement it? How do we choose to use the technology is how we express that decision?

Is our current social media system a misnomer? Does the integration of large language models show this off? Is it really a broadcast platform? Do we want to participate in the attention economy where we are tenant and product? Is it time to call it something other than &#34;social media&#34;?</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/11/02/is_social_media_a_misnomer.md" length="6440" type="text/markdown" />
    </item>    <item>
      <title>Simple websites using Markdown</title>
      <link>https://rsdoiel.github.io/blog/2025/10/14/simple_websites_using_Markdown.html</link>
      <description>
        <![CDATA[Markdown allows many more people to participate in creating content for the web. The [Antenna App](https://rsdoiel.github.io/antennaApp) is a tool for creating Markdown focused websites. This post explores creating a simple website using Antenna App,
        your favorite text editor and your web browser.]]>
      </description>
      <source:markdown># Simple Websites using Markdown

&#62; The [Antenna App](/antennaApp) is shaping the way I write and build websites

By R. S. Doiel, 2025-10-14

The Web is a __networked hypertext system__. A human friendly way of expressing hypertext is [Markdown](https://en.wikipedia.org/wiki/Markdown). Markdown has allowed many people to participate in creating content for the web. I think it deserves more focus. Between Markdown and a sprinkle of CSS and you can create entire websites using your own computer or hosted on a static site service[^1]. In this post I show how Markdown can be used for basic websites[^2]. All it requires is knowing a little Markdown, a text editor, web browser and the [Antenna App](https://rsdoiel.github.io/antennaApp).

[^1]: Examples: [GitHub Pages](https://pages.github.com), S3 buckets setup as static websites at AWS, Dreamhost, my favorite [Mythic Beasts](https://www.mythic-beasts.com/)

Demonstrated in this post:

- A means of creating a web page (HTML page) using Markdown and the Antenna App
- A means of creating a simple multi page website with navigation using an Antenna App theme
- A means of styling the web site using CSS expressed as part of an Antenna App theme 

[^2]: Antenna supports creating many types of websites. Examples include [microblogs](https://en.wikipedia.org/wiki/Microblogging), [blogs](https://en.wikipedia.org/wiki/Blog), [linkblogs](https://en.wikipedia.org/wiki/Linklog), [wikis](https://en.wikipedia.org/wiki/Wiki) and hybrids of these. Pretty much what you need to be social on the web without resorting to the silos of Big Corp.

## What you need to get started

- A computer
- A terminal app
- A text editor
- A web browser
- Antenna App

Chances are you have the first four if you&#39;re reading this. What I describe in this ports should work on computers running Windows, macOS and Linux based systems. If you need to acquire a computer and already have a HDMI television I recommend getting a Raspberry Pi Computer. The Raspberry Pi 500 Kit is a nice middle range device that runs about $120.00 US. It includes a book about using Raspberry Pi computers. If you can afford a bit more the Raspberry Pi 500+ Kit should be out by Christmas 2025 and that&#39;ll probably run in the ballpark of $240.00 US. That&#39;s a nice computer with lots of storage and the bells and whistles to do image, audio and light video work. If you are on a budget then you can squeak by with a Raspberry Pi Zero 2W running the software. That setup will set you back about $50.00 US with the minimum accessories needed like case, keyboard, mouse, power supply and cables to connect things up. The Official Raspberry Pi Handbook print edition runs between $25.00 US but the electronic edition that you can read on your phone or tablet is free. My point is that for a little bit of investment or using your existing resources you have the means of building websites for yourself and others. You don&#39;t need to be a programmer or even a web designer if you are comfortable editing text and doing a bit of writing. 

The [Antenna App](https://rsdoiel.github.io/antennaApp) is software I wrote to prove to myself that building websites could be made simpler. I currently use it to curate my own websites include my blog and personal news site, &#60;https://rsdoiel.github.io/antenna&#62; and &#60;https://rsdoiel.github.io/antennaApp/INSTALL.html&#62;.

## Installing Antenna App

If you need to install the Antenna App you need to use your terminal to run the following command on macOS and Linux like systems.

~~~shell
curl https://rsdoiel.github.io/antennaApp/installer.sh | sh
~~~

or on Windows in Powershell you can use this command.

~~~shell
irm https://rsdoiel.github.io/antennaApp/installer.ps1 | iex
~~~

You can find detailed instructions in getting the latest version of the Antenna App at &#60;https://rsdoiel.github.io/antennaApp/INSTALL.html&#62;.

## Build a simple website

On your computer open the Terminal application. The basic steps you&#39;ll be taking are the following.

1. Create a project directory, &#34;simple&#34;, and change into it
2. Use the Antenna App to setup your site using the &#34;init&#34; command
3. Create a home page in Markdown called &#34;index.md&#34;
4. Generate the HTML page, &#34;index.html&#34; using Antenna
5. Preview your website in your web browser

Here is what you type in the terminal for steps one through two.

~~~shell

mkdir simple
cd simple
antenna init
~~~

If you look at the contents of that directory.

~~~shell

ls
~~~

You should see the following files in the simple directory.

~~~shell

antenna.yaml
page.yaml
~~~

These files are used by Antenna App to generate your website. The &#34;antenna.yaml&#34; is the main configuration file. The &#34;page.yaml&#34; file is used by Antenna App to know how to construct an HTML page.

For step three we need to use a text editor. On Raspberry Pi mousepad is the the default editor. On Windows it is notepad and macOS its textedit. Many people use [VS Code](https://code.visualstudio.com/download) editor which is available on macOS, Windows and Raspberry Pi OS. That is what I&#39;ll be using in my examples. You can change the &#34;code&#34; references to the editor you prefer.

On my Raspberry Pi I can run the editor creating &#34;index.md&#34; when I save it. Here is the command I type into my terminal to do that.

~~~shell

code index.md
~~~

In the editor window you should see an empty file. Type in the following and save the file as &#34;index.md&#34; in the simple directory.

~~~markdown

Hello World! 

~~~

Now we want to use Antenna App &#34;page&#34; command to turn that &#34;index.md&#34; into our home web page. This is the command you type into your Terminal window.

~~~shell

antenna page index.md
~~~

If you check the contents of the simple directory.

~~~shell

ls
~~~

You should see three four files now.

~~~

antenna.yaml
index.md
index.html
page.yaml
~~~

We can new preview the website by run the Antenna App&#39;s &#34;preview&#39; command. This command runs a little web server on your computer so you can view the results of your handy work in your web browser.

~~~shell

antenna preview
~~~

Started your web browser and open the URL &#60;http://localhost:8000&#62;.  You should see your Hello World homepage. If you open &#60;view-source:http://localhost:8000&#62; you&#39;ll see the HTML source generated by the &#34;page&#34; command. When you are finished go back to your Terminal window and hold down the control key and press the &#34;C&#34; key (Ctrl-C). This exits the preview command. If you reload the pages in your web browser you&#39;ll get your browser&#39;s default page indicating that the pages are not available.

### Expanding our site

Our site isn&#39;t useful website yet. There are no links. Modify &#34;index.md&#34; file to look like this.

~~~markdown

# Hello World!

Learn about [Markdown](https://en.wikipedia.org/wiki/Markdown) on
John Grubber&#39;s website &#60;https://daringfireball.net/projects/markdown/&#62;.

~~~

Once you&#39;ve saved this revision use the page command to update the HTML. After that run the preview command like before. Switch back to your terminal and type the following.

~~~shell

antenna page index.md
antenna preview
~~~

Refresh (reload) your web browser page for the URL &#60;http://localhost:8000&#62;.  Our &#34;Hello World&#34; homepage, &#34;index.md&#34;, now should be transformed with &#34;Hello World&#34; displaying as a heading and our added paragraph with links to Wikipedia and Daring Fireball websites. Refresh the &#34;view-source&#34; version of the pages and see the additional HTML markup generated by the page command.

### Elaboration

A single page website is very limited.  You can create more pages going through the process we used for &#34;index.md&#34;. Here&#39;s the basic sequence.

1. create or edit the markdown document in your editor, save it
2. use the page command to render the HTML version
3. use the preview command and your web browser to see your handy work

Let&#39;s add a page called &#34;fruit.md&#34; with a list of heading and list fruits.

~~~shell

code fruit.md
~~~

Here&#39;s the text of the fruits.md page.

~~~Markdown

# Fruit

- Dragon Fruit
- Lime
- Mango

~~~

You turn the page into HTML using the page command and then preview the website again

~~~shell

antenna page fruit.md
antenna preview
~~~

You have two pages in your website now. The URL are

- http://localhost:8000/index.html
- http://localhost:8000/fruit.html

Look at each in your web browser. Notice the pages aren&#39;t linked in anyway. That&#39;s not ideal. What we need is an easy way to navigation the main pages of our site. Antenna App supports a concept called themes. This let&#39;s you define how a page is generated through assigning content to specific parts of the page. One of the parts of an Antenna generated page is called &#34;nav&#34; and it is expressed as Markdown. Other parts include header, footer, top_content and bottom_content (the last two are before and after the section holding the contents our   original Markdown documents). 

### theming

To create a theme we need to first create a directory. Then we need to include the desired parts in the theme. Let&#39;s create a theme that has the nav element for our website that will make it easy to switch from the homepage to the fruit page. The steps we&#39;ll take to create our theme follows.

1. create a directory called &#34;theme&#34;
2. create a file in the &#34;theme&#34; directory called &#34;nav.md&#34;
3. Add two links using Markdown to &#34;nav.md&#34; and save the file
4. &#34;apply&#34; our theme
5. regenerate our HTML pages and preview the site

Here&#39;s what I&#39;d type in the terminal for steps one and two.

~~~shell

mkdir theme
code theme/nav.md
~~~

The content to type into &#34;nav.md&#34; is as follows.

~~~Markdown

[home](index.html) [fruit](fruit.html)
~~~

Save the result then in the terminal window type the following to finish steps four and five.

~~~shell

antenna apply theme
antenna page index.md
antenna page fruit.md
antenna preview
~~~

You can now refresh the browser pages and exploring our site by clicking on the links provided by the navigation.

## Antenna Themes

An Antenna App theme is defined as a directory with Markdown files to express the visible common HTML elements. Other non-visible elements in the head element of an HTML page are also able to be set with in a theme. Those aren&#39;t expressed in Markdown. Here&#39;s the names of files which are recognized by the Antenna App as valid parts of a theme.

- header.md
- nav.md
- top_content.md
- bottom_content.md
- footer.md
- head.yaml
- style.css

The files ending with &#34;.md&#34; are Markdown files. Markdown describes the content we want to include in those sections of our webpage. Any Markdown content can be used. If one of the files does not exist in the theme then that element is not populated in the resulting page. Example if you include nav.md then navigation will be included otherwise it will not be included. The page order of the Markdown elements are 

1. header.md
2. nav.md
3. top_content.md
4. The content of the Markdown document used with the page command
5. bottom_content.md
6. footer.md

When we create a file like &#34;index.md&#34; and &#34;fruits.md&#34; the resulting HTML is inserted between the &#34;top_content&#34; and &#34;bottom_content&#34; sections of the webpage. 

To keep things simple I started by showing you how to create the &#34;nav&#34; element using the &#34;nav.md&#34; site your theme folder.
The theme name can be anything, it&#39;s just a directory. Here&#39;s the basic steps in creating a new theme

1. create a directory to hold the theme parts
2. Inside your directory create Markdown documents for the parts of the them you want defined
3. Apply the theme by using the directory name and the apply command 
4. Generate or regenerate HTML for each page in your site
5. Preview the site

Editing the theme is just a matter of adding, editing or removing elements from the theme folder. The two elements that are not Markdown documents are &#34;style.css&#34; and &#34;head.yaml&#34; if you choose to include them. The &#34;style.css&#34; file should contain valid CSS.
The &#34;head.yaml&#34; gives you find grained control of the meta, link and script elements that get rendered into the head element of your web page.

### Spicing pages up with CSS

So far the website is functional but relies on the defaults provided by your web browser. The visual appearance can be enhanced through using CSS. CSS is a language that describes to the web browser how it should layout the HTML page contents. The Antenna App&#39;s themes supports embedding a style element in the HTML page&#39;s head element by including a &#34;style.css&#34; file inside the theme directory. Let&#39;s do that now.

Use your terminal to create &#34;theme/style.css&#34;

~~~shell

code theme/style.css
~~~

Paste in the follow CSS. It&#39;ll turn your top level page titles vertical using CSS.

~~~css

/* Turn the H1 elements vertical */
h1 {
  writing-mode: vertical-rl;
  transform: rotate(180deg);
  text-orientation: mixed;
}
~~~

After creating and save the &#34;style.css&#34; you theme directory should have two files.

~~~

theme/nav.md
theme/style.css
~~~

You can now apply the theme, regenerate the HTML pages and preview them.

~~~shell

antenna apply theme
antenna page index.md
antenna page fruits.md
antenna preview
~~~

When you preview the site you should see the effect the CSS had on how the page displayed. The changes to layout using CSS can be very elaborate and CSS is a topic I encourage you to explore on your own. A good reference and tutorial website for CSS is &#60;https://developer.mozilla.org/en-US/docs/Web/CSS&#62;. A historic example of what people have done with CSS can be seen at [CSS Zen Garden](https://csszengarden.com/). A web search on [DuckDuckGo](https://duckduckgo.com?q=learning CSS) or other search engine will turn up lots of additional resources. There are also book available through [Open Library](https://openlibrary.org/search?q=CSS&#38;mode=everything).

## Next steps

- Create a few more pages for your website using Markdown for the text content
- Explore adding additional theme elements like &#34;header&#34; and &#34;footer&#34;

Happy site building!</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/10/14/simple_websites_using_Markdown.md" length="14516" type="text/markdown" />
    </item>    <item>
      <title>Computer related terminology that bugs me</title>
      <link>https://rsdoiel.github.io/blog/2025/10/01/words_that_bug_me.html</link>
      <description>
        <![CDATA[AI, Artificial Intelligence, Cyber\* are poor choices of terms but pickup because
        they marketed well.]]>
      </description>
      <source:markdown># Computer related words that bug me

Sometimes specific terms bug me on popular usage. This is particularly true when related to computer science (or better term, information science). 

Two terms really grate on my nerves these days.

1. AI was a marketing term to get the DOD to fund researching in to cognitive approaches to computation.  The generals like the term Artificial Intelligence in the 1970s because the idea of robot solders who could be destroyed with out the having to write the letters to their parents was desirable after Vietnam and previous wars. For the researchers it was sufficiently vague to be a goldmine of grant possibilities.
2. Information Security became Cyber Security for marketing reasons

Terms that change for marketing reasons quickly loose meaning. In the computers as communication devices space this isn&#39;t a good feature.

The stupidest phrase I&#39;ve heard a politician and business leaders say in interviews, &#34;We&#39;ve got to have cyber&#34;.  In my head I change &#34;cyber&#34; to &#34;coffee&#34; so I don&#39;t think have to remember their names.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/10/01/words_that_bug_me.md" length="1399" type="text/markdown" />
    </item>    <item>
      <title>Getting started with Go</title>
      <link>https://rsdoiel.github.io/blog/2025/09/22/getting_started_with_go.html</link>
      <description>
        <![CDATA[A quick intro to Go and tool chain]]>
      </description>
      <source:markdown># Getting started with Go

By R. S. Doiel, 2025-09-22

This is just a quick tutorial covering installing the Go compiler, tool chain and learning the basics through creating three programs -- hello world, a web server and a Markdown server. It is not a [tutorial](https://go.dev/learn/#selected-tutorials) on the Go language. The [Go website](https://go.dev) has that covered. This is really just the bare minimum to get started with the `go` command. 

Here&#39;s the files source files we&#39;ll be using in this tutorial

- [helloworld.go](helloworld/helloworld.go)
- [webserver.go](webserver/webserver.go)
- [markdown.go](mdserver/markdown.go)
- [markdown_test.go](mdserver/markdown_test.go)
- [handler.go](mdserver/handler.go)
- [handler_test.go](mdserver/handler_test.go)
- [mdserver&#39;s main.go](mdserver/cmd/mdserver/main.go)


## Installing Go and making sure it works

 It covers the basics of how the Go compiler and tool change work by walking throw three Go programs. The first one is to create a hello world program to make sure Go is available and working. The steps will be as follows.

1. Download and install Go, see &#60;https://go.dev/dl/&#62; (latest is currently 1.25.1)

## Creating hello world, making sure Go tool chain is working

1. Open a terminal
2. create a directory/folder you&#39;ll use for you creating a hello world program
3. Change into that directory
4. Type in the hello world program below and save it as &#34;hello.go&#34;
5. Try running &#34;hello.go&#34;
6. Follow the instruction to &#34;build&#34; the hello.go program (you&#39;re compiling the Go source into an executable)
7. Run you newly created executable

Here&#39;s the steps we&#39;ll take starting with step #3.

~~~shell

mkdir helloworld
cd helloworld
edit hello.go
go run hello.go
go build hello.go
./hello
cd ..
~~~

Here&#39;s the source code to hello world in Go

~~~go

package main

import (
    &#34;fmt&#34;
) 

func main() {
    fmt.Println(&#34;Hello World!&#34;)
}
~~~

If you got this far you&#39;ve created your first Go program.

What are the parts of the source code package to take note of?

`package main`
: Identifies what &#34;package&#34; or module that the source file belongs to. The &#34;main&#34; package is special in that it what represents a stand alone program.

`import`
: This is a statement (including the contents inside the parenthesis) that shows what modules your program relies on.

`func`
: This defines a functions, in this case the main function.

`&#34;fmt&#34;`
: This is the name of the module for formatting text, includes functions like Println that display a line of text.

Things to keep in mind.  Go is a typed language. Variables and constants can be declared explicitly or be implicitly used on their first assignment.  Scope is block level like Python, TypeScript or ES6 when using &#34;let&#34;.

Capitalized functions, variables and constants are &#34;exported&#34; from the current module and can be referenced when that package is later imported. When we use the imported &#34;fmt&#34; package with the &#34;Println&#34; function, the fact that it is capitalized means it can be used in the scope of our package. Other things inside of &#34;fmt&#34; which are not capitalized don&#39;t get exported and are not visible inside the current package scope. Capitalization is like the &#34;export&#34; keyword in TypeScript or JavaScript.

Packages and modules are Go&#39;s reusable unit of code. Originally Go called them &#34;packages&#34;, hence the &#34;package&#34; declaration in our program.  There are some technical distinctions. A packages are a collection of Go files in a directory.  They don&#39;t necessarily carry dependencies in them. This can be an issue for you are importing packages that might break between versions. Modules are implemented using packages but carry the dependency information with them so you compiles are consistent regardless of the latest release of a package that was imported. See the &#34;mod&#34; command below.

Let&#39;s look at the go command before looking at the source code. The go command is actually a collection of tools. The commons I used most frequently are the following.

run
: Run the main package Go file

build
: This compiles the program into an executable. It can also be used to cross compile if you set some environment variables

fmt
: Format the source code to a standard set by the Go community

vet
: Perform analysis on your Go source file(s) and indicate potential issues

We&#39;ll use the following two when we build our web server since it&#39;ll be constructed as a module.

mod
: Setup and maintain go module (this is be covered later)

test
: Test a go module (e.g. files that end in &#34;_test.go&#34;)

## Build a static web server

We&#39;ll be doing the following next.

1. Create a new directory for our web server
2. Change into it.
3. Go to the http package docs, &#60;https://pkg.go.dev/net/http#example-FileServer-DotFileHiding&#62;
4. Copy the example for the dot file hiding file server and save as webserver.go
5. Create a hello world index.html file.
6. Use Go &#34;run&#34; our static web server

~~~shell

mkdir webserver
cd webserver
edit webserver.go
edit hello.html
go run webserver.go
# You can open your web browser to http://localhost:8080/hello.html
cd ..
~~~

## Build a Markdown web server

1. Create a new directory called &#34;mdserver&#34;
2. Change into it
3. Initialize the Go module we&#39;ll build (e.g. &#39;github.com/rsdoiel/mdserver&#39;, you&#39;d use something that makes sense for you)
4. Add a Go modules for working with Markdown, &#34;github.com/yuin/goldmark&#34;
5. Write a function to convert Markdown into HTML in a file called markdown.go
6. Write a test function called MarkdownToHTML in a file called markdown_test.go
7. Run the test to make sure it works
8. Write a the function MarkdownHandler function and save it markdown.go
8. mkdir called &#34;cli&#34;
9. Copy &#34;../webserver/webserver.go&#34; to &#34;cli/mdserver.go&#34;
10. Update &#34;cmd/mdserver/mdserver.go&#34; to import our mdserver package
11. Wrap the dot file handler function with our MarkdownHandler function
12. Using go run to test our web server

~~~shell

mkdir mdserver
cd mdserver
go mod init &#39;github.com/rsdoiel/mdserver&#39;
go mod tidy
go get &#34;github.com/yuin/goldmark&#34;
edit markdown.go
edit markdown_test.go
go test
# Now we&#39;re going to create our handler function and test of it
edit handler.go
edit handler_test.go
go test
# Now let&#39;s create our wrapping cli
mkdir -p cmd/mdserver
copy ../webserver/webserver.go cmd/mdserver/main.go
# Import our mdserver package and wrap the dotFileHandler function with the Markdown function
edit cmd/mdserver/main.go
go run cmd/mdserver/main.go
~~~


We can make a mdserver executable using the build command and specify an output name.

~~~shell

go build -o bin/mdserver cmd/mdserver/main.go
~~~

You&#39;ve not create Go program that is defined by a module/package &#34;mdserver&#34; and runs that package in a command line program. What&#39;s left is to learn the language itself, get familiar with the standard library and write more Go programs.

## Next Steps

- Go through some of the Go.dev tutorials to better understand Go
  - See [Go by Example](https://gobyexample.com/), [Effective Go](https://go.dev/doc/effective_go) and [Awesome Go](https://awesome-go.com/)
  - Familiarize yourself with the [standard library](https://pkg.go.dev/std#section-directories)
- Try adding a YAML configuration to our two web server example, [gopkg.in/yaml.v3](https://pkg.go.dev/gopkg.in/yaml.v3)
- Try out cross compiling to other computer platforms
  - See [Go command documentation](https://pkg.go.dev/cmd/go#hdr-Compile_and_run_Go_program) and &#60;https://rakyll.org/cross-compilation/&#62;</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/09/22/getting_started_with_go.md" length="7738" type="text/markdown" />
    </item>    <item>
      <title>RSS 23 years on</title>
      <link>https://rsdoiel.github.io/blog/2025/09/11/RSS_23_years_on.html</link>
      <description>
        <![CDATA[Reflection on Dave Winer's recent post "It's Really Simple"]]>
      </description>
      <source:markdown># RSS 23 years on

By R. S. Doiel, 2025-09-11

I think Dave&#39;s post, [It&#39;s Really Simple](http://scripting.com/2025/09/11/123936.html?title=itsReallySimple), is an important read. It&#39;s shortness lends itself to reflection. **Dave takes the long view.** I think that is very healthy for the web. It&#39;s a view I&#39;ve increasingly embraced over the last decade or three.

Dave&#39;s post points out 23 years after the introduction of RSS we could have seen much more creativity. In 2025 the primary contribution of web corporations is to stifle innovation. The &#34;network effect&#34; is their business model.They insure we&#39;re are both tenant and product. Is the solution to make complex standards that embrace the characteristics that enabled the walled gardens in the first place? I think RSS 2.0, twenty three years later, still provides a better path forward.

Do we want to be both tenant and product? If so than perhaps ATProto or ActivityPub make sense. RSS&#39;s path is far simpler. I don&#39;t think it is accidental that both Mastodon and BlueSky embraced RSS output. In 2025 lots of languages come with mature RSS libraries. Even if you need to write your own the spec is simple enough to do so.

Here are three systems I think show the longevity and possibility of RSS. Dave Winer&#39;s [FeedLand](https://feedland.org), Matt Mullenweg&#39;s [WordPress](https://wordpress.com), and Andrew Shell&#39;s port of rssCloud support from Frontier to a standalone server, [rssCloud v2](https://rpc.rsscloud.io/)[^1]. The FeedLand and WordPress show us user facing content systems with real inter-opt. Andrew&#39;s rssCloud shows us how to build a real time RSS service without tying it into a content management system directly. These are examples of a social web we can own. We&#39;re just not calling them that.

[^1]: See Andrew&#39;s notes on [his blog](https://blog.andrewshell.org/notes/rsscloud-server/) or checkout the GitHub repository at [github.com/rsscloud/rsscloud-server](https://github.com/rsscloud/rsscloud-server)

&#62; RSS 2.0 is for the long view. As the car commercial used to say, &#34;it&#39;s built to last&#34;.

I think BlueSky and Mastodon missed the boat skipping support for RSS 2.0 as input. I think part of the problem was how they were guided to re-implement a solution from walled gardens as an open source project. A common characteristic of systems developed by X, Meta and Google was complexity. For those companies the complexity was useful. I don&#39;t think we need that complexity if we are not building walled gardens. This missed opportunity was to assumed the &#34;network effect&#34; was the goal. I think communication is the goal. It can exists without scaling out to centralized walled gardens.

Here&#39;s a thought experiment, &#34;Do complex protocols lend themselves to a diversity of implementations? Are they better suited to preservation?&#34; I think the answer is no. I can read and process a tab delimited file from the 1970s or a comma delimited one from the 1980s. Why? It&#39;s a simple format. Simple is preservation friendly. I am certain I&#39;ll be able to read and process an RSS 2.0 file in 2050. Why, it&#39;s a simple standard. I don&#39;t think I&#39;ll be able to do that with the complex alternatives. My guess is they fall out of favor overtime and fade precisely because they&#39;re complex. Simple is a built-in preservation quality.

RSS 2.0 is stable and extensible. It has enabled Podcasting which Apple, YouTube and Spotify have yet to wall off. Podcasting, whether audio, video or text, offers us a way out. RSS allows us to say goodbye to the gate keepers like X, Meta, Google, Apple et el. We don&#39;t need a __Web 3.0__. What we need to embrace is the web and internet as common nouns. Write and deploy from our own machines. Connect them through the networks and eventually land content on the __Web__ or __Internet__ based on open standards and inter-opt. Let&#39;s skip the business models of wall gardens this time around.

When people mention the foundations of the web they typically mention HTTP, HTML, CSS and JavaScript as if their are just four pillars of the web. I think they are missing a fifth column, RSS 2.0. It provides the proven means of an open syndication system that continues to work after 23 years. 

The final reflection I&#39;d like to focus on in Dave&#39;s post is Markdown. Markdown is a hypertext document format that is easier to type and proof read than HTML. I think it is where an alternative to the walled gardens can shine.  It works as an enclosure in the [Textcasting](https://textcasting.org) context.  Markdown with front matter allows non-programmers to manage the metadata associated with their documents be they blog posts or academic papers[^2]. It&#39;s a simple format, simpler than HTML. It&#39;s a format that lends itself to preservation in much the same way RSS 2.0 does.

[^2]: An example is R-Studio and RMarkdown or Jupyter Notebooks used in the academic and research science community. Functionality and features are enabled or activated by the metadata presented as front matter encoded using [YAML](https://yaml.org/).

In my own modest effort to create [the writing environment I want](https://rsdoiel.github.io/antennaApp &#34;antennaApp is a feed oriented static content system build around Markdown and RSS 2.0&#34;), I have relied on Markdown and RSS 2.0. Why? Because RSS 2.0 is a stable spec. I&#39;ve embraced [CommonMark](https://commonmark.org) alongside with John Grubber&#39;s [Markdown](https://daringfireball.net/projects/markdown/) for writing because I can proof read it easier than proof reading HTML. It&#39;s a format you can read without conversion and if you do want to render as HTML that is easy to do too. With Markdown I can start something on my phone in a simple text editor. I can add that document to my blog to be rendered as HTML or into an RSS 2.0 feed. When I do so it all works because of inter-opt provided by RSS 2.0.

The web of 2025 can be open if we choose to open it up. Embrace not the &#34;Web&#34; as proper noun owned by corporations but the &#34;web&#34; as a common noun which we can also own a part. Step out the silos and see the wild lands of the real web. See their beauty and create them for yourself.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/09/11/RSS_23_years_on.md" length="6376" type="text/markdown" />
    </item>    <item>
      <title>Two recent articles on working with Large Language Models</title>
      <link>https://rsdoiel.github.io/blog/2025/09/10/Two_Recent_Articles_on_LLM.html</link>
      <description>
        <![CDATA[Two online articles discussing application and challenges in using large
        language models for generative work (e.g. generating software source code).
        They are written in clear non technical language.]]>
      </description>
      <source:markdown># Two recent articles on working with Large Language Models

By R. S. Doiel, 2025-09-10

I&#39;ve recently read two articles on Large Language Models which I think explain clearly some of the issues around using LLM in developing software.

- [AI: Why generative AI hallucinate. all the causes you should know](https://paolozaino.wordpress.com/2025/09/09/ai-why-generative-ai-hallucinates-all-the-causes-you-should-know/) by Paolo Fabio Zaino
- [Building AI-Resistant Technical Debt](https://www.oreilly.com/radar/building-ai-resistant-technical-debt/) by Andrew Stellman

What I liked about these articles is they present the challenges in clear language that is readable by both software writer and lay people who are familiar with using a computer. The later is really helpful when collaborating with non-developers or with management. I think we&#39;re still in the &#34;AI is hammer and all problems are nails&#34; phase of the hype cycle around large language models and these types of articles help break the spin. Nicely done.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/09/10/Two_Recent_Articles_on_LLM.md" length="1718" type="text/markdown" />
    </item>    <item>
      <title>Build a Blog with Antenna</title>
      <link>https://rsdoiel.github.io/blog/2025/09/05/Build_a_Blog_with_Antenna_App.html</link>
      <description>
        <![CDATA[This is a short post on using antennaApp to build a traditional blog.
        All you need is antennaApp, a text editor and a little knowledge of
        Markdown.
        
        antennaApp uses the front matter expressed as YAML as metadata for
        processing the blog post and collecting the metadata for rendering
        a page listing all the posts (an aggregation of posts) as HTML and
        as an RSS 2.0 feeed.
        
        HTML pages can be customized for your sight in a simple YAML
        configuration file.]]>
      </description>
      <source:markdown># Build a Blog with Antenna

By R. S. Doiel, 2025-09-05

Antenna is a feed orient content management tool. It can use to build a and run a blog. What follows are the steps needed to create a simple blog using only Antenna application, Markdown and the Antenna YAML configuration files.

## Setting up

The first thing we need is a directory to hold our website. That can be done from a terminal on Linux, macOS or Windows using the following command.

~~~shell
mkdir myblog
~~~

We want to change into that directory.

~~~shell
cd myblog
~~~

Now we&#39;re ready to begin.

## Initializing the blog

The Antenna application, `antenna` has an &#34;initialization&#34; action. This creates the configuration file needed for your blog. Here&#39;s what you need to type in the terminal (same for Linux, macOS and Windows).

~~~shell
antenna init
~~~

This will result in a file called &#34;antenna.yaml&#34; being created. If you are using macOS or Linux you can type the following and see it.

~~~shell
ls -l
~~~

On Windows you would use the `dir` command instead.

~~~shell
dir
~~~

## Defining our blog collection

A blog is built from three parts

- file system path(s) holding the blog posts[^1]
- One or more HTML pages showing the available posts in reverse chronological order
- An RSS feed of the recent blog posts

[^1]: The file paths to posts can be whatever you want. Antenna doesn&#39;t impose an structure. Traditionally a structure broken down for year, two digit month and another for two digit days is common. So posts are contained in a director called &#34;blog&#34; it&#39;ll have a path broken to by year, month and day. The day directory holds the blog post. Example I have a post called &#34;updates.md&#34; for the date August 5th, 2025. That might be held in `blog/2025/08/05/updates.md` Markdown file.

In terms of Antenna these are contained in a collection which needs to be defined. Then
we use the post action to add Markdown documents, with front matter, as blog posts. The post
action also generates an HTML version of the post based on the settings in the front matter. Finally
the RSS feed and HTML aggregation page are render by Antenna&#39;s generate action. That&#39;s pretty much it.

Let&#39;s first create our collection. I am going to call it &#34;index.md&#34;. The reason I call the collection &#34;index.md&#34; is it&#39;ll result in an HTML page called &#34;index.html&#34;, a RSS file called &#34;index.xml&#34; a configuration page page called &#34;index.yaml&#34; and an SQLite3 database file called &#34;index.db&#34;.  Here&#39;s an example of a Markdown document defining the blog.

~~~markdown
  ---
  title: My blog
  ---

  # Welcome to My Blog

  This is My blog where I use the Antenna app to curate a simple blog.
  The Markdown forms the definition of the &#34;index.md&#34; collection. The blog
  will be managed in the &#34;index.db&#34; SQLite3 database. It can be configured by
  modifying the &#34;index.yaml&#34; file generated when this collection is added to the
  Antenna configuration using the &#34;add&#34; action.
~~~

That&#39;s all that is needed, save this Markdown document as &#34;index.md&#34;. Now let&#39;s add this to our Antenna collection. We only need to do this once.

~~~shell
antenna add index.md
~~~

If you list the directory you should see &#34;index.db&#34; and &#34;index.yaml&#34;. You can modify the &#34;index.yaml&#34; to set the various HTML elements that will host either the list of blog posts or the individual blog post content.

~~~shell
ls -1 index.*
~~~

or for Windows.

~~~shell
dir index.*
~~~

## Adding the first blog post

I am going to assume the first blog post is called &#34;welcome.md&#34;. I am also going to assume you&#39;ll using a blog oriented directory structure. For this example today&#39;s date is September 5th, 2025. I need to create a place to hold my blog post &#34;welcome.md&#34;. I will first create a directory to hold it.

~~~shell
mkdir -p blog/2025/09/05
~~~

On Windows

~~~shell
New-Item -ItemType Directory -Force -Path blog\2025\09\05
~~~

Now you need to open the a new file in that directory called &#34;welcome.md&#34;. If you have VS Code or Codium installed as your editor you can do the following.

~~~shell
code blog/2025/09/05/welcome.md
~~~

Or for Windows.

~~~shell
code blog\2025\09\05\welcome.md
~~~

In that file create our welcome post. We need to include the following attributes in our front matter, &#34;postPath&#34;, &#34;link&#34;, &#34;pubDate&#34;. Here&#39;s my version of &#34;welcome.md&#34; Markdown.

~~~markdown
  ---
  title: Welcome
  postPath: &#34;blog/2025/09/05/welcome.md&#34;
  pubDate: &#34;2025-09-05&#34;
  ---

  # Welcome

  This is a demonstration of Blogging with Antenna.

~~~

NOTE: If you are on Windows, you&#39;ll want to use the version of postPath that looks like this, `postPath: blog\2025\09\05\welcome.html`. You can now add the the post using the post action.

~~~shell
antenna post index.md blog/2025/09/05/welcome.md
~~~

Or on Windows

~~~shell
antenna post index.md blog\2025\09\05\welcome.md
~~~

We&#39;re are almost done. You should see the version of welcome.md you created and a new &#34;welcome.html&#34; generated when you use the post action. We need to generate the index.html and index.xml files with the updated post.

~~~shell
antenna generate
~~~

You can preview your new blog post at `http://localhost:8000` using the preview action and pointing your web browser at that URL.

~~~shell
antenna preview
~~~

## Updating a post

Any time you run the post command on your Markdown file the post with the matching link and postPath gets updated. Below I open and update the &#34;welcome.md&#34; file. Then I post it again to regenerate the HTML page.

~~~shell
code blog/2025/09/05/welcome.md
antenna post index.md blog/2025/09/05/welcome.md
~~~

That&#39;s it you now can add and update posts for your blog. Antenna will manage the index.html and index.xml documents for you when you run the `antenna generate` command again. You then use the preview action to view it in your web browser.

NOTE: If this was your blog you&#39;d change the value I used for the link element to match how your website is structured and use it&#39;s URL.  I used a localhost URL with port number just to keep things simple and to allow us to test using the `antenna preview`.

## Enhancing you blog 

You will likely want some navigation and other text on your blog pages. This is accomplished by updating the &#34;index.yaml&#34; file. In this file you can set the path to your custom CSS, to any JavaScript script you might want to include. You can also set your site header, footer and nav elements. Finally you can even include HTML elements before and after the generated content. Here&#39;s an example of a customized &#34;index.yaml&#34; file.


~~~yaml
css: css/site.css
header: |
  &#60;header&#62;&#60;h1&#62;Welcome to My Blog&#60;/h1&#62;&#60;/header&#62;

nav: |
  &#60;nav&#62;
    &#60;ul&#62;
      &#60;li&#62;&#60;a href=&#34;/&#34;&#62;Home&#60;/a&#62;&#60;/li&#62;
    &#60;/ul&#62;
  &#60;/nav&#62;

footer: |
  &#60;footer&#62;&#60;!-- your custom footer insert HTML goes here --&#62;&#60;/footer&#62;
~~~

Now re-post your welcome.md then generate followed by preview to see the changes.

~~~shell
antenna post index.md blog/2025/09/05/welcome.md
antenna generate
antenna preview
~~~

You&#39;re blog is staged you can now publish on the Internet using the tools provided by your host. If you&#39;re using GitHub that might mean committing and pushing to a specific branch setup for your website (see GitHub pages documentation). 

Happy blogging!</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/09/05/Build_a_Blog_with_Antenna_App.md" length="7888" type="text/markdown" />
    </item>    <item>
      <title>Building Your Own Antenna</title>
      <link>https://rsdoiel.github.io/blog/2025/08/31/Building_Your_Own_Antenna.html</link>
      <description>
        <![CDATA[I have been prototyping my personal news site called [Antenna](https://rsdoiel.github.io/antenna)
        since 2023. It was inspired by Dave Winer's [news.scripting.com](https://news.scripting.com). It
        has always run on a Raspberry Pi 3B+ running Raspberry Pi OS (Linux). The content is available
        over my home network and I make it public by publishing via GitHub pages. Over the last two years it
        has become the first place I go to consume content from the web (second is Dave's news.scripting.com).]]>
      </description>
      <source:markdown># Building Your Own Antenna

By R. S. Doiel, 2025-08-31

I have been prototyping my personal news site called [Antenna](https://rsdoiel.github.io/antenna) since 2023. It was inspired by Dave Winer&#39;s [news.scripting.com](https://news.scripting.com). It has always run on a Raspberry Pi 3B+ running Raspberry Pi OS (Linux). The content is available over my home network and I make it public by publishing via GitHub pages. Over the last two years it has become the first place I go to consume content from the web (second is Dave&#39;s news.scripting.com).

This past month (August 2025), my prototype website lead me to write a new application I am calling [antenna](https://github.com/rsdoiel/antennaApp) after the website that inspired it. 

## What I learned from running Antenna website

Having the feeds in a simple text format means I curate the feeds more. When I relied on OPML it became just enough extra work hand editing XML that I wanted something simpler. I started with a feed file format inspired by [Newsboat](https://newsboat.org). It was a little too simple, wound up writing scripts to convert that format to something more general (e.g. OPML). I decided to pick something better known for my new application. That got me thinking. Lots of people know Markdown. A collection is a list of zero or more links. That&#39;s is easy to express as Markdown. It has the advantage of being able to be convert to other formats as needed (example using a Pandoc filter to render as OPML).

What about other configuration I might need?

The Markdown community has supported Metadata for the Markdown document as &#34;front matter&#34; expressed in YAML. That suggested YAML has been seen by the many in in the Markdown community. YAML front matter is also documented. Why not use it for specific configuration of my collection of feeds? I can use it to express the extra fields that can be included in the channel metadata for rendering RSS 2.0 files. YAML can also serve as the configuration language for the Antenna application. Seems like a good match. I think people who work with Markdown maybe interested in generating static websites, they may have knowledge of YAML so that aligns with my applcation.

How do you add a collection to the &#34;antenna.yaml&#34; file? A prototype can be implemented with a hand edited YAML file but that&#39;s not good for everyone. While YAML is easier to type and read than JSON it&#39;s not bullet proof. The Antenna application can create a default &#34;antenna.yaml&#34; file as an initialization action. The Antenna command can include a means of adding a collection (expressed as a Markdown document) and removing a collection. While someone can dive into the YAML (and comments in the default YAML can document what things are), they don&#39;t have to dive into YAML if I provide sensible defaults. That&#39;s a win for simplicity and flexibility.

Once a collection is defined and added to the Antenna YAML configuration it can be harvested. The harvested content then can be used to generate HTML and RSS 2.0 XML files.

You can find my current experimental implementation of the Antenna application as &#60;https://github.com/rsdoiel/antennaApp/releases&#62;. There are zip files for macOS, Linux and Windows for both x86_64 and aarch64 (ARM 64) machines.

## What is the Antenna application really?

Antenna, the application, is a feed oriented static website generator. It stores each collection of feed items in an SQLite3 database. You can have as many collections as you want.  In addition to aggregating feed content you can &#34;post&#34; or &#34;unpost&#34; items to a collection. The Antenna application can harvest collections then generate an HTML page and RSS 2.0 XML for each collection. The application provides a preview action so you can see the results without publishing a website.

Supporting in bound and out bound RSS allows an Antenna website to become a distributed node on the open social web. It&#39;s simple, it&#39;s RSS 2.0, it proven. You can &#34;follow&#34; someone by adding their feed. They can &#34;follow&#34; you back by adding yours. You only need to host a copy of your Antenna site on the open web and promote the feeds you produce. It doesn&#39;t require permission, it remains under your control. It doesn&#39;t need to surveil anyone or sell anything to work. It&#39;s just out there as long as you have a site on the open web.

Antenna requires minimal resources. I run it on a Raspberry Pi 3B+ running the current version of Raspberry Pi OS. It can even run on a Raspberry Pi Zero running Raspberry Pi OS without the desktop. I have run tested it on Windows 10, Windows 11 and macOS and found it runs just fine there too. 

Because it can run on minimal hardware it is possible to create your own Antenna appliance. You could even host a public access point to step off the public Internet and into the community digital spectrum like little digital libraries.

## How do I install the experimental Antenna application?

The Antenna application can be [installed](https://rsdoiel.github.io/antennaApp/INSTALL.html) on Raspberry Pi OS/Linux, macOS and Windows. The executables are distributed via GitHub at &#60;https://github.com/rsdoiel/antennaApp/releases&#62;. Unzip the file for your operating system and CPU type then copy the executable in the &#34;bin&#34; directory to where it&#39;ll be available in a terminal (i.e. someplace in the PATH). If you can run `antenna -version` from your Terminal then it is installed and working. See [INSTALL.md](https://rsdoiel.github.io/antennaApp/INSTALL.html)

It is important to remember that Antenna is an experimental proof of concept, it&#39;ll have bugs and will likely change over time. As I use it more the things that bother me will get fixed or improved. Look at the [license](https://rsdoiel.github.io/antennaApp/LICENSE) for details before adopting it.

## High level Workflow

The basic workflow for generating the Antenna website can be thought of as 

1. Curating your feed collections, posting Markdown documents to a collection
2. Harvest the collections
3. Generate HTML pages for the collections

Steps two and three can be run on a schedule or run on demand. I use a cronjob to run them each morning and evening on my venerable Pi 3B+.

## Developing the Antenna application

### Figuring out features

What are the features of the Antenna application?  Dave Winer&#39;s [Textcasting](https://textcasting.org) was a good guide for thinking about the problem. It&#39;ll likely guide the feature evolution moving forward. Two years of running my Antenna website with a bundle of Bash scripts, Pandoc templates and a custom feed manager provided more insight. The hairball of the prototype lead to a single application that you run in a terminal. It supports a syntax of &#34;actions&#34; and simple &#34;modifiers&#34;. Configuration is generally managed by the application but can be customized via editing YAML files. Each collection has a YAML file that specifies how to write the content in HTML. The &#34;antenna.yaml&#34; file descriptions which collections to process and includes SQL to filter them aggregated content.

Collection curation is done in Markdown. Markdown describes a list of links. Each link is a feed. The links are extracted from the Markdown document when they have this form, `- [FEED_LABEL](FEED_URL &#34;OPTIONAL_FEED_DESCRIPTION&#34;)`. Front Matter in the collection document will be used to provide channel metadata used when generating RSS and HTML. Having the collection expressed as Markdown is an advantage. When the collection is rendered as RSS the channel level information can be taken from the YAML front matter of the collection document much like it works with RMarkdown or Pandoc. Mean while Markdown also makes sense for post to a collection. Again the additional front matter can provide insights into how Antenna should handle the post.

Rules of thumb:

- __content__ think Markdown
- __configuration__ think YAML

Here&#39;s the initial feature set I&#39;ve landed for the 0.0.1 version of the Antenna application.

1. Define a feed collections as a list of links in a Markdown file
2. Use YAML front matter for additional configuration and metadata
3. Use YAML for configuration of the Antenna App
4. Use a share page structure among all generated pages (easier to style)
5. Allow customization of the pages by injecting HTML blocks
6. Use a simple command structure for the program, `antenna ACTION [MODIFIERS]`

There were something things I wanted to avoid.

- Avoid template languages, they get too involved to document and explain
- Avoid extra software dependencies, the Antenna application should be a single stand alone executable available for Raspberry Pi OS/Linux, macOS and Windows
- While Antenna is being implemented as a command line tool, keep it simple where the syntax is action followed by modifiers

What are minimum set of actions needed by the Antenna application based on my [prototype website](https://rsdoiel.github.io/antenna)?

init
: Initial (create/validate) an Antenna YAML configuration file.

add COLLECTION_MARKDOWN
: Add a feed collection defined by COLLECTION_MARKDOWN, a Markdown file. The Markdown file can contain a list of links that look like `- [FEED_LABEL](FEED_URL, &#34;FEED_DESCRIPTION&#34;)`. All other content in the Markdown file is ignored. That means you can actually document things about the feed or publish the Markdown file as it&#39;s own webpage along side the aggregated results.

del COLLECTION_MARKDOWN
: Remove a feed collection defined by the COLLECTION_MARKDOWN file name from the collections managed in the antenna.yaml file.

harvest | harvest COLLECTION_MARKDOWN
: Harvest all collections or specific collections.

generate | generate COLLECTION_MARKDOWN
: Generate HTML and RSS 2.0 XML for all collections or a specific collection. The generated files have a similar name to the Markdown file defining the collection. Example, &#34;myfeeds.md&#34; would result in the aggregation HTML page of &#34;myfeeds.html&#34; and the RSS 2.0 file called &#34;myfeeds.xml&#34;.

post COLLECTION_MARKDOWN  MARKDOWN_FILE
: Add the POST_MARKDOWN file to the harvested collection of COLLECTION_MARKDOWN. Depending on the he front matter the &#34;post&#34; will either exist as a collection item record or may also be configured to be rendered as HTML in the web tree curated by Antenna.

unpost COLLECTION_MARKDOWN LINK_TO_POST
: Remove a item with the LINK_TO_POST value from the harvested collection of COLLECTION_MARKDOWN. This works both for &#34;post&#34; items and any other item in the feed collection.

In the case of &#34;post&#34; the MARKDOWN_FILE front matter provides the metadata
needed to complete the processing. This allows you to inject items into a collection that just show up in the aggregate or to do full blog posts where the the post content is written to an HTML file.

When you run a harvest action, the COLLECTION_MARKDOWN file is read and each of the feeds defined will be harvested with the feed items and channel information stored in the &#34;channels&#34; and &#34;items&#34; table of the SQLite3 database associated with COLLECTION_MARKDOWN filename. Example, &#34;travel.md&#34; could define a list of links related to travel. That is the COLLECTION_MARKDOWN file. When the &#34;travel.md&#34; collection is added to the antenna configuration a database is generated called &#34;travel.db&#34;. When I run generation action both an HTML page will show the aggregation of those feeds and a new RSS 2.0 XML file will be generated, &#34;travel.html&#34; and &#34;travel.xml&#34;.  

A Markdown document which includes  &#34;postPath&#34; and &#34;link&#34; in the front matter  tells the Antenna application to convert the Markdown into HTML and write it to the location specified by &#34;postPath&#34;. 

## How do you manage individual pages for your website?

A collection doesn&#39;t have to contain feeds from external websites. It can be a collection of &#34;posts&#34;. A post isn&#39;t proscriptive. It just describes something that will be aggregated in a collection, if you include &#34;postPath&#34; it also describes where to render HTML of the post contents. That&#39;s it. You can organize the &#34;postPath&#34; of each document however you want. I happen to like the blog directory style of &#34;posts/YEAR/MONTH/DAY/POST_FILE&#34; but you might prefer a wiki layout or just something ad-hoc. Antenna doesn&#39;t have opinions, you tell it what you want and it does it. It&#39;s your choice.

## Do feed items accumulate forever?

Each collection defined in the &#34;antenna.yaml&#34; file includes a filter attribute. A filter is a sequence of SQL statements. These statements are run against the SQLite3 database associated with the collection. By default two filter statements are supplied. 

- Set everything to review, `UPDATE items SET status = &#39;review&#39;`
- Set the recently (previous three weeks) published items to published, `UPDATE items SET status = &#39;published&#39; WHERE pubDate &#62;= date(&#39;now&#39;, &#39;-21 days&#39;);`

The statements are applied one after the other in order. You can choose to include statements that remove stale items. You could set all items to published too. Each statement gets executed that you include. Since the database is a standard SQLite3 database you can use the SQLite3 command line program to explore and test what statements you might want to use yourself.

## Work flows

### Curating aggregations

When I want to add a feed to a collection I just open the Markdown file for the collection and add another list element that points to the feed. Example, `- [Robert&#39;s blog](https://rsdoiel.github.io/rss.xml)`.  To remove a feed from a collection, I just edit the Markdown file and delete that line.

The Markdown file is read each time you run `antenna harvest`. It&#39;ll only harvest items it finds matching the simple markdown list item pattern I&#39;ve shown above.

### Updating your Antenna website

This involves two steps

1. harvest, e.g. `antenna harvest`
2. generate, e.g. `antenna generate`

This will cause the aggregation pages to be updated with the last content of the collection feeds. This is easy to run in a terminal as well as schedule via your operating system&#39;s scheduler (e.g. via cron, systemd or launchd daemons).

### Setting up a new Antenna

1. Create a directory
2. Change into that directory
3. Initialize your Antenna instance
4. Add a feed collection
5. Customize the collection&#39;s YAML files
6. Harvest
7. Generate
8. Preview your website at &#60;http://localhost:8000&#62;

Here&#39;s the steps taken in the terminal.

~~~shell
mkdir mysite
cd mysite
antenna yaml
edit index.md # define my initial collection, save the file
antenna add index.md
edit index.yaml # Customize the HTML wrapping the aggregated content.
antenna harvest
antenna generate
antenna preview
~~~

NOTE: A site generated in this way can be published to the Web via a static website service. I happen to use GitHub pages but there are lots of others out there.

### Adding a micro blog post to &#34;mysite&#34;

Here is a simple micro blog &#34;helloworld.md&#34; post Markdown file.

~~~markdown

Hello World!
~~~

I can added to the &#34;index.md&#34; collection using the &#34;post&#34; action.

~~~shell
antenna post index.md hellworld.md
~~~

I can see the updated post by running generate and preview.

~~~
antenna generate
antenna preview
~~~

This micro blog post only lives in the feed&#39;s item table. There is no landing page. It will only be written out to RSS and HTML if it remains in the &#34;published&#34; state. If the item is removed from the items table the micro blog post will not get rendered the next to you render the feed&#39;s page. This makes them somewhat emphermal.

### Adding a blog post to &#34;mysite&#34;

If I update &#34;helloworld.md&#34; to include a &#34;postPath&#34; and &#34;link&#34; then the post will get written into the the directory tree managed by my Antenna application. Here&#39;s the updated Markdown file.

~~~Markdown
  ---
  link: http://localhost:8000/helloworld.html
  postPath: helloworld.md
  pubDate: &#34;2025-08-31&#34;
  ---
  
  Hello World!
~~~

Run the post action, generate and then preview.

~~~shell
antenna post index.md helloworld.md
antenna generate
antenna preview
~~~

You should now see &#34;htdocs/helloworld.html&#34; and be able to navigate to it directly using &#60;http://localhost:8000/helloworld.html&#62;.

NOTE: If each time you run the post action on that file it&#39;ll regenerate the HTML.

There you have it, you have your own Antenna for aggregating your personal news site and website generation rolled together in one application.

## How would Antenna integrate into am existing website?

Antenna plays nice with other blogging tools. It only generates those pages associated with the aggregated collections and any posted Markdown files. The HTML generate is suitable to process with [PageFind](https://pagefind.app) if your static sites provides search. You can place the source Markdown files into your web tree an leverage the JSON API generated by [FlatLake](https://flatlake.app). If your using Pandoc or another means of rendering your website Antenna just gets used along side it. It doesn&#39;t require additional software or modify the web tree beyond what you specify.

## SQLite3 database as content storage

The Antenna application takes advantage of SQLite3 database(s) to persist the web content it manages. It is a binary format so you may want to back it up outside of your version control system (outside of Git). It&#39;s really up to you.

Because SQLite3 is used that means you can do any additional ad-hoc processing you want via SQL. This is a powerful feature and allows for potential integration with other systems. You could as example write a program to render come posts to other systems like BlueSky or Mastodon. I leave that as an exercise for the reader.

In the antenna.yaml file each collection has an attribute called &#34;filter&#34;. The filter holds a list of SQL statements that are run before generating the HTML pages. You can list as many as you want, though by default two are provided (set everything to review, then set recent items to published).

## How do I manage individual web pages?

The Antenna application is feed oriented. By that I mean it&#39;ll generate an aggregation page and RSS file. A collection doesn&#39;t have to include links at all. A collection that just has posts is completely valid. The posts will be used as content for the aggregated HTML page and populate the outbound RSS feed. You can use a simple filter to ensure all posts are published before generating the feed page, `update items set status = &#39;published&#39;`. That&#39;s it. All items in the collection will always be published. They&#39;ll always show up in the aggregated HTML and RSS file.

When you update a page you re-post it to the collection. When each item is posted Antenna generates the HTML page when the postPath and link are set. Then you can &#34;preview&#34; the site you can see the updated page.

## Other possibilities

Antenna application let&#39;s us easily setup our own new aggregation websites. We don&#39;t need to rely on Google, Facebook, etc. As long as there is an RSS/Atom or JSON feed, we can included it in our own aggregation.

Antenna can also serve as a site rendering tool for Markdown content. It supports posts as well as content that is harvested for aggregation pages. All collections include outbound RSS automatically. 

By combining aggregation of feeds with publication Antenna supports a distributed social web based on RSS 2.0. You can skip the complexity of [AT Protocol](https://en.wikipedia.org/wiki/AT_Protocol &#34;Blue Sky&#39;s AT protocol&#34;) or [ActivityPub](https://en.wikipedia.org/wiki/ActivityPub &#34;Mastodon&#39;s native protocol&#34;) and still be social. On my own Antenna site I consume content from both Blue Sky and Mastodon. Others are working on tools that will take an RSS feed and replicate the content using AT Protocol or Activity Pub but in the mean time let&#39;s just keep using RSS. It works as it always has. 

Final thoughts, because the feed collections are stored in a SQLite3 database you can opt to write your own page generates, connectors, API or what have you. This includes things like adding support for hash tag lists and &#39;@&#39; tags too. Since it runs on your machine you could extract your &#39;@&#39; tags form your posts and then ping some via SMS or email if you like. You could even write something that would post the item via their preferred social media platform. Where you go with it is up to you. Those features are beyond what I need for Antenna but that shouldn&#39;t stop you from taking it further.

- Project Website: &#60;https://rsdoiel.github.io/antennaApp&#62;
- Quick Install: &#60;https://rsdoiel.github.io/antennaApp/INSTALL.html&#62;
- GitHub Repository: &#60;https://github.com/rsdoiel/antennaApp&#62;</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/08/31/Building_Your_Own_Antenna.md" length="21208" type="text/markdown" />
    </item>    <item>
      <title>Generating RSS with FlatLake</title>
      <link>https://rsdoiel.github.io/blog/2025/08/10/generating_rss_with_flatlake.html</link>
      <description>
        <![CDATA[A show post describing a prototype in Deno+TypeScript for generating RSS 2.0 feeds from a FlatLake generated JSON API.]]>
      </description>
      <source:markdown># Generating RSS with FlatLake

By R. S. Doiel, 2025-08-10

One of the must have features missing from many static websites are RSS feeds.
Many people produce blogs using CommonMark/Markdown documents with front matter.
[FlatLake](https://flatlake.app &#34;an open source tool from Cloud Cannon&#34;)
produces a JSON API from front matter. The API makes it easy to generate various
types of feeds. All that is needed is something to translate the JSON API
documents to the feed format you want. In this post I will show a proof of
concept using Deno+TypeScript to process the Flatlake generate JSON API into an
RSS feed.

## Requirements for your site

You need [FlateLake](https://flatlake.app) installed. You also need to have
front matter in your CommonMark/Markdown documents. No front matter, no API. I
rely on front matter in my publication process for blog so FlatLake is a good
fit. I only use a mimum of features in FlatLake. Here&#39;s the configuration file I
use.

```yaml
global:
  outputs:
    - &#34;list&#34;
collections:
  - output_key: &#34;posts&#34;
    page_size: 24
    sort_key: &#34;datePublished&#34;
    sort_direction: &#34;desc&#34;
    list_elements:
      - &#34;data&#34;
      - &#34;content&#34;
    inputs:
      - path: &#34;./blog&#34;
        glob: &#34;**/*{md}&#34;
```

## Generating the json api

Once you have FlatLake configured generating the JSON API is as simple as
running the command

```shell
flatlake
```

That&#39;s it. It scans the directory named &#34;blog&#34; in the directory where I stage my
website. The output goes to a directory called &#34;api&#34;. If you prefer a diffent
path FlatLake has options to support that. See `flatlake --help` or the website
[FlatLake/docs](https://flatlake.app/docs). The verbose and logging options are
helpful in understanding what FlatLake is doing.

## Short anatomy of RSS XML

The RSS XML has two parts. A channel description and the items in the feed. The
items map the data about your blog posts. The channel description isn&#39;t directly
available from the FlatLake generated data. The easy solution I&#39;ve chosen is to
describe the channel in a YAML. Here&#39;s the example I use for my blog.

```yaml
title: &#34;Robert&#39;s Ramblings&#34;
description: &#34;Robert&#39;s website and blog posts&#34;
link: &#34;https://rsdoiel.github.io&#34;
```

## JSON API to RSS

In my publication process the next thing I do is used a Deno+TypeScript program
to convert the JSON API into an RSS feed. I have one feed for my site contains
recent posts. The path in the api tree I use it `api/posts/all/page-1.json`.
Based on my configuration file this contains the most recent 24 posts sorted by
descending publication date. At this point it&#39;s just a matter of cross walking
the content of &#34;page-1.json&#34; into an RSS XML feed file.

Here&#39;s a TypeScript module I knock together to test out the concept.

~~~TypeScript
/**
 * flatlakeToRSS2.ts translate a FlatLake JSON API document to RSS2 XML.
 *  
 *  Copyright (C) 2025  R. S. Doiel
 * 
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Affero General Public License as
 *  published by the Free Software Foundation, either version 3 of the
 *  License, or (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Affero General Public License for more details.
 *
 *  You should have received a copy of the GNU Affero General Public License
 *  along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.
 */
// Define the structure of your input data
export interface Post {
  content: string;
  data: {
    abstract: string;
    author: string;
    dateCreated?: string;
    dateModified?: string;
    datePublished?: string;
    keywords: string[];
    pubDate?: string;
    title: string;
    url: string;
  };
}

// Define the structure of the RSS feed
export interface RSSFeed {
  title: string;
  description: string;
  link: string;
  language?: string;
  copyright?: string;
  managingEditor?: string;
  webMaster?: string;
  items: RSSItem[];
}

export interface RSSItem {
  title: string;
  description: string;
  link: string;
  pubDate: string;
  author: string;
  categories: string[];
}

// Function to convert JSON data to RSS feed with YAML configuration
export function convertToRSS(
  posts: Post[],
  channelInfo: Partial&#60;RSSFeed&#62;,
): RSSFeed {
  const feed: RSSFeed = {
    title: channelInfo.title || &#34;My Blog&#34;,
    description: channelInfo.description || &#34;A collection of blog posts&#34;,
    link: channelInfo.link || &#34;http://example.com&#34;,
    language: channelInfo.language,
    copyright: channelInfo.copyright,
    managingEditor: channelInfo.managingEditor,
    webMaster: channelInfo.webMaster,
    items: [],
  };

  posts.forEach((post) =&#62; {
    const item: RSSItem = {
      title: post.data.title,
      description: post.data.abstract,
      link: `http://example.com/${post.data.url}`,
      pubDate: new Date(
        post.data.datePublished || post.data.pubDate || post.data.dateCreated ||
          post.data.dateModified || Date.now(),
      ).toUTCString(),
      author: post.data.author,
      categories: post.data.keywords,
    };
    feed.items.push(item);
  });

  return feed;
}

// Function to generate the RSS XML
export function generateRSS(feed: RSSFeed): string {
  const itemsXML = feed.items
    .map(
      (item) =&#62; `
        &#60;item&#62;
            &#60;title&#62;&#60;![CDATA[${item.title}]]&#62;&#60;/title&#62;
            &#60;description&#62;&#60;![CDATA[${item.description}]]&#62;&#60;/description&#62;
            &#60;link&#62;${item.link}&#60;/link&#62;
            &#60;pubDate&#62;${item.pubDate}&#60;/pubDate&#62;
            &#60;author&#62;${item.author}&#60;/author&#62;
            ${
        item.categories.map((category) =&#62; `&#60;category&#62;${category}&#60;/category&#62;`)
          .join(&#34;&#34;)
      }
        &#60;/item&#62;
    `,
    )
    .join(&#34;&#34;);

  return `&#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34; ?&#62;
&#60;rss version=&#34;2.0&#34;&#62;
    &#60;channel&#62;
        &#60;title&#62;&#60;![CDATA[${feed.title}]]&#62;&#60;/title&#62;
        &#60;description&#62;&#60;![CDATA[${feed.description}]]&#62;&#60;/description&#62;
        &#60;link&#62;${feed.link}&#60;/link&#62;
        ${feed.language ? `&#60;language&#62;${feed.language}&#60;/language&#62;` : &#34;&#34;}
        ${feed.copyright ? `&#60;copyright&#62;${feed.copyright}&#60;/copyright&#62;` : &#34;&#34;}
        ${
    feed.managingEditor
      ? `&#60;managingEditor&#62;${feed.managingEditor}&#60;/managingEditor&#62;`
      : &#34;&#34;
  }
        ${feed.webMaster ? `&#60;webMaster&#62;${feed.webMaster}&#60;/webMaster&#62;` : &#34;&#34;}
        ${itemsXML}
    &#60;/channel&#62;
&#60;/rss&#62;`;
}
~~~

## Putting it all together

The demo program can be compiled with the follow.

```shell
git clone https://github.com/rsdoiel/flatlakeToRSS2
cd flatlakeToRSS2
deno task build
```

The program in the `bin` directory can then be used to generate the RSS XML from
your FlatLake API. Copy it to some place in your PATH.

Now I go to my website staging directory. I can take the following steps to test
RSS generation from FlatLake.

```shell
cd $HOME/Sites/rsdoiel.github.io
flatlake
edit rss_channel.yaml # Create the YAML file above
flatlakeToRSS2 rss_channel.yaml api/posts/all/page-1.json &#62;rss.xml
```

## Lessons learned

There are some very good feed libraries out there like
[Dave Winer&#39;s](https://scripting.com)
[ReallySimple](https://github.com/scripting/reallysimple). That could save using
format strings to render XML. It also has the advantage that is can be use to
read feeds as well.

Ideally I&#39;d like the generation of RSS feeds to be in integrated into my
[BlogIt](/BlogIt) tool I am working on.

This experiment showed that inspite of the limitted documentation of the
FlatLake website that it&#39;s worth exploring as an off the shelf open source
solution. It might just earn a place along side [PageFind](https://pagefind.app)
and [Pandoc](https://pandoc.org) as part of my regular web toolkit.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/08/10/generating_rss_with_flatlake.md" length="8187" type="text/markdown" />
    </item>    <item>
      <title>Opensearch Description Document needs love</title>
      <link>https://rsdoiel.github.io/blog/2025/08/09/opensearch_description_documents_need_love.html</link>
      <description>
        <![CDATA[Opensearch Description Document is a specification for describing how a site search can integrate into your web browser. In 2025 it is still supported by Firefox, Safari and Chrome. It lets you integrate your site search into your browsers URL box (a.k.a. omnibox) as a first class search citizen. It is a means for us to take back search.
        In this article I use a simple case study of integrating a PageFind search using an Opensearch Description Document.]]>
      </description>
      <source:markdown># Opensearch Description Document needs love

&#62; Allow your readers to skip the ad-tech and search your site directly

By R. S. Doiel, 2025-08-09

Opensearch Description Documents are an XML file that describes how search works on your website. The specification dates back to 2005[^01]. Opensearch Description Documents (example &#34;osd.xml&#34;) should not be confused with [Opensearch](https://opensearch.org/) the search engine Amazon and friends forked from [ElasticSearch](https://www.elastic.co/). Despite the overlap in name they are not the same thing[^02].

The Opensearch Description Document role is to provide a little metadata about the search system and how to make a query. With this bit of information the web browser can then make that search available in the URL box (omnibox) of your web browser. Early on this was automatic but as companies have tried to monopolize search they have used security as the excuse to restrict the usage of this metadata. It still works safely in Firefox but in Safari and Chrome you have to take extra steps to use it.

## What does this mean to a site reader?

If implemented correctly the Opensearch Description Document allows integration as a &#34;search engine&#34; in your web browser. On Firefox that means I can see my web site&#39;s site search in the pull down list of available search engines and just add it. For [Safari](https://support.apple.com/guide/safari/search-sfria1042d31/18.0/mac/15.0 &#34;This was the documentation I could find for Safari search options&#34;) and [Chrome](https://support.google.com/chrome/answer/95426?hl=en-IS&#38;co=GENIE.Platform%3DDesktop &#34;This is the doucmentation I could find from Google for Chome site search options&#34;) you must go into the settings to enable your site search.

## What does it mean as a site owner?

As a site owner I can allow my readers to search without ad-tech and spyware.  Plus I can allow my readers to initiate a site search from any web page since the URL box (omnibox) is always available regardless of which page I&#39;m reading. The eliminates URL tricks like pasting in the search URL followed by a `?q=My%20Search%20Terms` at the end of it.

## How do I implement this?

Integrating Opensearch Description Documents requires including additional HTML elements in the head element of the HTML document. This tells the web browser where to find the XML document. It requires the creation of the XML document (e.g. &#34;osd.xml&#34;). It also requires that your search engine have a structured URL where the search terms can be dropped. More on that in our use case.

## Implementing the Opensearch Description Document.

The [MDN](https://developer.mozilla.org/en-US/docs/Web/XML/Guides/OpenSearch &#34;see the docs at the Mozilla Developer Network&#34;) has a good page describing the details of creating this document. (NOTE: While XML looks a lot like HTML it is less forgiving than HTML. It must be a valid XML file.)

Below is a minimum Opensearch Description Document XML document I use on my website. It is in the root of the website and is called &#34;osd.xml&#34;, &#60;https://rsdoiel.github.io/osd.xml&#62;.

~~~xml
&#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&#62;
&#60;OpenSearchDescription xmlns=&#34;http://a9.com/-/spec/opensearch/1.1/&#34;&#62;
  &#60;ShortName&#62;rsdoiel&#60;/ShortName&#62;
  &#60;Description&#62;Search Robert&#39;s Ramblings and Blog&#60;/Description&#62;
  &#60;Url type=&#34;text/html&#34; method=&#34;get&#34; template=&#34;https://rsdoiel.github.io/search.html?q={searchTerms}&#34;/&#62;
  &#60;InputEncoding&#62;UTF-8&#60;/InputEncoding&#62;
  &#60;Image height=&#34;16&#34; width=&#34;16&#34;&#62;https://rsdoiel.github.io/favicon.ico&#60;/Image&#62;
&#60;/OpenSearchDescription&#62;
~~~

The first element, **OpenSearchDescription** element, wraps the document.
Next up is **ShortName**. As the element tag suggests this is for a &#34;show name&#34;. ShortName should be less than 14 alphanumeric characters only. I&#39;ve chose my subdomain name, &#34;rsdoiel&#34;, from the host name &#34;rsdoiel.github.io&#34;. It can be anything you want.

After **ShowName** is an element called **Description**. This is a simple text string up to 1024 characters long. I short description. In one sentence you want to describe your search engine. In my case I chose, &#34;Search Robert&#39;s Ramblings and Blog&#34;.

**Description** is followed by the **Url** element. This is the element that describes how to make a query. It includes the full URL to your search page and any query options needed to cause the page to return search results.  On my site I use a browser side search engine called [PageFind](https://pagefind.app). I leverage the URL query parameters to trigger the search and therefore use a &#34;get&#34; method. The template attribute describes how to form the URL used to retrieve search results. The text, `{searchTerms}` will get replaced by the text your type in the URL box (omnibox) of your web browser. I&#39;ve setup PageFind to look at the query parameters for a &#34;q&#34; field and my &#34;template&#34; maps the value of &#34;q&#34; to `{searchTerms}`.

Next are **InputEncoding** which sets the encoding type and **Image** that associates an image for your search. For me I always use UTF-8 for web things and have repurposed the favicon.ico as the icon value associated with my site search.

## Hooking in &#34;osd.xml&#34; to your web pages.

I use CommonMark documents to build for my website and Pandoc to turn that into a webpage. I do this using a template. To integrate the Opensearch Description Document into my site I&#39;ve updated the templates adding the following link element inside the head element.

~~~HTML
  &#60;link rel=&#34;search&#34;
        type=&#34;application/opensearchdescription+xml&#34;
        title=&#34;Robert&#39;s Rambling Search Engine&#34;
        href=&#34;osd.xml&#34;&#62;
~~~

## Updating PageFind

PageFind is a search engine that works in your browser rather than on a server. It does this be creating indexes that can be downloaded as needed by your web browser. If you look at the [PageFind docs](https://pagefind.app/docs) website the basic example looks like this.

~~~JavaScript
window.addEventListener(&#39;DOMContentLoaded&#39;, (event) =&#62; {
  new PagefindUI({ element: &#34;#search&#34;, showSubResults: true });
});
~~~

This will add a search box to the element with the id of &#34;#search&#34;. When you type in a search term it will show results. This is fine if you directly navigate to that page. What we want is to trigger results when you navigate to the page including a &#34;q&#34; query parameter, &#60;https://rsdoiel.github.io/search.html?q=osd.xml&#62;

The first thing to support this is to retrieve the query parameter. Here&#39;s a simple function to do that.

~~~JavaScript
function getQueryParam(name) {
  const urlParams = new URLSearchParams(window.location.search);
  return urlParams.get(name);
}
~~~

When the page is loaded we use that function to retrieve any value for &#34;q&#34; that is available.

~~~JavaScript
const queryString = getQueryParam(&#34;q&#34;) || &#34;&#34;;
~~~

Then when the `PageFindUI` is created we can use that object&#39;s `triggerSearch` method to trigger our search results.

~~~JavaScript
    pagefindUI.triggerSearch(queryString);
~~~

Putting it all together you add the following side the body element.

~~~JavaScript
&#60;link href=&#34;/pagefind/pagefind-ui.css&#34; rel=&#34;stylesheet&#34;&#62;
&#60;script src=&#34;/pagefind/pagefind-ui.js&#34;&#62;&#60;/script&#62;
&#60;div id=&#34;search&#34;&#62;&#60;/div&#62;
&#60;script&#62;
// Fetch the query &#34;q&#34; form the URL
function getQueryParam(name) {
  const urlParams = new URLSearchParams(window.location.search);
  return urlParams.get(name);
}
// When the page is loaded setup PageFindUI object.
window.addEventListener(&#39;DOMContentLoaded&#39;, (event) =&#62; {
  const pagefindUI = new PagefindUI({ element: &#34;#search&#34;, showSubResults: true });

  const queryString = getQueryParam(&#34;q&#34;);
  // Trigger a search query if &#34;q&#34; is available.
  if (queryString) {
    pagefindUI.triggerSearch(queryString);
  }
});
&#60;/script&#62;
~~~

You can see a much more elaborate version on my website, [search.html](view-source:https://rsdoiel.github.io/search.html). You can try it at &#60;https://rsdoiel.github.io/search.html&#62;. On my site I am bundling indexes from other GitHub repository websites to provide a common searchable collection.

## Using your site search from the URL box (omnibox)

If you&#39;re using Firefox you&#39;re in luck. When you load a website search page and then clear the menu bar you should see your search icon and pull down list of search options on the left of the URL box (omnibox).

&#60;img src=&#34;Firefox_URL_Box.png&#34; alt=&#34;Picture of the Firefox URL box&#34; style=&#34;max-width:50%&#34;&#62;

Clicking on the downward caret lists the  available search engines. Notice the &#34;Robert&#39;s Rambling Search Engine&#34; at the top. Clicking this will add it to the list of search engines available when typing in the URL box (omnibox).

&#60;img src=&#34;Firefox_Search_Engine_Choice.jpg&#34; alt=&#34;Picture of the Firefox Search Engine List&#34; style=&#34;max-width: 30%&#34;&#62;

Now when you want to search my web site you can click the &#34;rsdoiel&#34; (short name) from the pull down list.


[^01]: See https://en.wikipedia.org/wiki/OpenSearch_(specification)

[^02]: Since Amazon pushed the fork I&#39;ve wondered if the name choices was deliberate to sow confusion. They want lock in just as much as the next Big Co.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/08/09/opensearch_description_documents_need_love.md" length="9888" type="text/markdown" />
    </item>    <item>
      <title>Build a CommonMark Processor</title>
      <link>https://rsdoiel.github.io/blog/2025/07/26/building_cmarkprocess.html</link>
      <description>
        <![CDATA[In this post I go over the process of building a TypeScript module called `commonMarkDoc.ts` along with a simple command line CommonMark processor called `cmarkprocess`.
        CommonMark pre-processor features are
        
          - support `@include-code-block` for including code samples as code blocks
          - support `@include-text-block` for include plain text into a CommonMark document
          - transform Hashtags into front matter
          - transform @Tags into front matter]]>
      </description>
      <source:markdown># Building a CommonMark Processor in Deno+TypeScript

By R. S. Doiel, 2025-07-26

CommonMark and Markdown are easier to proof read and edit than HTML. CommonMark is a super set of John Grubber&#39;s original Markdown. It incorporates common practices and extensions to the original Markdown. I&#39;ve found over the years the &#34;Markdown&#34; I type is really &#34;CommonMark&#34;. I use [Pandoc](https://pandoc.org) for processing my CommonMark documents into HTML. There are a few transforms I&#39;d like to make before I send the text off to Pandoc. That is what I&#39;ll be covering.

My CommonMark Processor will be responsible for several things. The features I miss are simple. Here&#39;s the short list.

- support `@include-code-block` for including code samples as code blocks
- support `@include-text-block` for include plain text into a CommonMark document
- transform Hashtags into front matter
- transform web Mentions into front matter

My homepage is built from a sequence of plain text files.
I also commonly need to include source code in my blog posts.  That has lead me to think about an include mechanisms. A source file should be included in a CommonMark code block while plain text can be included as is. 

The include blocks, text and code, can also be detected through regular expression. The difference for those is they require reading files from disk. That needs to be handled.

Here&#39;s the syntax I&#39;d use for code block and included texts.

  &#62; `@include-code-block` `FILEPATH [LANGUAGE]`

  &#62; `@include-text-block` `FILEPATH`

Finally I&#39;d to add support for [Hashtags](https://en.wikipedia.org/wiki/Hashtag) and web [Mentions](https://en.wikipedia.org/wiki/Mention_(blogging)). I want to explore integrating both with facets in search results, for that I&#39;ll need to track them in the front matter. Overtime I&#39;ll explore new features. The `commonMarkDoc.ts` module needs to be simple to extend.

How do I extract Hashtags and Mentions? Both are function similar though are used for different purposes. A regular expression should be suitable to pick them out. The difference between extracting a Hashtag or a Mention is the prefix, &#34;#&#34; or &#34;@&#34;. A function that code use the supplied text and prefix could return a list of tagged results.

I want to easily extend the processor. I can create modules based on the transforms I need. Each module will include a function that implements the transform. The `process` method will be responsible for sequencing the transforms and updating the CommonMark object with the results.

What do I need in my CommonMark document object? I need to take the markup, parse it and have the object holding the CommonMark document split into front matter and content. Similarly I will need to reassemble the parts into back into a CommonMark text.  Those functions will be called `parse` and `stringify`. These names are idiomatic in JavaScript and TypeScript. The object type will be called `CommonMarkDoc`.

Here is the basic outline of the `CommonMarkDoc` object without the `process` method.

~~~TypeScript
/**
 * commonMarkDoc is a Deno TypeScript module for working with CommonMark documents.
 * Copyright (C) 2025 R. S. Doiel
 * 
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.
 * 
 * You should have received a copy of the GNU Affero General Public License
 * along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.
 * 
 * @contact: rsdoiel@gmail.com
 * @issues: https://github.com/rsdoiel/commonMarkDoc/issues
 * 
 */
import * as yaml from &#34;@std/yaml&#34;;

import { extractTags, mergeTags } from &#34;./extractTags.ts&#34;;
import { includeCodeBlock } from &#34;./includeCodeBlock.ts&#34;;
import { includeTextBlock } from &#34;./includeTextBlock.ts&#34;;

export class CommonMarkDoc {
  frontMatter: Record&#60;string, unknown&#62; = {};
  content: string = &#34;&#34;;

  // Parse a CommonMark or Markdown document into content and front matter
  parse(text: string) {
    const frontMatterRegex: RegExp = /^---([\s\S]+?)---/;
    const match: Array&#60;string&#62; | null = text.match(frontMatterRegex) as Array&#60;
      string
    &#62;;

    if (match) {
      this.frontMatter = yaml.parse(match[1]) as Record&#60;string, unknown&#62;;
      this.content = text.slice(match[0].length).trim();
    } else {
      this.frontMatter = {};
      this.content = text;
    }
  }

  // Return this object as a CommonMark document with front matter and content.
  stringify(): string {
    if (Object.keys(this.frontMatter).length &#62; 0) {
      return `---
${yaml.stringify(this.frontMatter)}---

${this.content}`;
    }
    return this.content;
  }
}


~~~

This object makes it easy to work the front matter and the main content parts of a CommonMark document. The `parse` and `stringify` methods can bookend the `process` method implementing the transform sequence. This provides the functionality needed to implement a CommonMark Processor.

The `process` method will evolve overtime. I need to minimize its complexity. The `process` method is only responsible for sequencing the transforms defined in their own modules.

## Inclusion mechanisms

Before writing the `process` method I will work through the transform modules.

I need two inclusion mechanisms. One will support plain text file inclusion. The other will wrap the included file in the CommonMark markup for code blocks. Here&#39;s the syntax I want to use in my CommonMark document.

&#62; `@include-text-block` `FILENAME`

&#62; `@include-code-block` `FILENAME LANGUAGE`

Each of these will be implemented in their own module. Let&#39;s look at the one for `@include-text-block`. Here&#39;s what the include text module looks like.

~~~TypeScript
/**
 * includeTextBlock takes a text string and replaces the code blocks
 * based on the file path included in the line and the langauge name
 * The generate code block uses the `~~~` sequence to delimit the block
 * with the language name provided in the opening delimiter.
 *
 * @param text:string to be transformed
 * @returns the transformed text as a string
 */
export function includeTextBlock(text: string): string {
  // Find the include-text-block directive in the page.
  const insertBlockRegExp:RegExp = /@include-text-block\s+([^\s]+)(?:\s+(\w+))?/g;

  // Insert the code blocks
  return text.replace(insertBlockRegExp, replaceTextBlock);
}

// replaceTextBlock does that actual replacement work with the result
// of the matched RegExp.
function replaceTextBlock(_fullMatch: string, filePath:string):string {
  let fileContent:string = &#39;&#39;;
  try {
    fileContent = Deno.readTextFileSync(filePath);
  } catch (error) {
    console.error(`Error inserting block from ${filePath}, ${error}`);
  }
  if (fileContent) {
    return fileContent;
  } else {
    return `@include-text-block ${filePath}`;
  }
}

~~~

The public function handles finding the file referenced, reading it into a string before including it the transformed content block.  Let&#39;s look at the one for `@include-code-block`.

~~~TypeScript
/**
 * commonMarkDoc is a Deno TypeScript module for working with CommonMark documents.
 * Copyright (C) 2025 R. S. Doiel
 * 
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.
 * 
 * You should have received a copy of the GNU Affero General Public License
 * along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.
 * 
 * @contact: rsdoiel@gmail.com
 * @issues: https://github.com/rsdoiel/commonMarkDoc/issues
 * 
 * includeCodeBlock.js is a package implementing the `@include-code-block` extension to
 * CommonMark markup.
 */

/**
 * includeCodeBlock takes a text string and replaces the code blocks
 * based on the file path included in the line and the langauge name
 * The generate code block uses the `~~~` sequence to delimit the block
 * with the language name provided in the opening delimiter.
 *
 * @param text (string) to be transformed
 * @returns the transformed text as a string
 */
export function includeCodeBlock(text: string):string {
  // Find the include-code-block directive in the page. 
  const insertBlockRegExp: RegExp = /@include-code-block\s+([^\s]+)(?:\s+(\w+))?/g;

  // Insert the code blocks
  return text.replace(insertBlockRegExp, replaceCodeBlock);
}

// replaceCodeBlock does that actual replacement work with the result
// of the matched RegExp.
function replaceCodeBlock(_fullMatch:string, filePath: string, language:string = &#34;&#34;): string {
  const fileContent:string = Deno.readTextFileSync(filePath);
  if (fileContent) {
    return &#34;~~~&#34; + language + &#34;\n&#34; + fileContent + &#34;\n~~~&#34;;
  } else {
    console.error(`Error inserting block from ${filePath}`);
    return `@include-code-block ${filePath} ${language}`;
  }
}

~~~

These modules are very similar. I&#39;ve implemented them as separate modules because I want the option of evolving them independently. I don&#39;t want to entangled the two functions unnecessarily. Keeping both simple in separate modules aligns with that.

## Hashtags and Mentions

The other transforms needed in the `process` method are extracting the Hashtags and Mentions which will be used to update the front matter. Both Hashtags and Mentions are similar. They have a prefix, &#34;#&#34; or &#34;@&#34; followed by a sequence of alphanumeric characters, period and underscores. Trailing periods are stripped. They should be kept separate in the front matter. Each plays different content roles. 

Collecting  tags in the text is easy using a regular expression. I have to make a choice about the resulting list. One approach would just be list a tag each time it is encountered. This will result in repeated tags. That can be problematic. Instead I would like the list of tags returned to be a unique list. The extraction function will need a parameter for the source text and the prefix. It should return a list of unique tags. I&#39;m going to call this function, `extractTags`.

As I collect tags I will need an ability to merge the unique tag. That suggestions a merge function. That function will take one or more lists of tags and return a single list of unique tags.  I&#39;m going to call these function `mergeTags`. Both relate to tags and exist because of extraction.  I&#39;ll put them in a module called `extractTags.ts`.  Here is my implementation.

~~~TypeScript
// Extract tags. By default it extracts HashTags. You may provide
// another prefix like &#39;@&#39; to extract @Tags.
export function extractTags(text: string, prefix: string = &#34;#&#34;): string[] {
  // Regular expression to match tags based on the prefix, including alphanumeric,
  // periods, and underscores.
  const regex = new RegExp(`${prefix}[\\w.]+`, &#34;g&#34;);
  //const regex = new RegExp(`${prefix}[\\w.]+?(?=\\s|$|[^\\w.])`, &#39;g&#39;);
  const tags = text.match(regex);
  if (tags === null) {
    return [];
  }
  // Further process the tags to remove any trailing periods
  return tags.map((tag) =&#62; tag.replace(/\.$/, &#34;&#34;));
}

// mergeTags takes a list of tag lists and merges them
// into a single list of unique tags.
export function mergeTags(...tagLists: string[][]): string[] {
  // Use a Set to automatically handle uniqueness
  const uniqueTags = new Set&#60;string&#62;();

  // Iterate over each list of tags
  tagLists.forEach((tagList) =&#62; {
    // Add each tag to the Set
    tagList.forEach((tag) =&#62; uniqueTags.add(tag));
  });

  // Convert the Set back to an array and return it
  return Array.from(uniqueTags);
}

~~~

## The `process` method

I have the set of transforms I want. I will need to sequence them in the `process` method.  Here is a look at the internals of the `process` method.

~~~TypeScript

// Process uses the existing CommonMark object to create a new one transforming the front matter
// and content accordingly. E.g. handling HashTags, @Tags, `@include-code-block`.
//
// NOTE: This function uses a synchonous read to include content for code blocks. If have a slow disk
// access or lots of included code blocks this will raise the execution time of this method.
//
// If a code block can&#39;t be read it will leave the `@include-code-block` text in place.
process() {
  // Clone a copy of this object.
  const cmark: CommonMarkDoc = new Object(this) as CommonMarkDoc;

  // Handle included text blocks
  cmark.content = includeTextBlock(cmark.content);

  // Extract any HashTags from content
  const hashTags: string[] = extractTags(cmark.content, &#34;#&#34;);
  const atTags: string[] = extractTags(cmark.content, &#34;@&#34;);

  // Process our HashTags adding them to our keywords list
  if (
    cmark.frontMatter.hashTags === undefined ||
    cmark.frontMatter.hashTags === null
  ) {
    cmark.frontMatter.hashTags = [];
  }
  cmark.frontMatter.hashTags = mergeTags(cmark.frontMatter.hashTags as string[], hashTags);

  // Process our @Tags and add them to an AtTag list.
  if (
    cmark.frontMatter.atTags === undefined ||
    cmark.frontMatter.atTags === null
  ) {
    cmark.frontMatter.atTags = [];
  }
  cmark.frontMatter.atTags = mergeTags(cmark.frontMatter.atTags as string[], atTags);

  // Handle include code blocks
  cmark.content = includeCodeBlock(cmark.content);

  // We can now return the revised object.
  return cmark;
}

~~~

To use the the transform functions in the `process` method I need to import them.

~~~TypeScript

import { extractTags, mergeTags } from &#34;./extractTags.ts&#34;;
import { includeCodeBlock } from &#34;./includeCodeBlock.ts&#34;;
import { includeTextBlock } from &#34;./includeTextBlock.ts&#34;;


~~~

## The complete CommonMark processor module

Here is the complete `commonMarkDoc.ts` module.

~~~TypeScript
import * as yaml from &#34;@std/yaml&#34;;

import { extractTags, mergeTags } from &#34;./extractTags.ts&#34;;
import { includeCodeBlock } from &#34;./includeCodeBlock.ts&#34;;
import { includeTextBlock } from &#34;./includeTextBlock.ts&#34;;

export class CommonMarkDoc {
  frontMatter: Record&#60;string, unknown&#62; = {};
  content: string = &#34;&#34;;

  // Parse a CommonMark or Markdown document into content and front matter
  parse(text: string) {
    const frontMatterRegex: RegExp = /^---([\s\S]+?)---/;
    const match: Array&#60;string&#62; | null = text.match(frontMatterRegex) as Array&#60;
      string
    &#62;;

    if (match) {
      this.frontMatter = yaml.parse(match[1]) as Record&#60;string, unknown&#62;;
      this.content = text.slice(match[0].length).trim();
    } else {
      this.frontMatter = {};
      this.content = text;
    }
  }

  // Return this object as a CommonMark document with front matter and content.
  stringify(): string {
    if (Object.keys(this.frontMatter).length &#62; 0) {
      return `---
${yaml.stringify(this.frontMatter)}---

${this.content}`;
    }
    return this.content;
  }

  // Process uses the existing CommonMark object to create a new one transforming the front matter
  // and content accordingly. E.g. handling HashTags, @Tags, `@include-code-block`.
  //
  // NOTE: This function uses a synchonous read to include content for code blocks. If have a slow disk
  // access or lots of included code blocks this will raise the execution time of this method.
  //
  // If a code block can&#39;t be read it will leave the `@include-code-block` text in place.
  process() {
    // Clone a copy of this object.
    const cmark: CommonMarkDoc = new Object(this) as CommonMarkDoc;

    // Handle included text blocks
    cmark.content = includeTextBlock(cmark.content);

    // Extract any HashTags from content
    const hashTags: string[] = extractTags(cmark.content, &#34;#&#34;);
    const atTags: string[] = extractTags(cmark.content, &#34;@&#34;);

    // Process our HashTags adding them to our keywords list
    if (
      cmark.frontMatter.hashTags === undefined ||
      cmark.frontMatter.hashTags === null
    ) {
      cmark.frontMatter.hashTags = [];
    }
    cmark.frontMatter.hashTags = mergeTags(cmark.frontMatter.hashTags as string[], hashTags);

    // Process our @Tags and add them to an AtTag list.
    if (
      cmark.frontMatter.atTags === undefined ||
      cmark.frontMatter.atTags === null
    ) {
      cmark.frontMatter.atTags = [];
    }
    cmark.frontMatter.atTags = mergeTags(cmark.frontMatter.atTags as string[], atTags);

    // Handle include code blocks
    cmark.content = includeCodeBlock(cmark.content);

    // We can now return the revised object.
    return cmark;
  }
}

~~~

## The CommonMark processor application

To use this module I need to wrap it so I can execute it from the command line. My processor is going to be called `cmarkprocess` so I&#39;ll name the module that becomes the command line program is `cmarkprocess.ts`. This module will include a &#34;main&#34; function, that function will handle command line options and parameters as well as read data from either standard input or a file.  It&#39;ll use the `CommonMarkDoc` `process` method and write the results to standard out.

~~~TypeScript
import * as yaml from &#34;@std/yaml&#34;;
import { parse as parseArgs } from &#34;@std/flags&#34;;

import { CommonMarkDoc } from &#34;./commonMarkDoc.ts&#34;;

// main implements a light weight CommonMark processor
// called `cmarkprocess`. It demonstrates the features of the
// CommonMarkDoc module.
async function main() {
  const appName = &#34;cmarkprocess&#34;;
  const args = parseArgs(Deno.args, {
    boolean: [&#34;help&#34;, &#34;version&#34;, &#34;license&#34;],
    alias: {
      h: &#34;help&#34;,
      v: &#34;version&#34;,
      l: &#34;license&#34;,
    },
  });
  const version:string = &#39;1&#39;;
  const licenseText: string = `${appName} is a program to process CommonMark documents with front matter.
    Copyright (C) 2025 R. S. Doiel

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as
    published by the Free Software Foundation, either version 3 of the
    License, or (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.`;

  if (args.help) {
    console.log(`%${appName} | user manual

# NAME

${appName}

# SYNOPSIS

${appName} [COMMONMARK_FILENAME]

# DESCRIPTION

This is a CommonMark processor that can read a CommonMark text
transforming it into an update CommonMark text base on the following
features.

- support for HashTags and Mentions, merging them into the document&#39;s
  front matter
- \`@include-text-block\` allows for text includes
  - Example: \`@include-text-black\` \`FILENAME\`
- \`@include-code-block\` allows for including code blocks
  - Example: \`@include-code-black\` \`FILENAME [LANGUAGE]\`

# OPTION

-h, --help
: display help

-v,--version
: display version

-l, --license
: display license

`);
    Deno.exit(0);
  }

  if (args.version) {
    console.log(`${appName} ${version}`);
    Deno.exit(0);
  }

  if (args.license) {
    console.log(licenseText);
    Deno.exit(0);
  }

  const cmark = new CommonMarkDoc();
  const filePath = args._[0] as string;
  let text: string = &#34;&#34;;
  if (args._.length === 0) {
    const decoder = new TextDecoder();
    for await (const chunk of Deno.stdin.readable) {
      text += decoder.decode(chunk);
    }
  } else {
    text = await Deno.readTextFile(filePath);
  }
  cmark.parse(text);
  try {
    await cmark.process();
  } catch (error) {
    console.error(`ERROR (${filePath}): ${error}`);
    Deno.exit(1);
  }
  const output = cmark.stringify();
  console.log(output);
}

if (import.meta.main) {
  await main();
}

~~~

Now that we have our wrapping modules, how do I get a nice executable using Deno?

~~~shell
deno compile --allow-read -o bin/cmarkprocess cmarkprocess.ts
~~~

The result is an executable, `bin/cmarkprocess`. This executable can read from standard input or from a file path. It will write to standard output.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/07/26/building_cmarkprocess.md" length="21518" type="text/markdown" />
    </item>    <item>
      <title>Signed Binaries and Business Models</title>
      <link>https://rsdoiel.github.io/blog/2025/07/24/WHY_NO_SIGNED_BINARIES.html</link>
      <description>
        <![CDATA[This post explains why I don't provide signed binaries in the open source software I create and release.]]>
      </description>
      <source:markdown># Thoughts on signed binaries and business models

By R. S. Doiel, 2025-07-24

## Why no Apple signed binaries?

To sign the binaries I would need to be an &#34;Apple Developer&#34;. To be an &#34;Apple Developer&#34;, I or someone on my behalf would need to pay an annual fee. I thinks this is ridiculous. Why do you need to pay someone to write software for hardware that was already paid for? It feels like being extorted for your lunch money. I am stubborn and refuse to be an Apple Developer. I write Open Source software so anyone can compile and run it. Should they choose this includes Apple employees. I do not extort lunch money.

Apple&#39;s insistence on requiring payment is a little rich. Apple&#39;s operating systems, macOS and iOS, are derived from Open Source software. Apple&#39;s current operating systems are derived from the Mach kernel and BSD. The developer tools are derived from Clang. Another open source software project. The software I write is also open source. There&#39;s a trend here.

## Why no signed binaries for other companies?

When I considered Apple&#39;s policies, I rejected the fee to write software. When Microsoft, Google, et el. followed a similar path my eye brows raised in extreme skepticism. I am concerned for my eye brows.

I believe this requirement is less about actual security and more about business models. After all signing could be done in an open, secure and distributed manner. It&#39;s as easy as using a standard form of public signing (e.g. PGP, SSH keys), having the OS supported the signed binaries and hosting the public keys of the signed document a location controlled by the individual or organization signing the executable. A distributed approach does not require corporate sponsorship, implementation or blessings. You could even implement a signing mechanism similar to Let&#39;s Encrypt used for HTTPS certificates.

Security is being used as an excuse to enable vendor lock in or drive people to rent software from the &#34;cloud&#34;. This seems incredibly silly to me. I am a stubborn person. I am **NOT** going along with this practice. I apologize for the inconvenience but insist I avoid feeling a part of a lunch money extortion plot. I feel I should be free to practice my vocation of writing open source software without being required to participate in business models that neither benefit my employer, my community or myself.

## What are your options to use the software?

There are ways to allow the operating system to run the software. You can follow the instructions complete with the scary warnings to do so. You can also download the source code and build the software yourself. It is open source software after all.

- [installing unsigned software macOS](INSTALL_NOTES_macOS.txt)
- [installing unsigned software Windows](INSTALL_NOTES_Windows.txt)

# ABOUT THIS DOCUMENT

These are my opinions. My opinions do not reflect my employer&#39;s policies, procedures or practices.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/07/24/WHY_NO_SIGNED_BINARIES.md" length="3428" type="text/markdown" />
    </item>    <item>
      <title>Simplifying BlogIt</title>
      <link>https://rsdoiel.github.io/blog/2025/07/21/Simplifying_BlogIt.html</link>
      <description>
        <![CDATA[BlogIt is a command I've written many times over the years. At it's core
        is did a two simple things. 
        
        1. Copy CommonMark files into a blog directory three
        2. Use Front Matter as a source for aggregated blog metadata
        
        In it the new incarnation it is primarily focus on curating the front matter
        followed by copying the document into the blog directory structure. 
        
        1. Curate CommonMark file front matter
        2. Copy CommonMark files into the blog directory tree
        
        Other tools can aggregate blog metadata like [FlatLake](https://flatlake.app).]]>
      </description>
      <source:markdown># Simplifying BlogIt

By R. S. Doiel, 2025-07-21

NOTE: This post was updated to include minor bug fixes, RSD 2025-07-28.

__BlogIt__ is a command I&#39;ve written many times over the years. Previously it was intended to perform two tasks.

1. Copy CommonMark documents into a blog directory tree
2. Aggregate metadata from the document for my blog

I am updating the way my website and blog are is built. I am adopting [FlatLake](https://flatlake.app) for fulfill the role of aggregator. This changes the role __BlogIt__ plays.  Since I am relying on an off the shelf tool to perform front matter aggregation it becomes more important to make the front matter consist. The new priorities for __BlogIt__ are.

1. Curating the front matter of the CommonMark document
2. When ready to publish, update the front matter and Copy CommonMark document into the blog directory tree

With curating front matter the priority some additional features will be helpful.

- check a file for well formed front matter with the minimum field requirements
- check directories for CommonMark documents and their front matter

## BlogIt command line

Here&#39;s what that might look like on the command line.

~~~shell
blogit [OPTIONS] ACTION COMMONMARK_FILE [DATE_OF_POST]
~~~

## OPTIONS

- help
- version
- license
- prefix BLOG_BASE_PATH (to set an explicit path to the &#34;blog&#34; directory)

## ACTION

The following actions need to be supported in the new implementation.

- check COMMONMARK_FILE | DIRECTORY 
  - validate the front matter in a file or directory of CommonMark documents
- draft
  -  set the front matter draft attribute to true, clear the published date
- edit COMMONMARK_FILE [FRONT_MATTER_FIELD ...]
  - edit all or a subset of standard front matter fields
- publish COMMONMARK_FILE
  - read front matter
  - set draft to false
  - set publish date and update modified date
  - validate front matter
  - on success, save the updates then copy into blog directory tree
    - prompt if it will overwrite a file

## Editing front matter

__BlogIt__ is a terminal application. The programs scans the source CommonMark file for existing front matter. For each expected element the current (or default) value of the element is displayed with a prompt to edit it. If editing is chosen then the value is presented in the default editor for update. The saved value is then used to update the front matter element. A temporary file is used to communicate between __BlogIt__ and the system provided text editor.

Complex fields like keywords are provided to the text edit as YAML. The default should show the desired structure as YAML with placeholder values to be edited.

## Front Matter

The basic front matter I want to use is straight forward as my blog started almost a decade ago. Essentially it is title, author, abstract, dateCreated, dateModified, datePublished and keywords. Some blog items have a series name and number so I will support those fields as well.

__BlogIt__ will be written in TypeScript this time. I can cover my bases with the following interfaces.

~~~TypeScript
/* This describes the front matter metadata object */
interface Metadata {
    title: string; /* Optional because they are optional in RSS 2 */
    author: string;
    abstract: string; /* Maps to description in RSS 2 */
    dateCreated: string; /* ISO 8601 date */
    dateModified: string; /* ISO 8601 date */
    datePublished?: string; /* ISO 8601 date */
    draft?: boolean /* if true then BlogIt processes document as a draft */
    keywords?: string[];
    series?: string;
    seriesNo?: number;
    copyrightYear?: string; /* Four digit year */
    copyrightHolder?: string;
    license?: string; /* Text of license or a URL pointing at the license */
}
~~~

BlogIt expectations

- working directory contains a directory called &#34;blog&#34; (this is customary but not always the place the blog resides)
  - An explicit blog directory can be set using the `prefix` option
- The directory structure is formed as `&#60;prefix&#62;/&#60;YEAR&#62;/&#60;MONTH&#62;/&#60;DAY&#62;` where year is four digits, month and day are two digits (zero padded).
- the default date is today, may explicitly be provided by the front matter as `.datePublished`
- the date fields automatically supported are `dateCreated`, `dateModified` and `datePublished`. The `dateModified` should be updated automatically each time __BlogIt__ changes the document. `dateCreated` is set the first time the front matter is created or edited.  `datePublished` is set the first time the CommonMark document  is &#34;published&#34; into the blog directory tree. This also results in the draft field being removed.

Recursive blog maintenance could be supported by allowing the tool to walk a directory tree and when it encounters CommonMark documents the front matter is validate. Errors are written to standard out. This feature would ensure that the CommonMark documents are ready for processing by the website build process.

## Checking for Front Matter

Front Matter traditionally is found at the start of the CommonMark file. It starts with the a line matching `---` and terminates with same `---` line. Anything between the two is treated as YAML.  Checking the front matter means identifying the YAML source, parsing it and comparing the result with the interface definition. If an expected field is missing then prompt for it and if the response is &#34;y&#34; create a temp file of the content and invoke a default editor for the system. When the editor is exited the source is read back in and the front matter is updated.

## Processing the Front Matter

Aside from extracting the YAML front matter from the text, the standard Deno library (`@std/yaml`) can be used to populate the interface for validation and editing.

The task for __BlogIt__ is primarily orchestrating the use of existing Deno TypeScript modules implementing the functionality I want from __BlogIt__.

## Rewriting the CommonMark document

If the front matter changes then the CommonMark document should be written to a backup file (e.g. &#34;.bak&#34;). If changes are made the interface should prompt before saving the backup and writing out the updates to the source CommonMark document.

## Draft versus datePublished

If the front matter includes the value `draft: true` __BlogIt__ will exit after updating the front matter.  If `draft: true` is not in the front matter (e.g. `draft: false` or doesn&#39;t exist), the value  of `datePublished` needs to be set to the current date if not already populated. The `datePublished` is used to calculate the target path for coping the CommonMark document.

The action &#34;draft&#34; will set the `draft` value to `true` and clear `datePublished`.

The action &#34;publish&#34; will remove the `draft` attribute setting the publication and modification dates. If the front matter is valid then it will save the updated metadata and proceed to copy the revised CommonMark document into the blog tree.

## The Program

### editor module

Calling out to the system&#39;s text editor and running the editor as a sub process should be implemented as it&#39;s own module. This will allow me to improve the process independently and potentially use it in other applications.

~~~TypeScript
/**
 * editor.ts module handles the setup and access to a text editor for updating front matter. It is part of BlogIt program.
 *
 *  Copyright (C) 2025  R. S. Doiel
 * 
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Affero General Public License as
 *  published by the Free Software Foundation, either version 3 of the
 *  License, or (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Affero General Public License for more details.
 *
 *  You should have received a copy of the GNU Affero General Public License
 *  along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.
 */
import { exists } from &#39;@std/fs/exists&#39;;

/**
 * pickEditor
 * @return string
 */
function pickEditor(): string {
  let editor: string | undefined = Deno.env.get(&#34;EDITOR&#34;);
  if (editor === undefined) {
    if (Deno.build.os === &#34;windows&#34;) {
      editor = &#34;notepad.exe&#34;;
    } else {
      editor = &#34;nano&#34;;
    }
  }
  return editor as string;
}

export function getEditorFromEnv(envVar?: string): string {
  const editor: string | undefined = (envVar === undefined)
    ? undefined
    : Deno.env.get(envVar);
  if (editor === undefined) {
    return pickEditor();
  }
  return editor as string;
}

// editor.ts assumes Micro Editor in order to simplify testing.
const editor: string = pickEditor();

// editFile takes an editor name and filename. It runs the editor using the
// filename (e.g. micro, nano, code) and returns success or failure based on
// the the exit status code. If the exit statuss is zero then true is return,
// otherwise false is returned.
export async function editFile(
  editor: string,
  filename: string,
): Promise&#60;{ ok: boolean; text: string }&#62; {
  const command = new Deno.Command(editor, {
    args: [filename],
    stdin: &#34;inherit&#34;,
    stdout: &#34;inherit&#34;,
    stderr: &#34;inherit&#34;,
  });
  const child = command.spawn();
  const status = await child.status;
  if (status.success) {
    const txt = await Deno.readTextFile(filename);
    return { ok: status.success, text: txt };
  }
  return { ok: status.success, text: &#34;&#34; };
}

// editTempData will take data in string form, write it
// to a temp file, open the temp file for editing and
// return the result. If a problem occurs then an undefined
// value is returns otherwise is the contents of the text file
// as a string.
export async function editTempData(val: string): Promise&#60;string&#62; {
  const tmpFilename = await Deno.makeTempFile({
    dir: &#34;./&#34;,
    prefix: &#34;blogit_&#34;,
    suffix: &#34;.tmp&#34;,
  });
  if (val !== &#34;&#34;) {
    await Deno.writeTextFile(tmpFilename, val);
  }
  const res = await editFile(editor, tmpFilename);
  if (await exists(tmpFilename, {isFile: true})) {
    await Deno.remove(tmpFilename);
  }
  if (res.ok) {
    // NOTE: string is returned via standard out not the text of the file.
    return res.text;
  }
  return val;
}

~~~

### Front Matter

The front matter handling is implemented as it&#39;s own TypeScript module, `frontMatter.ts`. This module defines all the front matter schema and the operations that maybe performed on it including the interactive prompts. 

~~~TypeScript
/**
 * frontMatterEditor.ts module curates front matter for Common Mark or Markdown documents. It is part of the BlogIt project. 
 * 
 *  Copyright (C) 2025  R. S. Doiel
 * 
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Affero General Public License as
 *  published by the Free Software Foundation, either version 3 of the
 *  License, or (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Affero General Public License for more details.
 *
 *  You should have received a copy of the GNU Affero General Public License
 *  along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.
 */
import { parse, stringify } from &#34;@std/yaml&#34;;
import { CommonMarkDoc } from &#34;./commonMarkDoc.ts&#34;;
import { editTempData } from &#34;./editor.ts&#34;;

export const metadataFields: Array&#60;keyof Metadata&#62; = [
  &#34;title&#34;,
  &#34;author&#34;,
  &#34;contributor&#34;,
  &#34;abstract&#34;,
  &#34;draft&#34;,
  &#34;dateCreated&#34;,
  &#34;dateModified&#34;,
  &#34;datePublished&#34;,
  &#34;keywords&#34;,
  &#34;series&#34;,
  &#34;seriesNo&#34;,
  &#34;copyrightYear&#34;,
  &#34;copyrightHolder&#34;,
  &#34;license&#34;
];


export interface Metadata {
  title?: string;
  author: string;
  contributor?: string;
  abstract: string;
  dateCreated: string;
  dateModified: string;
  datePublished?: string;
  draft?: boolean;
  keywords?: string[];
  series?: string;
  seriesNo?: number;
  pubType?: string;
  copyrightYear?: number;
  copyrightHolder?: string;
  license?: string | URL;
}


function yyyymmdd(s: string): boolean {
  const dateFormatRegex = /^\d{4}-\d{2}-\d{2}$/;
  return dateFormatRegex.test(s);
}

function assignValue&#60;T extends keyof Metadata&#62;(
  frontMatter: Record&#60;string, unknown&#62;,
  field: T,
  newValue: string,
) {
  let seriesNo = 0;
  if (newValue.trim() === &#39;&#39;) {
    delete frontMatter[field];
    return;
  }
  switch (field) {
    case &#34;abstract&#34;:
      frontMatter[field] = newValue;
      break;
    case &#34;draft&#34;:
      if (newValue.toLowerCase() === &#34;true&#34;) {
        frontMatter[field] = true;
      }
      break;
    case &#34;keywords&#34;:
      frontMatter[field] = parse(newValue);
      //FIXME: if there are no keywords then we could remove the field
      break;
    case &#34;series&#34;:
      frontMatter[field] = newValue;
      break;
    case &#34;seriesNo&#34;:
      seriesNo = Number(newValue) || 0;
      if (seriesNo &#62; 0) {
        frontMatter.seriesNo = seriesNo;
      } else {
        delete frontMatter.seriesNo;
      }
      break;
    case &#34;copyrightYear&#34;:
      if (newValue.trim() === &#39;&#39;) {
        delete frontMatter.copyrightYear;
      } else {
        frontMatter[field] = Number(newValue);
      }
      break;
    case &#34;dateCreated&#34;:
    case &#34;dateModified&#34;:
    case &#34;datePublished&#34;:
      if (newValue.trim() !== &#39;&#39; &#38;&#38; yyyymmdd(newValue.trim())) {
        frontMatter[field] = newValue.trim();
        delete frontMatter[&#39;draft&#39;];
      } else {
        delete frontMatter[field];
      }
      break;
    default:
      frontMatter[field] = newValue.trim();
      break;
  }
}

function getDefaultValueAsString(frontMatter: Record&#60;string, unknown&#62;, field: string): string {
  switch (field) {
    case &#34;draft&#34;:
	  if (frontMatter.datePublished === undefined || frontMatter.datePublished === null || frontMatter.datePublished === &#39;&#39;) {
      	//NOTE: The default value is draft === true
      	return &#34;true&#34;;
	  }
	  return &#39;&#39;;
    case &#34;dateCreated&#34;:
    case &#34;dateModified&#34;:
      return (new Date()).toISOString().split(&#34;T&#34;)[0];
    default:
      return &#34;&#34;;
  }
}

function getAttributeAsString(
  frontMatter: Record&#60;string, unknown&#62;,
  field: string,
): string {
  if (frontMatter[field] === undefined) {
    return &#39;&#39;;
  }
  switch (field) {
    case &#34;keywords&#34;:
      return stringify(frontMatter[field]);
    case &#34;seriesNo&#34;:
    case &#34;copyrightYear&#34;:
    case &#34;draft&#34;:
      return `${frontMatter[field]}`;
  }
  return frontMatter[field] as string;
}

async function promptToEditFields(
  cmarkDoc: CommonMarkDoc,
  fields: Array&#60;keyof Metadata&#62;,
) {
  let keys = metadataFields;
  if (fields.length &#62; 0) {
    keys = fields;
  }

  let changed = false;
  for (const key of keys) {
    // dateModified gets updated when the changed record is saved. We can skip it.
    if (key === &#39;dateModified&#39;) {
      continue
    }
    if (cmarkDoc.frontMatter[key] === undefined) {
      assignValue(cmarkDoc.frontMatter, key, getDefaultValueAsString(cmarkDoc.frontMatter, key));
    }
    // NOTE: draft and pub date are connected. A draft can&#39;t have a datePublished
    if (key === &#39;draft&#39; &#38;&#38; cmarkDoc.frontMatter.draft)  {
      delete cmarkDoc.frontMatter.datePublished;
    }
    // NOTE: Need to handle the case where draft has been set to false and a pub date is not yet set.
    // It should default to today like dateCreated and dateModified do.
    if (key === &#39;datePublished&#39; &#38;&#38; cmarkDoc.frontMatter.datePublished === &#39;&#39;) {
      if (cmarkDoc.frontMatter.draft === undefined || cmarkDoc.frontMatter.draft === false) {
        cmarkDoc.frontMatter.datePublished = (new Date()).toISOString().split(&#34;T&#34;)[0];
      }
    }
    // NOTE: we need to display the value in string form to prompt for editing.
    let val: string = getAttributeAsString(cmarkDoc.frontMatter, key);
    if (confirm(`${key}:\n${val}\nedit ${key}?`)) {
      const oldVal = val;
      //FIXME: call the editor to edit the value then convert it back usign assignValue
      val = await editTempData(val);
      if (oldVal !== val) {
        changed = true;
        assignValue(cmarkDoc.frontMatter, key, val);
      }
    }
  }
  cmarkDoc.changed = changed;
  // Make sure that date modified is updated on change
  if (cmarkDoc.changed &#38;&#38; cmarkDoc.frontMatter !== undefined &#38;&#38; cmarkDoc.frontMatter.dateModified !== &#34;&#34;) {
    cmarkDoc.frontMatter.dateModified = (new Date()).toISOString().split(&#34;T&#34;)[0];
  }
}

export async function editFrontMatter(
  cmarkDoc: CommonMarkDoc,
  fields: Array&#60;keyof Metadata&#62;,
) {
  await promptToEditFields(cmarkDoc, fields);
}

export function applyDefaults(cmarkDoc: CommonMarkDoc, defaults: Record&#60;string, unknown&#62;) {
  for (const k of Object.keys(defaults)) {
    switch (cmarkDoc.frontMatter[k]) {
      case undefined:
        cmarkDoc.frontMatter[k] = defaults[k];
        cmarkDoc.changed = true;
        break;
      case null:
        cmarkDoc.frontMatter[k] = defaults[k];
        cmarkDoc.changed = true;
        break;
      case &#39;&#39;:
        cmarkDoc.frontMatter[k] = defaults[k];
        cmarkDoc.changed = true;
        break;
      case 0:
        cmarkDoc.frontMatter[k] = defaults[k];
        cmarkDoc.changed = true;
        break;
    }
  }
}

~~~

### CommonMark module

My website is implemented using CommonMark documents that include front matter. It is helpful to be able to handle the documents 
in a uniform way. This is accomplished through a TypeScript module called `commonMarkDoc.ts`.  It defines an interface, `CommonMarkDoc` that contains three attributes, `frontMatter`, `markdown` and `changed`. The latter is a boolean flag that is set when something changes in either `frontMatter` or `markdown`.

The module also supports an Object called CMarkDoc that include a pre-processor function called `process` providing two useful features.

- mapping of &#34;.md&#34; file links to &#34;.html&#34; file links
- including code blocks from external files

~~~TypeScript
/**
 * commonMarkDoc.ts is a module for handling CommomMark and Markdown documents with front matter.
 * It is part of the BlogIt project.
 * 
 *  Copyright (C) 2025  R. S. Doiel
 * 
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Affero General Public License as
 *  published by the Free Software Foundation, either version 3 of the
 *  License, or (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Affero General Public License for more details.
 *
 *  You should have received a copy of the GNU Affero General Public License
 *  along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.
 */
import { parse, stringify } from &#34;@std/yaml&#34;;

export interface CommonMarkDoc {
  frontMatter: Record&#60;string, unknown&#62;;
  markdown: string;
  changed: boolean;
}

/**
 * stringToCommonMarkDoc takes a string and splits it into a record with frontmatter and markdown
 * @param content string, the text string to transform into a CommonMarkDoc type.
 * @return `{frontMatter: Record&#60;string, unknown&#62;; markdown: string }`
 */
export function stringToCommonMarkDoc(
  content: string,
): CommonMarkDoc {
  const frontMatterRegex = /^---([\s\S]+?)---/;
  const match = content.match(frontMatterRegex);
  let frontMatter: Record&#60;string, unknown&#62; = {};
  let markdown: string = content;
  if (match) {
    frontMatter = parse(match[1]) as Record&#60;string, unknown&#62;;
    markdown = content.slice(match[0].length).trim();
  }
  return { frontMatter: frontMatter, markdown: markdown, changed: false };
}

/**
 * commonMarkDocToString takes a CommonMarkDoc object and renders it into string prefixed by the front matter block.
 * @param cmarkDoc CommonMarkDoc
 * @return string
 */
export function commonMarkDocToString(
  cmarkDoc: CommonMarkDoc,
): string {
  if (Object.keys(cmarkDoc.frontMatter).length &#62; 0) {
    return `---
${stringify(cmarkDoc.frontMatter)}---

${cmarkDoc.markdown}`;
  }
  return cmarkDoc.markdown;
}

/**
 * commonMarkDocPreprocessor takes a CommonMarkDoc object and maps the &#34;.md&#34; links to &#34;.html&#34; links
 * and includes code blocks using the `@include-code-block` directive.
 *
 * @param cmarkDoc: CommmonMarkDoc
 * @return string
 */
export function commonMarkDocPreprocessor(
  cmarkDoc: CommonMarkDoc,
): string {
      // Convert markdown links to HTML links
    const markdownLinkRegex = /\[([^\]]+)\]\(([^)]+\.md)\)/g;
    let processedMarkdown = cmarkDoc.markdown.replace(markdownLinkRegex, (_fullMatch, linkText, filePath) =&#62; {
      const htmlFilePath = filePath.replace(/\.md$/, &#39;.html&#39;);
      return `[${linkText}](${htmlFilePath})`;
    });

    // include code blocks from external files
    const insertCodeBlockRegex = /@include-code-block\s+([^\s]+)(?:\s+(\w+))?/g;
    processedMarkdown = processedMarkdown.replace(insertCodeBlockRegex, (_fullMatch, filePath, language = &#39;&#39;) =&#62; {
      let fileContent = &#39;&#39;;
      try {
        fileContent = Deno.readTextFileSync(filePath);
      } catch (error) {
        return `Error reading ${filePath}, ${error}`;
      }
      if (fileContent !== &#39;&#39;) {
        return `~~~${language}\n${fileContent}\n~~~`;
      } else {
        return `Error inserting block from ${filePath}`;
      }
    });
    // include code blocks from external files
    const insertTextBlockRegex = /@include-text-block\s+([^\s]+)(?:\s+(\w+))?/g;
    processedMarkdown = processedMarkdown.replace(insertTextBlockRegex, (_fullMatch, filePath, language = &#39;&#39;) =&#62; {
      let fileContent = &#39;&#39;;
      try {
        fileContent = Deno.readTextFileSync(filePath);
      } catch (error) {
        return `Error reading ${filePath}, ${error}`;
      }
      if (fileContent !== &#39;&#39;) {
        return fileContent;
      } else {
        return `Error inserting block from ${filePath}`;
      }
    });
    if (processedMarkdown !== cmarkDoc.markdown) {
      return commonMarkDocToString({
          frontMatter: cmarkDoc.frontMatter,
          markdown: processedMarkdown,
          changed: true
      });
    }
    return commonMarkDocToString(cmarkDoc);
}

/**
 * CMarkDoc implements the interface CommonMarkDoc
 * It supports working with CommonMark documents that contain front matter
 */
export class CMarkDoc implements CommonMarkDoc {
  frontMatter: Record&#60;string, unknown&#62; = {};
  markdown: string = &#39;&#39;;
  changed: boolean = false;

  /**
   * parse takes a string hold CommonMark text and parses it into the CMarkDoc object structure.
   */
  parse(src: string): boolean {
    const cmarkDoc: CommonMarkDoc = stringToCommonMarkDoc(src);
    this.frontMatter = cmarkDoc.frontMatter;
    this.markdown = cmarkDoc.markdown;
    return (this.markdown.length &#62; 0);
  }
  
  /**
   * stringify takes this object and returns a CommonMark representation including front matter.
   */
  stringify(): string {
    return commonMarkDocToString(this);
  }

  /**
   * processSync is a CommonMark pre-processor implementing two features. It performs two
   * fucntions.
   *   1. converts links to markdown files (ext. &#34;.md&#34;) to their HTML file counter parts
   *   2. Any `@include-code-block` will include a source code file block in the resulting
   *      source document.
   */
  processSync(): string {
    return commonMarkDocPreprocessor(this);
  }
}


~~~

### Main

The main module, `mod.ts`, will allow for processing the command line option and performing the requested actions.

~~~TypeScript
/**
 * mod.ts - The main entry point for BlogIt, a Common Mark front matter validator and curation tool. It is part of the BlogIt project.
 *
 *  Copyright (C) 2025  R. S. Doiel
 * 
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Affero General Public License as
 *  published by the Free Software Foundation, either version 3 of the
 *  License, or (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Affero General Public License for more details.
 *
 *  You should have received a copy of the GNU Affero General Public License
 *  along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.
 */
// mod.ts
import { parse as parseArgs } from &#34;@std/flags&#34;;
import { exists } from &#34;@std/fs/exists&#34;;
import * as yaml from &#34;@std/yaml&#34;;
import { checkDirectory, checkFile, createBackup, publishFile, showFrontMatter } from &#34;./src/blogit.ts&#34;;
import { Metadata, editFrontMatter, applyDefaults } from &#34;./src/frontMatter.ts&#34;;
import {
  CommonMarkDoc,
  commonMarkDocPreprocessor,
  commonMarkDocToString,
  stringToCommonMarkDoc,
} from &#34;./src/commonMarkDoc.ts&#34;;
import { licenseText, releaseDate, releaseHash, version } from &#34;./version.ts&#34;;
import { helpText, fmtHelp } from &#34;./helptext.ts&#34;;

async function main() {
  const appName = &#39;BlogIt&#39;;
  const args = parseArgs(Deno.args, {
    boolean: [&#34;help&#34;, &#34;version&#34;, &#34;license&#34;, &#34;draft&#34;, &#34;check&#34;, &#34;edit&#34;, &#34;publish&#34;, &#34;process&#34;, &#34;show&#34; ],
    string: [&#34;prefix&#34;, &#34;apply&#34;],
    alias: {
      h: &#34;help&#34;,
      v: &#34;version&#34;,
      l: &#34;license&#34;,
      a: &#34;apply&#34;,
      p: &#34;prefix&#34;,
      c: &#34;check&#34;,
      d: &#34;draft&#34;,
      e: &#34;edit&#34;,
      P: &#34;process&#34;,
      s: &#34;show&#34;,
    },
    default: {
      prefix: &#34;blog&#34;,
      apply: &#34;&#34;,
    },
  });

  if (args.help) {
    console.log(
      fmtHelp(helpText, appName, version, releaseDate, releaseHash)
    );
    Deno.exit(0);
  }

  if (args.version) {
    console.log(`${appName} ${version} ${releaseDate} ${releaseHash}`);
    Deno.exit(0);
  }

  if (args.license) {
    console.log(licenseText);
    Deno.exit(0);
  }

  // Handle verb commands without dash prefix.
  switch (args._[0] as string) {
    case &#34;apply&#34;:
      // Shift &#34;apply&#34; off the args, then assign the value from the next parameter
      args._.shift();
      if (args._[0] !== undefined) {
        args.apply = args._.shift() as string;
      } else {
        console.error(`Missing defaults YAML filename`);
        Deno.exit(1);
      }
      break;
    case &#34;check&#34;:
      args.check = true;
      args._.shift();
      break;
    case &#34;edit&#34;:
      args.edit = true;
      args._.shift();
      break;
    case &#34;draft&#34;:
      args.draft = true;
      args._.shift();
      break;
    case &#34;process&#34;:
      args.process = true;
      args._.shift();
      break;
    case &#34;publish&#34;:
      args.publish = true
      args._.shift();
      break;
    case &#34;show&#34;:
      args.show = true
      args._.shift();
      break;
  }

  const filePath = args._[0] as string; // Explicitly assert filePath as string
  const dateOfPost = args._[1] as string | undefined; // Explicitly assert dateOfPost as string or undefined

  if (args.check) {
    if (await exists(filePath, {isDirectory: true})) {
      console.log(`Checking the directory ${filePath}`);
      await checkDirectory(filePath);
    }
    if (await exists(filePath, { isFile: true})) {
      console.log(`Checking the file ${filePath}`);
      await checkFile(filePath)
    }
    Deno.exit(0);
  }


  if (!filePath) {
    console.error(&#34;No file specified.&#34;);
    Deno.exit(1);
  }

  // Make sure file exists and is readable before proceeding.
  if (!await exists(filePath, { isFile: true, isReadable: true })) {
    console.error(`Cannot find ${filePath}`);
    Deno.exit(1);
  }

  const content = await Deno.readTextFile(filePath);
  const cmarkDoc: CommonMarkDoc = stringToCommonMarkDoc(content);

  if (args.draft || args.edit || args.apply !== &#34;&#34;) {
    // Set to draft is args.draft is true
    if (args.draft) {
      if (cmarkDoc.frontMatter.draft === false) {
        cmarkDoc.frontMatter.draft = true;
        delete cmarkDoc.frontMatter.datePublished;
        cmarkDoc.changed = true;
      }
    }

    // Apply defaults if requested
    if (args.apply !== &#34;&#34;) {
      const data = await Deno.readTextFile(args.apply);
      const dafaults: Record&#60;string, unknown&#62; = yaml.parse(data) as Record&#60;string, unknown&#62;;
      applyDefaults(cmarkDoc, dafaults);
    }

    // if args.edit then edit the front matter
    if (args.edit) {
      const fields = args._.slice(1) as Array&#60;keyof Metadata&#62;; // Explicitly assert fields as string array
      await editFrontMatter(cmarkDoc, fields);
    }

  	// Display the front matter
    showFrontMatter(cmarkDoc);
    // NOTE: either edit or draft setting caused a change, backup, write it out and exit
    if (cmarkDoc.changed) {
      if (confirm(`save ${filePath}?`)) {
        // Backup original file
        await createBackup(filePath);
        // Write output updated version
        await Deno.writeTextFile(filePath, commonMarkDocToString(cmarkDoc));
        console.log(`Wrote ${filePath}`);
      }
    }
    Deno.exit(0);
  }

  if (args.show) {
    // Display the front matter
    showFrontMatter(cmarkDoc);
    Deno.exit(0);
  }

  if (args.publish) {
    // OK, we must intend to engage the publication process.
    await publishFile(filePath, args.prefix, args.process, dateOfPost);
    Deno.exit(0);
  }

  // If args.process then run the preprocessor and write the output to standard out
  if (args.process) {
    let src: string = &#39;&#39;;
    try {
      src = commonMarkDocPreprocessor(cmarkDoc);
    } catch (err) {
      console.error(err);
      Deno.exit(1);
    }
    if (src === &#39;&#39;) {
      console.error(`no content after preprocessor ran for ${filePath}`)
      Deno.exit(1);
    }
    console.log(src);
    Deno.exit(0);
  }
}

if (import.meta.main) {
  main().catch(console.error);
}

~~~

## Reference

- &#60;https://github.com/rsdoiel/BlogIt&#62;
- [Website](https://rsdoiel.github.io/BlogIt)</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/07/21/Simplifying_BlogIt.md" length="30684" type="text/markdown" />
    </item>    <item>
      <title>Build a Static Web Server with Deno</title>
      <link>https://rsdoiel.github.io/blog/2025/06/30/Build_a_Static_Web_Server.html</link>
      <description>
        <![CDATA[This post discusses static web server implementation using Deno.]]>
      </description>
      <source:markdown># Build your own static web server with Deno

By R. S. Doiel, 2025-06-30

One of things I have in my web toolbox is a static site web server. It only runs on localhost. It amazes me how often I wind up using it. PHP and Python can launch one easily from the command line but I have always found they were lacking. What I want is a simple web server that runs only on localhost. It can serve content from a specified directory and should handle common content types appropriately (e.g. JavaScript files are served as &#34;application/javascript&#34; not as &#34;text/plain&#34;). I should be able choose the port the server runs on. I should be able to specify a document root for the content I want to expose. It should default to a sensible location like the &#34;htdocs&#34; directory in my current working directory.

When I started working with the web (when people used the NCSA web server), web servers were considered complex and hard to implement. I remember most network systems were presumed complex. Today most programming languages have some sort of library, module or package that makes implementing a web server trivial. This is true for JavaScript running under a JavaScript run time engine.

Deno is a JavaScript and TypeScript runtime. I prefer Deno over other JavaScript runtimes like NodeJS. Deno runs sandboxed. This is similar to how the web browser treats JavaScript. Deno&#39;s standard library aligns with web browser implementation too. Deno has a good set of standard modules. Many modules can also be used browser side. 

I&#39;ll be using some Deno standard JavaScript modules in this post. The standard module &#34;@std/http/file-server&#34; provides most of what you need to implement a static content server. Two other modules will round things out in how I want my web server to behave. They are &#34;@std/fs/exists&#34; and &#34;@std/yaml/parse&#34;.

Let&#39;s build a simple but useful static web server and add it to our web toolbox.

Before I build my static web server I need some web content. I&#39;m going to need an HTML file and a JavaScript file. This will provide content to test. The web content should be created in a directory called &#34;htdocs&#34;. On macOS, Linux and Windows the command I run from in the terminal application to create the &#34;htdocs&#34; directory is `mkdir htdocs`. Using your my editor, I created the HTML file called &#34;helloworld.html&#34; inside the &#34;htdocs&#34; directory.

~~~html
&#60;!DOCTYPE html&#62;
&#60;head&#62;
  &#60;script type=&#34;module&#34; src=&#34;helloworld.js&#34;&#62;&#60;/script&#62;
&#60;/head&#62;
&#60;html lang=&#34;en-US&#34;&#62;
  &#60;body&#62;Hello World&#60;/body&#62;
&#60;/html&#62;
~~~

I created a &#34;helloworld.js&#34; inside the &#34;htdocs&#34; directory too.

~~~JavaScript
const body = document.querySelector(&#34;body&#34;);
const elem = document.createElement(&#34;div&#34;);
elem.innerText = &#39;Hello World 2!&#39;;
body.append(elem);
~~~

This provides content to test my prototypes. Using these files I can make sure the prototype properly serves out a web page, handles a file listing and properly services the JavaScript.

Your directory tree should look something like this.

~~~shell
tree htdocs
htdocs/
 helloworld.html
 helloworld.js

1 directory, 2 files
~~~

## First prototype

Using a text editor, I create a file called `webserver_v1.js`. I need to do several things in JavaScript to build our static web server.

1. import a function called `serveDir` from the &#34;@std/http/file-server&#34; module
2. I need to set two constants, a port number and a root document path
3. It is helpful to display the setting for the port and document root when the server starts up
4. I can using Deno&#39;s built in `serve` method to handle inbound requests and then dispatch them to `serveDir`

Let&#39;s start with the import, `&#34;@std/http/file-server`.  Notice that it starts with and &#34;@&#34;. This indicates to the JavaScript runtime that the full URL to the module is defined by an import map. When you build a Deno project you can generate a file called `deno.json`. It can include an import map. The `deno add` command provides a really easy way to manage this mapping. As of Deno 2 the standard modules are available from [jsr.io](https://jsr.io), a reliable JavaScript registry. This includes our standard module `@std/http/file-server`. I can &#34;add&#34; it  to my project using the following command.

~~~shell
deno add jsr:@std/http/file-server
~~~

If the &#34;deno.json&#34; file does not exist this command will create it. If it does exist Deno will update it to reflect the new module. I can look inside the &#34;deno.json&#34; file after running this command and see my import map.

~~~json
{
  &#34;imports&#34;: {
    &#34;@std/http&#34;: &#34;jsr:@std/http@^1.0.18&#34;
  }
}
~~~

The Deno runtime knows how to contact jsr.io and use it to retrieve the module requested.  By default it picks the current stable version. In my case that is v1.0.18. Deno updates happen pretty steadily through out the year. When I try this a month from now it&#39;ll probably be a different version number.

Now that Deno is setup, I need to write my first prototype static web server.

~~~JavaScript
/**
 * webserver_v1.js - A simple static file server for serving files from the &#34;htdocs&#34; directory.
 */
import { serveDir } from &#34;@std/http/file-server&#34;;

const port = 8000;
const rootPath = &#34;htdocs&#34;; // Serve files from the &#34;htdocs&#34; directory

console.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);

// Start a simple server
Deno.serve({
  port,
}, async (req) =&#62; {
  try {
    // Serve files from the specified directory
    return await serveDir(req, {
      fsRoot: rootPath,
      urlRoot: &#34;&#34;,
      showDirListing: true,
      showDotfiles: false, // Exclude files starting with a period
    });
  } catch (err) {
    console.error(err);
    // Return a 404 response if something goes wrong
    return new Response(&#34;404: Not Found&#34;, { status: 404 });
  }
});
~~~
 
The `Deno.serve` manages the inbound request and the async anonymous function handles the mapping to the file server module function called `serveDir`. A try catch wraps the `serveDir` function. If that function fails a 404 response is created and returned. Pretty simple.

Let&#39;s see if the code we typed in works. Deno provides three helpful commands for working with your program code 

1. check 
2. lint
3. fmt

Check reads the JavaScript (or TypeScript) file and makes sure it makes sense from the compilation point of view.  The lint command goes a step further. It checks to see if best practices have been followed. Lint is completely optional but check needs to pass before Deno will attempt to run or compile the program. The `fmt` command will format your source code in a standard way. I&#39;m going to use check and lint.

~~~shell
deno check webserver_v1.js
deno lint webserver_v1.js
~~~

All went well. In both cases I see a line indicating it checked the file. If I had made errors check and lint would have complained and included lines describing errors.

Deno can run our JavaScript and TypeScript files. To test my program I try the following. 

~~~shell
deno run webserver_v1.js
~~~

When I tried this I saw the following message. 

~~~shell
Server running on http://localhost:8000/, serving htdocs
   Deno requests net access to &#34;0.0.0.0:8000&#34;.
 Requested by `Deno.listen()` API.
 To see a stack trace for this prompt, set the DENO_TRACE_PERMISSIONS environmental variable.
 Learn more at: https://docs.deno.com/go/--allow-net
 Run again with --allow-net to bypass this prompt.
 Allow? [y/n/A] (y = yes, allow; n = no, deny; A = allow all net permissions) &#62; 
~~~

I type &#34;y&#34; and press enter. New lines appear.

~~~shell
Server running on http://localhost:8000/, serving htdocs
 Granted net access to &#34;0.0.0.0:8000&#34;.
Listening on http://0.0.0.0:8000/ (http://localhost:8000/)
~~~

I point my web browser to &#34;http://localhost:8000/&#34;. Do I see anything? No. In my terminal window I see another prompt about permissions.

~~~shell
   Deno requests read access to &#34;htdocs&#34;.
 Requested by `Deno.stat()` API.
 To see a stack trace for this prompt, set the DENO_TRACE_PERMISSIONS environmental variable.
 Learn more at: https://docs.deno.com/go/--allow-read
 Run again with --allow-read to bypass this prompt.
 Allow? [y/n/A] (y = yes, allow; n = no, deny; A = allow all read permissions)
~~~

Again answer &#34;y&#34;. I then see something this in my terminal window.

~~~shell
[2025-06-30 16:27:31] [GET] / 200
No such file or directory (os error 2): stat &#39;/Users/rsdoiel/Sandbox/Writing/Books/A_Simple_Web/htdocs/favicon.ico&#39;
[2025-06-30 16:27:31] [GET] /favicon.ico 404
~~~

I reload my web browser page, what do I see? A list of files. I know that file directory listing works.
One of the files is &#34;helloworld.html&#34;.  I click on it. I my simple web page with the words &#34;Hello World&#34; and &#34;Hello World 2&#34;. Yippee, I&#39;ve created a static web server.

You might be wondering how I shutdown the web server. In the terminal window I press control and the letter c, aka &#34;Ctrl-C&#34;. This will shuts down the web server. I can confirm it is shutdown in the web browser by reloading the page. I see an connection error page now.

I don&#39;t want to answer questions about permissions each time I run my prototype. I can specify the permissions I want to grant on the command line.  I know from my test that my program needs &#34;net&#34; and &#34;read&#34; permissions. I can grant this using the following command.

~~~shell
deno run --allow-net --allow-read webserver_v1.js
~~~

Better yet I can compile our JavaScript program into an executable file. An executable is handy because I can run it without Deno being installed on a different computer as long as it runs the same operating system and has the same CPU type. Compiling to an executable makes this prototype similar to our tools in my web tool box. It let&#39;s me treat it just like my terminal application, text editor and web browser.

~~~shell
deno compile --allow-net --allow-read webserver_v1.js
~~~

This results in a file being created called &#34;webserver_v1&#34; (or on Windows, &#34;webserver_v1.exe&#34;). This file can be run from this directory or moved to another directory where I store other programs (e.g. `$HOME/bin` or `$HOME\bin` on Windows).

## Improving on v1

While webserver_v1.js is helpful it could be more friendly. What if I want to use a different port number? What if I want to server out content my current directory or maybe I want to service content on a different mounted drive? I can do that by adding support for command line arguments.

~~~JavaScript
/**
 * webserver_v2.js - A simple static file server with configurable port and root directory.
 */
import { serveDir } from &#34;@std/http/file-server&#34;;

const defaultPort = 8000;
const defaultRoot = &#34;htdocs&#34;;

// Parse command-line arguments
const args = Deno.args;
let rootPath = defaultRoot;
let port = defaultPort;

// Check the command arguments and set the port and rootPath appropriately
if (args.length &#62; 0) {
  // Check if the first argument is a port number
  const portArg = parseInt(args[0], 10);
  if (!isNaN(portArg)) {
    port = portArg;
  } else {
    // If not a port number, assume it&#39;s the root path
    rootPath = args[0];
  }

  // Check if the second argument is a root path
  if (args.length &#62; 1) {
    rootPath = args[1];
  }
}

console.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);

// Start a simple server
Deno.serve({
  port,
}, async (req) =&#62; {
  try {
    // Serve files from the specified directory
    return await serveDir(req, {
      fsRoot: rootPath,
      urlRoot: &#34;&#34;,
      showDirListing: true,
      showDotfiles: false, // Exclude files starting with a period
    });
  } catch (err) {
    console.error(err);
    // Return a 404 response if something goes wrong
    return new Response(&#34;404: Not Found&#34;, { status: 404 });
  }
});
~~~

I can compile that using the following deno compile command

~~~shell
deno compile --allow-net --allow-read webserver_v2.js
~~~

We can run the new webserver using the following command.

~~~shell
./webserver_v2 8001 .
~~~

Point the web browser at &#60;http://localhost:8001&#62;. What do I see the directory? Yep, I see the files in my root directory of my project including the &#34;htdocs&#34; directory I created. Can I find and display &#34;helloworld.html&#34;? Yep and it works as in the first prototype. I shutdown the web server and then start it again using just the executable name.

macOS and Linux

~~~shell
./webserver_v2
~~~

on Windows

~~~shell
.\webserver_v2
~~~

What do you see? Can you find &#34;helloworld.html&#34;? Stop the web server. I copy &#34;helloworld.html&#34; to &#34;index.html&#34;. After copying I restart the web server again.

On macOS and Linux

~~~shell
cp htdocs/helloworld.html htdocs/index.html
./webserver_v2
~~~

On Windows

~~~pwsh
copy htdocs\helloworld.html htdocs\index.html
.\webserver_v2
~~~

I point the web browser at &#60;http://localhost:8000&#62;, what do I see? I don&#39;t see the file directory any more, I see the contents of  I copied into the &#34;index.html&#34; file, &#34;Hello World&#34; and &#34;Hello World 2&#34;.

Can this be improved?  It&#39;d be nice to web able to just type &#34;webserver_v2&#34; and have the program using a default port and htdocs directory of my choice. That can be supported by using a configuration file. YAML is an easy to read and easy to type notation. It even supports comments which is nice in configuration files. YAML expresses the same types of data structures as JSON (JavaScript Object Notation). Below an example of a configuration file. I type it in and save it using the filename &#34;webserver.yaml&#34;.

~~~yaml
# Set root path for web content to the current directory.
htdocs: .
# Set the port number to listen on to 8002
port: 8002
~~~

From the point of the view of my prototype it&#39;ll need to check if the &#34;webserver.yaml&#34; file exists before attempting to read it. Deno has a module for that. It&#39;ll also need to read the YAML, parse it and get an object that exposes my preferred settings. Deno has a standard model for working with YAML too. The modules I&#39;m interested in are `@std/fs/exists` and `@std/yaml`. I&#39;ll need to &#34;add&#34; them to my deno project.

~~~shell
deno add jsr:@std/fs/exists
deno add jsr:@std/yaml
~~~

Time for an improved version of the static web server. This prototype should be called, &#34;webserver_v3.js&#34;.

~~~JavaScript
/**
 * webserver_v3.js - A simple static file server with configurable port and root directory via YAML.
 */
import { serveDir } from &#34;@std/http/file-server&#34;;
import { parse } from &#34;@std/yaml/parse&#34;;
import { exists } from &#34;@std/fs/exists&#34;;

const defaultPort = 8000;
const defaultRoot = &#34;htdocs&#34;;

// Function to read and parse YAML configuration file
async function readConfigFile(filePath) {
  try {
    const fileContent = await Deno.readTextFile(filePath);
    return parse(fileContent);
  } catch (err) {
    console.error(&#34;Error reading or parsing YAML file:&#34;, err);
    return null;
  }
}

// Parse command-line arguments
const args = Deno.args;
let rootPath = defaultRoot;
let port = defaultPort;

if (args.length &#62; 0) {
  // Check if the first argument is a port number
  const portArg = parseInt(args[0], 10);
  if (!isNaN(portArg)) {
    port = portArg;
  } else {
    // If not a port number, assume it&#39;s the root path
    rootPath = args[0];
  }

  // Check if the second argument is a root path
  if (args.length &#62; 1) {
    rootPath = args[1];
  }
} else {
  // Check for YAML configuration file
  const configFilePath = &#34;webserver.yaml&#34;;
  if (await exists(configFilePath)) {
    const config = await readConfigFile(configFilePath);
    if (config) {
      rootPath = config.htdocs || defaultRoot;
      port = config.port || defaultPort;
    }
  }
}

console.log(`Server running on http://localhost:${port}/, serving ${rootPath}`);

// Start a simple server
Deno.serve({
  port,
}, async (req) =&#62; {
  try {
    // Serve files from the specified directory
    return await serveDir(req, {
      fsRoot: rootPath,
      urlRoot: &#34;&#34;,
      showDirListing: true,
      showDotfiles: false, // Exclude files starting with a period
    });
  } catch (err) {
    console.error(err);
    // Return a 404 response if something goes wrong
    return new Response(&#34;404: Not Found&#34;, { status: 404 });
  }
});
~~~

Like before I compile it with the my desired permissions.

~~~shell
deno compile --allow-net --allow-read webserver_v3.js
./webserver_v3
~~~

I point the web browser at &#60;http://localhost:8002&#62;. What do I see? I see the contents of the index.html file. Can I display &#34;helloworld.html&#34; too? Yep. I remove the &#34;index.html&#34; file, then use my browser back button to go to the initial URL, yep I see a file directory listing again. Looks like this prototype works.

I think I have a useful localhost static content web server. It&#39;s time to rename my working prototype, compile and install it so it is available in my toolbox.

1. Copy `webserver_v3.js` to `webserver.js` 
2. Use `deno compile` to create an executable
3. Create a &#34;$HOME/bin&#34; directory if necessary
4. Move the executable to a location in the executable PATH with, example &#34;$HOME/bin&#34;
5. Try running the program

NOTE: If you are following along and have to create &#34;$HOME/bin&#34; then you may need to added to your environment&#39;s PATH.

On macOS and Linux

~~~shell
cp webserver_v3.js webserver.js
deno compile --allow-net --allow-read webserver.js
mkdir -p $HOME/bin
mv ./webserver $HOME/bin
webserver
~~~

On Windows

~~~shell
copy webserver_v3.js webserver.js
deno install --global --allow-net --allow-read webserver.js
New-Item -ItemType Directory -Path &#34;$HOME\bin&#34; -Force
move webserver.exe $HOME\bin\
webserver
~~~

There you have it. I have a new convenient static web server for serving static content on localhost.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/06/30/Build_a_Static_Web_Server.md" length="18161" type="text/markdown" />
    </item>    <item>
      <title>Rethinking REST</title>
      <link>https://rsdoiel.github.io/blog/2025/06/07/Rethinking-REST.html</link>
      <description>
        <![CDATA[I am re-thinking my reliance on REST's implementation of the CRUD abstraction in favor of the simpler
        read write file abstraction in my web application. This can be accomplished in SQL easily. This post
        covers an example of doing this in SQLite3 while also implementing JSON object versioning.
        
        Coverted are implenting the write abstraction using an upsert operation based on `insert` and SQLite3's
        `on conflict` clause. The object versioning is implemented using a simple trigger on the JSON column.
        The trigger maintains the version number and updated timestamp.]]>
      </description>
      <source:markdown># Rethinking REST

## How to embrace the read write abstraction using SQL databases

By R. S. Doiel, 2025-06-07

[Roy Fielding](https://en.wikipedia.org/wiki/Roy_Fielding)&#39;s 2000 dissertation describing [REST](https://en.wikipedia.org/wiki/REST) is a brilliant work. It revolutionized web services. I&#39;ve spent a good chunk of my career implementing back end systems using a REST approach. REST&#39;s superpower is the mapping of HTTP methods to core database operations of create (POST), read (GET), update (PUT), and delete (DELETE). The has simplified machine to machine communication. That is a good thing.

REST has a browser problem. A quarter century after Fielding presented REST, the web browser still requires JavaScript to talk directly to a REST service. The core problem is the REST methods are not defined in the semantics of HTML. They are only available in HTTP protocol layer. JavaScript plays the role of solving the mapping of actions to REST methods. I can program over that impedance server side, browser side or both. The penalty is increased complexity. I think this complexity unnecessarily.

**What abstraction aligns with the grain of both web service and web browser?**

Sir Tim invented HTTP and HTML on a NeXT cube. The NeXT cube was a Unix system like the systems used by the Physicists at CERN where Sir Tim was employed. From Unix you can trace the concept of &#34;everything is a file&#34;. File interaction can be boiled down to reads and writes. A second influence was the practice of using plain text to encode data. These characteristics influenced Sir Tim&#39;s choices when he invented HTTP and HTML. These characteristics inform the grain of the modern web.

What are the challenges of building on a read write abstraction rather than the database abstraction of create, read, update and delete? Do we toss out the database completely? That would be a too high of a cost.  Databases solve some important problems. This includes managing concurrent access, data protections and versatile query support. Database are the right choice in most cases for web applications. **So how do I get to a read write (RW) abstraction? The database wants create, read, update and delete (CRUD)?**

The short answer is we already do it. It&#39;s just messy. Typically we do this server side. It can be implemented browser side using JavaScript. Sometimes both places. We may layer that step as a micro service or embedded in some monolithic monstrosity. It&#39;s there someplace. It doesn&#39;t need to be a mess.

Let&#39;s consider that for a moment. Server side the web service receives a request containing web form data. The service decodes the web form, hopefully validates the contents, then figures out if it is a &#34;create&#34; or &#34;update&#34; in the database system before attempting either an `insert` or `update` operation. The database schema usually reflects the form data. If the form has repeating fields then you might have more than one table and need to maintain relationships between the tables. This can quickly become complex.

Server side this complexity was answered via object relational models (ORM).  Browser side we&#39;ve seen similar approaches to the ORM in the development of frameworks that &#34;bind&#34; data in to an object model that can be sent to a back end system (often a REST API). The problem with both the server side ORM and browser side data binding frameworks is they tend to add allot of complexity. Ultimately they wind up dictating the approach you take to solve problems. Over time the frameworks become more complex too as they try to be a generalize solution to complex schema implementations. This accrues another source of complexity. The price of either becomes loss of flexibility, loss of performances and often deep levels of knowledge about the framework or ORM.  The longer lived your application is the more likely that this will not end well. I believe we can avoid this by taking stock of where database systems and the web have evolved since 2000.


**What am I proposing?**

Let&#39;s look at the deepest layer in our stack, the relational database. Several changes have happen on the database side that I think can help us build web application aligned with the read write abstraction core to our web browsers.  The first is a concept called upsert. Upsert is the idea of combining the behavior of `insert` and `update` into one operation. The upsert gives us our write operation.

What about the mapping of a web form&#39;s data?  The second change in relational database world is the wide adoption of JSON column support. We can treat web form contents as a JSON expression. Modern SQL can query the JSON columns along with the other supported data types.

A third changed was the arrival of SQLite in 2000. SQLite is SQL engine that does not require a separate database management system. Since 2000 SQLite has grown in usage. It now is used more commonly than Microsoft SQL server, Oracle, MySQL or PostgreSQL. The old requirement of using a stand alone database management system as part of the web stack has now turned into an option.

SQLite3 provides support for both JSON columns and upsert. The upsert concept is implemented as an `on conflict` clause in your `insert` statement.  SQLite3 also support SQL triggers. Using the JSON column, upsert and triggers the SQLite3 database can handle the mapping of data as well as mapping our read write (RW) operations to the database CRUD operations. Better yet SQLite3 is an embedded SQL engine so you don&#39;t have to run a database management system at all. 

Use of JSON columns can radically simplify your JSON schema for many use cases. The model I am suggesting can be used to implement simple content management systems, metadata managers and form processor systems. Here&#39;s a table design suitable to many simple web applications.

~~~SQL
CREATE TABLE IF NOT EXISTS data (
   id TEXT NOT NULL PRIMARY KEY,
   src JSON DEFAULT NULL,
   updated DATETIME DEFAULT CURRENT_TIMESTAMP,
   version INT DEFAULT 0
);
~~~

The `id` holds a unique identifier like a file path does in a file system. The `src` column holds our JSON source. The `updated` column records the ISO-8601 timestamp of when your object is updated.  You might be wondering about `version` column and a missing `created` column. SQL can be used to automate data versioning and reduce create and update into a write operation. This is done by adding a second table. The scheme change in the second table from the first is how the primary key is defined.

~~~SQL
CREATE TABLE IF NOT EXISTS data_history (
   id TEXT NOT NULL,
   src JSON DEFAULT NULL,
   updated DATETIME DEFAULT CURRENT_TIMESTAMP,
   version INT DEFAULT 0,
   PRIMARY KEY (id, version)
);
~~~

The SQL engine (SQLite3) does the actual version management using an SQL trigger. The &#34;on conflict&#34; of an insert triggers an &#34;update&#34; operation. The &#34;update&#34; action then triggers the `write_data` action before it completes.

Here is how our upsert is implemented.

~~~SQL
INSERT INTO data (id, src) values (?, ?) 
ON CONFLICT (id) DO
  UPDATE SET src = excluded.src
  WHERE excluded.id = id;
~~~

The `write_data` trigger is responsible for two things. Inserts a new row into the `data_history` table using the the current row&#39;s values. Next it updates the `data` table&#39;s `version` number and `updated` timestamp automatically.

~~~SQL
CREATE TRIGGER write_data BEFORE UPDATE OF src ON data
BEGIN
  -- Now insert a new version into data_history.
  INSERT INTO data_history (id,src, updated, version)
    SELECT id, src, updated, version FROM data WHERE id =id; 
  -- Handle updating the updated timestamp and version number
  UPDATE data SET updated = datetime(), version = version + 1
    WHERE old.id = new.id;
END; 
~~~


So when I insert a new object there is no conflict so a simple insert is performed on the `data` table.  The row&#39;s version and `upgrade` columns get populated by the schema defaults. The next time the row is update it triggers the `write_data` operation where the row is recorded (copied) to the `data_history` table before being updated to reflect the changed values.

How do you find out when a record was created without a column called created?

In the follow SQL we perform a left join with the `data_history` table. We filter the history table for a row with the same id but a version number of 0. If a row is found then the value of `data_history.updated` will not be null. A `ifnull` function can be used to pick that value otherwise we use the `data.updated` value from `data` table. Here is how that SQL would look.

~~~SQL
SELECT data.id as id, 
  data.src as src,
  data.updated as updated,
  ifnull(data_history.updated, data.updated) as created,
  data.version
FROM data LEFT JOIN data_history ON
  ((data.id = data_history.id) and (data_history.version = 0))
WHERE data.id = ?;
~~~

The complexity of mapping CRUD to RW is now completely contained in the SQL engine. While I have use SQLite3 for this specific example in practice these features are available in most modern relational database management systems. It&#39;s matter of knowing the specifics of the dialect.

Isn&#39;t this a whole lot of SQL to write? Perhaps. By leveraging JSON columns the needs to elaborate on this SQL are minimal. Effectively these four statements can function like an SQL component. I think the investment is small. It solves a large class of web application storage needs.  You could even use a template to automatically generate them. Once written your can re-use them as needed.

**Why did I focus on SQLite3?**

Because reducing the layers in our web stack reduces complexity. With SQLite3 we don&#39;t need database management system running. It&#39;s one less thing to manage, monitor and defend. In a cloud environment it can mean renting one less service.

**What layers remain? What are their responsibilities?**

In 1999 web applications had a data management component, a user management component and an authentication and authorization component. The point of the application was the data management component. You were required to implement the others to keep the data safe while it was on the Internet.

Today authentication and authorization can be handled by single sign-on systems. In the academic and research settings you typically see combinations like Apache2 + Shibboleth or NginX + Shibboleth. On the commercial Internet you see systems like OpenID and OAuth2. For a decade or more the systems I&#39;ve designed and implemented take advantaged of single sign-on.  My application doesn&#39;t have to have a user management component or an authentication and authorization component at all.

I do need a layer that validates the inputs and returns the resources requested. I usually implement this as a &#34;localhost&#34; web service that relies on the &#34;front end&#34; web service for authentication and authorization. If my layer uses SQLite3 for data storage then the &#34;stack&#34; is just a &#34;front end&#34; web server providing authentication and authorization and a &#34;back end&#34; persistence layer providing validation, storage and retrieval.

An advantage of this simple stack is I can develop, test and improve the localhost web service and know it&#39;ll plug into the front end when I am ready for a production deployment.  The front end deals in requests and responses, the back end deals in requests and responses. Meanwhile I have all the advantages of a SQL database on the &#34;back end&#34;.

Are there times I might need more layers?  Sure.  If I was managing millions of objects I would not store them in a single SQLite database.I&#39;d use a database management system like PostgreSQL.  If I need a rich full text search engine I might use Solr or Open Search for that. If I am storing large objects then I might have a middle ware that can speak S3 protocol to store or retrieve those objects. My point is those are no longer a requirement. Extra layers or parallel services are now only options. They are available if and only if I need them.

Example.  If I want to basic full text search, SQL databases have index types that support this.  SQLite3 is included there.
By leveraging SQL triggers I can extract data from my stored JSON column and populate full text search columns or even other tables as needed.I can get allot of the advantages of a full text search before I reach for an external system like Solr.

So here are my take way items for you.

1. The web and databases continue to evolve.
2. Take advantage of the improvements to simplify your code and your implementations
3. Evaluate if you really need that heavy stack when you build your next application
4. Use the simplest of abstractions that solve the problem required
5. Consider a simple data interaction model like read write before you reach for REST

Enjoy.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/06/07/Rethinking-REST.md" length="13860" type="text/markdown" />
    </item>    <item>
      <title>PowerShell and Edit for macOS, Linux and Windows</title>
      <link>https://rsdoiel.github.io/blog/2025/06/05/PowerShell_and_Edit.html</link>
      <description>
        <![CDATA[One of the challenges of multi platform support is the variance in tools. Unix and related operating systems are pretty unified these days. The differences are minor today as opposed to twenty years ago. If you need to support Windows too it's a whole different story. You can jump to Linux Subsystem for Windows but that is really like using a container inside Windows and doesn't solve the problem when you need to work across the whole system. 
        
        Windows' shell experience is varied. Originally it was command com, essentially a enhanced CP/M shell. Much later as Windows moved beyond then replaced MS-DOS they invented PowerShell. Initially a Windows only system. Fast forward today things have change. PowerShell runs across Windows, macOS and Linux. It is even licensed under an MIT style license.
        
        ...]]>
      </description>
      <source:markdown># PowerShell and Edit on Windows, macOS and Linux

By R. S. Doiel, 2025-06-05

One of the challenges of multi platform support is the variance in tools. Unix and related operating systems are pretty unified these days. The differences are minor today as opposed to twenty years ago. If you need to support Windows too it&#39;s a whole different story. You can jump to Linux Subsystem for Windows but that is really like using a container inside Windows and doesn&#39;t solve the problem when you need to work across the whole system. 

Windows&#39; shell experience is varied. Originally it was command com, essentially a enhanced CP/M shell. Much later as Windows moved beyond then replaced MS-DOS they invented PowerShell. Initially a Windows only system. Fast forward today things have change. PowerShell runs across Windows, macOS and Linux. It is even licensed under an MIT style license.

- &#60;https://github.com/PowerShell/PowerShell&#62;

PowerShell is intended as a system scripting language and as such is focused on the types of things you need too to manage a system. It has vague overtones of Java, .NET and F#. If you are familiar with those it probably feels familiar for me it wasn&#39;t familiar. Picking up PowerShell has boiled down to my thinking I can do X in Bash then doing a search to find out the equivalent in PowerShell 7 or above.  There are something examples out there that are Windows specific because there isn&#39;t a matching service under that other operating systems but if you focus on PowerShell itself rather than Windows particular feature it is very useful. It also means while you&#39;re picking up how Windows might approach something you can re-purpose that knowledge on the other operating systems. That&#39;s really handy for admin type tasks.

One of the things I&#39;ve been playing with is creating a set of scripts that have a common name but deal with the specifics of the target OS. That way when I need to run a generalized task I can deploy the OS specific version to the platform but then start thinking about managing the heterogeneous environments in a unified way. E.g. scripts like &#34;require-reboot.ps1&#34;, &#34;safe-to-reboot.ps1&#34;, &#34;disk-is-used-by.ps1&#34;.

Once you start getting serious about learning a system admin script language you also learn you need to vet the quality of your the scripts you are writing. On Unix I use a program called [shellcheck](https://www.shellcheck.net/) and [shfmt](https://github.com/patrickvane/shfmt) to format my scripts. How do you do that for Powershell?

Recently I discovered recently is PSScriptAnalyzer. Like shellcheck it will perform static analysis on your script and let you know of lurking issues to be aware of. The beauty of it is I can evaluate a script in PowerShell on macOS and know that I&#39;ve caught issues that would have pinched me if I ran it Linux or Windows.  That&#39;s kinds of sweet.

You need to [install PSScriptAnalyzer](https://learn.microsoft.com/en-us/powershell/utility-modules/psscriptanalyzer/overview?view=ps-modules) but it is easy to do under PowerShell.

~~~pwsh
Install-Module -Name PSScriptAnalyzer -Force
~~~

or

~~~pwsh
Install-PSResource -Name PSScriptAnalyzer -Reinstall
~~~

If I want [run the analyzer](https://learn.microsoft.com/en-us/powershell/utility-modules/psscriptanalyzer/using-scriptanalyzer?view=ps-modules&#38;source=recommendations) on a script called `installer.ps1` I&#39;d run lit like

~~~psh
Invoke-ScriptAnalyzer -Path ./installer.ps1 -Settings PSGallery -Recurse
~~~

Formatting PowerShell scripts I am currently testing out [PowerShell-Beautifier](https://github.com/DTW-DanWard/PowerShell-Beautifier). It&#39;s a &#34;cmdlet&#34; and an easy install into PSGallery following the instructions in the GitHub repo.

Here&#39;s an example of formatting the previous example so it uses tabs instead of spaces.

~~~pwsh
Edit-DTWBeautifyScript ./installer.ps1 -IndentType Tabs
~~~

# Now that I got a shell running across systems, what about an editor?

MS-DOS acquired an editor called &#34;edit&#34; at some point in time (version 4 or 5?).  I remember it was a simple full screen non model editor. Recently Microsoft has created a similar editor that runs in a terminal on Windows and Linux. Like PowerShell it arrived as an Open Source Project. You don&#39;t mind some rough edges I have compiled it successfully on macOS. A few features are not implemented but it can open and save files.  It builds with the latest Rust (e.g. run `rustup update` if you haven&#39;t in a while) and generates an installable executable using Cargo. While I&#39;m not likely to switch editors it&#39;s nice to have one that is really cross platform in doesn&#39;t require odd Unix adapter libraries to be installed to compile it. It&#39;s just a Rust program so like Go the build process is pretty consistent when you compile on Windows, Linux or macOS.

- &#60;https://github.com/Microsoft/edit&#62;

So you have an administrative shell environment and a common editor. If you happen to have to document admin chores across systems at least now you can document one admin language and editor and know it&#39;ll run if installed.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/06/05/PowerShell_and_Edit.md" length="6272" type="text/markdown" />
    </item>    <item>
      <title>A quick note on types in Deno+TypeScript</title>
      <link>https://rsdoiel.github.io/blog/2025/05/25/a_quick_notes_on_types.html</link>
      <description>
        <![CDATA[Understanding the plumbing of a program that is built with Deno in TypeScript can be challenging if you can't identify the type of variables or constants.  TypeScript inherits the JavaScript function, `typeof`. This works nicely for simple types like `string`, `boolean`, `number` but is  less useful when compared to a class or interface name of a data structure.
        
        There are three approaches I've found helpful in my exploration of type metadata when working with Deno+TypeScript. (NOTE: in the following
        the value `VARIABLE_OR_CONSTANT` would be replaced with the object you are querying for type metadata)
        
        ...]]>
      </description>
      <source:markdown># A quick note on types in Deno+TypeScript

Understanding the plumbing of a program that is built with Deno in TypeScript can be challenging if you can&#39;t identify the type of variables or constants.  TypeScript inherits the JavaScript function, `typeof`. This works nicely for simple types like `string`, `boolean`, `number` but is  less useful when compared to a class or interface name of a data structure.

There are three approaches I&#39;ve found helpful in my exploration of type metadata when working with Deno+TypeScript. (NOTE: in the following
the value `VARIABLE_OR_CONSTANT` would be replaced with the object you are querying for type metadata)

`typeof`
: This is good for simple types but when a type is an object you get `[object object]` response.

`Object.protototype.toString.call(VARIABLE_OR_CONSTANT)`
: This is what is behind the `typeof` function but can be nice to know. It returns the string representation of the `VARIABLE_OR_CONSTANT` you pass to it.

`VARIABLE_OR_CONSTANT.constructor.name`
: This will give you the name derived from the object&#39;s constructor, effectively the class name. It doesn&#39;t tell you if the the `VARIABLE_OR_CONSTANT` is an interface. If you construct an object as an object literal then the name returned will be `Object`.

Here&#39;s the three types in action.

~~~TypeScript
  let fp = await Deno.open(&#39;README.md&#39;);
  console.log(typeof(fp));
  console.log(Object.prototype.toString.call(fp);
  console.log(fp.constructor.name);
  await fp.close()
  
  let t = { &#34;one&#34;: 1 };
  console.log(typeof(t));
  console.log(Object.prototype.toString.call(t);
  console.log(t.constructor.name);
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/05/25/a_quick_notes_on_types.md" length="2708" type="text/markdown" />
    </item>    <item>
      <title>New Life for Fielded Search</title>
      <link>https://rsdoiel.github.io/blog/2025/04/10/New_Life_for_Fielded_Search.html</link>
      <description>
        <![CDATA[A day dreaming in response to a Simon Willison post on using language models
        to convert queries into fielded searches. In this post I extrapolate how this
        could result in a more private search experience and allow for an enhanced
        search experience for static websites.]]>
      </description>
      <source:markdown># New Life for Fielded Searches

By R. S. Doiel, 2025-04-10

[Simon Willison](https://simonwillison.net/2025/Apr/9/an-llm-query-understanding-service/) posted an article yesterday that caught my eye. He was pointing out how even small language models can be used to breakdown a search query into fielded tokens.  Some of the earliest search engine, way before Google&#39;s one box, search engines were built on SQL databases. Full text search was tricky. Eventually full text search become a distinct service, e.g. [Solr](https://solr.apache.org). The full text search engine enabled the simple search to become the expected way to handle search. Later search engines like Google&#39;s use log analysis to improve this experience further. When you use a common search string like &#34;spelling of anaconda&#34;, &#34;meaning of euphemism&#34;, &#34;time in Hawaii&#34; these results are retrieved from a cache. The ones that are location/time sensitive can be handled by simple services that either populate the cache or return a result then populate the cache with a short expatriation time. Life in search land was good.  Then large language models hit the big time and the &#34;AI&#34; hyperbole cranked up to 11.

There has been flirtation with replacing full text engines or more venerable SQL databases with large language models.  There is a catch. Well many catches but let me focus on just one. The commercial large language models are a few years out of date. When you use a traditional search engine you expect the results to reflect recent changes. Take shopping a price from two years ago isn&#39;t has useful as today&#39;s price given the tariff fiasco. Assembling large language models takes allot of time, compute resources and energy. Updating them with today&#39;s changes isn&#39;t a quick process even if you can afford the computer and energy costs. So what do? One approach as been to allow the results of a large language model to have agency. Agency is to use a traditional search engine to get results.  They&#39;re serious challenges with this approach. These include performance, impact on other web services and of course security.

What if the language model is used in the initial query evaluation stage?  This is what Simon&#39;s article is about. He points out that even a smaller language model can be used to successfully take a query string and break it down into a fielded JSON object. Let&#39;s call that a search object. The search object then can be run against a traditional full text search engine or a SQL engine. These of course can be provided locally on your own servers.  [Ollama](https://ollama.app) provides an easy JSON API on localhost that can be used as part of your query parsing stack. This may be leveling especially if your organization has collection specific content to search (e.g. a library, archive or museum).

Constructing an language model enable stack could look something like this.

1. front end web service (accepts the queries)
2. Ollama is used to turn the raw query string into a JSON search object
3. The JSON search object is then sent to your full text search engine or SQL databases to gather results
4. results are formatted and returned to the browser.

The key point in Simon Willison&#39;s post is that you can use a smaller language model. This means you don&#39;t need more hardware to add the Ollama integration. You can shoe horn it into your existing infrastructure. 

This pipeline is a simple to construct.  This trick part is finding the right model and evaluating the results and deciding when the LLM translation to a JSON search object is good enough. Worst case is the original query string can still be passed off to your full text engine. So far so good. A slightly more complex search stack with hopefully improved usefulness.

## a few steps down the rabbit hole

Where I think things become interesting is when you consider where search processing can happen. In the old days the whole stack had to be on the server. Today that&#39;s isn&#39;t true.  The LLM piece might still be best running server side but the full text search engine can be provided along with your statically hosted website. You can even integrate with a statically hosted JSON API. In light of that let&#39;s revisit my sequence.

1. Front end web service response with a search page to the browser
2. Browser evaluates the search page, gets the query string
3. The browser then sends it to the Ollama web service that is returns a JSON search object (fielded object)
4. The browser applies the object to our static JSON API calculating some results, it also runs query string through the static site search getting results
5. results are merged and displayed in the browser

So you might be wonder about the static site search (browser side search) and JSON API I mentioned. For static site search I&#39;ve found [PageFind](https://pagefind.app) to be really handy. For the JSON API I&#39;d go with [FlatLake](https://flatlake.app). The two eliminate much of what used to be required from dynamic websites like those provided by Drupal and WordPress. A key observation here is that your query string only leaves the browser once in order to get back the language model result. This is a step toward a more private search but it doesn&#39;t pass the goal of avoiding log traces of searches.

I first encounter browser side search solution with Oliver Nightingale&#39;s [Lunrjs](https://lunrjs.com). I switch to PageFind because Cloud Cannon had the clever idea to partition the indexes and leverage WASM for delivering the search. PageFind has meant providing full text search for more than 10K objects.

**Could a PageFind approach work for migrating the language model service browser side?**

If the answer were yes, then would be a huge win for privacy. It would benefit libraries, archives and museums by allowing them to host rich search experiences while also taking advantage of the low cost and defensibilty of static site deployments.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/04/10/New_Life_for_Fielded_Search.md" length="6630" type="text/markdown" />
    </item>    <item>
      <title>LLM first impressions a few weeks in</title>
      <link>https://rsdoiel.github.io/blog/2025/03/30/LLM_first_impressions_a_few_weeks_in.html</link>
      <description>
        <![CDATA[A first take of LLM use for coding projects.]]>
      </description>
      <source:markdown># LLM first impressions a few weeks in

By R.S. Doiel, 2025-03-30

Writing code with an LLM is a far cry from what the hype cycle promises. It requires care. An attention to detail. It imposes significant compute resources. The compute resources requires a significant amount of electricity consumption. It doesn&#39;t bring speed of usage. Even cloud hosted LLM are slow beyond the first few iterations. If you want to find a happy medium between describing what you want and how you want it done, you need to commit to a non trivial amount of effort. Depending on your level of experience it may be faster to limit code generation to specific parts of a project. It maybe faster to code simple projects yourself.

When I compare working with an LLMs like Gemma, Llama, Phi, Mistral, Chat-GPT to traditional [RAD](https://en.wikipedia.org/wiki/Rapid_application_development &#34;Rapid Application Development&#34;) tools the shiny of the current &#34;AI&#34; hype is diminished. RAD tools often are easier to operate. They have been around so long we have forgotten about them. They use significantly less compute resources. RAD tools are venerable in 2025.

The computational overhead as well as the complexity of running an integrated LLM environment that supports [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation &#34;Retrieval Augmented Generation&#34;), [agency](https://en.wikipedia.org/wiki/Software_agent &#34;software agent explained&#34;) and [tools](https://www.forbes.com/councils/forbestechcouncil/2025/03/27/your-essential-primer-on-large-language-model-agent-tools/ &#34;A Forbes article on tool use with large language models&#34;) is much more than a simple code generator. A case in point. The best front end I&#39;ve tried so for in my LLM experiments is [Msty.app](https://mysty.app). It takes a desperate set of services and presents a near turn-key solution. You don&#39;t even need to install [Ollama](https://ollama.com) as it ships with it. It checks all the boxes, RAG, agency and tools. But this simplicity only masks the complexity. Msty is a closed source solution. It is only offered on the x86 platform. The x86 computers are known for their energy consumption. So that&#39;s a downer. There are more energy efficient ARM and RISC-V solutions out there.  A Raspberry Pi 500 can run Ollama and should also be able to run Msty.app. But since Msty.app is closed source, I can&#39;t download and compile an ARM version myself. I can&#39;t compile a version for my Windows ARM machine either. That machine has much more RAM than the Pi. It even has more RAM than the x86 Windows box I run Msty on. Unfortunately I&#39;m out of luck, the best I can do is running Msty.app under emulation. Not ideal. Scaling up with Msty.app means using a remote hosted LLM solution. How much energy am I saving when I run remote? When things go South the complexity costs in diagnosing problems with distributed systems is more than running things on a single box. I understand their business model.  Providing a Freemium version is a classic loss leader. People who like Msty will pay to upgrade to paid plans. From a big picture perspective it makes less sense to me.

- How do we get more bang for the energy consumed?
- Does the LLM enhanced coding environment push development forward?
- Are LLM yet an appropriate tool in the toolbox of our profession?
- Why am I continuing to explore using LLM for code development?

A few weeks in I am looking for reasons to continue exploring LLM for code generation. I have a much better understanding of how it could fit into my approach to software development. I see reason to be hopeful but I also continue to have serious reservations.

My hands and wrists aren&#39;t the same as when I started in my career. I am hoping that an LLM can reduce my typing and reduce the risk of [RSI](https://en.wikipedia.org/wiki/Repetitive_strain_injury &#34;repetitive strain injury&#34;). I am also hoping that I can reduce the need for close reading and therefore eye strain. I&#39;m currently experimenting with taking advantage of the slowness of the LLM eval cycle. Accessibility features in the OS will read back parts of the screen for me. Are these features worth burning down forests, floods and other climate driven issues? No they are not. Am I convinced that these LLM benefits will pan out? No. I&#39;m hoping this aspect of LLM usage gets studied. I&#39;m generally interested in tools that are a good ergonomic benefit for developers over the long haul. We still need a livable planet regardless.

Without using an LLM my normal development approach is to write out a summary of what I want to do in a project. I clarify the modules I want and the operations I need to perform. I code tests before coding the modules.  For many people this approach appears upside down. I&#39;ve found this inverted approach yields better documentation. Writing the documentation clarifies my software architecture and how I will organize the required code. Writing tests first means I don&#39;t write more code than needed. The result is software I can look at a year or more later and still understand.

The first step in my current approach is well suited to adopting an LLM for code generation.  The LLM I&#39;ve tried are decent at writing minimal test code. A good front end to Ollama or other LLM runner can automate some of the development cycle. The process is not fast. Once you get beyond the trivial the process is slow. It is slow like the compilers I used in my youth. It works OK for the most part. I can speed things up a little with faster hardware. That cost is increased electricity consumption. 

An intriguing analog to developing with an LLM environment is [Literate programming](https://en.wikipedia.org/wiki/Literate_programming) advocated by [Knuth](https://en.wikipedia.org/wiki/Donald_Knuth). The LLM I&#39;ve tried have used a chat interface. Even if the chat conversation is read back there is allot of duplicate text to listen to. Current chat responses are usually presented in Markdown with embedded code blocks. This is feels like a simplified version of Knuth&#39;s approach using TeX with embedded code blocks. It feels familiar to me. The Chat UI model presents many of the challenges I remember trying out literate programming. For some people a literate approach resulted in better code. I&#39;m not sure it was true for everyone. For some an LLM might encourage more thinking as dialog can be useful that way. It might be easier to be more objectively when vetting because the ego isn&#39;t challenged in the generated code. It is hard to say with certainty.

I have found one significant advantage of the LLM generated code. It is a byproduct of large training sets. The generated code tends to look average. It tends to avoid obfuscation. It mostly is reasonably readable. If the computing resources were significantly lower I&#39;d feel more comfortable with this approach over the long run. The current state needs radical simplification. The LLM development environment is overly complex. If it is running on my hardware why does it need to be implemented as a web service at all? I think allot of the complexity is intentional. It benefits consolidation of tooling in the cloud. The cloud where we rent the tools of our trade. LLM development environment&#39;s complexity and energy consumption weight heavily on me as I explore this approach. It is too early to tell if it should be a regular tool in my toolbox. It is too early to know if it is useful for sustainable software practices.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/03/30/LLM_first_impressions_a_few_weeks_in.md" length="7909" type="text/markdown" />
    </item>    <item>
      <title>Building Web Components using Large Language Models</title>
      <link>https://rsdoiel.github.io/blog/2025/03/13/Building_Web_Component_using_an_LLM.html</link>
      <description>
        <![CDATA[Quick discussion of my recent experience bootstrapping the CL-web-components project]]>
      </description>
      <source:markdown># Building Web Components using Large Language Models

By R. S. Doiel, 2025-03-13

In March I started playing around with large language models to create a couple web components. I settled on a paid subscription for Mistral&#39;s Le Chat. You can see the results of my work project at &#60;https://github.com/caltechlibrary/CL-web-compoments&#62;. The release at or before &#34;v0.0.4&#34; were generated using Mistral&#39;s Le Chat.

The current state of LLM offered online do not replace a human programmer. I&#39;m sure that is surprising to those who thing of this as a solved problem. I found that is was my experience as a developer that I could detect the problems in the generated code. It was also required in coming up with the right prompts to get the code I wanted. 

The key to success of using an LLM for code generation is domain knowledge. You need domain knowledge of the problem you&#39;re solving by creating new software and domain knowledge of the machine or run time that the software will target.

The ticket to working with an LLM using a chat interface is your domain knowledge. That&#39;s how you know what to ask. That starting point is useful when using the LLM explore **widely documented** topics and approaches. The LLM is not good at inferring novel solutions but rather at using what it has been trained on.  That has been the key with the current crop of LLM I tried.

I think using an LLM to generate code alongside a human client has potential. The human client brings subject knowledge. The human software developer brings more domain knowledge. Between the two they can guide the LLM in generating useful code. Its a little like training a literalist four year old that has the ability to type. I think this three way collaboration has possibilities. The LLM may prove helpful in bridging the human client and software developer divide.

## my experiment proceeded

Working with a chat interface and LLM is a non-trivial iterative process. It can be frustratingly slow and pedantic. I often felt I took two steps forward then a step backward. I am unsure if I arrived at the desire code faster using the LLM. I am unsure the result was better quality software.

I spent two weeks of working with a Mistral&#39;s Le Chat LLM on two web components. I am happy with the results at this stage of development. I am not certain the web components will continue to be developed with an LLM. My experience left me with questions about how LLM generated code will help or hurt sustainable software.

## Generating web components

My experiment focused on two web components I needed for a work project. The first successfully completed component was called `AToZUL` in &#34;[a_to_z_ul.js](https://raw.githubusercontent.com/caltechlibrary/CL-web-components/refs/heads/main/a_to_z_ul.js). This web component is intended to wrap a simple UL list of links. It turns the wrapped UL into an A to Z list. Taking the wrapping approach of a native HTML element was my idea not the LLM&#39;s. I asked the LLM to implement a fallback but each LLM I tried this with inevitably relied on JavaScript. This fails when JavaScript is unavailable in the browser[^1]. How can the LLM do better than rehashing of the training data?

[^1]: By providing a non JavaScript implementation I can interact with the web page using terminal based browser or using simple web scraping libraries.

The second web component successfully generated is called `CSVTextarea` in the &#34;[csvtextarea.js](https://raw.githubusercontent.com/caltechlibrary/CL-web-components/refs/heads/main/csvtextarea.js)&#34;. This web component wraps a TEXTAREA that has CSV data in it&#39;s inner text. The web component presents a simple table for data entry. It features auto complete for columns using DATALIST elements. The `CSVTextarea` emits &#34;focused&#34; and &#34;changed&#34; events when cells receive focus or change. Additional methods are implemented to push data into the component or retrieving data from the component. Both web components include optional attributes &#34;css-href&#34; to include CSS that overrides the default that ships with the component.

At the start of March a spent a couple days working with the free versions of several LLM. I found Chat-GPT to be useless. I found Claude and Mistral to be promising. In all cases I found the &#34;free&#34; versions to be crippled for generating web components. I settled on a paid subscription to Mistral&#39;s Le Chat. The code generation was nearly as good as Claude but less expensive per month.

Out side of work hours I tested code generation for several personal projects as well as code porting.  I was unhappy with the results for porting code from Go to Deno 2.x and TypeScript. The LLM are not trained on recent data. They seem to be about two years out. This includes the paid ones. As the result the best I could do was generate code for Deno 1.x. 

TypeScript and Go have some strong parallels. I&#39;d previously hand ported code. I compared my code with the LLM results and was surprised at the deficits in the generated code. I think this boils down to the training sets used. 

Of all the programming tasks I tried the best results for code generation were targeting JavaScript running in a web browser. This isn&#39;t surprising as the LLM likely trained on web data.

## My development setup for the experiments

I initially tried VSCode and CoPilot. For me it us annoying and highly unproductive[^2]. My reaction certainly is a reflection on my background. I prefer minimal IDE. I am happy with an editor that is limited to syntax highlighting, auto indent and line numbering. When I use VSCode I turn most features off. Your mileage may very with your preferences.

[^2]: It was annoying enough that I initially dismissed using an LLM. My colleague, Tommy, however encouraged me to give it a more serious try. If you lived through the &#34;editor wars&#34; of the early ARPAnet get ready for them to reignite. We&#39;re going to see allot of organizations claiming superior dev setups for integrating LLM into IDE.

I ran my tests using a terminal app, a text editor, a localhost web server and Firefox. The log output of the web server was available in one terminal window and another for my text editor. One browser tab was open to Mistral&#39;s Le Chat. The other tabs open to the HTML pages I used for testing the results of the generated code. It required a fair amount of cut and paste which is tedious. This is far from an ideal setup.

I looked at [Ollama](https://ollama.com) to see about running the LLM from the command line.  Long run this seems like a better approach for me. Unfortunately to use Mistral.ai&#39;s trained models I would need to purchase a more expensive subscription. The price point is roughly the same as Anthropic&#39;s Claude, a closed sourced option. For now I am sticking with cut and paste.

Using Ollama there is the possibility of using Mistral&#39;s open source models and training them further on data I curate. At some point I&#39;ll give it a try. I remain concerned about the electric bill. If collectively we&#39;re going to run these systems they will need clean alternative sources of electricity. Otherwise we are in for an environmental impact like we saw with BitCoin and Block-Chains. I think this is a major problem in the LLM space. I don&#39;t think we can ignore it even when the &#34;AI&#34; hype cycle is hand waving it away.

I liked using Ollama to test free models to understand their differences. I recommend this as an option if you are working from macOS or Windows. The quality of generated code varies considerably between models. It is also true that the speed and processing requirements varies considerably between models. I am sure this is why hardware vendors think they will be able to sell hardware with &#34;AI Chips&#34; built in. I&#39;m skeptical.

I think small language models targeting specific domains could really improve the use case for language models generating code. They could shine for specific tasks or programming languages. They might be reasonable to run on embedded platforms too. I think small language models are an overlooked area in the current &#34;AI&#34; hype cycle.

## What I took away from this experiment

I think a good front end developer[^3] could find an LLM useful as an &#34;assistant&#34;. I think a novice developer will easily be lead astray. As a community of practitioners we should guard against the bias, &#34;computer must be correct&#34;. This is not new. It happened when &#34;expert systems&#34; were the rage before the first AI winter. It&#39;ll be an easy trap to fall into for the public.

[^3]: I am not a front end developer. I have spent most of my career writing back end systems and middleware.

I have a great deal of concern about compromised training data. There is so much mischief possible. It has already been established that &#34;poisoning&#34; the LLM via training data and prompts will result in generating dubious code[^4]. I don&#39;t see much attention paid to the security and provenance of training data, let alone good tools to vet the generated code. This is a security bomb waiting to explode.

[^4]: See [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1). Think of the decades waste on SQL injections then multiply that by magnitudes as people come to trust the results of LLM to build really complex things. 

Today&#39;s software systems are really complex. Reproducibility has become a requirement in mitigating the problem. This is why you&#39;ve seen a rise in virtual environments. The LLM itself doesn&#39;t improve this situation. I&#39;ve used the same text prompts with the same LLM but different chat session and gotten significantly different results. The LLM as presently implemented exacerbate the problems of reproducibility. 

To an certain extent we can strengthen our efforts around quality assurance. The trouble I&#39;ve found is this too is a domain where LLMs are being applied. If the quality assurance LLM is tainted we don&#39;t get the assurance we need. There also is the very human problem of typos in our prompts. That&#39;s a very deep rabbit hole by itself.

## Lessons learned

I got the best results by composing a long specification as an initial state prompt to kick off code generation. I still needed to fix bugs using short prompts interactively. With the CSVTextarea I threw away five versions before I got to something useful. Each chat session lasted a couple hours. They were spread out over many days.

There was a clear point when additional prompts don&#39;t improve the results of generated code. I found three cases where I lead the LLM down a rabbit hole.

1. the terms I used weren&#39;t what the LLM was trained on so it couldn&#39;t respond the way I wanted
2. the visible results, e.g. the web component failing to render, doesn&#39;t indicate the underlying problem, this leads to prompt churn
3. their was a subtle assumption in the generated code I didn&#39;t pick up and correct early on

A beneficial result of using an LLM to generate code is that it encourages reading the source. Reading code is generally not taught enough. You&#39;re going to be reading allot of code using the current crop of commercially available LLM. That is a good thing in my book.

To solve the rabbit hole problem I adopted the following practices. Keep all my prompts in a Markdown document, along with notes to myself. When I felt the LLM had gone down a rabbit whole have the LLM generate a &#34;comprehensive detailed specification&#34; of the code generated. Compare the LLM specification against my own prompts helped simplify things when starting over.

Be willing to throw away the LLM results and start over. This is important in exploratory programming but also when you&#39;re using LLM generated code to solve a known problem. If you can tolerate the process of writing and refining the prompts in your native language the LLM will happily attempt to generate code for them. I&#39;d love to get some English and Philosophy graduates using LLM for code generation. It&#39;d be interesting to see how their skills may out perform those of traditionally educated software engineering graduates. I think the humanities fields that could benefit from a quantitative approach may find LLM to generate code to do analysis really compelling.

While I like the results I got for this specific tests I remain on the fence about **general usefulness** of LLM in the area of code generation. I suspect it&#39;ll take time before the shared knowledge and practice in using them emerges. There is also the problem of energy consumption.  This feels like the whole &#34;proof of work&#34; problem consuming massive amounts of electricity in the block chain tech. That alone was enough to turn me off of block chain for most of its proposed applications. Hopefully alternatives will be developed to avoid that outcome with large language models.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/03/13/Building_Web_Component_using_an_LLM.md" length="13386" type="text/markdown" />
    </item>    <item>
      <title>Setting up my Raspberry Pi 500, a Portable Workstation</title>
      <link>https://rsdoiel.github.io/blog/2025/02/14/Review_Pi-500_as_portable_workstation.html</link>
      <description>
        <![CDATA[Quick notes on configuring a Raspberry Pi 500 as a portable workstation along with a price list.]]>
      </description>
      <source:markdown># Setting up my Raspberry Pi 500, a Portable Workstation

By R. S. Doiel, 2025-02-14

I&#39;m  writing this on a newly setup Raspberry Pi 500. So far I&#39;m impressed. I was sceptical about moving from my Pi 400 but when it died I was forced to upgrade. It has been worth it.

I&#39;ve now had a chance to install the various pieces of software I regularly use. Even do some web browsing with it. Image some SD cards for a RISC OS project I&#39;m working on. It feels much quicker than my Pi 400. I think the speed increase is in part the faster CPU but I suspect the program I am running is taking advantage of the 8 Gig RAM instead of the old 4. As I&#39;ve testing this machine out I noticed I had stopped asking myself the question about launching more than one large program at a time (e.g. my browser is running and being used as I type this in VS Code). When I tried VS Code on my Pi 4B+ and 400 it just felt too sluggish. I&#39;m typing this review in VS Code now on the Pi 500 is keeping up OK.

When I purchased the Pi-500 and I also decided to go with the new Raspberry Pi Monitor. Together I have a complete portable workstation. While my previous powered WaveShare monitor was higher resolution the Pi Monitor it  also used allot more power.  With the combination of Pi Monitor and Pi-500 I am now contemplating exploring power consumption so I can find a one battery to power the monitor and Pi-500.

Here&#39;s the spec and pricing for this new machine. Some parts I didn&#39;t purchase because I already owned them (e.g. the 27W Pi power supply) but I have included the list prices for those who might be interested.

Item                                List Price    Notes                   
----------------------------------  ------------  -----------------------
Raspberry Pi 500 unit               $90.00        included  32G SD card  
Raspberry Pi 27W Power supply       $13.65        used existing one      
Raspberry Pi Mouse                  $9.25         used existing one      
Micro HDMI to standard HDMI cable   $5.75         used existing one      
Raspberry Pi Monitor                $100.00                              
Raspberry Pi 15W Power supply       $8.00         for monitor            
Raspberry Pi 128G SD Card           $16.95        upgraded storage       


## Software Setup

I created an &#34;image&#34; on my 128G SD Card using the Raspberry Pi Imager on another computer.  The imager lets you setup initial user account, WiFi configuration and whether you want SSH services running the first boot. I like using the vanilla 64 bit Raspberry Pi OS distribution. I picked the one for the Raspberry Pi 5 series.

When I booted the Pi-500 connected to the Pi Monitor it seemed to cause the screen to cycle through and Pi splash page a few times. Once it completed its first boot it hasn&#39;t done that again. I am assuming that was some negotiation between the monitor and the Pi. After a quick click around test I rebooted the machine by doing a full shutdown and power off and the power back up.

### Development and Writing Software

I am planning to use the Pi-500 as a light weight development machine and as a machine for preparing updates for my blog. I installed the additional software below.

- Pandoc 3 via installed via deb package from Pandoc GitHub releases
- Rust installed via Rustup
- PageFind installed via Cargo
- Flatlake installed via Cargo
- htmlq installed via Cargo
- ncal installed via Cargo
- Deno installed via website using CURL
- Go installed via website&#39;s tar ball
- jq installed via apt
- SQLite3, libsqlite3-dev installed via apt
- Hunspell and US English dictionary installed via apt

The &#34;build-essential&#34; package was already installed. I noticed that Git and GNU Make were immediately available with my first boot.

I this setup as a portable  workstation. It feels quick and snappy but is small enough to toss in computer bag. I did test compile a few things. It is possible to peg the CPU but then again it&#39;s a little machine after all. I didn&#39;t get any warning lights or notifications like I used to on the Pi-400.

With a connection to my home WiFi network (not a fast connection) it took me about an hour or so to download and install all my extras. This was quicker than the last time I setup a Pi. Some of the time saved was the better hardware and net work performance but much of the time saved was due to the fact that I did not have to compile software from scratch. That was a change from the last time I setup a Pi up.  I guess  Pi and aarch64 processors are common enough that projects are now including it in their regular builds.

If  I pickup the right capacity battery I suspect I will have a lovely deconstructed Laptop to use as a portable workstation.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/02/14/Review_Pi-500_as_portable_workstation.md" length="5262" type="text/markdown" />
    </item>    <item>
      <title>Book review, &#34;Man and the Computer&#34;</title>
      <link>https://rsdoiel.github.io/blog/2025/02/10/Man_and_the_Computer.html</link>
      <description>
        <![CDATA[A book review of a vintage computer publication, "Man and the Computer" by
        John G. Kemeny, published 1972, ISBN: 0684130092
        Read at the Open Library, <https://openlibrary.org/books/OL5282840M/Man_and_the_computer>]]>
      </description>
      <source:markdown># Book review, &#34;Man and the Computer&#34;

By R. S. Doiel, 2025-02-10

Open Library has a wonderful collection of classic Computer related texts. This is a review of one of them.  &#34;Man and the Computer&#34; was written by [John G. Kemeny](https://en.wikipedia.org/wiki/John_G._Kemeny) and published in 1972. The book covers the evolution of the Dartmouth Time Sharing System (DTSS) and BASIC[^1].

[^1]: Prof. Kemeny was co-developer of BASIC along with [Thomas Kurtz](https://en.wikipedia.org/wiki/Thomas_E._Kurtz) The author, Kemeny, as a math professor at Dartmouth and eventual became president of the university.

It is an interesting weekend read. You can read the text at [Open Library](https://openlibrary.org/books/OL5282840M/Man_and_the_computer). The book is short (160 pages or so). It is written for casual reading like a talk given to a small group. 

The first part goes into the innovations that resulted for the undergraduate students who used, developed and extended the systems. The later part of the book covers what the implications of the system had been by the 1970s and what it suggestions for the future through 1990. Given that the book was written before DARPAnet, before Internet and before Personal Computers a surprising amount of the  Kemeny&#39;s predictions were on target and remaining relevant today. He anticipated Open Access and Cloud Computing and the benefits the were possible. My take away is it is a charming reflection of where computing was at for Dartmouth and its alumni in the start of the 1970s.

Stepping back to a bigger picture of computing in the 1960s and 1970s the book does have blind spots.  This is not surprising because the communications were so much more restrictive before the Internet in terms of technical exchange in computing. It is no wonder that there is but one line that mentions the innovations in time-sharing that occurred at RAND with the [Johniac Open Shop System](https://en.wikipedia.org/wiki/JOSS) (aka. JOSS-1, JOSS-2). They system have some parallels with the early DTSS/BASIC incarnations and may have preceded by as much as a few years[^2]. JOSS lead to other systems like CAL, PIL/I, FOCAL and MUMPs. The later, like JOSS, bare similarities to what would be developed at Dartmouth in the form of BASIC.

[^2]: JOSS dates from approximately 1963 and DTSS was released in 1964. Kemeny was published in research memorandum in 1953 and probably talked about his ideas. https://en.wikipedia.org/wiki/JOSS

It is quite reasonable for this text to the have these blind spots. First it was not intended to be a history of computing (see the Preface of the book) but rather a high level look at what the promise was for people who could work interactively with a computer. I feel it gives insights into the early era where computing access was rapidly increasing and the sense of promise that carried. Well worth the reading time if you&#39;re a hobbyist or arm chair computer history buff.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/02/10/Man_and_the_Computer.md" length="3691" type="text/markdown" />
    </item>    <item>
      <title>Working with Structured Data in Deno and TypeScript</title>
      <link>https://rsdoiel.github.io/blog/2025/02/03/working_with_structured_data.html</link>
      <description>
        <![CDATA[A short discourse on working with structured data in TypeScript and easily
        converting from JSON, YAML and XML representations.]]>
      </description>
      <source:markdown># Working with Structured Data in Deno and TypeScript

One of the features in Go that I miss in TypeScript is Go&#39;s [DSL](https://en.wikipedia.org/wiki/Domain-specific_language &#34;Domain Specific Language&#34;) for expressing data representations.  Adding JSON, YAML and XML support in Go is simple. Annotating a struct with a string expression. There is no equivalent feature in TypeScript. How do easily support multiple representations in TypeScript?

Let&#39;s start with JSON. TypeScript has `JSON.stringify()` and `JSON.parse()`. So getting to JSON representation is trivial, just call the stringify method. Going from text to populated object is done with `JSON.parse`. But there is a catch.

Let&#39;s take a simple object I&#39;m defining called &#34;ObjectN&#34;. The object has a single attribute &#34;n&#34;. &#34;n&#34; holds a number. The initial values is set to zero. What happens when I instantiate my ObjectN then assign it the result from `JSON.parse()`.

~~~TypeScript
class ObjectN {
    n: number = 0;
    addThree(): number {
        return this.n + 3;
    }
}
let src = `{&#34;n&#34;: 1}`;
let o: ObjectN = new ObjectN();
o = JSON.parse(src);
// NOTE: This will fail, addThree method isn&#39;t available.
console.log(o.addThree());
~~~

Huston, we have a problem. No &#34;addThree&#34; method. That is because JSON doesn&#39;t include method representation. What we really want to do is inspect the object returned by `JSON.parse()` and set the values in our ObjectN accordingly. Let&#39;s add a method called `fromObject()`.
(type the following into the Deno REPL).

~~~TypeScript
class ObjectN {
    n: number = 0;
    addThree(): number {
        return this.n + 3;
    }
    fromObject(o: {[key: string]: any}): boolean {
        if (o.n === undefined) {
            return false;
        }
        // Validate that o.n is a number before assigning it.
        const n = (new Number(o.n)).valueOf();
        if (isNaN(n)) {
            return false;
        }
        this.n = n;
        return true;
    }
}
let src = `{&#34;n&#34;: 1}`;
let o: ObjectN = new ObjectN();
console.log(o.addThree());
o.fromObject(JSON.parse(src));
console.log(o.addThree());
~~~

Now when we run this code we should see a &#34;3&#34; and then a &#34;4&#34; output. Wait, `o.fromObject(JSON.parse(src));` looks weird. Why not put `JSON.parse()` inside &#34;fromObject&#34;? Why not renamed it &#34;parse&#34;?

I want to support many types of data conversion like YAML or XML. I can use my &#34;fromObject&#34; method with the result of produced from `JSON.parse()`, `yaml.parse()` and `xml.parse()`. One function works with the result of all three. Try adding this.

~~~TypeScript
import * as yaml from &#39;jsr:@std/yaml&#39;;
import * as xml from &#34;jsr:@libs/xml&#34;;
src = `n: 2`;
o.fromObject(yaml.parse(src));
console.log(o.addThree());
src = `&#60;n&#62;3&#60;/n&#62;`;
o.fromObject(xml.parse(src));
console.log(o.addThree());
~~~

That works!

Still it would be nice to have a &#34;parse&#34; method too. How do I do that without winding up with a &#34;parseJSON()&#34;, &#34;parseYAML()&#34; and &#34;parseXML()&#34;? What I really want is a &#34;parseWith&#34; method which accepts the text and a parse function. TypeScript expects type information about the function being passed. I solve that problem by including a &#34;ObjectParseType&#34; definition that works across the three parsing objects -- JSON, yaml and xml.

~~~TypeScript
import * as yaml from &#39;jsr:@std/yaml&#39;;
import * as xml from &#34;jsr:@libs/xml&#34;;

// This defines my expectations of the parse function provide by JSON, yaml and xml.
type ObjectParseType = (arg1: string, arg2?: any) =&#62; {[key: string]: any} | unknown;

class ObjectN {
    n: number = 0;
    addThree(): number {
        return this.n + 3;
    }
    fromObject(o: {[key: string]: any}) : boolean {
        if (o.n === undefined) {
            return false;
        }
        // Validate that o.n is a number before assigning it.
        const n = (new Number(o.n)).valueOf();
        if (isNaN(n)) {
            return false;
        }
        this.n = n;
        return true;
    }
    parseWith(s: string, fn: ObjectParseType): boolean {
        return this.fromObject(fn(s) as unknown as {[key: string]: any});
    }
}

let o: ObjectN = new ObjectN();
console.log(`Initial o.addThree() -&#62; ${o.addThree()}`);
console.log(`o.toString() -&#62; ${o.toString()}`);

let src = `{&#34;n&#34;: 1}`;
o.parseWith(src, JSON.parse);
console.log(`parse with JSON, o.addThree() -&#62; ${o.addThree()}`);
console.log(`JSON.stringify(o) -&#62; ${JSON.stringify(o)}`);

src = `n: 2`;
o.parseWith(src, yaml.parse);
console.log(`parse with yaml, o.addThree() -&#62; ${o.addThree()}`);
console.log(`yaml.stringify(o) -&#62; ${yaml.stringify(o)}`);

src = `&#60;?xml version=&#34;1.0&#34;?&#62;
&#60;n&#62;3&#60;/n&#62;`;
o.parseWith(src, xml.parse);
console.log(`parse with xml, o.addThree() -&#62; ${o.addThree()}`);
console.log(`xml.stringify(o) -&#62; ${xml.stringify(o)}`);
~~~

As long as the parse method returns an object I can now update my ObjectN instance
from the attributes of the object expressed as JSON, YAML, or XML strings. I like this approach because I can add validation and normalization in my &#34;fromObject&#34; method and use for any parse method that confirms to how JSON, YAML or XML parse works. The coding cost is the &#34;ObjectParseType&#34; type definition and the &#34;parseWith&#34; method boiler plate and defining a class specific &#34;fromObject&#34;. Supporting new representations does require changes to my class definition at all.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/02/03/working_with_structured_data.md" length="5882" type="text/markdown" />
    </item>    <item>
      <title>Moving beyond git template repositories with CodeMeta</title>
      <link>https://rsdoiel.github.io/blog/2025/01/31/moving_beyond_git_templates.html</link>
      <description>
        <![CDATA[An exploration of using CodeMeta objects to generate of software artifacts as an alternative to Git template repositories.]]>
      </description>
      <source:markdown># Moving beyond git template repositories with CodeMeta

By R. S. Doiel, 2025-01-31 (updated: 2025-02-03)

A nice feature of GitHub is the ease in starting a new repository with a complete set of documents[^1]. This feature creates a problem. The &#34;template&#34; documents require editing. Then they require more editing to keep them from being stale. If you&#39;re serious about keeping documentation up to date then the copy edit work must be continuous. Copy edit work is tedious. Is there a path beyond the git repository templates that avoid stale [software artifacts](https://en.wikipedia.org/wiki/Artifact_(software_development))?

[^1]: The feature is built around using another Git repository as a template. GitHub has a nice UI for this, essentially it is forking the &#34;template repository&#34; when you create your new repository.

Existing development tools suggestion a solution. For decades software developers have had tools to extract documentation from the comments in source code.  [Knuth](https://en.wikipedia.org/wiki/Donald_Knuth) pioneered this with his [literate programming](https://en.wikipedia.org/wiki/Literate_programming) approach. A simplified take on this is [Javadoc](https://en.wikipedia.org/wiki/Javadoc) used in the Java programming community. Today most open source programming languages have similar tools available. In 2025 it is trivial to generate source code documentation directly from the comments in your source code. Can an approach like this be used for more of our software artifacts?

I believe leveraging [CodeMeta](https://codemeta.github.io &#34;See the section titled, Motivation&#34;), developed by the research programming community, gives us a path forward for general software development.

## How is CodeMeta relevant? 

The CodeMeta data is meant to be machine readable and human manageable. That&#39;s a developer sweet spot. The CodeMeta object let&#39;s you track metadata like name, description, version, authorship and contributors. It includes where the project source repository is hosted and the license used. Over time the list of metadata attributes in the CodeMeta object has grown. Today we have a long list of a project&#39;s metadata that can be tracked and acted upon.

When I started including CodeMeta files in my projects at Caltech Library I did so at software release. It was a task to do at the end of the process like the task of reviewing and updating the cloned documentation files. Treating the CodeMeta as an after thought created a burden. It was a disincentive to adopting CodeMeta. 

I think we make the CodeMeta object relevant to developers by treating it as primary source code. Editing it should be the first rather than last thing updated before a release. **An accurate CodeMeta file is actionable.** Ignored this and you ignore CodeMeta&#39;s super power.

Can CodeMeta be used to help lower the documentation burden? Yes.

In 2025 there is enough information in a complete CodeMeta object to create structured documents. This includes documents like a README and INSTALL. Keeping track of the content as metadata can lower the effort in managing stale documentation. It may be enough to eliminate the need for git template repositories. At a minimum it can shrink what is needed from template repositories.

Here&#39;s the steps I used to take setting up a project.

1. Write the README
2. Document how the software will work
3. Write tests to confirm the software works
4. Write the software
5. Create/update my software artifacts to reflect the current state such as the codemeta.json file.

Repeat as needed. The last step was always tedious. The longer a project is around the more artifacts need to be managed. It was easy to want to short cut that last step.

Here&#39;s what I have been experimenting with.

1. Create or update the CodeMeta file
2. Document or update how the software should work
3. Create or update tests to confirm the software works as documented
4. Write or update the software to pass the tests
5. **Generate** additional software artifacts from the CodeMeta document.

Step five is automated. In practice step five can be integrated with your standard build processes. Us humans focus on steps one through four. Life just got a little easier for the busy developer.

Several questions are suggested by this proposal.

1. How do I make it easy to create and update the CodeMeta file?
2. How do I generate the software artifacts?
3. What artifacts can be generated in a reliable way?

The CodeMeta object is stored as a JSON document in your repository. That means you can readily build tooling around it.

Today you can use the CodeMeta [generator](https://codemeta.github.io/codemeta-generator/) to bootstrap the creation of a CodeMeta file.  If you&#39;re willing to do some cut and paste work the generator can even be used to maintain your CodeMeta file.  

What about generating our artifacts?

For the last few years I&#39;ve relied on Pandoc templates, see [codemeta-pandoc-examples](https://github.com/caltechlibrary/codemeta-pandoc-examples). I use Pandoc and these templates to generate files like CITATION.cff, about.md, installer scripts and version files for the Python, Go and TypeScript programming languages. The trouble with this approach is Pandoc tends to be well knowing in the data science community but not in the general developer community. The Pandoc template language is obscure. This has lead me to believe new tools are needed beyond Pandoc and templates.

***

## CMTools

I recently started prototyping two programs for working with [CodeMeta](https://codemeta.github.io) objects. The prototype code is available at [github.com/caltechlibrary/CMTools](https://github.com/caltechlibrary/CMTools). The two programs are for editing (`cme`) and for transforming (`cmt`) the CodeMeta object. I am currently testing the prototypes in selected Caltech Library projects.

### What challenges have the prototypes raise?

The CodeMeta object is sprawling. `cme` was started as command line alternative to the generator.  It was initially designed with an interactive, prompt and response, user interface. It would iterate over all the top level attributes prompting for changes. This can be tedious. I quickly added support to shorten the process by including a list of specific attributes to edit. 

~~~shell
cme codmeta.json                       # &#60;-- iterate over all
cme codemeta.json version dateModified # &#60;-- just listed attributes
~~~

The prompt and response approach works well for simple attribute types like name, description and version. The more complex attributes like author or contributor were challenging. To avoid the need to increase the types of prompts or be forced into a menu system I&#39;m experimenting with using YAML to display the current value and accept YAML as the user response. YAML is much easier to type and copy edit than JSON. It is easy to transform into JSON. The downside is you need to know the structure and attribute names ahead of time. That gives `cme` a training cost.

Multi line values are tricky to work with if you rely on standard input. To address this I added a feature to allow the use the editor of your choice.  If you are on macOS or Linux the default editor is nano. On Windows it is notepad.exe.  You can pick a different editor by setting the EDITOR environment variable.  In the example below I&#39;ve chosen the [Micro Editor](https://micro-editor.github.io) for setting values. It didn&#39;t solve the problem of knowing the YAML attributes in advance but does make it easier to copy edit the YAML. Micro Editor is Open Source and available for macOS, Linux and Windows. Support for other editors could be added. Further prototyping and development work is needed to support alternatives to editing YAML.

On macOS and Linux

~~~shell
export EDITOR=micro
cme codemeta.json author -e
~~~

or on Windows

~~~ps1
Set-Variable EDITOR micro
cme codemeta.json author -e
~~~

In the 0.0.9 release of the `cme` prototype I added the ability to set the attribute values from the command line. This has helped in automated environments. CodeMeta attribute name is used as the key. An equal sign, &#34;=&#34;, is the delimiter. What follows the equal sign is treated as the value. This works well for simple fields, e.g. version, dateModified.

~~~shell
cme codemeta.json version=&#34;0.0.2&#34; dateModified=&#34;2025-01-30&#34;
~~~

Complex attribute editing using this approach is very challenging.

### What can CMTools generate?

The `cmt` prototype has limited abilities. It can render about.md and CITATION.cff files. It can generate &#34;version&#34; source code files for Python (version.py), Go (version.go), TypeScript (version.ts) and JavaScript (version.js). I am actively working on porting the remaining Pandoc templates from codemeta-pandoc-examples to `cmt`. README and INSTALL will be added after the template port is complete.

~~~shell
cmt codemeta.json about.md CITATION.cff version.py \\
  README.md INSTALL.md installer.sh installer.ps1
~~~

## What&#39;s next?

CMTools is at an early stage of development (January 2025). The project is focused finding the balance of editing and generating. Improvements will flow base on our usage.

The [v0.0.14 release](https://github.com/caltechlibrary/CMTools/releases/tag/v0.0.14) includes the basic features discussed in this post for both `cme` and `cmt`. RSD 2025-02-03</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/01/31/moving_beyond_git_templates.md" length="10051" type="text/markdown" />
    </item>    <item>
      <title>Deno 2.1.7, Project Idioms</title>
      <link>https://rsdoiel.github.io/blog/2025/01/29/project_idioms.html</link>
      <description>
        <![CDATA[Notes on some of the file and code idioms I'm using with Deno+TypeScript projects.]]>
      </description>
      <source:markdown># Deno 2.1.7, Project Idioms

I&#39;ve noticed a collection of file and code idioms I&#39;ve been used in my recent Deno+TypeScript projects at work. I&#39;ve captured them here as a future reference.

## Project files

My project generally have the following files, these are derived from the [CodeMeta](https://codemeta.github.io) file using [CMTools](https://caltechlibrary.github.io/CMTools).

codemeta.json
: Primary source of project metadata, used to generate various files

CITATION.cff
: used by GitHub for citations. version, dateModified, datePublished and releaseNotes


about.md
: A project about page. This is generated based on the codemeta.json file.

README.md, README
: A general read me describing the project and pointing to INSTALL.md, user_manual.md as appropriate

INSTALL.md
: These are boiler plate description of how to install and compile the software

user_manual.md
: This is an index document, a table of contents. It points to other document including Markdown versions of the man page(s).

For TypeScript projects I also include a following

version.ts
: This hold project version information used in the TypeScript co-debase. It is generated from the codemeta.json via CMTools.

helptext.ts
: This is where I place a function, `fmtHelp()`, for rendering response to the &#34;help&#34; command line option.

I&#39;m currently ambivalent about &#34;main.ts&#34; file which is created by `deno init`. My ambivalent is that most of my projects wind up producing more than one program from a shared code base. a single &#34;main.ts&#34; doesn&#39;t really fit that situation.

The command line tool will have a TypeScript with it&#39;s name. Inside this file I&#39;ll have a main function and use the Deno idiom `if (import.meta.main) main();` to invoke it. I don&#39;t generally put the command line  TypeScript in my &#34;mod.ts&#34; file since it&#39;s not going to work in a browser or be useful outside my specific project.

mod.ts
: I usually re-export modules here that maybe useful outside my project (or in the web browser).

deps.ts
: I use this if there are allot of files consistently being imported across the project, otherwise I skip it.

## What I put in Main

I use the main function to define command line options, handle parameters such as data input, output and errors. It usually invokes a primary function modeled in the rest of the project code.

Here is an example Main for a simple &#34;cat&#34; like program.

~~~TypeScript
import { parseArgs } from &#34;jsr:@std/cli&#34;;
import { licenseText, releaseDate, releaseHash, version } from &#34;./version.ts&#34;;
import { fmtHelp, helpText } from &#34;./helptext.ts&#34;;

const appName = &#34;mycat&#34;;

async function main() {
  const app = parseArgs(Deno.args, {
    alias: {
      help: &#34;h&#34;,
      license: &#34;l&#34;,
      version: &#34;v&#34;,
    },
    default: {
      help: false,
      version: false,
      license: false,
    },
  });
  const args = app._;

  if (app.help) {
    console.log(fmtHelp(helpText, appName, version, releaseDate, releaseHash));
    Deno.exit(0);
  }
  if (app.license) {
    console.log(licenseText);
    Deno.exit(0);
  }
  if (app.version) {
    console.log(`${appName} ${version} ${releaseHash}`);
    Deno.exit(0);
  }

  let input: Deno.FsFile | any = Deno.stdin;

  // handle case of many file names
  if (args.length &#62; 1) {
    for (const arg of args) {
      input = await Deno.open(`${arg}`);
      for await (const chunk of input.readable) {
        const decoder = new TextDecoder();
        console.log(decoder.decode(chunk));
      }
    }
    Deno.exit(0);
  }
  if (args.length &#62; 0) {
    input = await Deno.open(Deno.args[0]);
  }
  for await (const chunk of input.readable) {
    const decoder = new TextDecoder();
    console.log(decoder.decode(chunk));
  }
}

if (import.meta.main) main();
~~~

## helptext.ts

The following is an example of the [helptext.ts](helptext.ts) file for the demo [mycat.ts](mycat.ts).

```TypeScript
export function fmtHelp(
  txt: string,
  appName: string,
  version: string,
  releaseDate: string,
  releaseHash: string,
): string {
  return txt.replaceAll(&#34;{app_name}&#34;, appName).replaceAll(&#34;{version}&#34;, version)
    .replaceAll(&#34;{release_date}&#34;, releaseDate).replaceAll(
      &#34;{release_hash}&#34;,
      releaseHash,
    );
}

export const helpText =
  `%{app_name}(1) user manual | version {version} {release_hash}
% R. S. Doiel
% {release_date}

# NAME

{app_name}

# SYNOPSIS

{app_name} FILE [FILE ...] [OPTIONS]

# DESCRIPTION

{app_name} implements a &#34;cat&#34; like program.

# OPTIONS

Options come as the last parameter(s) on the command line.

-h, --help
: display help

-v, --version
: display version

-l, --license
: display license


# EXAMPLES
~~~shell
{app_name} README.md
{app_name} README.md INSTALL.md
~~~
`;
```

## Generating version.ts

The [version.ts](version.ts) is generated form two files, [codemeta.json] and [LICENSE] using the CMTools, `cmt` command.

~~~
cmt codemeta.json veresion.ts
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/01/29/project_idioms.md" length="5390" type="text/markdown" />
    </item>    <item>
      <title>Deno 2.1.7, Points of Friction</title>
      <link>https://rsdoiel.github.io/blog/2025/01/26/points_of_friction.html</link>
      <description>
        <![CDATA[A short discussion of working with file input in TypeScript+Deno coming from the
        perspective of Go's idiomatic use of io buffers.]]>
      </description>
      <source:markdown># Deno 2.1.7, Points of Friction

By R. S. Doiel, 2025-01-26

I have run into a few points of friction in my journey with Deno coming from Go. I miss Go&#39;s standard &#34;io&#34; and &#34;bufio&#34; packages. With the Go code I&#39;m porting TypeScript I&#39;d often need to handle standard input or input from a named file interchangeably. Seems like this should be easy in Deno&#39;s TypeScript but there are a few bumps in the road.

Here&#39;s the Go idiom I commonly use.

~~~go
var err error
input := io.Stdin
if inFilename != &#34;&#34; {
    input, err := os.Open(inFilename)
    if err !== nil {
        // ... handle error
    }
    defer input.Close();
}
// Now I can just pass &#34;in&#34; around for processing.
~~~

Conceptually this feels simple though verbose. I can pass around the &#34;input&#34; for processing in a way that is agnostic as to file or standard input. This type of Go code works equally on POSIX and Windows.

Deno provide access to [standard input](https://docs.deno.com/api/deno/~/Deno.stdin). Deno supports streamable files. From the docs here&#39;s an simple example.

~~~TypeScript
// If the text &#34;hello world&#34; is piped into the script:
const buf = new Uint8Array(100);
const numberOfBytesRead = await Deno.stdin.read(buf); // 11 bytes
const text = new TextDecoder().decode(buf);  // &#34;hello world&#34;
~~~

Setting aside the buffer management code it seems simple and straight forward. It is easy to understand and you could wrap it in a function easily to hide the buffer management part. Yet it doesn&#39;t provide the same flexibility as the more verbose Go version. Surely there is an an idiomatic why of doing this in TypeScript already? 

## Stability Challenge

Deno currently is a rapidly evolving platform. My first impulse was to reach for packages like `jsr:@std/fs` or `jsr:@sys/fs`. When I search for examples they mostly seem to reference specific versions of &#34;std/fs&#34; that are not available via jsr. So what&#39;s the &#34;right&#34; way to approach this?

## Repl to the rescue.

Poking around in the Deno repl I tried assigning `Deno.stdin` to a local variable. Playing with command line completion I realized it has most of the the methods you would get if you used `Deno.open()` to open a named file.

Here&#39;s a little test I ran in the repl after creating a &#34;hellworld.txt&#34; text file.

~~~deno
deno
const stdin = Deno.stdin;
let input = Deno.open(&#39;helloworld.txt&#39;)
stdin.isTerminal();
input.isTerminal();
stdin.valueOf();
input.valueOf();
Deno.exit(0);
~~~

The `valueOf()` reveals their type affiliation. It listed them as `Stdin {}` and `FsFile {}` respectively. I used TypeScript&#39;s typing system to let us implement &#34;mycat.ts&#34;. You can assign multiple types to a variable with a `|` (pipe) symbol in TypeScript. 

Used that result to write a simple cat file implementation.

~~~TypeScript
async function catFile() {
    let input : Stdin | FsFile = Deno.stdin;

    if (Deno.args.length &#62; 0) {
        input = await Deno.open(Deno.args[0]);
    }

    const decoder = new TextDecoder();

    // NOTE: the .readable function is available on both types of objects.
    for await (const chunk of input.readable) {
        console.log(decoder.decode(chunk));
    }
}

if (import.meta.main) catFile();
~~~

You can &#34;run&#34; this deno to see it in action. Try running it on your &#34;helloworld.txt&#34; file.

~~~shell
deno run --allow-read mycat.ts helloworld.txt
~~~

You can also read from standard input too. Try the command below type in some text then press Ctrl-D or Ctrl-Z if you&#39;re on Windows.

~~~shell
deno run --allow-read mycat.ts
~~~

Looks like we have a nice solution. Now I can compile &#34;mycat.ts&#34;.

## trouble in paradise

While you can &#34;run&#34; the script you can&#39;t compile it. It doesn&#39;t pass &#34;check&#34;. This is the error I get with Deno 2.1.7.

~~~shell
deno check mycat.ts
Check file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts
error: TS2304 [ERROR]: Cannot find name &#39;Stdin&#39;.
    let input : Stdin | FsFile = Deno.stdin;
                ~~~~~
    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:17

TS2552 [ERROR]: Cannot find name &#39;FsFile&#39;. Did you mean &#39;File&#39;?
    let input : Stdin | FsFile = Deno.stdin;
                        ~~~~~~
    at file:///C:/Users/rsdoi/Sandbox/Writing/Articles/Deno/mycat.ts:3:25

    &#39;File&#39; is declared here.
    declare var File: {
                ~~~~
        at asset:///lib.deno.web.d.ts:622:13

Found 2 errors.
~~~

It seems like what works in the repl should also compile but that&#39;s isn&#39;t the case. I have an open question on Deno&#39;s discord help channel and am curious to find the &#34;correct&#34; way to handle this problem.

## Update 2025-01-26, 5:00PM

I heard back on Deno Discord channel for help.  With the help of [crowlKat](https://github.com/crowlKats) sorted the problem out.

The compile and runnable version of [mycat.ts](mycat.ts) looks like this.

~~~typescript
async function main() {
    let input : Deno.FsFile | any = Deno.stdin;

    if (Deno.args.length &#62; 0) {
        input = await Deno.open(Deno.args[0]);
    }

    const decoder = new TextDecoder();

    // NOTE: the .readable function is available on both types of objects.
    for await (const chunk of input.readable) {
        console.log(decoder.decode(chunk));
    }
}

if (import.meta.main) main();
~~~

The &#34;any&#34; type feels a little ugly but since I am assinging the default value is `Deno.stdin` it covers that case where the `Deno.FsFile` covers the case of a name file.  Where does this leave me? I have a nice clean idiom that does what I want for interacting with standard input or a file stream.  Not necessarily the fast thing on the planet but it works.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2025/01/26/points_of_friction.md" length="6144" type="text/markdown" />
    </item>    <item>
      <title>Installing Deno via Cargo and other options</title>
      <link>https://rsdoiel.github.io/blog/2024/12/13/installing-via-cargo-etc.html</link>
      <description>
        <![CDATA[Notes on setting up a Debian flavored Linux boxes, macOS and Windows to install Deno via `cargo install deno`,
        `curl -fsSL https://deno.land/install.sh | sh` or
        `iwr https://deno.land/install.ps1 -useb | iex`]]>
      </description>
      <source:markdown># Installing Deno via Cargo and other options

By R. S. Doiel, 2024-12-13

I&#39;ve recently needed to install Deno on several Debian flavored Linux boxes.  I wanted to install Deno using the `cargo install --locked deno` command. Notice the `--locked` option, you need that for Deno. This worked for the recent Ubuntu 22.04 LTS release. I needed alternatives for Ubuntu 20.04 LTS and Raspberry Pi OS.

## Using Cargo

Prerequisites:

- Rust (install with [Rustup](https://rustup.rs))
- CMake
- Clang, LLVM dev, Clang DEV and the lld (clang) linker
- SQLite3 and LibSQLite3 dev
- pkg config
- libssh dev, libssl dev

The Debian flavors I work with are recent (Dec. 2024) Ubuntu 22.04 LTS release[^1].

Recently when I was installing Deno 2.1.4 I got errors about building the `flate2` module. I had forgotten to include the `--locked` option in my cargo command. I found this solution in Deno GitHub issue [9524](https://github.com/denoland/deno/issues/9524).

```shell
sudo apt install -y build-essential cmake clang libclang-dev llvm-dev lld \
                    sqlite3 libsqlite3-dev pkg-config libssh-dev libssl-dev
rustup update
cargo install deno --locked
```

## Other options

For Ubuntu 20.04 LTS and Raspberry Pi OS, use `curl -fsSL https://deno.land/install.sh | sh` to install.

For Windows on ARM64 use `iwr https://deno.land/install.ps1 -useb | iex`.

 `curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh`
On Raspberry Pi OS I added a `nice` before calling `cargo`. Without the &#34;nice&#34; it failed after the &#34;spin&#34; module.

[^1]: I failed to install Deno this way on Ubuntu 20.04 LTS, just use the cURL + sh script.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/12/13/installing-via-cargo-etc.md" length="2345" type="text/markdown" />
    </item>    <item>
      <title>When Deno+TypeScript, when Go?</title>
      <link>https://rsdoiel.github.io/blog/2024/12/06/when_deno_when_go.html</link>
      <description>
        <![CDATA[Brief discussion of when I choose Deno+TypeScript versus Go for work projects.]]>
      </description>
      <source:markdown># When Deno+TypeScript, when Go?

By R. S. Doiel, 2024-11-06

Last month I gave a [presentation](https://caltechlibrary.github.io/cold/presentations/presentation1.html) on a project written in [Deno](https://deno.com)+[TypeScript](https://typescriptlang.org). The project included code that needed to be shared server and browser side.  At the conclusion of my talk the question came up, &#34;When would I choose Go versus Deno+TypeScript for a project?&#34;[^1]. The answer I came up with at the time was I would choose Deno+TypeScript when I needed to share code between server and browser. That was the question that had lead me to Deno in the first place. Deno+TypeScript shared many of the advantages of Go[^2]. I like writing in Go. The weak point in my Go based projects I thought was limited to porting Go code to JavaScript when I needed it run browser side. I went home and slept well that night. Since then I have been reflecting a little more on the question. 

[^1]: I have presented allot of Go based projects to the SoCal Code4Lib group

[^2]: Deno lets to cross compile TypeScript to binary executables. This, like Go, makes it trivial to develop and deploy. Deno provides tooling that seems inspired by the Go command.

Learning Go isn&#39;t difficult and the learning resources are pretty good. I find Go well suited to the library and archive software domain. I believe typed languages are good for working with structured metadata. Go compiles to a binary and is trivial to deploy. Go has really good tooling making it easier to write better quality code.

My colleagues and I all know Python. Python is our language of collaboration. I&#39;m the only one that knows Go on our team of four. The leap from Python to Go isn&#39;t huge but it is a leap. I made that leap because I needed the features that Go provided at the time[^3]. Since that time the uptake in libraries and archives of coding in Go has been minimal[^4].

[^3]: This was over a decade ago, back when Go was very much a child of Robert Griesemer, Rob Pike and Ken Thompson.

[^4]: After a decade I know only a half dozen or so Go programmers working in the library and archive domain.

My colleagues and I know JavaScript. Most of the people I&#39;ve met through Code4Lib know JavaScript. TypeScript is a superset of JavaScript and Deno can compile it[^5]. The path from JavaScript to TypeScript is less of a leap and more of a stretch. Valid JavaScript is valid TypeScript after all.

[^5]: Deno can also compile TypeScript/JavaScript making your project as easy to deploy as a Go project. TypeScript is a typed programming language so offers similar benefits when working with structured metadata. Deno has tooling inspired by the Go command&#39;s tooling.

Learning Deno command is easy and not a big ask. It is certainly allot easier learning Deno than Git. Git knowledge has grown over the decade that I&#39;ve known the Code4Lib community. I suspect Deno will be easier to adopt. When I take software sustainability into consideration I suspect those projects I write in Deno+TypeScript may out live the ones I&#39;ve written in Go.

In hindsight I don&#39;t think my answer about sharing code between browser and server is the whole criteria. Libraries and archives tend to have a small team to zero software development staff. While Go is a very popular language in 2024 few writing software for libraries and archives know it. Go adoption in our community hasn&#39;t materialized.  In the meantime most of the people I&#39;ve met via Code4Lib know JavaScript. The Go developers I know of in our community also know JavaScript.

TypeScript is a small stretch to pickup if you know JavaScript. TypeScript has good, free, online documentation[^6]. TypeScript is by definition a superset of JavaScript. If your project requires participation by other developers and you choose Deno+TypeScript over Go you have a wider pool of possible helping hands. Deno+TypeScript gives most of the benefits that Go offers today. I think this is compelling for software sustainability. 

[^6]: See [www.typescriptlang.org](https://www.typescriptlang.org/docs/) for TypeScript and [MDN](https://developer.mozilla.org/en-US/docs/Web/JavaScript) for JavaScript.

When would I pick Deno+TypeScript over Go? I now have three criteria questions I ask myself.

- Do I need to share code between browser and server?
  - If yes, maybe this is a Deno+TypeScript project.
- Do I need to largest pool of programmers available to lend a hand?
  - If yes, maybe this is a Deno+TypeScript project.
- Do I want to require Go knowledge to participate in the project?
  - If yes then it might be a Go project.

If my answers are &#34;yes&#34;, &#34;yes&#34;, &#34;no&#34; then the project should proceed with Deno+TypeScript. If I answer the last one is &#34;no&#34; then it shouldn&#39;t be a Go project.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/12/06/when_deno_when_go.md" length="5344" type="text/markdown" />
    </item>    <item>
      <title>Transpiling &#38; Bundling with Emit</title>
      <link>https://rsdoiel.github.io/blog/2024/11/21/transpiling-and-bundling-with-emit.html</link>
      <description>
        <![CDATA[A brief discussion of using the Deno emit module to transpile and bundle
        TypeScript.]]>
      </description>
      <source:markdown># Transpiling &#38; Bundling with Emit

One of the nice features of Deno is native TypeScript support.  One of the selling strength though is that the same source can run both server side and browser side.  The challenge is that TypeScript does not have native TypeScript support. This is easy to remedy using Deno&#39;s [emit](https://jsr.io/@deno/emit) module.

The emit module supports to important functions, `transpile` and `bundle`. Both will render your TypeScript as JavaScript in a browser friendly manner. The `transpile` function turns a single TypeScript file into an equivalent JavaScript file. Bundle can do that with a TypeScript and all the files it imports so you have a self contained JavaScript file with everything you need.

&#60;!-- The emit module website shows how to write a short TypeScript program to transpile and bundle.  When you combine that with a Deno task it is trivial to automatically make that happen. --&#62;


Here&#39;s what my `transpile.ts` looks like.

~~~typescript
import { transpile } from &#34;jsr:@deno/emit&#34;;
import * as path from &#34;jsr:@std/path&#34;;

/* Transpile directory_client.ts to JavaScript and render it to 
   htdocs/js/directory_client.js */
const js_path = path.join(&#34;htdocs&#34;, &#34;js&#34;);
const js_name = path.join(js_path, &#34;directory_client.js&#34;);
const url = new URL(&#34;./directory_client.ts&#34;, import.meta.url);
const result = await transpile(url);
const code = await result.get(url.href);

await Deno.mkdir(js_path, { mode: 0o775, recursive: true });
Deno.writeTextFile(js_name, code);
~~~

You can run that with the following long command line.

~~~shell
deno run --allow-import --allow-env --allow-read --allow-write --allow-net transpile.ts
~~~

Of course you can easily turn this into a [Deno task](https://docs.deno.com/runtime/reference/cli/task_runner/).

If our `directory_client.ts` file contained other modules you can instead use the `bundle` function.  Here&#39;s an example of bundling our `directory_client.ts` saving the result as `htdocs/modules/directory_client.js`.

~~~typescript
/**
 * bundle.ts is an example of &#34;bundling&#34; the type script file directory_client.ts
 * into a module and writing it to htdocs/modules.
 */
import { bundle } from &#34;jsr:@deno/emit&#34;;

const js_path = path.join(&#34;htdocs&#34;, &#34;modules&#34;);
const js_name = path.join(js_path, &#34;directory_client.js&#34;);
const result = await bundle(&#34;./directory_client.ts&#34;);
const { code } = result;
await Deno.mkdir(js_path, { mode: 0o775, recursive: true });
await Deno.writeTextFile(js_name, code);
~~~

You can run that with the following long command line.

~~~shell
deno run --allow-import --allow-env --allow-read --allow-write --allow-net bundle.ts
~~~

The bundle will contain the transpiled TypeScript from `directory_client.ts` but also any modules that `directory_client.ts` relied on. If you don&#39;t want to include the imported modules then you can set the value of `recursive` to false.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/11/21/transpiling-and-bundling-with-emit.md" length="3411" type="text/markdown" />
    </item>    <item>
      <title>Raspberry Pi 4 &#38; 400 Power Supply Issues</title>
      <link>https://rsdoiel.github.io/blog/2024/11/20/power-supply-issues.html</link>
      <description>
        <![CDATA[Quick notes on some low voltage issues I ran into with my Raspberry Pi 4 and 400 using the stock power supply with thumb drives.]]>
      </description>
      <source:markdown># Raspberry Pi 4 &#38; 400 Power Supply Issues

By R. S. Doiel, 2024-11-20

I have a Raspberry Pi 4 and Raspberry Pi 400. The former builds my personal website and my [Antenna](https://rsdoiel.github.io/antenna). I use the latter as a casual portable development box.

Over the last year or two I noticed I would get voltage warnings popping up. Typical this was prevalent when I was compiling Go, Pandoc or Deno. The power supply I was using was the Official Raspberry Pi 4 power supply that came with the Raspberry Pi Desktop Kit and Raspberry Pi 400 Computer Kit.  At first I thought the power supplies were going bad or had become damaged.  I tried replacing the power supply with one of the extras I had picked up (power supplies can and do fail). Still had problems with low voltage.

I did some digging but found nothing useful than the old recommendations of making sure the power supplies were appropriately and to spec. I did read somewhere in my searching that the official power supplies didn&#39;t have allot of headroom. This got me thinking.  The Raspberry Pi OS has been updated many times since I got these devices. Maybe there are more power demands happening than I realized.  I&#39;ve also moved away from using the internal micro SD cards to using an external thumb drive to boot, that would place an additional power requirement on the system.

I looked at [PiShop.us](https://www.pishop.us) which is linked from the Raspberry Pi website as an official retailer.  They had the official supplies that came with my kits but also had one that had a higher wattage rating. I ordered [Raspberry Pi 27W USB-C Power Supply White US](https://www.pishop.us/product/raspberry-pi-27w-usb-c-power-supply-white-us/). Connected it up to my Raspberry Pi 400 via the powered monitor. Booted from the thumb drive and then tried rebuilding Go from Go 1.19 that ships with Raspberry Pi OS compiling Go 1.21.9 and 1.23.3 from source. No voltage errors.  Looks like I just out grew the stock power supply.  If this remains working without problems I will order another one and use it with my old Pi Drive enclosure and my Pi 4. Keeping my fingers crossed.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/11/20/power-supply-issues.md" length="2775" type="text/markdown" />
    </item>    <item>
      <title>Rust tools for Web Work</title>
      <link>https://rsdoiel.github.io/blog/2024/11/06/rust-tools-for-web-work.html</link>
      <description>
        <![CDATA[A quick review of a PageFind and FlatLake by Cloud Cannon. A brief description of how I use them.]]>
      </description>
      <source:markdown># Two Rust command line tools for web work

By R. S. Doiel, 2024-11-06

I&#39;ve noticed that I&#39;m using more tools written in the Rust language. I&#39;d like to highlight two that have impressed me. Both are from [Cloud Cannon](https://cloudcannon.com). They make static website development interesting without unnecessarily increasing complexity.

Historically static site development meant limited interactivity browser side. Then JavaScript arrived. That enabled the possibility of an interactive static site. Two areas remained challenging. First was search. An effective search engine used to required running specialized software like [Solr](https://solr.apache.org) or using a SAAS[^1] search solution.  When the renaissance in static sites happened these options were seen as problematic.

[^1]: SAAS, Software as a Service. This is how the web provides application software and application functionality. SAAS are convenient but often have significant drawbacks in terms of privacy and content ownership. SAAS dates back to pre-Google era. Early search engines like Alta Vista provided search as a service. Today many sites use Google, Bing or DuckDuckGo to provide search as a service.

Statically hosted search arrived when [Oliver Nightingale](https://github.com/olivernn) created [LunrJS](https://lunrjs.com/). LunrJS provides a Solr like search experience run from within your web browser. You needed to write some JavaScript to generate indexes and of course JavaScript had to be available in the browser to run the search engine. In spite of having to write some JavaScript to perform indexing it was easier to setup and manage than Solr. LunrJS added benefit of avoiding running a server. Things were good but there were limitations.  If you wanted to index more than ten thousand or so objects the indexes grew to be quite large. This made search painfully slow. That&#39;s because your web browser needed to download the whole search index before search could run in the browser.  There were variations on Oliver&#39;s approach but none really seemed to solve the problem completely. Still for small websites LunrJS was very good.

Fast forward and a little over a decade ago Cloud Cannon emerged as a company trying to make static site content management easier.  One of the things they created was a tool called [PageFind](https://pagefind.app). Like LunrJS PageFind provides a search engine experience for static websites that runs in the browser.  But it includes a few important improvements. First you don&#39;t need to write a program to build your indexes. PageFind comes with a Rust based tool called, unsurprisingly, &#34;pagefind&#34;. It builds the indexes for you with minimal configuration. The second difference from LunrJS is PageFind builds collection of partial indexes that can be loaded on demand. This means you can index sites with many more than 10K objects and still use a browser side search. That&#39;s huge. I&#39;ve used it on sites with as many as hundred thousand objects. That&#39;s a magnitude difference content that can be searched! A very clever feature of PageFind is that you can combined indexes from multiple sites.  This means the search with my blog can also cover my GitHub project sites that use PageFind for their web documentation. Very helpful.

PageFind does have limitations. It only indexes HTML documents. Unlike Solr it&#39;s not going to easily serve as a search engine for your collection of PDFs. At least without some effort on your part. Like LunrJS it also requires JavaScript to be available in the web browser. So if you&#39;re using RISC OS and a browser like NetSurf or Dillo, you&#39;re out of luck. Still it is a viable solution when you don&#39;t want to run a server and you don&#39;t want to rely on a SAAS solution like Google or Bing.

&#62; Wait! There&#39;s more from Cloud Cannon!

If you start providing JavaScript widgets for your website content you&#39;ll probably miss having a database backed JSON API. You can create one as part of your site rendering process but it is a big chore. Cloud Cannon, in their quest for a static site content management system, provides an Open Source solution for this too. It&#39;s called [FlatLake](https://flatlake.app). Like PageFind it is a command line tool. Instead of analyzing HTML documents FlatLake works on Markdown documents. More specifically Markdown documents with YAML front matter[^2]. It uses that to render a read only JSON API from the metadata in your documents front matter.  You define which attributes you want to expose in your API in a YAML file. FlatLake creates the necessary directory structure and JSON documents to reflect that. Want to create a tag cloud? Your JSON API can provided the data for that. You want to provide alternate views into your website such as indexes or article series views?  Again you now have a JSON API that aggregates your metadata to render that. Beyond some small amount of configuration FlatLake does the heavy lifting of generating and managing the JSON API for your site. It&#39;s consistent and predictable. Start a new site and add FlatLake and you get a familiar JSON API out of the box.

[^2]: Front matter is a block of text at the top of your Markdown document usually expressed in YAML. Site building tools like [Pandoc](https://pandoc.org/chunkedhtml-demo/8.10-metadata-blocks.html#extension-yaml_metadata_block) can use the YAML block to populate and control templating. Similarly R-Markdown provides a similar functionality. FlatLake takes advantage of that. 


## PageFind and FlatLake in action

My personal website is indexed with PageFind.  The indexes are located at &#60;https://rsdoiel.github.io/pagefind&#62;. The search page is at &#60;https://rsdoiel.github.io/search.html&#62;. I index my content with the following command (which will run on macOS, Windows or Linux).

~~~shell
pagefind --verbose --exclude-selectors=&#34;nav,header,footer&#34; --output-path ./pagefind --site .
~~~

That command index all the HTML content excluding nav, header and footers.  The JavaScript in my [search.html](https://rsdoiel.github.io/search.html) page looks like this.

~~~JavaScript
let pse = new PagefindUI({
    element: &#34;#search&#34;,
    showSubResults: true,
    highlightParam: &#34;highlight&#34;,
    mergeIndex: [{
        bundlePath: &#34;https://rsdoiel.github.io/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/shorthand/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/pttk/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/skimmer/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/scripttools/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/fountain/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/osf/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/fdx/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/stngo/pagefind&#34;,
        bundlePath: &#34;https://rsdoiel.github.io/opml/pagefind&#34;
    }]
})
window.addEventListener(&#39;DOMContentLoaded&#39;, (event) =&#62; {
    let page_url = new URL(window.location.href),
        query_string = page_url.searchParams.get(&#39;q&#39;);
    if (query_string !== null) {
        console.log(&#39;Query string: &#39; + query_string);
        pse.triggerSearch(query_string);
    }
});
~~~

This supports searching content in some of my GitHub project sites as well as my blog.

One of the things I am working on is updating how I render my website.  I have a home grown tool called [pttk](https://rsdoiel.github.io/pttk &#34;Plain Text Toolkit&#34;) that includes a &#34;blogit&#34; feature. Blog it takes cares care of adding Markdown documents to my blog and generates a a JSON document that contains metadata from the Markdown documents.  That later feature is no longer needed with the arrival of FlatLake. FlatLake also has the advantage that I can define other metadata collections to include in my JSON API. The next incarnation of pttk will be simpler as the JSON API will be provided by FlatLake.

FlatLake analyzes the Markdown documents in my website and build out a JSON API as folders and JSON documents. If I do this as the first step in rendering my site the rendering process can take advantage of that. It replace my &#34;blog.json&#34; file. It can even replace the directory traversal I previously needed to use with building the site. That&#39;s because I can take advantage of the exposed metadata in a highly consistent way. You can explore the JSON API being generated at &#60;https://rsdoiel.github.io/api&#62;. I haven&#39;t yet landed on my final API organization but when I do I&#39;ll be able to trim the code for producing my website significantly. Here&#39;s the outline I expect to follow.

1. Run FlatLake on the Markdown content and update the JSON API content
2. Read the JSON API and render my site
3. Run PageFind and update my site indexes
4. Deploy via GitHub Pages with `git` or `gh`

## Installing PageFind and FlatLake

Both PageFind and FlatLake are written in Rust. If you have Rust installed on your machine then Cargo will do all your lifting.  When I have a new machine I install [Rust](https://www.rust-lang.org/) with [Rustup](https://rustup.rs). On an running Machine I run `rustup upgrade` to get the latest Rust.  I then install (or updated) PageFind and FlatLake with Cargo.

~~~shell
rustup upgrade
cargo install pagefind
cargo install flatlake
~~~

I&#39;ve run PageFind on macOS, Windows, Linux. On ARM 64 and Intel CPUs. I even run it on Raspberry Pi!. Rust supports all these platforms so where Rust runs PageFind and FlatLake can follow.

PageFind solves my search needs.  FlatLake is simplifying the tooling I use to generate my website. My plain text toolkit (pttk) needs to do much less. It feels close to the grail of static website management system built from simple precise tools. Git hands version control. Pandoc renders the Markdown to HTML. PageFind provides search and FlatLake provides the next generation JSON API. A Makefile or Deno task can knit things together into a publication system.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/11/06/rust-tools-for-web-work.md" length="10495" type="text/markdown" />
    </item>    <item>
      <title>Limit and offset for row pruning</title>
      <link>https://rsdoiel.github.io/blog/2024/10/31/limit_and_offset_for_row_pruning.html</link>
      <description>
        <![CDATA[Noted are how to combine a select statement with limit and offset clauses with a delete statement to prune rows.]]>
      </description>
      <source:markdown># Limit and offset for row pruning

By R. S. Doiel, 2024-10-31

I recently needed to prune data that tracked report requests and their processing status. The SQLite3 database table is called
&#34;reports&#34; and has four columns.

- `id` (uuid)
- `created` (request date stamp)
- `updated` (status updated date stamp)
- `src` (a JSON column with the status details)

The problem is the generated report can be requested as needed. I wanted to maintain the request data for the most recent one. The &#34;src&#34; column has the report name and status. That is easily checked using the JSON notation supported by SQLite3 (v3.47.0). It&#39;s easy to get the most recent completed row with a simple SELECT statement using both an ORDER clause and LIMIT clause.

~~~sql
select id
  from reports
  where src-&#62;&#62;&#39;report_name&#39; = &#39;myreport&#39; and
        src-&#62;&#62;&#39;status&#39; = &#39;completed&#39;
  order by updated desc
  limit 1
~~~

This gives me the key to the most recent record.  How do I get a list of he rows I want to prune?  The answer is to use the LIMIT cause with an OFFSET
modifier. The OFFSET let&#39;s us skip a certain number of rows before applying the limit.  In this case I want to skip one row and show the rest. This database table doesn&#39;t get that big so I can use a limit like one thousand. Here&#39;s what that looks like.

~~~sql
select id
  from reports
  where src-&#62;&#62;&#39;report_name&#39; = &#39;myreport&#39; and
        src-&#62;&#62;&#39;status&#39; = &#39;completed&#39;
  order by updated desc
  limit 1000 offset 1
~~~

Now that I have my list of ids I can combine it with a DELETE statement which has a WHERE clause. The WHERE clause will use the IN operator to iterate over the list of ids from my select statement.

Putting it all together it looks like this.

~~~sql
delete from reports
  where id in (
    select id
      from reports
      where src-&#62;&#62;&#39;report_name&#39; = &#39;myreport&#39; and
            src-&#62;&#62;&#39;status&#39; = &#39;completed&#39;
      order by updated desc
      limit 1000 offset 1
)
~~~

The nice thing is I can run this regularly. It will never delete the most recent row because the offset value is one.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/10/31/limit_and_offset_for_row_pruning.md" length="2621" type="text/markdown" />
    </item>    <item>
      <title>SQLite3 json_patch is a jewel</title>
      <link>https://rsdoiel.github.io/blog/2024/10/31/sqlite3_json_patch.html</link>
      <description>
        <![CDATA[Quick note about json_path function in SQLite3]]>
      </description>
      <source:markdown># SQLite3 json_patch is a jewel

By R. S. Doiel, 2024-10-31

If youre working with an SQLite3 database table and have JSON or columns you need to merge with other columns then the `json_path` function comes in really handy.
I have a SQLite3 database table with four columns.

- _key (string)
- src (json)
- created (datestamp)
- updated (datestamp)

Occasionally I want to return the `_key`, `created` and `updated` columns as part of the JSON held in the `src` column.  In SQLite3 it is almost trivial.

~~~sql
select 
  json_patch(json_object(&#39;key&#39;, _key, &#39;updated&#39;, updated, &#39;created&#39;, created), src) as object
  from data;
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/10/31/sqlite3_json_patch.md" length="1114" type="text/markdown" />
    </item>    <item>
      <title>Quick tour of Deno 2.0.2</title>
      <link>https://rsdoiel.github.io/blog/2024/10/18/a-quick-tour-of-deno-2.html</link>
      <description>
        <![CDATA[A quick tour of Deno 2 and the features I enjoy. Deno includes thoughtful tooling, good language support,
        ECMAScript module support and a good standard library. Deno has the advantage of being able to cross compile
        TypeScript to an executable which makes deployment of web services as easy for me as it is with Go.]]>
      </description>
      <source:markdown># Quick tour of Deno 2.0.2

By R. S. Doiel

I&#39;ve been working with TypeScript this year using Deno. Deno has reached version 2.0. It has proven to be a nice platform for projects. Deno includes thoughtful tooling, good language support, ECMAScript module support and a [good standard library](https://jsr.io/@std).  As a TypeScript and JavaScript platform I find it much more stable and compelling than NodeJS. Deno has the advantage of being able to cross compile TypeScript to an executable which makes deployment of web services as easy for me as it is with Go.

## Easy install with Cargo

Deno is written in Rust. I like installing Deno via Rust&#39;s Cargo. You can installed Rust via [Rustup](https://rustup.rs). When I install Deno on a new machine I first check to make sure my Rust is the latest then I use Cargo to install Deno.

~~~shell
rustup update
cargo install deno
~~~

## Easy Deno upgrade

Deno is in active development. You&#39;ll want to run the latest releases.  That&#39;s easy using Deno. It has a self upgrade option.

~~~shell
deno upgrade
~~~

## Exploring TypeScript

When I started using Deno this year I wasn&#39;t familiar with TypeScript. Unlike NodeJS Deno can run TypeScript natively. Why write in TypeScript? TypeScript is a superset of JavaScript. That means if you know JavaScript you know most of TypeScript already. Where TypeScript differs is in the support for type safety and other modern language features. Writing TypeScript for Deno is a joy because it supports the web standard ECMAScript Models. That means the code I develop to run server side can be easily targetted to work in modern browsers too. TypeScript began life as a transpiled language targeting JavaScript. With Deno&#39;s emit module I can easily transpile my TypeScript to JavaScript. No more messying about with NodeJS and npm.

## Exploring Deno

As a learning platform I find Deno very refreshing. Deno provides a [REPL](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop). That means you can easily try out TypeScript interactively. Deno is smart about when it runs &#34;programs&#34; versus running as a REPL. This is an improvement over NodeJS.

Deno, like your web browser, runs TypeScript and JavaScript in a sand boxed environment. The REPL gives you full access to your machine but running programs via Deno requires you to give explicit permissions to resources like reading from your file system, accessing your environment, accessing the network or importing models from remote systems. This might sound tedious but Deno makes it easy in practice.

Deno projects use a `deno.json` file for initialization. Creating the file is as easy as typing `deno init` in your project directory. Here&#39;s an example of setting up a `happy_deno` project.

~~~shell
mkdir happy_deno
cd happy_deno
deno init
~~~

If you list your directory you will see a `deno.json` file (Windows Powershell also supports &#34;ls&#34; to list directories).

~~~shell
ls 
~~~

The init action created the following files.

`deno.json`
: The project configuration for Deno. It includes default tasks and module imports.

`main.ts`
: This is the &#34;main&#34; program for your project. It&#39;s where you&#39;ll add your TypeScript code.

`main_test.ts`
: This is a test program so you can test the code you&#39;ve written in your &#34;main&#34; module.

The task action by itself will list currently defined tasks, e.g. `deno task` (the &#34;dev&#34; task
was defined by the init action).

~~~shell
Available tasks:
- dev
    deno run --watch main.ts
~~~

Looking at the `deno.json` file directly we see.

~~~json
{
  &#34;tasks&#34;: {
    &#34;dev&#34;: &#34;deno run --watch main.ts&#34;
  }
}
~~~

What does that do? The &#34;dev&#34; task will start deno using the &#34;run&#34; action passing it the &#34;watch&#34; option when running the file &#34;main.ts&#34;. What does mean? The &#34;watch&#34; option will notice of the &#34;main.ts&#34; file changes on disk. It it changes it will re-run the &#34;main.ts&#34; program.  Save a change to &#34;main.ts&#34; in your editor deno and automagically it runs &#34;main.ts&#34; again. The really helps when you are write web services, the service automatically restarts.

Here&#39;s an example of the output of running the &#34;dev&#34; task with the command `deno task dev`.

~~~
Task dev deno run --watch main.ts
Watcher Process started.
Add 2 + 3 = 5
Watcher Process finished. Restarting on file change...
~~~

Using your editor, add a &#34;hello world&#34; log message to the code in &#34;main.ts&#34; so it looks like this.

~~~typescript
export function add(a: number, b: number): number {
  return a + b;
}

// Learn more at https://docs.deno.com/runtime/manual/examples/module_metadata#concepts
if (import.meta.main) {
  console.log(&#34;Add 2 + 3 =&#34;, add(2, 3));
  console.log(&#34;Hello World!&#34;);
}
~~~

Save your program and look what happens.

~~~
Watcher File change detected! Restarting!
Add 2 + 3 = 5
Hello World!
Watcher Process finished. Restarting on file change...
~~~

Adding additional tasks is just a matter of editing the `deno.json` file and adding them to the `tasks` attributes.

See [deno task](https://docs.deno.com/runtime/reference/cli/task_runner/) documentation for details.

### Modules in Deno

TypeScript and JavaScript support &#34;modules&#34;. Specifically Deno supports [ES](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules) modules. The nice thing about this is ES modules can be used with the same import export syntax in your web browser supports. Deno supports local modules and remote modules accessed via URL just like your browser. At work I have our project documentation hosted on GitHub. I can write a TypeScript modules there too. I can then import them into a another project just by using the URL.

Why is the significant? I don&#39;t need to rely on an external system like [npm](https://npmjs.com) for module repositories. All I need is a simple static website. Modules in the Deno community often use &#60;https://jsr.io/&#62; as a common module registery. This includes Deno&#39;s standard library modules.  Let&#39;s add the standard &#34;fs&#34; and &#34;path&#34; module to our happy deno project. Use Deno&#39;s &#34;add&#34; action.

~~~shell
deno add jsr:@std/fs
deno add jsr:@std/path
~~~

If you look at the `deno.json` now it should look something like this.

~~~json
{
  &#34;tasks&#34;: {
    &#34;dev&#34;: &#34;deno run --watch main.ts&#34;
  },
  &#34;imports&#34;: {
    &#34;@std/assert&#34;: &#34;jsr:@std/assert@1&#34;,
    &#34;@std/fs&#34;: &#34;jsr:@std/fs@^1.0.4&#34;,
    &#34;@std/path&#34;: &#34;jsr:@std/path@^1.0.6&#34;
  }
}
~~~

To quit my deno dev task I can press the control key and the &#34;c&#34; key (a.k.a. Ctrl-C) in my terminal window. 

I mentioned Deno runs programs in a sand box. That is because Deno tries to be secure by default. You must explicitly allow Deno to reach outside the sand box. One resource outside the sand box is the file system. If you use our remote modules we need to give Deno permission to do that too. See [security and permissions](https://docs.deno.com/runtime/fundamentals/security/) on Deno&#39;s documentation website for more details.

To allow reading files on the local file system with the &#34;dev&#34; task I would modify the &#34;dev&#34; command to look like.

~~~
    &#34;dev&#34;: &#34;deno run --allow-read --watch main.ts&#34;
~~~

You can include multiple permissions by adding the related &#34;allow&#34; option (E.g. `--allow-import`, `--allow-env`, `--allow-net`). It is important to realize that importing a moddel doesn&#39;t give you permission, you need to explicitly allow Deno to do that. When you compile a program the permissions you allow will also be allowed in the compiled version.

### An exercise for the reader

Create a TypeScript file called [show_deno_json.ts](show_deno_json.ts). Read in and display the contents of the &#34;deno.json&#34; file in the same directory.

Here&#39;s so links to documentation that may be helpful in finishing the exercise.

- [reading files](https://docs.deno.com/examples/reading-files/)

Additional reading.

- [fundamentals](https://docs.deno.com/runtime/fundamentals/)
- [file system access](https://docs.deno.com/runtime/fundamentals/security/#file-system-access)
- [standard modules](https://docs.deno.com/runtime/fundamentals/standard_library/)
- [modules](https://docs.deno.com/runtime/fundamentals/modules/)
- [deno.json](https://docs.deno.com/runtime/fundamentals/configuration/)
- [security](https://docs.deno.com/runtime/fundamentals/security/)

## Compiling TypeScript to executable code

One of the really nice things about Deno + TypeScript is that your development experience can be interactive like interpretive languages (e.g. Python, Lisp) and as convenient to deploy as a [Go](https://golang.org) executable. You can compile our &#34;main.ts&#34; file with the following command.

~~~
deno compile --allow-read main.ts
~~~

Listing my directory in our project I see the following.

~~~shell
deno.json    deno.lock    happy_deno   main.ts      main_test.ts
~~~

~~~
./happy_deno
~~~

NOTE: On a Windows the compiled program is named `happy_deno.exe`, to execute it I would type `.\happy_deno.exe` in your Powershell session.

By default Deno uses the project directory for the executable name. You can explicitly set the executable name with a [command line option](https://docs.deno.com/runtime/getting_started/command_line_interface/). You can also use command line options with the compile action to [cross compile](https://en.wikipedia.org/wiki/Cross_compiler) your executable similar to how it is done with Go.

Why compile your program?  Well it runs slightly fast but more importantly you can now copy the executable to another machine and run it even if Deno isn&#39;t installed. This means you no longer have the version dependency problems I typically experience with deploying code from Python and NodeJS projects.  Like Go the Deno compiler is a cross compiler. That means I can compile versions for macOS, Windows and Linux on one machine then copy the platform specific executable to the machines where they are needed. Deno&#39;s compiler provides similar advantages to Go.

## TypeScript to JavaScript with Deno

JavaScript is a first class language in modern web browsers but TypeScript is not.  When TypeScript was invented it was positioned as a [transpiled](https://en.wikipedia.org/wiki/Source-to-source_compiler) language. Deno is a first class TypeScript environment but how do I get my TypeScript transpiled to JavaScript?  Deno provides an [emit](https://jsr.io/@deno/emit) module for that. With a five lines of TypeScript I can write a bundler to convert my TypeScript to JavaScript. I can even add running that as a task to my `deno.json` file. Here&#39;s an example of &#34;main_to_js.ts&#34;.

~~~typescript
import { bundle } from &#34;jsr:@deno/emit&#34;;
const url = new URL(&#34;./main.ts&#34;, import.meta.url);
const result = await bundle(url);
const { code } = result;
console.log(code);
~~~

The command I use to run `main_to_js.ts` is

~~~shell
deno run --allow-read --allow-env main_to_js.ts
~~~

My `deno.json` file will look like this with a &#34;transpile&#34; task.

~~~json
{
  &#34;tasks&#34;: {
    &#34;dev&#34;: &#34;deno run --allow-read --watch main.ts&#34;,
    &#34;transpile&#34;: &#34;deno run --allow-read --allow-env main_to_js.ts&#34;
  },
  &#34;imports&#34;: {
    &#34;@std/assert&#34;: &#34;jsr:@std/assert@1&#34;,
    &#34;@std/fs&#34;: &#34;jsr:@std/fs@^1.0.4&#34;,
    &#34;@std/path&#34;: &#34;jsr:@std/path@^1.0.6&#34;
  }
}
~~~

Now when I want to see the `main.ts` in JavaScript I can do `deno task transpile`.

## Contrasting Deno + TypeScript with Go and Python

For me working in Go has been a pleasure in large part because of its tooling. The &#34;go&#34; command comes with module management, code formatter, linting, testing and cross compiler right out of the box. I like a garbage collected language. I like type safety. I like the ease which you can work with structured data. I&#39;ve enjoyed programming with the excellent Go standard library while having the option to include third party modules if needed.

Deno with TypeScript gives me most of what I like about Go out of the box. The `deno` command includes a task runner, module manager, testing, linting (aka check), cross compiler and formatter out of the box. TypeScript interfaces provide a similar experience to working with `struct` types in Go. Unlike Go you can work with Deno interactively similar to using the REPL in Python, Lisp or your favorite SQL client. I like the ES module experience of Deno better than Go&#39;s module experience.

What makes Deno + TypeScript compelling over writing web services over Python is Deno&#39;s cross compiler.  Like Go I can compile executables for macOS, Windows and Linux on one box and target x86_64 and ARM 64 CPUs.No more need to manage virtual environments and no more sorting out things when virtual environments inevitably get crossed up. Copying an executable to the production machines is so much easier.  Many deployments boil down to an `scp` and restarting the services on the report machines. Example `scp myservice apps.example.edu:/serivces/bin/; ssh apps.example.edu &#34;systemctl restart myservice&#34;`.  It also means curl installs are trivial. All you need is an SH or Powershell script that can download a zip file, unpack it and copy it into the search path of the host system. Again the single self contained executable is a huge simplifier.

One feature I miss in Deno + TypeScript is the DSL in Go content strings embedded in struct type definitions. This makes it trivial to write converts for XML, JSON and YAML.  Allot of code in libraries and archives involves structured metadata and that feature ensures the structures definition are consistent between formats. I think adding to/from methods will become a chore at some point.

If you are working in Data Science domain I think Python still has the compelling code ecosystem. It works, it mature and there is lots of documentation and community out there. While you can run Deno from a [Jupyter notebook](https://docs.deno.com/runtime/reference/cli/jupyter/) I think it&#39;ll take a while for TypeScript/JavaScript to reach parity with Python for this application domain.

Switching from Go to Deno/TypeScript has been largely a matter of getting familiar with Deno, the standard library and remembering JavaScript while adding the TypeScript&#39;s type annotations. I&#39;ve also had to learn TypeScript&#39;s approach to type conversions though that feels similar to Go. If I need the same functional code server side and browser side I think the Deno + TypeScript story can be compelling.

Python, Rust, Go and Deno + TypeScript all support creating and running WASM modules.  Of those languages Rust has the best story and most complete experience. Deno runs a close second. Largely because it is written in Rust so what you learn about WASM in rust carries over nicely. The Python story is better than Go at this time. This is largely a result of how garbage collection is integrated into Go.  If I write a Go WASM module there is a penalty paid when you move between the Go runtime space and the hosts WASM runtime space. This will improve over time but it isn&#39;t something I&#39;ve felt comfortable using in my day to day Go work (October 2024, Go v1.23.2).

Deno makes TypeScript is a serious application language. I suspect more work projects to be implemented in TypeScript where shared server and browser code is needed. I has be useful exploring Deno and TypeScript.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/10/18/a-quick-tour-of-deno-2.md" length="15960" type="text/markdown" />
    </item>    <item>
      <title>Web GUI and Deno</title>
      <link>https://rsdoiel.github.io/blog/2024/07/08/webgui_and_deno.html</link>
      <description>
        <![CDATA[My notes on two Web GUI modules available for Deno.]]>
      </description>
      <source:markdown># Web GUI and Deno

By R. S. Doiel, 2024-07-08

I&#39;ve been looking at various approaches to implement graphical interfaces for both Deno and other languages.  I had been looking primarily at webview bindings but then stumbled on [webui](https://webui.me). Both could be a viable way to implement a local first human user interface.

Here&#39;s an example of the webview implementation of hello world.

~~~typescript
import { Webview } from &#34;@webview/webview&#34;;

const html = `
&#60;html&#62;
  &#60;head&#62;&#60;/head&#62;
  &#60;body&#62;
    &#60;h1&#62;Hello from deno v${Deno.version.deno}&#60;/h1&#62;
    &#60;script&#62;console.log(&#34;Hi There!&#34;);&#60;/script&#62;
  &#60;/body&#62;
&#60;/html&#62;
`;

const webview = new Webview();

webview.navigate(`data:text/html,${encodeURIComponent(html)}`);
webview.run();
~~~

Now here is a functionally equivalent version implemented using webui.

~~~typescript
import { WebUI } from &#34;https://deno.land/x/webui/mod.ts&#34;;

const myWindow = new WebUI();

myWindow.show(`
&#60;html&#62;
  &#60;head&#62;&#60;script src=&#34;webui.js&#34;&#62;&#60;/script&#62;&#60;/head&#62;
  &#60;body&#62;
    &#60;h1&#62;Hello from deno v${Deno.version.deno}&#60;/h1&#62;
    &#60;script&#62;console.log(&#34;Hi There!&#34;);&#60;/script&#62;
    &#60;/body&#62;
&#60;/html&#62;`);

await WebUI.wait();
~~~

Let&#39;s call these [thing1.ts](thing1.ts) and [thing2.ts](thing2.ts).  To run thing1 I need a little prep since I&#39;ve used an `@` import. The command I need to map the `webview/webview` module is the `deno add` command.

~~~shell
deno add @webview/webview
~~~

Here&#39;s how I check and run thing1.

~~~shell
deno check thing1.ts
deno run -Ar --unstable-ffi thing1.ts
~~~

Since I didn&#39;t use an `@` import in the webui version I don&#39;t need to &#34;add&#34; it to Deno. I check and run thing2 similar to thing1.

~~~shell
deno check thing2.ts
deno run -Ar --unstable-ffi thing2.ts
~~~

Both will launch a window with our hello world message. Conceptually the code is similar but the details differ.  In the case of webview you are binding the interaction from the webview browser implementation. You populate your &#34;page&#34; using a data URL call (see `webview.navigate()`. Webview is a minimal web browser. It is similar to but not the same as evergreen web browsers like Firefox, Chrome, or Edge. Depending how var you want to push your CSS, JavaScript and HTML this may or may not be a problem.

Webui uses a lighter weight approach. It focuses on a web socket connection between your running code and the user interface. It leaves the browser implementation to your installed browser (e.g. Chrome, Edge or Firefox). There is a difference in how I need to markup the HTML compared to the webview version. In the webui version I have a script element in the head. It loads &#34;webui.js&#34;. This script is supplied by webui C level code. It &#34;dials home&#34; to connect your program code with the web browser handling the display. Webui at the C library level is functioning as a web socket server.

Conceptually I like the webui approach. My program code is a &#34;service&#34;, webui manages the web socket layer and the web browser runs the UI. Web browsers are complex. In the web UI approach my application&#39;s binary isn&#39;t implementing one. In the webview approach I&#39;m embedding one. Feels heavy. At a practical level of writing TypeScript it may not make much differences. When I compiled both thing1 and thing2 to binaries thing2 was approximately 1M smaller. Is that difference important? Not really sure.

What about using webview or webui from other languages? Webview has been around a while. There are many bindings for the C++ code of webview and other languages.  Webui currently supports Rust, Go, Python, TypeScript/JavaScript (via Deno), Pascal as well as a few exotic ones. TypeScript was easy to use either. I haven&#39;t tried either out with Python or Go. I&#39;ll leave that for another day.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/07/08/webgui_and_deno.md" length="4156" type="text/markdown" />
    </item>    <item>
      <title>Transpiling with Deno</title>
      <link>https://rsdoiel.github.io/blog/2024/07/03/transpiling_with_deno.html</link>
      <description>
        <![CDATA[[Deno](https://deno.land) is a fun environment to work in for learning TypeScript.  As I have become comfortable writing server side TypeScript code I know I want to also be able to use some modules in JavaScript form browser side. The question is then how to you go from TypeScript to JavaScript easily with getting involved with a bunch-o-npm packages?  Turns the solution in deno is to use the [deno_emit](https://github.com/denoland/deno_emit/blob/main/js/README.md) module.  Let's say I have a TypeScript module called `hithere.ts`. I want to make it available as JavaScript so I can run it in a web browser. How do I use the `deno_emit` module to accomplish that?
        
        - Write a short TypeScript program
          - include the transpiler module provided with emit
          - use the transpiler to generate the JavaScript code
          - output the JavaScript code
        
        Here's what `transpile.ts` might look like ...]]>
      </description>
      <source:markdown># Transpiling with Deno

[Deno](https://deno.land) is a fun environment to work in for learning TypeScript.  As I have become comfortable writing server side TypeScript code I know I want to also be able to use some modules in JavaScript form browser side. The question is then how to you go from TypeScript to JavaScript easily with getting involved with a bunch-o-npm packages?  Turns the solution in deno is to use the [deno_emit](https://github.com/denoland/deno_emit/blob/main/js/README.md) module.  Let&#39;s say I have a TypeScript module called `hithere.ts`. I want to make it available as JavaScript so I can run it in a web browser. How do I use the `deno_emit` module to accomplish that?

- Write a short TypeScript program
  - include the transpiler module provided with emit
  - use the transpiler to generate the JavaScript code
  - output the JavaScript code

Here&#39;s what `transpile.ts` might look like:

~~~typescript
/* Get the transpiler module from deno&#39;s emit */
import { transpile } from &#34;https://deno.land/x/emit/mod.ts&#34;;

/* Get the python to my CL.ts as a URL */
const url = new URL(&#34;./hithere.ts&#34;, import.meta.url);
/* Transpile the code returning a result */
const result = await transpile(url);

/* Get the resulting code and write it to standard out */
const code = result.get(url.href);
console.log(code);
~~~

Here&#39;s the `hithere.ts` module:

~~~typescript
/**
 * hithere takes a name and returns a string of &#34;hi there &#34;, a name and &#34;!&#34;. If the name is null
 * it returns &#34;Hello World!&#34;.
 *
 * @param {string | null} name
 * @returns {string}
 */
function hithere(name: string | null): string {
	if (name === null) {
		return &#34;Hello World!&#34;;
	}
	return `hi there ${name}!`;
}
~~~

To compile the module I need to give transpile.ts some permissions.

- --allow-read (so I can read my local module
- --allow-env (the transpiler needs the environment)
- --allow-net (the deno emit module is not hosted locally)

The command line could look like this.

~~~shell
deno run --allow-read --allow-env --allow-net \
  transpile.ts
~~~

The result is JavaScript. It still has my comments in the code but doesn&#39;t have the TypeScript specific
annotations.

~~~javascript
/**
 * hithere takes a name and returns a string of &#34;hi there &#34;, a name and &#34;!&#34;. If the name is null
 * it returns &#34;Hello World!&#34;.
 *
 * @param {string | null} name
 * @returns {string}
 */ function hithere(name) {
  if (name === null) {
    return &#34;Hello World!&#34;;
  }
  return `hi there ${name}!`;
}
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/07/03/transpiling_with_deno.md" length="3836" type="text/markdown" />
    </item>    <item>
      <title>Bootstrapping a Text Oriented Web</title>
      <link>https://rsdoiel.github.io/blog/2024/06/14/tow_bootstraping.html</link>
      <description>
        <![CDATA[First order of business is to shorten "text oriented web" to TOW.  It's easier to type and say.  I'm considering the bootstrapping process from three vantage points. 
        
        1. content author
        2. the server software
        3. client software 
        
        The TOW approach is avoids invention in favor of reuse. HTTP protocol is well specified and proven. [Common Mark](https://commonmark.org) has a specification as does [YAML](https://yaml.org/). TOW documents are UTF-8 encoded. A TOW document is a composite of Common Mark with YAML blocks. TOW documents combined with HTTP provide a simplified hypertext platform. 
        
        ...]]>
      </description>
      <source:markdown># Bootstrapping a Text Oriented Web

By R. S. Doiel, 2024-06-14

First order of business is to shorten &#34;text oriented web&#34; to TOW.  It&#39;s easier to type and say.  I&#39;m considering the bootstrapping process from three vantage points. 

1. content author
2. the server software
3. client software 

The TOW approach is avoids invention in favor of reuse. HTTP protocol is well specified and proven. [Common Mark](https://commonmark.org) has a specification as does [YAML](https://yaml.org/). TOW documents are UTF-8 encoded. A TOW document is a composite of Common Mark with YAML blocks. TOW documents combined with HTTP provide a simplified hypertext platform. 


## Authoring TOW documents

TOW seeks to simplify the content author experience. TOW removes most of the complexity of content management systems rendering processes. A TOW document only needs to be place in a directory supported by a TOW server. In that way it is as simple as [Gopher](https://en.wikipedia.org/wiki/Gopher_(protocol)). The content author should only need to know [Markdown](https://en.wikipedia.org/wiki/Markdown), specifically the [Common Markdown](https://commonmark.org/) syntax. If they want to create interactive documents or distribute metadata about their documents they will need to be comfortable creating and managing YAML blocks embedded in their Common Mark document. Use of YAML blocks is already a common practice in the Markdown community.

Describing content forms using YAML has several advantages. First it is much easier to read than HTML source. YAML blocks are not typically rendered by Markdown processor libraries. I can write a simple preprocessor which tenders the YAML content form as HTML. Since HTML is allowed in Markdown documents these could then be run through a standard Markdown to HTML converter.  In the specific case of Pandoc a filter could be written to perform the pre-processor step. It should be possible to always render a TOW document as an HTML5 document. This is deliberate, it should be possible to use the TOW documents to extrapolate a traditional website.

## Server and client software

TOW piggy backs on the HTTP protocol. A TOW document is a composite of Common Mark with embedded YAML blocks when needed. It differs from the existing WWW content only in its insistence that Common Mark and YAML be first class citizens forming a viable representation of a hypertext document. A TOW document URL looks the same as a WWW URL. The way TOW documents distinguish themselves from ordinary web content is via their content type, &#34;text/tow&#34; or &#34;text/x-tow&#34;.  Content forms are sent to a TOW service using the content type &#34;application/yaml&#34; content type instead of the various urlencoded content types used by WWW forms. 

TOW browsers eschew client side programming. I have several reasons for specifying this. First the TOW concept is a response to current problems and practices in the WWW. I don&#39;t want to contribute to the surveillance economy. It also means that&#39;s what the client receives avoids on vector if hijacking that the WWW has battled over the years. Importantly this also keeps the TOW browser model very simple. The TOW browser renders TOW content once per load. TOW is following the path that [Gemini protocol](https://geminiprotocol.net/) and [Gemtext](https://hexdocs.pm/gemtext/Gemtext.html). Unlike Gemini it does not require a new protocol and leverages an existing markup. Like Gemini TOW is not replacing anything but only supplying an alternative.

My vision for implementing TOW is to use existing HTTP protocol. That means a TOW URL looks just like a WWW URL. How do I distinguish between WWW and TOW?  HTTP protocol supports headers. TOW native interaction should use the content type &#34;text/tow&#34; or &#34;text/x-tow&#34;. Content forms submitted to a TOW native server should submit their content encoded as YAML and use the content type &#34;text/tow&#34; or &#34;text/x-tow&#34;. This lets the server know that the reply should remain in &#34;text/tow&#34; or &#34;text/x-tow&#34;.  A TOW enabled browser can be described as a browser that knows how to render TOW documents and submit YAML responses.

## How to proceed?

A TOW document needs to be render-able as HTML+CSS+JavaScript because that is what is available today to bootstrap TOW. The simplest TOW server just needs to be able to send TOW content to a requester with the correct content type header, e.g. &#34;text/tow&#34;.  That means a server can be easily built in Go using the standard [net/http](https://gopkg.in/net/html) package. That same package could then be combined with a web server package to adapt it into a TOW server supporting translation to HTML+CSS+JavaScript during the bootstrap period.  If the TOW web server received a request where &#34;text/tow&#34; wasn&#39;t in the acceptable response list then it would return the TOW document translated to HTML+CSS+JavaScript.

A TOW native browser could be built initially as a [PWA](https://en.wikipedia.org/wiki/Progressive_web_app). It just needs to render TOW native documents as HTML5+CSS+JavaScript and be able to send TOW content forms back as YAML using the &#34;text/tow&#34; content type. Other client approaches could be taken, e.g. write plugin for the [Dillo browser](https://dillo-browser.github.io/), build something on [Gecko](https://developer.mozilla.org/en-US/docs/Glossary/Gecko), build something on [WebKit](https://webkit.org/), or use [Electron](https://www.electronjs.org/). A PWA is probably good enough for proof of concept.

A minimal TOW proof of concept would be the web service that can handle the translation of TOW documents to HTML+CSS+JavaScript. A complete proof of concept could be implemented TOW native support via [PWA](https://en.wikipedia.org/wiki/Progressive_web_app). 

1. tow2html5
2. towtruck (built using tow2html5)
3. towby (initially built as tow2html5 WASM module as PWA)

## Proposed programs

tow2html5
: This can be implemented in Go as both a package and command line interface. The command line interface could function either in preprocessor mode (just translating the YAML forms into HTML5) or as a full processor using an existing Common Mark package. It could also be compiled to a WASM module to support implementing a TOW browser as PWA.

towtruck
: This would be a simple web service that performed tow2html5 translation for tow document requests from non-TOW native browsers. If the accepted content type requested includes TOW native then it&#39;d just hand back the TOW file untranslated. I would implemented this as a simple static HTTP web service running on localhost then use Lighttpd, Apache 2 or NginX for a front end web server. This simplifies the TOW native server.

towby
: A [PWA](https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps) based TOW browser proof of concept</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/06/14/tow_bootstraping.md" length="7826" type="text/markdown" />
    </item>    <item>
      <title>RISC OS 5.30, GCC 4.7 and Hello World</title>
      <link>https://rsdoiel.github.io/blog/2024/06/08/riscos_gcc_and_hello.html</link>
      <description>
        <![CDATA[These are my notes on learning to program a Raspberry Pi Zero W
        under RISC OS using GCC 4.7 and RISC OS 5.30]]>
      </description>
      <source:markdown># RISC OS 5.30, GCC 4.7 and Hello World

By R. S. Doiel, 2024-06-08 (updated: 2024-06-16)

Presently am I learning RISC OS 5.30 on my Raspberry Pi Zero W. I want to write some programs and I learned C back in University. I am familiar with C on POSIX systems but not on RISC OS. These are my notes to remind myself how things work differently on RISC OS.

I found two resources helpful. First James Hobson had a YouTUBE series on RISC OS and C programming. From this I learned about allocating more space in the Task Window via the Tasks window display of Next and Free memory. Very handy to know. Watching his presentation it became apparent he was walking through some
one&#39;s tutorial. This lead to some more DuckDuckGo searches and that is when I stumbled on Steve Fryatt&#39;s [Wimp Programming In C](https://www.stevefryatt.org.uk/risc-os/wimp-prog). James Hobson&#39;s series (showing visually) help and the detail of Steve Fryatt&#39;s tutorial helped me better understanding how things work on RISC OS.

I think these both probably date from around 2016. Things have been evolving in RISC OS since then. I&#39;m not certain that OSLib today plays the same role it played in 2016. Also in the case of Steve Fryatt&#39;s tutorial I&#39;m not certain that the DDE and Norcroft compilers are essential in the say way. Since I am waiting on the arrival of the ePic SD Card I figured I&#39;d get started using the
GCC and tools available via Packman and see how far I can get.

## Getting oriented

What I think I need.

1. Editor
2. C compiler
3. Probably some libraries

You need an editor fortunately RISC OS comes with two, `!Edit` and `!StrongED`. You can use both to create C files since they are general purpose text edits.

You need a C compiler, GCC 4.7.4 is available via Packman. That is a click,
and download away so I installed that.

I had some libraries already installed so I skipped installing additional ones since I wasn&#39;t sure what was currently required.

## Pick a simple goal

When learning a new system I find it helpful to set simple goals. It helps from feeling overwhelmed.

My initial goal is to understand how I can compile a program and run it in the Task Window of RISC OS. The Task Window is a command line environment for RISC OS much like a DOS Window was for MS Windows or the Terminal is for modern macOS.  My initial program will only use standard in and out. Those come with the standard library that ships with the compiler. Minimal dependencies simplifies things. That goes my a good simple intial goal.

&#62; I want to understand the most minimal requirements to compile a C program and run it in Task Window

## Getting started

The program below is a simple C variation on the &#34;Hello World&#34;  program tought to beginner C programers.  I&#39;ve added a minimal amount of parameter handlnig to se how that works in the Task Window environment. This program will say &#34;Hello World!&#34; but if you include parameters it will say &#34;Hi&#34; to those too.

The code looks like this.

~~~C
#include &#60;stdio.h&#62;

int main(int argc, char *argv[]) {
  int i = 0;
  printf(&#34;Hello World!\n&#34;);
  for (i = 1; i &#60; argc; i++)  { 
       printf(&#34;Hi %s\n&#34;, argv[i]);
  }
  return 0;
}
~~~

In a POSIX system I would name this &#34;HelloWorld.c&#34;. On RISC OS the &#34;.&#34; (dot)
is a directory delimiter. There seems to be two approaches to translating POSIX paths to RISC OS. Samba mounted resources seem to have a simple substitution translatio. A dot used for file extensions in POSIX becomes a slash. The slash directory delimiter becomes a dot. Looking at it from the POSIX side the translation is flipped. A POSIX path like &#34;Project/HelloWorld/HelloWorld.c&#34; becomes &#34;Project.HelloWorld.HelloWorld/c&#34; in RISC OS.

In reading of the RISC OS Open forums I heard discussions about a different approach that is more RISC OS centric. It looks like the convention in RISC OS is to put the C files in a directory called &#34;c&#34; and the header files in a directory called &#34;h&#34;. Taking that approach I should instead setup my directory paths like &#34;Project.HelloWorld.c&#34; which in POSIX would be &#34;Project/HelloWorld/c&#34;. It seems to make sense to follow the RISC OS convensions in this case as I am not planning to port my RISC OS C code to POSIX anytime soon and if I did I could easily write a mappnig program to do that. My path to &#34;HelloWorld&#34; C source should look like `$.Projects.C_Programming.c.HelloWorld`.

After storting this bit out it is time to see if I can compile a simple program with GCC and run it in a Task Window. This is a summary of my initial efforts.

First attempt steps

1. Open Task Window
2. run `gcc --version`

This failed. GCC wasn&#39;t visible to the task window. Without understanding what I was doing I decided maybe I need to launch `!GCC` in `$.Apps.Development` directory. I then tried `gcc --version` again in the Task Window and this time
the error was about not enough memory available. I looked the &#34;Tasks&#34; window and saw plenty of memory was free. I did NOT realise you could drag the red bar for &#34;next&#34; and increase the memory allocation for the next time you opened a Task Window. I didn&#39;t find that out until I did some searching and stumbled on James Hobson&#39;s videos after watching the recent WROCC Wakefield Show held in Bedford (2024).

&#62; A clever thing about RISC OS is the graphical elements are not strictly informational. Often they are actionable. Dragging is not limited to icons.

Second attempt steps

1. Open the Tasks window, drag the memory (red bar) allocation to be more than 16K
2. Open a new Task Window
3. Find and Click on `!GCC`
4. In the task window check the GCC version number
5. Change the directory in the Task Window to where I saved &#34;HelloWorld&#34;
6. Check the directory with &#34;cat&#34;
7. Try to compile with `gcc HelloWorld -o app`, fails
8. Check GCC options with `--help`
9. Try to compiled with `gcc -x c HelloWorld -o app`, works

This sequence was more successful. I did a &#34;cat&#34; on the task window and saw I was not in the right folder where my &#34;HelloWorld&#34; was saved.  Fortunately James Hobson video shows any easy way of setting the working directory. I brought the window forward that held &#34;HelloWorld&#34;. Then I used the middle mouse button (context menu) to &#34;set directory&#34;. I then switched back to the Task Window and low and behold when I did a &#34;cat&#34; I could see my HelloWorld file.

I  tried to compile &#34;HelloWorld&#34;. In James Hobson video he shows how to do this but I couldn&#39;t really see what he typed.  When I tried this I got an error
about the file type not being determined.  Doing `gcc --help` listed the options
and I spotted `-x` can be used to explicitly set the type from the GCC point of view. This is something to remember when using GCC. It&#39;s a POSIX program running
on RISC OS which is not a POSIX system.  GCC will expect files to have a POSIX references in some case and not others. There&#39;s a bit of trial and error around
this for me.

Next I tried using the `-x c` option. I try recompiling and after a few moments
GCC creates a &#34;app&#34; file in the current directory. On initial creation it is a Textfile but then the icon quickly switches to a &#34;App/ELF&#34; icon.  Double clicking the App icon displays hex code in the Task Window. Not what I was expected. Back in the Task Window I type the following.

~~~shell
app Henry Mable
~~~

And I get out put of

~~~shell
Hello World!
Hi Henry
Hi Mable
~~~

My program works at the CLI level in a Task Window. My initial goal has been met.

## What I learned

1. Remember that RISC OS is a fully GUI system, things you do in windows can change what happens in the whole environment
2. Remember that the display elements in the GUI maybe actionable
3. When I double clicked on `!GCC` what it did is add itself to the search path.

I remember something from the Hobson video about setting that in `!Configure`, `!Boot` and picking the right boot configuration action.  I&#39;ll leave that for next time. I should also be able to script this in an Obey file and that might be a better approach.

There are some things I learned about StrongED that were surprising. StrongED&#39;s C mode functions like a &#34;folding&#34; editor. I saw a red arrow next to my &#34;main&#34; functions. If I click it the function folds up except for the function signature and opening curly bracket. Click it again the the arrow changes direction and the full function is visible again.

The &#34;build&#34; icon in StrongED doesn&#39;t invoke GCC at the moment. I think the build icon in the ribbon bar maybe looking for a Makefile. If so I need to install Make from Packman. This can be left for next time.

I&#39;d really like to change the editor colors as my eyes have trouble with white background. This too can be left for another day to figure out.

## Next Questions

1. How do I have the GCC compiled &#34;app&#34; so that I can double click in the file window and have it run without manually starting the Task Window and running it from there.  Is this a compiler option or do I need an Obey file?
2. Which libraries do I need to install while I wait on the DDE from ePic to arrive so that I can write a graphical version of Hello World?

## Updates

I got a chance to read more about [Obey files](https://www.riscosopen.org/wiki/documentation/show/CLI%20Basics) and also clicked through the examples in the `SDSF::RISCOSPi.$.Apps.Development.!GCC` directory (shift double click to open the GCC directory. In that directory is an examples
folder which contains a Makefile for compile C programs in various forms.
From there it was an easy stop to see how a simple Obey file could be used
to create a `!Build` and `!Cleanup` scripts.
where all the GCC setup lives). What follows are the two Obey files in the directory holding the &#34;c&#34; folder of HelloWorld.

Here&#39;s `!build`

~~~riscos
| !Build will run GCC on c.HelloWorld to create !HelloWorld
Set HelloWord$Dir &#60;Obey$Dir&#62;
WimpSlot -min 16k
gcc -static -O3 -s -O3 -o !HelloWorld c.HelloWorld
~~~

and `!Cleanup`

~~~riscos
| !Cleanup removes the binaries created with !Build
Set HelloWorld$Dir &#60;Obey$Dir&#62;
Delete !HelloWorld
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/06/08/riscos_gcc_and_hello.md" length="10754" type="text/markdown" />
    </item>    <item>
      <title>Exploring RISC OS 5.30 on a Raspberry Pi Zero W</title>
      <link>https://rsdoiel.github.io/blog/2024/06/04/exploring_riscos.html</link>
      <description>
        <![CDATA[In this post I talk about my exploration of using a Raspberry Pi Zero W
        as a desktop computer. This was made possible by the efficiency of 
        RISC OS 5.30 which includes native WiFi support for Raspberry Pi computers.]]>
      </description>
      <source:markdown># Exploring RISC OS 5.30 on a Raspberry Pi Zero W

By R. S. Doiel, 2024-06-04

Back on April, 27, 2024 [RISC OS Open](https://riscosopen.org) [announced](https://www.riscosopen.org/news/articles/2024/04/27/risc-os-5-30-now-available) the release of RISC OS 5.30. This release includes WiFi support for the Raspberry Pi Zero W. This may sound like a small thing. WiFi is taken for granted on many other operating systems.  This is the news I&#39;ve been waiting for before diving into RISC OS. My Pi Zero W running RISC OS **just works** with my wireless network. That is wonderful.

## RISC OS and Pi Zero W gives you a personal networked computer

Here&#39;s my setup.

- First generation Raspberry Pi Zero W (running RISC OS 5.30 with Pinboard2)
- Raspberry Pi Keyboard and Mouse (the keyboard provides a nice USB hub for the Zero)
- A powered portable monitory (the monitor provides power to the Pi Zero)

For additional disk storage I have a old Pi 3B+ with a 3.14G Western Digital hard drive. It is configured as a Samba file server running the bookworm release of Raspberry Pi OS[^1].

[^1]: RISC OS 5.3 only supports SMB with LANMAN1. LANMAN1 is turned off by default for Samba on R-Pi OS bookworm.

## Quick summary

I&#39;ve been playing around with RISC OS 5.30  on my Pi Zero for a couple weeks now. I&#39;ve really come to enjoy using it. RISC OS is different from macOS and Windows in its approach to the graphical user interface. Like with the Oberon Operating System you need to accept that difference and shed your assumptions about how things should and do work. I&#39;ve found the difference invigorating. My Pi Zero with RISC OS has become my &#34;fun desktop&#34; for recreational computing.

## Diving in

RISC OS is a small single user operating system. It requires minimal resources. It provides a rich graphical environment. Currently RISC OS 5.30 only runs on one core of an ARM CPU. The Raspberry Pi Zero has only one core so that&#39;s a nice fit.

There is a fair amount of information regarding RISC OS online. There are active user groups and communities too.
Some of the documentation is quite good. One of the things to keep in mind if you search for &#34;RISC OS&#34; on the Internet is that RISC OS has forked. This is a little like the old Unix fork of BSD versus System V. They come from the same origins but have taken different paths and approaches.

&#62;RISC OS&#39;s closed fork runs on vintage hardware and emulators only. It does not appear to be actively developed and the version numbers associated include version four and six.
&#62;
&#62; The [RISC OS Open](https://www.riscosopen.org) (i.e. RISC OS 5.x) fork is Open Source. It is being actively developed. It is licensed using the Apache 2 Open Source license. It runs on most versions of the Raspberry Pi. It actually runs on many single board computers. See &#60;https://www.riscosopen.org&#62; for details. Today it seems to be a well run Open Source project.

The reasons for the fork are complicated. From what I&#39;ve read they are due to how Acorn was broken up when the company ceased to operate as Acorn.  RISC OS as intellectual property wound up in two different companies. They had two different business models and were driven by maximizing profit in the diminished Acorn market. The net resulted was a divided RISC OS community. The division included a level of acrimony. Somehow RISC OS survived this. RISC OS 5 branch even survived a bumpy road to becoming a true Open Source operating system. Today RISC OS 5.30 is licensed under the widely used Apache 2.0 Open Source license. RISC OS 5 even have a small community of commercial software developers writing and updating software for it!

If, like me, you&#39;re starting your RISC OS 5.30 on a Raspberry Pi then you&#39;re in luck. The dust seems to have settled. I highly recommend first reading the User Guide found at the bottom of the [common](https://www.riscosopen.org/content/downloads/common) page in the downloads section of the RISC OS Open website. You can also buy a printed version of the User Guide. From there head over and explore RISC OS Open community forums at &#60;https://www.riscosopen.org/forum/&#62;. There is also a more general Acorn enthusiast community at [stardot.co.uk](https://www.stardot.org.uk/forums/). I found this particularly helpful in understanding the historical evolution of RISC OS. Stardot even has a presence of [GitHub](https://github.com/stardot).

What follows are a semi-random set of points that I had to wrap my head around in getting oriented on RISC OS 5.30 on the Raspberry Pi Zero W.

## Visibly different

When I search for RISC OS on the [Raspberry Pi Forums](https://forums.raspberrypi.com/search.php?keywords=RISC+OS) many of the questions seemed to have more to do with user assumptions about how RISC OS works than about things actually not working. RISC OS is different. It comes from a different era. It was designed with a vastly different set of assumptions than POSIX systems like macOS, Linux, BSD, or Windows. Check your assumptions at the door.

### The mouse and its pointer

RISC OS is a three button mouse oriented system. The left and middle mouse buttons play very specific roles.  The left button is for &#34;action&#34; and the middle button provides a &#34;context menu&#34;. RISC OS does not use &#34;application menu&#34; at the top of the screen or top of the window frame like macOS, Windows or Raspberry Pi OS. The &#34;context menu&#34; provides that functionality.

When you point your mouse at a screen element and press the middle button you get the context menu for that thing.  If you single or double click on an item with the left button you are taking an action.

- The left mouse button takes a action. Sometimes you use a single click and at others a double click
- The middle mouse button brings up the context menu, on RISC OS this is replaces the need for an application menu

### The Iconbar and Pinboard

When you start RISC OS you will see two main visual elements. The desktop is called the &#34;Pinboard&#34;. This is analogous to desktops on Windows, macOS and Raspberry Pi OS.  It contains a background image and icons.

The Iconbar is the other big visible element. It is found at the bottom of the screen. The Iconbar predates Window&#39;s task bar and may have inspired it.  The Iconbar is responsibly for allowing you to access system resources and the loaded applications and modules.

On the left side of the Iconbar are icons representing system resources. This includes the SD card holding RISC OS. On the right side of the Iconbar you&#39;ll find icons that represent running applications or modules.

RISC OS is a single user operating system and relies on cooperative multitasking. An application or module can be loaded into memory and left in memory for fast reuse.  When you &#34;launch&#34; an application it loads into memory. To use the application you can either left double click on the icon in the Iconbar or open a file associated with the application. If you do the later then the application will automatically get added to the Iconbar if it was not already present. When you close an application&#39;s window you&#39;re not removing the application. To close an application you need to go to the Iconbar and use the context menu (remember that middle mouse button) to &#34;quit&#34; it.

On the left, system resources side, data storage is manipulated via the filer windows. You open a filer window by left double clicking on the resource in the Iconbar. The filer window on RISC OS plays a role similar to the file manager on Windows or a finder window on macOS.

### Iconbar and file resources

The filer maintains metadata about files. This includes the file type and the last application used to save the file. The file type is explicit with RISC OS and is NOT based on a file extension like with macOS, Windows and Raspberry Pi OS. The file path notation is also significantly different from the POSIX path syntax.

Double click on a storage resource in the Iconbar opening a filer window. The window will list files, folders and applications. The file system is hierarchical.  An icon will indicate the file type. Some icons are folders, these are sub directories of the current filer window. Most files will have other icons associated based on file type. There are special &#34;files&#34; that begin with an exclamation symbol, &#34;!&#34;. In POSIX this is referred as a &#34;bang&#34; symbol but in RISC OS it is called a &#34;pling&#34;.  Filenames starting with the pling hold applications. Applications are implemented as a directory containing resources. Resources might be application&#39;s visual assets, configuration, scripts or executable binaries. Directories as application will be familiar to people running modern versions of macOS or who have used NeXTStep. RISC OS usage predated both those systems. As an end user you just click on the pling name to launch the application. It will then show up in the Iconbar ready for use.

On the right side of the Iconbar are your running applications. When you first startup RISC OS you&#39;ll see two running. First, on the far right is an icon representing the hardware you&#39;re running on. If you&#39;re on a Raspberry Pi it&#39;ll be raspberry icon.  If you&#39;re running on a Pine64 Pinebook it&#39;ll be a pine cone. Since I&#39;m on a Raspberry Pi Zero I will called it a Raspberry icon. Use your mouse and move the mouse pointer over the Raspberry icon. Left double click on the icon. This starts up a dialog box containing information about system resources. If you single click the middle button on the Raspberry icon you will get a operating system context menu. In the context menu you&#39;ll see items for &#34;Configure&#34; and &#34;Task Window&#34;. The Task window provides a command line interface for RISC OS. The configure menu lets you configure how RISC OS runs. This includes things like setting the desktop theme and configuring network services.

Pointing your mouse to the left of the Raspberry will place the cursor over an icon that looks like a computer monitor.  Double left clicking on this icon will open a dialog that lets you set the resolution of your monitory. Similarly a single middle click on the icon will open a context menu.  This is a major pattern in interacting with RISC OS on the Iconbar. In fact this is the general pattern of using the mouse through out the system. It&#39;s take an action (left) or select an action to take (middle).

The Now you should see it in the Iconbar towards the right.  Note it is only visible in the Iconbar right now. This is very different than double clicking on macOS Or Windows. If you move your mouse over the icon in the Iconbar and Left double click it&#39;ll open your applications main dialog or window (just like what happened when we click on the monitor or Raspberry icons). It&#39;s a two step process.

How do you get more applications on the Iconbar?

Use the filer. The default applications are in the &#34;Apps&#34; icon visible on the Iconbar. Left double click on it. It&#39;ll open a filer window with an abbreviate list of applications. In that filer window look for an application called &#34;!Alarm&#34; (pling alarm). Double left click on that application&#39;s icon will cause it to appear in the Iconbar towards the right side. Congratulation you&#39;ve loaded your first RISC OS application.

You will find more applications by looking at the contents of the SD Card. To do that double left click on the SD Card icon in the Iconbar. It&#39;ll open a filer window. You&#39;ll see a folder icon labeled &#34;Apps&#34;. This is an actual directory on the file system. It is where applications are usually installed on RISC OS. Open that folder by double left clicking on it. You can low launch additional application by left double clicking on their pling names. These like Alarm will show up on the right side of the Iconbar.

Having things running on the Iconbar is convenient.  If you want to start a new document associated with one of the running application you just left double the icon on the Iconbar.  If you want an application to be processed by something running in the Iconbar you can draft the document from the filer window to the icon on the Iconbar. If you want to save that new document start the application from the Iconbar then use the context menu in the application window or press F3 to get the save dialog box. In the dialog box give your document a name and drag the icon of the file from the dialog box to a filer window where you want to store the document. That last step is important to note. Icons are actionable even when they appear in a dialog box! This is very unlike Windows, macOS or Raspberry Pi OS. The dragging the icon is what links data storage with the application. The next time you want to edit the file you just fine the file and left double click on it to open it. If the application isn&#39;t running then it&#39;ll be launched and added to the Iconbar automatically.

Remember when you close your application&#39;s main window or dialog box it doesn&#39;t close the application. It only close the file you were working on. To actually unload the application (and free up additional memory) point your mouse at the icon in the Iconbar and middle click to see the context menu. One of the context menu items will be &#34;Quit&#34;, this is how you fully shutdown the application.

### Context menus

What&#39;s all this about context menus? On other operating systems the context menu is called an application menu. It might be at the top of the application window (e.g. Windows) or at the top of the screen (e.g. macOS). On Windows and macOS the words &#34;context menu&#34; usually mean a special sub-menu in the application (e.g. like when you use a context menu to copy a link in your web browser). In RISC OS the context menu is the application menu. It is always accessed via the middle mouse button just as we saw in introduction to the Iconbar.

Some applications like the &#34;!StrongEd&#34; editor do include a ribbon of icons that will make it easy to do various things. That&#39;s just a convenience. Other applications like &#34;!Edit&#34; don&#39;t provide this convenience. Personally I&#39;m ambivalent to ribbon bars. I think they are most useful where you&#39;d have to navigate down a level or two from the context menu to reach a common action you wanted to perform. There are things I don&#39;t like about the ribbon button approach. You loose window space to the ribbon. The second you are increasing the distance of mouse travel to take the action. On the other hand it beats traveling down three levels of nesting in the context menu. It&#39;s a trade off.

What ever the case remember the middle button as picking an action and the left mouse button as taking the action.

### Window frames

When you open an application from the Iconbar it&#39;ll become a window (a square on the screen containing a dialog or application content). Looking at the top of of the window frame there is are some buttons and application title area. In the upper left of the window frame you see two buttons. From left to right, the first button you see will look like two squares overlaid. Reminds me of the symbols often used when copying text.  That button is not a copy button. The two overlapping squares indicate that the current window can be sent to the bottom (furthest behind) of the stack of overlapping windows. If you think of a bunch of paper loosely stack and being read it&#39;s like taking the top paper and putting it at the back of the stack as you finish reading it. I found this very intuitive and a better experience than how you would take similar actions on macOS and Windows.

Next to the back window button is an button with an &#34;X&#34; on it. This button closes the window.

To the right of the close window button is the title bar. This usually shows the path to the document or data you the application is managing. You&#39;ll notice the path doesn&#39;t resemble a POSIX path or even a path as you&#39;d find on DOS or CP/M.  I&#39;ll circle back to the RISC OS path notation in a bit. The path maybe followed by an asterisk. If the asterisk is present it indicates the data has been modified. If you try to close the window without saving a filer dialog will pop up giving a choice to discard changes, cancel closing or saving the changes. A heads up is saving is approach differently in RISC OS than on macOS or Windows.

To the right of the title bar you&#39;ll see minimize button which looks like a dash and a maximize button that looks like a square. These work similar to what you&#39;d expect on macOS or Windows.

A scroll bar is visible on the right side and bottom. These are nice and wide. Very easy to hit using the mouse unlike modern macOS and Windows scroll bars which are fickle. There is one on the bottom the lower right of the window. The lower right that you can use to stretch or squeeze the window easily.

Along with the middle mouse button presenting the context menu you&#39;ll also notice that RISC OS makes extensive use of dialog boxes to complete actions. If you see an Icon in the dialog it&#39;s not there as a branding statement. That icon can be dragged to another window such as the Filer window to save a document. This is a significant difference between RISC OS and other operating systems. The Icon has a purpose beyond identification and branding. Moving the Icon and dropping it usually tells the OS to link two things.

### Some historical context and web browsing

RISC OS has a design philosophy and historical implementation. As I understand it the original RISC OS was written in ARM assembly language with BBC BASIC used to orchestrate the pieces written in assembly. That was how it achieved a responsive fluid system running on minimal resources. Looking back at it historically I kinda of think of BBC BASIC as a scripting language for a visual interface built from assembly language modules. The fast stuff was done in assembly but the high level bits were BBC BASIC. Today C has largely taken over the assembly language duties. It even has taken over some of the UI work too (e.g. Pinboard2 is written in C). The nice thing is BBC BASIC remains integrated and available.

&#62; BBC BASIC isn&#39;t the BASIC I remember seeing in college.  BBC BASIC appears more thorough. It includes an inline ARM assembly language support. Check out Bruce Smith&#39;s books on ARM Assembly Language on RISC OS if you want to explore that.

Even through RISC OS was originally developed with BBC BASIC and assembler it wasn&#39;t developed in a haphazard fashion. A very modular approach was taken. Extending the operating system often means writing a module.  The clarity of the approach as much as the tenacity of the community has enable RISC OS to survive long past the demise of Acorn itself. It has also meant that as things have shifted from assembly to C that the modularity has remained strong aspect of RISC OS design and implementation.

RISC OS in spite of its over 30 years of evolution has a remarkably consistent feel. This is in part a historical accident but also a technical one in how the original system was conceived. That consistency is also one is one of its strengths. Humans tend to prefer consistency when they can get it. It is also the reasons you don&#39;t see allot of direct ports of applications from Unix, macOS (even single user system 7) or Windows. When I work on a Linux machine I expect the GUI to be inconsistent. The X Window systems was designed for flexibility and experimentation. The graphical experience only becomes consistent within the specific Linux distribution (e.g. Raspberry Pi OS). Windows also has a history of being pretty lose about it user interface. Windows user seem much happier to accept a Mac ported application then Mac users using a Windows ported one.

Why do I bring this up? Well like WiFi many people presume availability of evergreen browsers. Unlike WiFi I think I can live without Chrome and friends.  There is a browser called NetSurf that comes with RISC OS 5.30. You can view forums and community website with targeting RISC OS with it. If you&#39;re a follower of the Tidleverse you&#39;ll find NetSurf sufficient too. It&#39;s nice not running JavaScript. If you visit GitHub though it&#39;ll look very different. Some of it will not work.  Fortunately there is a new browser on the horizon called Iris. I suspect like the addition of native WiFi support you&#39;ll see a bump in usability for RISC OS when it lands.

### Access external storage

RISC OS supports access to external storage services. There are several file server types supported but I found SMB the easiest to get setup. I have a Raspberry Pi 3B+ file server running Samba on Raspberry Pi OS (bookworm). The main configuration change I needed to support talking to RISC OS was to enable LANMAN1 protocol. By default the current Samba shipping with bookworm has LANMAN1 protocol turned off (they are good reasons for this). This is added in the global section of the smb.conf file. I added the following line to turn on the protocol.

~~~shell
server min protocol = LANMAN1
~~~

Because of the vintage nature of the network calls supported and problems of WiFi being easy to sniff I opted to remove my personal directory from Samba services. I also used a different password to access Samba from my user account (see the smbpasswd manual page for how to do that).

I don&#39;t store secret or sensitive stuff on RISC OS nor do I access it from RISC OS. Like running an DOS machine security on RISC OS isn&#39;t a feature of the operating system. For casual writing, playing retro games, not a big problem but I would not do my banking on it.

### Connecting to my Raspberry Pi File Server

Once I had Samba configured to support LANMAN1 I was able to connect to it from RISC OS on the Pi Zero but it was tricky to figure out how at first.  The key was to remember to use the context menu and to use the `!OMNI` application found in the &#34;Apps&#34; folder on the Iconbar.

First open the Apps folder, then right double click on `!OMNI`.  This will create what looks like a file server on the left side of the Iconbar (where other disk resources are listed).  If you place your mouse pointer over it and middle click the context menu will pop up. Then navigate to the protocol menu, followed by LanMan. Clicking on LanMan should bring up a dialog box that will let you set the connection name, the name of the file server as well as the login credentials for that server.  There is no OK button. When you press enter after entering your password the dialog box knows you&#39;re done and OMNI will try to make the connection. You&#39;ll see the mouse pointer turn into an hour glass as it negotiates the connection. If the connection is successful you&#39;ll see a new window open with the resource&#39;s available from that service.

You can save yourself time by &#34;saving&#34; the protocol setup via the context menu. To do that you point to the OMNI icon, pull up the context menu, select protocol then select save protocol.

### RISC OS Paths

The RISC OS path semantics are NOT the same as POSIX.  The RISC OS file system is hierarchical but does not have a common root like POSIX. Today RISC OS supports several types of file systems and protocols. As a result the path has a vague resemblance to a URI.

The path starts with the protocol. This is followed by a double colon then the resource name. The resource name is followed by a period (a.k.a. dot) and then the dollar sign. The dot and dollar sign represents the root of the file system resource. The dot (period) is used as a path delimiter. It appears to be a practice to use a slash to indicate a file extension. This is important to remember. A path like `myfile.txt` on RISC OS means there is a folder called &#34;myfile&#34; and a file called &#34;txt&#34; in side it. On the other hand `myfile/txt` would translate to the POSIX form of `myfile.txt`. Fortunately the SMB client provided by OMNI handles this translation for us.

## first impressions

I find working on RISC OS refreshing. It is proving to be a good writing platform for me. I do have a short wish list. RISC OS seems like a really good platform for exploring the text oriented internet.  I would like to see both a gopher client and server implemented on RISC OS. Similarly I think Gemini protocol makes sense too.  I miss not having Pandoc available as I use that to render my writing to various formats on POSIX systems (e.g. html, pdf, ePub).

What might I build in RISC OS?  I&#39;m not sure yet though I have some ideas. I really like RISC OS as a writing platform. The OS itself reminds me of some of the features I like in Scrivener. I wonder if Scrivener&#39;s Pinboard got its inspiration from RISC OS?

I&#39;ve written Fountain, Open Screen Play, Final Draft conversion tools in Go. I&#39;d like to have similar tools available along with a nice editor available on RISC OS. Not sure what the easiest approach to doing that is. I&#39;ve ordered the [ePic](https://www.riscosopen.org/content/sales/risc-os-epic/epic-overview) SD card and that&#39;ll have the [DDE](https://www.riscosopen.org/content/sales/dde) for Raspberry Pi. It might be interesting to port OBNC Oberon-07 compiler to RISC OS and see if I could port my Go code to Oberon-07 in a sensible way.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/06/04/exploring_riscos.md" length="25660" type="text/markdown" />
    </item>    <item>
      <title>A quick review of Raspberry Pi Connect</title>
      <link>https://rsdoiel.github.io/blog/2024/05/10/quick-review-rpi-connect.html</link>
      <description>
        <![CDATA[A review of Raspberry Pi Connect as an alternative to using RealVNC.]]>
      </description>
      <source:markdown># A quick review of Raspberry Pi Connect

The Raspberry Pi company has created a nice way to share a Pi Desktop. It is called Raspberry Pi Connect. It is built on the peer-to-peer capability of modern web browsers using [WebRTC](https://en.wikipedia.org/wiki/WebRTC). The connect service requires a Raspberry Pi 4, Raspberry Pi 400 or Raspberry Pi 5 running the [Wayland](https://en.wikipedia.org/wiki/Wayland_(protocol)) display server and Bookworm release of Raspberry Pi OS.

When I read the [announcement](https://www.raspberrypi.com/news/raspberry-pi-connect/) I wondered, why create Raspberry Pi Connect? RealVNC has works fine.  RealVNC even has a service to manage your RealVNC setups.

I think the answer has three parts. First it gives us another option for sharing a Pi Desktop. Second it is a chance to make things easier to use. Third if you can share a desktop using WebRTC then you can also provide additional services.

For me the real motivator is ease of use. In the past when I&#39;ve used RealVNC between two private networks I&#39;ve had to setup SSH tunneling. Not unmanageable but certainly not trivial.  I think this is where Raspberry Pi Connect shines. Setting up sharing is a three step process.

1. Start up your Pi desktop, install the software
2. Create a Raspberry Pi Connect account and register your Pi with the service
3. On another machine point your web browser at the URL for Raspberry Pi connect and press the connect button

The next time you want to connect you just turn on your Pi and login. If I have my Pi desktop to auto login then I just turn the Pi on and when it finishes booting it is ready and waiting. On my other machine I point my web browser at the connect website, login and press the connection button.

When I change computers I don&#39;t have to install VNC viewers. I don&#39;t have to worry about setting secure ssh tunnels. I point my web browser at the Raspberry Pi Connect site, login and press the connect button. The &#34;one less thing to worry about&#34; can make it feel much less cumbersome.

## How does it work?

The Raspberry Pi Connect architecture is intriguing. It leverages [WebRTC](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API). WebRTC supports peer to peer real time connection between two web browsers running in separate locations across the internet. Making a WebRTC requires the two sites to use a URL to establish contact. From that location perform some handshaking and see if the peer connection can be establish a directly between the two locations. If the direct connection can&#39;t be established then a relay or proxy can be provided as a fallback. 

The Raspberry Pi Connect site provides the common URL to contact. On the Pi desktop side a Wayland based service provides access to the Pi&#39;s desktop. On the other side you use a Web Browser to display and interact with the desktop. Ideally the two locations can establish a direct connection. If that is not possible then Raspberry Pi Connect hosts [TURN](https://en.wikipedia.org/wiki/Traversal_Using_Relays_around_NAT) in London as a fallback. A direct connection gives you a responsive shared desktop experience but if you&#39;re on the Pacific edge of North America or on a remote Pacific island then traffic being relayed via London can be a painfully slow experience.

The forum for the Raspberry Pi Connect has a [topic](https://forums.raspberrypi.com/viewtopic.php?t=370591&#38;sid=61d7cdf3c03a7ead49e3da837b0d4f06) discussing the routing algorithm and choices. The short version is exerted below.

&#62; Essentially, when the connection was being established both sides provided their internet addresses (local, and WAN) - and when both sides tested their ability to talk to the other side, they failed. Only after this failure is the TURN server used.

## Questions

Can I replace RealVNC with Raspberry Pi Connect?

It depends. I still use Raspberry Pi 2, 3 and some Zeros. I&#39;m out of luck using Pi Connect since these devices aren&#39;t supported. If you&#39;ve already installed RealVNC and it&#39;s working well for you then sharing via Pi connect is less compelling.

If I was setting up a new set of Raspberry Pi 4/400 or 5s then I&#39;d probably skip RealVNC and use Pi connect. It&#39;s feels much easier and unless the network situation forces you to route traffic through London is reasonably responsive.

Is screen sharing the only thing Raspberry Pi Connect provides?

I expect if Raspberry Pi Connect proves successful we&#39;ll see other enhancements. One of the ones mentioned in the forums was SSH services without the hassle of dealing with setting up tunnels. The folks in the Raspberry Pi company, foundation and community are pretty creative. It&#39;ll be interesting to see where this leads.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/05/10/quick-review-rpi-connect.md" length="5233" type="text/markdown" />
    </item>    <item>
      <title>Building Lagrange on Raspberry Pi OS</title>
      <link>https://rsdoiel.github.io/blog/2024/05/10/building-lagrange-on-pi-os.html</link>
      <description>
        <![CDATA[These are my quick notes on building the Lagrange Gemini browser on Raspberry Pi OS. They are based on instructions I found at <gemini://home.gegeweb.org/install_lagrange_linux.gmi>. These are in French and I don't speak or read French. My loss. The author kindly provided the specific command sequence in shell that I could read those. That was all I needed. When I read the site today I had to click through an expired certificate. That's why I think it is a good idea to capture the instructions here for the next time I need them.  I made single change to his instructions. I have cloned the repository from <https://github.com/skyjake/lagrange>.
        
        ...]]>
      </description>
      <source:markdown># Building Lagrange on Raspberry Pi OS

These are my quick notes on building the Lagrange Gemini browser on Raspberry Pi OS. They are based on instructions I found at &#60;gemini://home.gegeweb.org/install_lagrange_linux.gmi&#62;. These are in French and I don&#39;t speak or read French. My loss. The author kindly provided the specific command sequence in shell that I could read those. That was all I needed. When I read the site today I had to click through an expired certificate. That&#39;s why I think it is a good idea to capture the instructions here for the next time I need them.  I made single change to his instructions. I have cloned the repository from &#60;https://github.com/skyjake/lagrange&#62;.

## Steps to build

1. Install the programs and libraries in Raspberry Pi OS to build Lagrange
2. Create a directory to hold the repository, then change into it
3. Clone the repository
4. Add a &#34;build&#34; directory to the repository and change into it
5. Run &#34;cmake&#34; to build the release
6. Run &#34;make&#34; in the build directory and install
7. Test it out.

When you clone the repository you want to clone recursively and get the release branch. Below is a transcript of the commands I typed in my shell to build Lagrange on my Raspberry Pi 4.

~~~shell
sudo apt install build-essential cmake \
     libsdl2-dev libssl-dev libpcre3-dev \
     zlib1g-dev libunistring-dev git
mkdir -p src/github.com/skyjake &#38;&#38; cd src/github.com/skyjake 
git clone --recursive --branch release git@github.com:skyjake/lagrange.git
mkdir -p lagrange/build &#38;&#38; lagrange/build
cmake ../ -DCMAKE_BUILD_TYPE=Release
sudo make install
lagrange &#38;
~~~

That&#39;s about it. It worked without a hitch. I&#39;d like to thank Grald Niel who I think created the page on gegeweb.org. I attempted to leave a thank you via the web form but couldn&#39;t get past the spam screener since I didn&#39;t understand the instructions. C&#39;est la vie.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/05/10/building-lagrange-on-pi-os.md" length="2999" type="text/markdown" />
    </item>    <item>
      <title>Getting Started with Miranda</title>
      <link>https://rsdoiel.github.io/blog/2024/04/25/getting-started.html</link>
      <description>
        <![CDATA[I've been interested in exploring the Miranda programming language. Miranda influenced Haskell. Haskell was used for programs I use almost daily such as [Pandoc](https://pandoc.org) and [shellcheck](https://www.shellcheck.net/). I've given a quick review of [miranda.org.uk](https://miranda.org.uk) to get a sense of the language but to follow along with the [Miranda: The Craft of Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/Miranda_craft/) it is really helpful to have Miranda available on my machine. Today that machine is a Mac Mini, M1 processor, running macOS Sonoma (14.4.x) and the related Xcode C tool chain.  I ran into to minor hiccups in compilation and installation. Both easy to overcome but ones I will surely forget in the future. Thus I write myself another blog post.
        
        ...]]>
      </description>
      <source:markdown># Getting Started with Miranda

I&#39;ve been interested in exploring the Miranda programming language. Miranda influenced Haskell. Haskell was used for programs I use almost daily such as [Pandoc](https://pandoc.org) and [shellcheck](https://www.shellcheck.net/). I&#39;ve given a quick review of [miranda.org.uk](https://miranda.org.uk) to get a sense of the language but to follow along with the [Miranda: The Craft of Functional Programming](https://www.cs.kent.ac.uk/people/staff/sjt/Miranda_craft/) it is really helpful to have Miranda available on my machine. Today that machine is a Mac Mini, M1 processor, running macOS Sonoma (14.4.x) and the related Xcode C tool chain.  I ran into to minor hiccups in compilation and installation. Both easy to overcome but ones I will surely forget in the future. Thus I write myself another blog post.

## Compilation

First down load Miranda source code at &#60;http://miranda.org.uk/downloads&#62;. The version 2.066 is the most recent release I saw linked (2024-04-25), &#60;http://www.cs.kent.ac.uk/people/staff/dat/ccount/click.php?id=11&#62;. The [COPYING](https://www.cs.kent.ac.uk/people/staff/dat/miranda/downloads/COPYING) link shows the terms under which this source release is made available.

Next you need to untar/gzip the tarball you downloaded. Try running `make` to see if it compiles. On my Mac Mini I got a compile error that looks like

~~~shell
make
gcc -w    -c -o data.o data.c
data.c:666:43: error: incompatible integer to pointer conversion passing &#39;word&#39; (aka &#39;long&#39;) to parameter of type &#39;char *&#39; [-Wint-conversion]
                     else fprintf(f,&#34;%c%s&#34;,HERE_X,mkrel(hd[x]));
                                                        ^~~~~
1 error generated.
make: *** [data.o] Error 1
~~~

While I&#39;m rusty on C I read this as the C compiler being more strict today then it was back in the 1990s. That&#39;s a good thing generally.  Next I checked the compiler version. 

~~~shell
gcc --version
Apple clang version 15.0.0 (clang-1500.3.9.4)
Target: arm64-apple-darwin23.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
~~~

I&#39;m using clang and the website mentioned it should compile with clang for other platforms.  I reviewed the data.c file and notice other similar lines that invoked `mkrel(hd[x])` had a `(char *)` cast in front of `hd[x]`. This tells me that being explicit with the compiler might solve my problem. I edited line 666 of data.c to look like

~~~C
    else fprintf(f,&#34;%c%s&#34;,HERE_X,mkrel((char *)hd[x]));
~~~

Save the file and then ran Make again. It compile cleanly. I gave at quick test run of the `mira` command creating an simple function called `addone`

~~~miranda
mira
/edit
addone a = a + 1
:wq
addone (addone (addone 3))
6
/q
~~~

Miranda seems to work. The Makefile comes with a an install rule but the install defaults doesn&#39;t really work with macOS (it wants to install into `/usr`).
I&#39;d rather it install into my home directory so I copied the Makefile to `miranda.mak` and change the lines setting `BIN`, `LIB` and `MAN` to the following
lines.

~~~Makefile
BIN=$(HOME)/bin
LIB=$(HOME)/lib#beware no spaces after LIB
MAN=$(HOME)/man/man1
~~~

In my `.profile` I set the `MIRALIB` variable to point at `$HOME/lib/miralib`. I opened a new terminal session and ran `mira` and the interpreter was up and running.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/04/25/getting-started.md" length="4629" type="text/markdown" />
    </item>    <item>
      <title>A Text Oriented Web</title>
      <link>https://rsdoiel.github.io/blog/2024/02/25/text_oriented_web.html</link>
      <description>
        <![CDATA[The web is a busy place. There seems to be a gestalt resonant at the moment on the web that can be summarized by two phrases, "back to basics" and "simplification". It is not the first time I've seen this nor is it likely the last. This blog post describes a thought experiment about a simplification with minimal invention and focus on feature elimination. It's a way to think about the web status quo a little differently. My intention is to explore the implications of a more text centered web experience that could coexist as a subset of today's web.
        
        ...]]>
      </description>
      <source:markdown># A Text oriented web

By R. S. Doiel, 2024-02-25

The web is a busy place. There seems to be a gestalt resonant at the moment on the web that can be summarized by two phrases, &#34;back to basics&#34; and &#34;simplification&#34;. It is not the first time I&#39;ve seen this nor is it likely the last. This blog post describes a thought experiment about a simplification with minimal invention and focus on feature elimination. It&#39;s a way to think about the web status quo a little differently. My intention is to explore the implications of a more text centered web experience that could coexist as a subset of today&#39;s web.

## The web&#39;s &#34;good stuff&#34;

I think the following could form the &#34;good stuff&#34; in a Crockford[^1] sense of pairing things down to the essential.

- the transport layer should remain HTTP but be limited to a few methods (GET, POST and HEAD) and the common header elements (e.g. length, content-type come to mind)
- The trio of HTML, CSS and JavaScript is really complex, swap this out for Markdown augmented with YAML (Markdown and YAML already have a synergy in Markdown processors like Pandoc)
- A Web form is expressed using GHYITS[^2], it is delimited in the Markdown document by the familiar &#34;`^---$`&#34; block element, web form content would be encoded as YAML in the body of the POST using the content type &#34;`application/x-yaml`&#34;.
- Content would be served using the `text/markdown; charset: utf-8` content type already commonly used to identify Markdown content distinct from `plain/text`

I need a nice name for describing the arrangement of Markdown+YAML over HTTP arrangement. Is the descriptive acronym for &#34;text oriented web&#34;, i.e. &#34;tow&#34;, enough? Does it already have a meaning in software or the web? Would the protocol be &#34;tow://&#34;? I really need something a bit more clever and catchy if this is going to proceed beyond a thought experiment.

[^1]: Douglas Crockford &#34;discovered&#34; JSON, see &#60;https://en.wikipedia.org/wiki/Douglas_Crockford&#62;

[^2]: GHYITS, acronym, GitHub YAML Issue Template Syntax, see &#60;https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/syntax-for-issue-forms&#62;

## Prototyping Options

A proof of concept could be possible using off the self web server and web browser. The missing parts would be setting up the web server to add the `text/markdown; charset: utf-8` header for &#34;`.md`&#34; files and to handle processing POST with a content type of `application/x-yaml`. Client side could be implemented in a static web page via JavaScript or WASM module. The JS/WASM could convert the Markdown+YAML into HTML rendering the &#34;tow&#34; content. The web form on submit would be intercepted by the JavaScript event handler and reformulated as a POST with a content type of `application/x-yaml`.

Building a &#34;tow&#34; server and client should be straight forward in Go (probably many languages). The the standard &#34;http&#34; package can be used to implement the specialized http server. The `yaml.v3` package to process the YAML POST data. Similar you should be able to create a text client for the command line or even a GUI client via [Fyne](https://fyne.io)

## Exploratory Questions

- What does it mean to have a more text oriented web?
- What advantages could a text user interface have over a graphical user interface?
- Can &#34;tow&#34; provide enough simple interactivity to support interactive fiction?
- Could a simple specification be stated clearly in a few pages of text?
- What possibilities open up when a web browser can send a data structure via YAML to a service?
- Can we live with a simpler client than a modern evergreen web browser?
- With a conversation interaction model of &#34;listener&#34; and a &#34;speaker&#34;, does it make sense thinking in terms of client server architecture?
- How hard is it to support both traditional website and this minimal &#34;tow&#34; site using the same corpus?
- Can this be done sustainably?

## Extrapolations

From a thought experiment I can see how to implement this both from a proof of concept level but also from a service and viewer level. I think it even offers an opportunity to function in a peer to peer manner.  If we&#39;re focusing primarily on text then the storage requirements can be minimal and the service could even live in a database system like SQLite3 as a form of sandbox of content.  Leveraging HTTP/HTTPS means we don&#39;t need any special support for content traveling across the net. With a much smaller foot print you can scratch the itch of a simpler textual experience without the trackers, JavaScript ping backs, etc. It could re-emphasize the conversion versus broadcast metaphor popularized by the walled gardens.  It might provide a more satisifying experience on Mobile since the payloads delivered to the web browser could be much smaller.

## What is needed to demonstrate a standalone &#34;tow&#34;?

- A modified HTTP web server (easy to implement in Go and other languages)
- A viewer/browser, possible to implement via Fyne in Go or as a text application/command line interface in Go

## Why not Gopher or Gemini?

Tow is not there to replace anything, not Gopher, Not Gemini, the WWW. It is an exploration of a subset of the WWW protocols with a specific focused on textual interaction. I don&#39;t see why a server or browser couldn&#39;t support Gopher and Gemini as well as Tow. Given that Markdown can easily be rendered into Gem Text, and Markdown can be treated as plain text I suspect you should be able to support all three text rich systems from the same copy and easily derive a full HTML results if desired too.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/02/25/text_oriented_web.md" length="6519" type="text/markdown" />
    </item>    <item>
      <title>Two missing features from HTML5, an enhanced form.enctype and a list input type</title>
      <link>https://rsdoiel.github.io/blog/2024/02/23/enhanced_form_handling.html</link>
      <description>
        <![CDATA[I wish the form element supported a `application/json` encoding type and there was such a thing as a `list-input` element.
        
        I've been thinking about how we can get back to basic HTML documents and move away from JavaScript required to render richer web forms. When web forms arrived on scene in the early 
        1990s they included a few basic input types. Over the years a few have been added but by and large the data model has remained relatively flat. The exception being the select 
        element with `multiple` attribute set. I believe we are being limited by the original choice of urlencoding web forms and then resort to JavaScript to address it's limitations.
        
        What does the encoding of a web form actually look like?  The web generally encodes the form using urlencoding. It presents a stream of key value pairs where the keys are the form's 
        input names and the values are the value of the input element. With a multi-select element the browser simply repeats the key and adds the next value in the selection list to that 
        key.  In Go you can describe this simple data structure as a `map[string][]string`. Most of the time a key points to a single element array of string but sometimes it can have 
        multiple elements using that key and then the array expands to accommodate. Most of the time we don't think about this as web developers. The library provided with your programming 
        language decodes the form into a more programmer friendly representation. But still I believe this simple urlencoding has held us back. Let me illustrate the problem through a 
        series of simple form examples.
        
        Here's an example of a simple form with a multi select box. It is asking for your choice of ice cream flavors.
        
        ...]]>
      </description>
      <source:markdown># Two missing features from HTML5, an enhanced form.enctype and a list input type

## Robert&#39;s wish list for browsers and web forms handling

By R. S. Doiel, 2024-02-23

I wish the form element supported a `application/json` encoding type and there was such a thing as a `list-input` element.

I&#39;ve been thinking about how we can get back to basic HTML documents and move away from JavaScript required to render richer web forms. When web forms arrived on scene in the early 1990s they included a few basic input types. Over the years a few have been added but by and large the data model has remained relatively flat. The exception being the select element with `multiple` attribute set. I believe we are being limited by the original choice of urlencoding web forms and then resort to JavaScript to address it&#39;s limitations.

What does the encoding of a web form actually look like?  The web generally encodes the form using urlencoding. It presents a stream of key value pairs where the keys are the form&#39;s input names and the values are the value of the input element. With a multi-select element the browser simply repeats the key and adds the next value in the selection list to that key.  In Go you can describe this simple data structure as a `map[string][]string`[^1]. Most of the time a key points to a single element array of string but sometimes it can have multiple elements using that key and then the array expands to accommodate. Most of the time we don&#39;t think about this as web developers. The library provided with your programming language decodes the form into a more programmer friendly representation. But still I believe this simple urlencoding has held us back. Let me illustrate the problem through a series of simple form examples.

[^1]: In English this could be described as &#34;a map using a string to point at a list of strings&#34; with &#34;string&#34; being a sequence of letters or characters.

Here&#39;s an example of a simple form with a multi select box. It is asking for your choice of ice cream flavors.

~~~html
&#60;form method=&#34;POST&#34;&#62;
  &#60;label for=&#34;ice-cream-flavors&#34;&#62;Choose your ice cream flavors:&#60;/label&#62;
  &#60;select id=&#34;ice-cream-flavors&#34; name=&#34;ice-cream-flavors&#34; multiple &#62;
    &#60;option value=&#34;Chocolate&#34;&#62;Chocolate&#60;/option&#62;
    &#60;option value=&#34;Coconut&#34;&#62;Cocunut&#60;/option&#62;
    &#60;option value=&#34;Mint&#34;&#62;Mint&#60;/option&#62;
    &#60;option value=&#34;Strawberry&#34;&#62;Strawberry&#60;/option&#62;
    &#60;option value=&#34;Vanilla&#34;&#62;Vanilla&#60;/option&#62;
    &#60;option value=&#34;Banana&#34;&#62;Banana&#60;/option&#62;
    &#60;option value=&#34;Peanut&#34;&#62;Peanut&#60;/option&#62;
  &#60;/select&#62;
  &#60;p&#62;
  &#60;input type=&#34;submit&#34;&#62; &#60;input type=&#34;reset&#34;&#62;
&#60;/form&#62;
~~~

By default your web browser will packaged this up and send it using &#34;application/x-www-form-urlencoded&#34;. If you select &#34;Coconut&#34; and &#34;Strawberry&#34; then the service receiving your data will get an encoded document that looks like this.

~~~urlencoding
ice-cream-flavors=Coconut&#38;ice-cream-flavors=Strawberry
~~~

The ampersands separate the key value pairs. The fact that &#34;ice-cream-flavors&#34; name repeats means that the key &#34;ice-cream-flavors&#34; will point to an array of values.  In pretty printed JSON representation is a little clearer.

~~~json
{
    &#34;ice-cream-flavors&#34;: [ &#34;Coconut&#34;, &#34;Strawberry&#34; ]
}
~~~

So far so good. Zero need to enhance the spec. It works and has worked for a very long time. Stability is a good thing. Let&#39;s elaborate a little further.  I&#39;ve added a dish choice for the ice cream, &#34;Sugar Cone&#34; and &#34;Waffle Bowl&#34;. That web form looks like.

~~~html
&#60;form method=&#34;POST&#34;&#62;
&#60;label for=&#34;ice-cream-flavors&#34;&#62;Select the flavor for each scoop of ice cream:&#60;/label&#62;
&#60;select id=&#34;ice-cream-flavors&#34; name=&#34;ice-cream-flavors&#34; multiple&#62;
  &#60;option value=&#34;Chocolate&#34;&#62;Chocolate&#60;/option&#62;
  &#60;option value=&#34;Coconut&#34;&#62;Cocunut&#60;/option&#62;
  &#60;option value=&#34;Mint&#34;&#62;Mint&#60;/option&#62;
  &#60;option value=&#34;Strawberry&#34;&#62;Strawberry&#60;/option&#62;
  &#60;option value=&#34;Vanilla&#34;&#62;Vanilla&#60;/option&#62;
  &#60;option value=&#34;Banana&#34;&#62;Banana&#60;/option&#62;
  &#60;option value=&#34;Peanut&#34;&#62;Peanut&#60;/option&#62;
&#60;/select&#62;
&#60;p&#62;
&#60;fieldset&#62;
  &#60;legend&#62;Pick your delivery dish&#60;/legend&#62;
  &#60;div&#62;
    &#60;input type=&#34;radio&#34; id=&#34;sugar-cone&#34; name=&#34;ice-cream-dish&#34; value=&#34;sugar-cone&#34; /&#62;
    &#60;label for=&#34;sugar-cone&#34;&#62;Sugar Cone&#60;/label&#62;
  &#60;/div&#62;
  &#60;div&#62;
    &#60;input type=&#34;radio&#34; id=&#34;waffle-bowl&#34; name=&#34;ice-cream-dish&#34; value=&#34;waffle-bowl&#34; /&#62;
    &#60;label for=&#34;waffle-bowl&#34;&#62;Waffle Bowl&#60;/label&#62;
  &#60;/div&#62;
&#60;/fieldset&#62;
&#60;input type=&#34;submit&#34;&#62; &#60;input type=&#34;reset&#34;&#62;
&#60;/form&#62;
~~~

If we select &#34;Banana&#34; and &#34;Peanut&#34; flavors served in a &#34;Waffle Bowl&#34; the encoded document would reach the web service looking something like this.

~~~urlencoded
ice-cream-flavors=Banana&#38;ice-cream-flavors=Peanut&#38;ice-cream-dish=waffle-cone
~~~

That&#39;s not too bad. Again this is the state of web form for ages now. In JSON it could be represented as the following.

~~~json
{
    &#34;ice-cream-flavors&#34;: [ &#34;Banana&#34;, &#34;Peanut&#34; ],
    &#34;ice-cream-dish&#34;: &#34;waffle-cone&#34;
}
~~~

This is great we have a simple web form that can collect a single ice cream order.  But what if we want to actually place several individual ice cream orders as one order? Today we have two choices, multiple web forms that accumulate the orders (circa 2000) or use JavaScript create a web UI that can handle list of form elements. Both have their drawbacks.

In the case of the old school approach changing web pages just to update an order can be slow and increase uncertainty about your current order. That is why the JavaScript approach has come to be more common. But that JavaScript approach comes at a huge price. It&#39;s much more complex, we&#39;ve seen a dozens of libraries and frameworks that have come and gone trying to manage that complexity in various ways.

If we supported JSON encoded from submission directly in the web browser I think we&#39;d make a huge step forward. It could decouple the JavaScript requirement. That would avoid much of the cruft that we ship down to the web browser today because we can&#39;t manage lists of things without resorting to JavaScript.

Let&#39;s pretend there was a new input element type called &#34;list-input&#34;. A &#34;list-input&#34; element can contain any combination of today&#39;s basic form elements. Here&#39;s my hypothetical `list-input` based from example. In it we&#39;re going to select the ice cream flavors and the dish format (cone, bowl) as before but have them accumulate in a list. That form could be expressed in HTML similar to my mock up below.

~~~html
&#60;form&#62;
  &#60;label for=&#34;ice-cream-order&#34;&#62;Place your next order, press submit when you have all of them.&#60;/label&#62;
  &#60;list-input id=&#34;ice-cream-order&#34; name=&#34;ice-cream-order&#34;&#62;
    &#60;label for=&#34;ice-cream-flavor&#34;&#62;Select the flavor for each scoop of ice cream:&#60;/label&#62;
    &#60;select id=&#34;ice-cream-flavor&#34; name=&#34;ice-cream-flavor&#34; multiple&#62;
      &#60;option value=&#34;Chocolate&#34;&#62;Chocolate&#60;/option&#62;
      &#60;option value=&#34;Coconut&#34;&#62;Cocunut&#60;/option&#62;
      &#60;option value=&#34;Mint&#34;&#62;Mint&#60;/option&#62;
      &#60;option value=&#34;Strawberry&#34;&#62;Strawberry&#60;/option&#62;
      &#60;option value=&#34;Vanilla&#34;&#62;Vanilla&#60;/option&#62;
      &#60;option value=&#34;Banana&#34;&#62;Banana&#60;/option&#62;
      &#60;option value=&#34;Peanut&#34;&#62;Peanut&#60;/option&#62;
    &#60;/select&#62;
  &#60;p&#62;
  &#60;fieldset&#62;
    &#60;legend&#62;Pick your delivery dish&#60;/legend&#62;
    &#60;div&#62;
      &#60;input type=&#34;radio&#34; id=&#34;sugar-cone&#34; name=&#34;ice-cream-dish&#34; value=&#34;sugar-cone&#34; /&#62;
      &#60;label for=&#34;sugar-cone&#34;&#62;Sugar Cone&#60;/label&#62;
    &#60;/div&#62;
    &#60;div&#62;
      &#60;input type=&#34;radio&#34; id=&#34;waffle-bowl&#34; name=&#34;ice-cream-dish&#34; value=&#34;waffle-bowl&#34; /&#62;
      &#60;label for=&#34;waffle-bowl&#34;&#62;Waffle Bowl&#60;/label&#62;
    &#60;/div&#62;
  &#60;/fieldset&#62;
  &#60;/list-input&#62;
  &#60;input type=&#34;submit&#34;&#62; &#60;input type=&#34;reset&#34;&#62;
&#60;/form&#62;
~~~

With two additional lines of HTML the input form can now support a list of individual ice cream orders. Assuming only urlencoding is supported then how does that get encoded and sent to the web server? Here is an example set of orders

1. vanilla ice cream with a sugar cone
2. chocolate with a waffle bowl

~~~urlencoded
ice-cream-flavors=Vanilla&#38;ice-cream-flavors=Chocolate&#38;ice-cream-dish=sugar-cone&#38;ice-cream-dish=waffle-bowl
~~~

Which flavor goes with which dish?  That&#39;s the problem with urlencoding a list in your web form. We just can&#39;t keep the data alignment manageable.  What if the web browser used JSON encoding? 

~~~json
[
  {
      &#34;ice-cream-flavors&#34;: [ &#34;Vanilla&#34; ],
      &#34;ice-cream-dish&#34;: &#34;sugar-cone&#34;
  },
  {
      &#34;ice-cream-flavors&#34;: [ &#34;Chocolate&#34; ],
      &#34;ice-cream-dish&#34;: &#34;waffle-bowl&#34;
  }
~~~

Suddenly the alignment problem goes away. There is precedence for controlling behavior of the web browser submission through the `enctype` attribute. File upload was addressed by adding support for `multipart/form-data`.  In 2024 and for over the last decade it has been common practice in web services to support JSON data submission. I believe it is time that the web browser also supports this directly. This would allow us to decouple the necessity of using JavaScript in browser as we require today. The form elements already map well to a JSON encoding. If JSON encoding was enabled then adding a element like my &#34;list-input&#34; would make sense.  Otherwise we remain stuck in a world where hypertext markup language remains very limited and can&#39;t live without JavaScript.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/02/23/enhanced_form_handling.md" length="11264" type="text/markdown" />
    </item>    <item>
      <title>Installing pgloader from source</title>
      <link>https://rsdoiel.github.io/blog/2024/02/01/installing-pgloader-from-source.html</link>
      <description>
        <![CDATA[I'm working on macOS at the moment but I don't use Home Brew so the instructions to install pgloader are problematic for me. Except I know pgloader is a Lisp program and once upon a time I had three different Lisps running on a previous Mac.  So what follows is my modified instructions for bringing pgloader up on my current Mac Mini running macOS Sonoma 14.3 with Xcode already installed.
        
        ## Getting your Lisps in order
        
        pgloader is written in common list but the instructions at https://pgloader.readthedocs.io/en/latest/install.html specifically mention compiling with [SBCL](https://sbcl.org) which is one of the Lisps I've used in the past. But SBCL isn't (yet) installed on my machine and SBCL is usually compiled using SBCL but can be compiled using other common lists.  Enter [ECL](https://ecl.common-lisp.dev/), aka Embedded Common-Lisp. ECL compiles via a C compiler including the funky setup that macOS has. This means the prep for my machine should look something like
        
        1. Compile then install ECL
        2. Use ECL to compile SBCL
        3. Install SBCL
        4. Now that we have a working SBCL, follow the instructions to compile pgloader and install
        
        NOTE: pgloader requires some specific configuration of SBCL when SBCL is compiled
        
        ...]]>
      </description>
      <source:markdown># Installing pgloader from source

By R. S. Doiel, 2024-02-01

I&#39;m working on macOS at the moment but I don&#39;t use Home Brew so the instructions to install pgloader are problematic for me. Except I know pgloader is a Lisp program and once upon a time I had three different Lisps running on a previous Mac.  So what follows is my modified instructions for bringing pgloader up on my current Mac Mini running macOS Sonoma 14.3 with Xcode already installed.

## Getting your Lisps in order

pgloader is written in common list but the instructions at https://pgloader.readthedocs.io/en/latest/install.html specifically mention compiling with [SBCL](https://sbcl.org) which is one of the Lisps I&#39;ve used in the past. But SBCL isn&#39;t (yet) installed on my machine and SBCL is usually compiled using SBCL but can be compiled using other common lists.  Enter [ECL](https://ecl.common-lisp.dev/), aka Embedded Common-Lisp. ECL compiles via a C compiler including the funky setup that macOS has. This means the prep for my machine should look something like

1. Compile then install ECL
2. Use ECL to compile SBCL
3. Install SBCL
4. Now that we have a working SBCL, follow the instructions to compile pgloader and install

NOTE: pgloader requires some specific configuration of SBCL when SBCL is compiled

## Getting ECL up and running

This recipe is straight forward. 

1. Review ECL&#39;s current website, find latest releases
2. Clone the Git repository from GitLab for ECL
3. Follow the install documentation and compile ECL then install it

Here&#39;s the steps I took in the shell (I&#39;m installing ECL, SBCL in my home directory)

```
cd
git clone https://gitlab.com/embeddable-common-lisp/ecl.git \
          src/gitlab.com/embeddable-common-lisp/ecl
cd src/gitlab.com/embeddable-common-lisp/ecl
./configure --prefix=$HOME
make
make install
```

## Getting SBCL up and running

To get SBCL up and running I grab the sources using Git then compile it with the options recommended by pgloader as well as the options to compile SBCL with another common lisp, i.e. ECL. (note: the `--xc-host=&#39;ecl&#39;`)

```
cd
git clone git://git.code.sf.net/p/sbcl/sbcl src/git.code.sf.net/p/sbcl/sbcl
cd git clone git://git.code.sf.net/p/sbcl/sbcl
sh make.sh --with-sb-core-compression --with-sb-thread --xc-host=&#39;ecl&#39;
cd ./tests &#38;&#38; sh ./run-tests.sh
cd ..
cd ./doc/manual &#38;&#38; make
cd ..
env INSTALL_ROOT=$HOME sh install.sh
```

At this time SBCL should be available to compile pgloader.

## Install Quicklisp

Quicklisp is a package manager for Lisp. It is used by pgloader so also needs to be installed. We have two lisp on our system but since SBCL is the one I need to work for pgloader I install Quicklisp for SBCL.

1. Check the [Quicklisp website](https://www.quicklisp.org/beta/) and see how things are done (it has been a long time since I did some lisp work)
2. Follow the [instructions](https://www.quicklisp.org/beta/#installation) on the website to install Quicklisp for SBCL

This leaves me with the specific steps

1. Use curl to download quicklisp.lisp
2. Use curl to download the signature file
3. Verify the signature file
4. If OK, load into SBCL
5. From the SBCL repl execute the needed commands

```
curl -O https://beta.quicklisp.org/quicklisp.lisp
curl -O https://beta.quicklisp.org/quicklisp.lisp.asc
gpg --verify quicklisp.lisp.asc quicklisp.lisp
sbcl --load quicklisp.lisp
```

At this point you&#39;re in SBCL repl. You need to issue the follow command

```
(quicklisp-quickstart:install)
(quit)
```


## Compiling pgloader

Once you have SBCL and Quicklisp working you&#39;re now ready to look at the rest of the dependencies. Based on the what other Linux systems required I figure I need to have the following available

- SQLite 3, libsqlite shared library (already installed)
- unzip (already installed)
- make (already installed)
- curl (already installed)
- gawk (already installed)
- freetds-dev (not installed)
- libzip-dev (not installed)

Two libraries aren&#39;t installed on my system. I use Mac Ports so doing a quick search both appear to be available.

```
sudo port search freetds
sudo port search libzip
sudo port install freetds libzip
```


OK, now I think I am ready to build pgloader. Here&#39;s what I need to do.

1. Clone the git repo for pgloader
2. Invoke make with the right options
3. Test installation

```
cd
git git@github.com:dimitri/pgloader.git src/github.com/dimitri/pgloader
cd src/github.com/dimitri/pgloader
make save
./build/bin/pgloader -h
```

If all works well I should see the help/usage text for pgloader. The binary executable
is located in `./build/bin` so I can copy this into place in `$HOME/bin/` directory.

```
cp ./build/bin/pgloader $HOME/bin/
```

Happy Loading.
, &#34;PostgreSQL&#34;</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/02/01/installing-pgloader-from-source.md" length="6516" type="text/markdown" />
    </item>    <item>
      <title>vis for vi and fun</title>
      <link>https://rsdoiel.github.io/blog/2024/01/31/vis-for-vi-and-fun.html</link>
      <description>
        <![CDATA[I've been looking for a `vi` editor that my fingers would be happy with. I learned `vi` when I first encountered Unix in University (1980s). I was a transfer student so didn't get the "introduction to Unix and Emacs" lecture. Everyone used Emacs to edit programs but Emacs to me was not intuitive. I recall having a heck of a time figuring out how to exit the editor! I knew I needed to learn an editor and Unix fast to do my school work. I head to my college bookstore and found two spiral bound books [Unix in a Nutshell](https://openlibrary.org/works/OL8724416W?edition=key%3A/books/OL24392296M) and "Vi/Ed in a Nutshell". They helped remedy my ignorance. I spent the afternoon getting comfortable with Unix and learning the basics in Vi. It became my go to text editor. Somewhere along the line `nvi` came along I used that. Eventually `vim` replaced `nvi` as the default "vi" for most Linux system and adapted again.  I like one featured about `vim` over `nvi`. `vim` does syntax highlighting. I routinely get frustrate with `vim` (my old muscle memory throws me into the help systems, very annoying) so I tend to bounce between `nvi` and `vim` depending on how my eyes feel and frustration level. 
        
        ...]]>
      </description>
      <source:markdown># vis for vi and fun

By R. S. Doiel, 2024-01-31 (updated: 2024-02-02)


I&#39;ve been looking for a `vi` editor that my fingers would be happy with. I learned `vi` when I first encountered Unix in University (1980s). I was a transfer student so didn&#39;t get the &#34;introduction to Unix and Emacs&#34; lecture. Everyone used Emacs to edit programs but Emacs to me was not intuitive. I recall having a heck of a time figuring out how to exit the editor! I knew I needed to learn an editor and Unix fast to do my school work. I head to my college bookstore and found two spiral bound books [Unix in a Nutshell](https://openlibrary.org/works/OL8724416W?edition=key%3A/books/OL24392296M) and &#34;Vi/Ed in a Nutshell&#34;. They helped remedy my ignorance. I spent the afternoon getting comfortable with Unix and learning the basics in Vi. It became my go to text editor. Somewhere along the line `nvi` came along I used that. Eventually `vim` replaced `nvi` as the default &#34;vi&#34; for most Linux system and adapted again.  I like one featured about `vim` over `nvi`. `vim` does syntax highlighting. I routinely get frustrate with `vim` (my old muscle memory throws me into the help systems, very annoying) so I tend to bounce between `nvi` and `vim` depending on how my eyes feel and frustration level. 

## vis, the vi I wished for

Recently I stumbled on `vis`. I find it a  very interesting `vi` implementation. Like `vim` it mostly conforms to the classic mappings of a modal editor built on top of `ed`. But `vis` has some nice twists. First it doesn&#39;t try to be a monolithic systems like Emacs or `vim`. Rather then used an application specific scripting language (e.g. Emacs-lisp, vim-script) it uses Lua 5.2 as its configuration language. For me starting up `vis` feels like starting up `nvi`. It is quick and responsive where my typical `vim` setup feels allot like Visual Studio Code in that it&#39;s loading a whole bunch of things I don&#39;t use. 

Had `vis` just had syntax highlighting I don&#39;t know if I was would switched from `vim`. `neovim` is a better vim but I don&#39;t use it regularly and don&#39;t go out of my way to install it.  `vis` has one compelling feature that pushed me over the edge. One I didn&#39;t expect. `vis` supports [structured regular expressions](http://doc.cat-v.org/bell_labs/structural_regexps/se.pdf &#34;PDF paper explain structured regular expression by Rob Pike&#34;). This is the command language found in Plan 9 editors like [sam](http://sam.cat-v.org/) and [Acme](http://acme.cat-v.org/). The approach to regexp is oriented around streams of characters rather than lines of characters. It does this by supporting the concept of multiple cursors and operating on selections (note the plural) in parallel. This allows a higher degree of transformation, feels like a stream oriented AWK but with simpler syntax for the things you do all the time. It was easiest enough to learn that my finger quickly adapted to it. It does mean that in command mode my search and replace is different than what I used to type. E.g. changing CRLF to LF

```
:1,$x/\r/ c//
```

versus

```
:1,$s/\r//g
```

Just enough different to catch someone who is used to `vim` and `nvi` unaware.

## Be careful what you wish for on Ubuntu

When I decided I want to use `vis` as my preferred &#34;vi&#34; in went and installed it on all my work Ubuntu boxes. What surprised me was that when you install `vis` on an Ubuntu system it winds up becoming the default &#34;vi&#34;. That posed a problem because I hadn&#39;t consulted with the other people who use those machines. I thought I would type `vis` instead of `vi` to use it. Fortunately Ubuntu also provides a means of fixing which alternative programs can be used for things like &#34;vi&#34;.  I reverted the default &#34;vi&#34; to `vim` for my colleagues using the Ubuntu command `update-alternatives` (e.g. `sudo update-alternatives --config vi`). No surprises for them and I still get to use `vis`, I just type the extra &#34;s&#34;. 

## Getting to know structured regular expressions and case swapping

A challenge in making the switch to `vis` is learning a new approach to search and replace. Fortunately Marc Tanner gives you the phrases in his documentation.  Searching for &#34;structured regular expressions&#34; leads to Rob Pike&#39;s paper of the same name. The other thing Marc points out is his choices in implementing `vis`. `vis` is like `vi` meets the Sam editor of Plan 9 fame.  You can try Plan 9 Sam editor by installing [Plan 9 User Space](https://9fans.github.io/plan9port/). Understanding Sam made the transition to `vis` smoother. I recommend reading Rob Pike&#39;s paper on &#34;Structured Regular Expressions&#34;[^1], his &#34;Sam Tutorial&#34;[^2] then keeping the &#34;Sam Cheatsheet&#34;[^3] handy during the transition. The final challenge I ran into in making the switch is the old `vi` stand by for flipping case for letters in visual mode.  In the old `vi` you use the tilde key, `shift+~`. In `vis` you press `g` then `~` to change the case on a letter.  

[^1]: Rob Pike&#39;s [&#34;structured regular expressions&#34;](http://doc.cat-v.org/bell_labs/structural_regexps/se.pdf &#34;PDF document&#34;)
[^2]: [Sam Tutorial](http://doc.cat-v.org/bell_labs/sam_lang_tutorial/sam_tut.pdf &#34;PDF document&#34;)
[^3]: [Sam Cheat Sheet](http://sam.cat-v.org/cheatsheet/ &#34;html document containing an image&#34;)


## A few &#34;thank you&#34; or &#34;how did I stumble on vis?&#34;

I&#39;d like to say thank you to [Marc Andr Tanner](https://github.com/martanne) for writing `vis`, [Glendix](https://www.glendix.org/) for highlighting it and to OS News contributor [pablo_marx](https://www.osnews.com/submissions/?user=pablo_marx) for the story [Glendix: Bringing the Beauty of Plan 9 to Linux](https://www.osnews.com/story/20588/glendix-bringing-the-beauty-of-plan-9-to-linux/). With this I find my fingers are happier.

## Additional resources

- [Marc Andr Tanner](https://www.brain-dump.org/projects/vis/)&#39;s vis project page
- [vis on GitHub](https://github.com/martanne/vis/)
- [vis @ readthedocs](https://vis.readthedocs.io/en/master/vis.html)
- [Vis Wiki](https://github.com/martanne/vis/wiki)
- [GitHub Topic](https://github.com/topics/vis-editor)
- [Plugin collection](https://erf.github.io/vis-plugins/)</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/01/31/vis-for-vi-and-fun.md" length="7855" type="text/markdown" />
    </item>    <item>
      <title>Updated recipe, compiling PostgREST 12.0.2 (M1)</title>
      <link>https://rsdoiel.github.io/blog/2024/01/04/updated-recipe-compiling-postgrest_v12.0.2.html</link>
      <description>
        <![CDATA[These are my updated "quick notes" for compiling PostgREST v12.0.2 on a M1 Mac Mini using the current recommended
        versions of ghc, cabal and stack supplied [GHCup](https://www.haskell.org/ghcup).  When I recently tried to use
        my previous [quick recipe](/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.md) I was disappointed it failed with errors like 
        
        
        ...]]>
      </description>
      <source:markdown># Updated recipe, compile PostgREST 12.0.2 (M1)

by R. S. Doiel, 2024-01-04

These are my updated &#34;quick notes&#34; for compiling PostgREST v12.0.2 on a M1 Mac Mini using the current recommended
versions of ghc, cabal and stack supplied [GHCup](https://www.haskell.org/ghcup).  When I recently tried to use
my previous [quick recipe](/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.md) I was disappointed it failed with errors like 

~~~
Resolving dependencies...
Error: cabal: Could not resolve dependencies:
[__0] trying: postgrest-9.0.1 (user goal)
[__1] next goal: optparse-applicative (dependency of postgrest)
[__1] rejecting: optparse-applicative-0.18.1.0 (conflict: postgrest =&#62;
optparse-applicative&#62;=0.13 &#38;&#38; &#60;0.17)
[__1] skipping: optparse-applicative-0.17.1.0, optparse-applicative-0.17.0.0
(has the same characteristics that caused the previous version to fail:
excluded by constraint &#39;&#62;=0.13 &#38;&#38; &#60;0.17&#39; from &#39;postgrest&#39;)
[__1] trying: optparse-applicative-0.16.1.0
[__2] next goal: directory (dependency of postgrest)
[__2] rejecting: directory-1.3.7.1/installed-1.3.7.1 (conflict: postgrest =&#62;
base&#62;=4.9 &#38;&#38; &#60;4.16, directory =&#62; base==4.17.2.1/installed-4.17.2.1)
[__2] trying: directory-1.3.8.2
[__3] next goal: base (dependency of postgrest)
[__3] rejecting: base-4.17.2.1/installed-4.17.2.1 (conflict: postgrest =&#62;
base&#62;=4.9 &#38;&#38; &#60;4.16)

...

~~~

So this type of output means GHC and Cabal are not finding the versions of things they need
to compile PostgREST. I then tried picking ghc 9.2.8 since the default.nix file indicated
a minimum of ghc 9.2.4.  The `ghcup tui` makes it easy to grab a listed version then set it
as the active one.

I made sure I was working in the v12.0.2 tagged release version of the Git repo for PostgREST.
Then ran the usual suspects for compiling the project. I really wish PostgREST came with 
install from source documentation. It took me a while to think about looking in the default.nix
file for a minimum GHC version. That&#39;s why I am writing this update.

A similar recipe can be used for building PostgREST on Linux.

1. Upgrade [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)
    a. Use the curl example command to install it or `gchup upgrade`
    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)
2. Make sure GHCup is pointing at the version PostgREST v12.0.2 needs, i.e. ghc v9.2.8. I chose to keep &#34;recommended&#34; versions of Cabal and Stack
3. Clone &#60;https://github.com/PostgREST/postgrest&#62; to my local machine
4. Check out the version you want to build, i.e. v12.0.2
5. Run the &#34;usual&#34; Haskell build sequence with cabal
    a. `cabal clean`
    b. `cabal update`
    c. `cabal build`
    d. `cabal install` (I use the `--overwrite-policy=always` option to overwrite my old v11 postgrest install)

Here&#39;s an example of the shell commands I run (I&#39;m assuming you&#39;re installing GHCup for the first time).

~~~
ghcup upgrade
ghcup tui
mkdir -p src/github.com/PostgREST
cd src/github.com/PostgREST
git clone git@github.com:PostgREST/postgrest
cd postgrest
git checkout v12.0.2
cabal clean
cabal update
cabal build
cabal install --overwrite-policy=always
~~~

This will install PostgREST in your `$HOME/.cabal/bin` directory. Make sure
it is in your path (it should be if you&#39;ve sourced the GHCup environment after you installed GHCup).</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2024/01/04/updated-recipe-compiling-postgrest_v12.0.2.md" length="4138" type="text/markdown" />
    </item>    <item>
      <title>Finding Bluesky RSS feeds</title>
      <link>https://rsdoiel.github.io/blog/2023/12/23/finding-blue-sky-rss-feeds.html</link>
      <description>
        <![CDATA[# Find Bluesky RSS Feeds
        
        With the update to [1.60](https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i) of Bluesky we can now follow people on Bluesky via RSS feeds. This makes things much more convienient for me. 
        The RSS feed is visible via the HTML markup on a person's profile page (which are now public). E.g. My Bluesky profile page is
        at <https://bsky.app/profile/rsdoiel.bsky.social> and if you look at that pages HTML markup you'll see a link element in the head
        
        ...]]>
      </description>
      <source:markdown># Find Bluesky RSS Feeds

With the update to [1.60](https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i) of Bluesky we can now follow people on Bluesky via RSS feeds. This makes things much more convienient for me. 
The RSS feed is visible via the HTML markup on a person&#39;s profile page (which are now public). E.g. My Bluesky profile page is
at &#60;https://bsky.app/profile/rsdoiel.bsky.social&#62; and if you look at that pages HTML markup you&#39;ll see a link element in the head

```html
 &#60;link rel=&#34;alternate&#34; type=&#34;application/rss+xml&#34; href=&#34;/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss&#34;&#62;
```

That&#39;s the RSS feed. So now if you want to follow you can expand the URL to 

```
https://bsky.app/profile/did:plc:nbdlhw2imk2m2yqhwxb5ycgy/rss
```

And use if via your feed reader. This is a sweat feature. It allows me to move my reading from visiting the website
to getting updates via my feed reader.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/12/23/finding-blue-sky-rss-feeds.md" length="1784" type="text/markdown" />
    </item>    <item>
      <title>RSS and my web experience</title>
      <link>https://rsdoiel.github.io/blog/2023/12/07/rss-and-my-web-experience.html</link>
      <description>
        <![CDATA[RSS is alive and kicking and Bluesky should support it too. Explore my recipe for reading web news.]]>
      </description>
      <source:markdown># RSS and my web experience

by R. S. Doiel, 2023-12-07

I agree with [Dave Winer](http://scripting.com/2023/12/07/140505.html?title=whyWeWantFeedsInBluesky), Blue Sky should support RSS. Most web systems benefit from supporting RSS. RSS is a base level inter-opt for web software like HTML and JSON can be. While I may not be typical I am an example of a web user who experiences much of the web via RSS. I read blog and site content via RSS. I &#34;follow&#34; my friends and colleagues via RSS. This is true when they blog or when they post in a Mastodon community.  I track academic repositories content for [ETH Zurich Research](https://www.rc-blog.ethz.ch/en/feed) and [Caltech](https://feeds.library.caltech.edu/recent/combined.html) via RSS feeds. I check the weather via NOAA&#39;s [RSS feed](https://www.weather.gov/rss).  News sites often syndicate still via RSS and then Podcasts, if they are actual Podcasts are distributed via RSS.  All this is to say I think RSS is not dead. It remains easy to render can can be easy to consume.  If a website doesn&#39;t provide it it is possible to generate it yourself[1] or find a service to use that does[2]. RSS remains key to how I experience and use the web in 2023.

[1]: Go libraries like [Colly](https://go-colly.org/) and [Gofeeds](https://github.com/mmcdole/gofeed) make it possible to roll your own like the one in skimmer

[2]: https://firesky.tv/ is an example of a service that provides RSS for Bluesky via its raw API, [html2rss](https://html2rss.github.io/) is service that producing RSS feeds for popular sites that don&#39;t include them

My personal approach to feeds is very much tailored to me. It&#39;s probably overkill for most people but it works with my vision and cognitive limitations. He&#39;s the steps I take in feed reading. They essentially decompose a traditional feed reader and allow for more flexibility for my reading pleasure.

1. Maintain a list of feeds in a simple text file
2. Harvest those feeds with [skimmer](https://rsdoiel.github.io/skimmer), Skimmer stores the items in an [SQLite3](https://sqlite.org)
3. I filter the items using SQL and SQLite3 or via an interactive mode provided by Skimmer
4. Render saved items to Markdown with [skim2md](https://rsdoiel.github.io/skimmer/skim2md.1.html)
5. Use [Pandoc](https://pandoc.org) to render the Markdown and view Firefox

The nice thing about this approach is that I can easily script it with Bash or even a Windows bat. I can easily maintain separate lists and separate databases for personal and work related material.  A bonus is the database items can also serve as a corpus for a personal search engine too. If you want to save maintain a public reading list this setup is ideal too. Of course the list of curated items can be transformed into their own RSS feed as well.

[Skimmer](https://rsdoiel.github.io/skimmer/skimmer.1.html) is a deconstructed feed reader. Does that make it post modern feed reader?  Skimmer processes a list of feeds I follow and saves the results in an SQLite 3 database. That database can be used to filter the feeds and flag items as &#34;saved&#34;. Typically I filter by timestamps. Saved items can be processed with `skim2md` to render a markdown document. `skim2md` has an option to include a &#34;save to pocket&#34; button for each item in the output. I use Pandoc to render the page then view that result in Firefox. At my leisure I read the web page and press the &#34;Save to pocket&#34; button any item I want to read later. It&#39;s a very comfortable experience.

Skimmer lead me to think about a personal news page for myself and family. Skimmer lets me curate separate lists organized around themes. These can then be rendered to individual pages like pages of a newspaper. This has been captured in an experimental project I call [Antenna](https://rsdoiel.github.io/antenna). It even includes a feed search feature thanks to [PageFind](https://pagefind.app)</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/12/07/rss-and-my-web-experience.md" length="4505" type="text/markdown" />
    </item>    <item>
      <title>Postgres Quick Notes, take two</title>
      <link>https://rsdoiel.github.io/blog/2023/11/17/PostgreSQL-Quick-Notes.html</link>
      <description>
        <![CDATA[A collection of quick notes for setting and Postgres for development.]]>
      </description>
      <source:markdown># Postgres Quick Notes, take two

By R. S. Doiel, 2023-11-17

What follows is some quick notes to remind me of the things I do when
I setup a new instance of PostgreSQL on the various machines I work with.

## Installation approach

If possible I install Postgres with the system&#39;s package manager or follow
the directions suggested for installation on the [Postgres website](https://postgres.org).

### macOS and Postgres

For macOS that&#39;s not the route I take if possible is to install via [Postgres App](https://postgresapp.com/).
This provides a very nice setup of developing with Postgres on macOS and also allows you to easily
test multiple versions of Postgres.  It is not as convenient in the Mac Mini headless configuration
I also use Postgres on macOS in. In that case I use Mac Ports&#39; package manager to install Postgres.
Unfortunately just using ports command isn&#39;t enough to get running. What follows is my notes on the
additional steps I&#39;ve taken to get things working.

Install the version of Postgres you want (e.g. PostgreSQL 16) via ports

1. install postgresql16, postgresql16-server, postgres_select
2. make sure the postgres version is selected using the ports command
3. make a directory for the default postgres db
4. make sure the default db directory is owned by the postgres user
5. run the initialization scripts provided by the posts installer
6. use the ports command to load the plist
7. start up the server, make sure the log file is writable

Here&#39;s the commands I type in the shell

~~~shell
sudo port install postgresql16-server postgresql16 postgresql_select
# Answer y to the prompt
# After the install completes Ports will suggest the following to complete the process.
sudo port select postgresql postgresql16
sudo mkdir -p /opt/local/var/db/postgresql16/defaultdb
sudo chown postgres:postgres /opt/local/var/db/postgresql16/defaultdb
sudo -u postgres /bin/sh -c &#39;cd /opt/local/var/db/postgresql16 &#38;&#38; /opt/local/lib/postgresql16/bin/initdb -D /opt/local/var/db/postgresql16/defaultdb&#39;
sudo port load postgresql16-server
sudo -u postgres /bin/sh -c &#39;/opt/local/lib/postgresql16/bin/pg_ctl -D /opt/local/var/db/postgresql16/defaultdb -l /opt/local/var/log/postgresql16/postgres.log start&#39;
~~~

## Database users setup

This applies to most Postgres installations I do because I am using them to
develop software solutions. In a production setting you&#39;d want a more conservative
security approach.

1. Make sure you can connect as the postgres user
2.  For each developer
    a. Use the Postgres createuser tool to create superuser account(s)
    b. Use the Postgres createdb tool to create databases for those account(s)

Here&#39;s the commands I type in the shell

~~~shell
sudo -u postgres psql
~~~

When in the psql shell you should be able to use the slash commands like

\\l
: list the databases

\\dt
: list the tables in the database

\\d TABLE\_NAME
: list the schema for TABLE\_NAME

\\q
: quit the psql shell

Assuming we have a working Postgres I now create superuser accounts for
development and databases that match the username.

~~~shell
sudo -u postgres createuser --interactive $USER
createdb $USER
~~~

I should now be able to run the psql shell without specifying the
postgres username.

~~~shell
psql
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/11/17/PostgreSQL-Quick-Notes.md" length="3707" type="text/markdown" />
    </item>    <item>
      <title>Building A to Z list pages in Pandoc</title>
      <link>https://rsdoiel.github.io/blog/2023/10/18/A-to-Z-lists.html</link>
      <description>
        <![CDATA[Pandoc offers a very good template system. It avoids elaborate features in favor of a few simple ways to bring content into the page.  It knows how to use data specified in front matter (a YAML header to a Markdown document) as well as how to merge in JSON or YAML from a metadata file.  One use case that is common in libraries and archives that less obvious of how to handle is building A to Z lists or year/date oriented listings where you have a set of navigation links at the top of the page followed by a set of H2 headers with UL lists between them.  In JSON the typical data presentation would look something like
        
        ...]]>
      </description>
      <source:markdown># Building A to Z lists pages in Pandoc

By R. S. Doiel, 2023-10-18

Pandoc offers a very good template system. It avoids elaborate features in favor of a few simple ways to bring content into the page.  It knows how to use data specified in front matter (a YAML header to a Markdown document) as well as how to merge in JSON or YAML from a metadata file.  One use case that is common in libraries and archives that less obvious of how to handle is building A to Z lists or year/date oriented listings where you have a set of navigation links at the top of the page followed by a set of H2 headers with UL lists between them.  In JSON the typical data presentation would look something like

```json
{
  &#34;a_to_z&#34;: [ &#34;A&#34;, &#34;B&#34;],
  &#34;content&#34;: {
    &#34;A&#34;: [
      &#34;After a beautiful day&#34;,
      &#34;Afterlife&#34;
    ],
    &#34;B&#34;: [
      &#34;Better day after&#34;,
      &#34;Better Life&#34;
    ]
  }
}
```

The trouble is that while YAMLs outer dictionary (key/value map) works fine in Pandoc templates there is no way for the the for loop to handle maps of maps like we have above.  Pandoc templates really want to iterate over arrays of objects . Thats nice thing! It gives us more ways to transform the data to provide more flexibility in our template implementation. Heres how I would restructure the previous JSON to make it easy to process via Pandocs template engine.  Note how Ive taken our simple array of letters and turned them into an object with an href and label attribute. Similarly Ive enhanced the content objects.

```json
{
  &#34;a_to_z&#34;: [ {&#34;href&#34;: &#34;A&#34;, &#34;label&#34;: &#34;A&#34;}, {&#34;href&#34;: &#34;B&#34;, &#34;label&#34;: &#34;B&#34;} ],
  &#34;content&#34;: [
      {&#34;letter&#34;: &#34;A&#34;, &#34;title&#34;: &#34;After a beautiful day&#34;, &#34;id&#34;: &#34;after-a-beautiful-day&#34;},
      {&#34;title&#34;: &#34;Afterlife&#34;, &#34;id&#34;: &#34;afterlife&#34;},
      {&#34;letter&#34;: &#34;B&#34;, &#34;title&#34;: &#34;Better day after&#34;, &#34;id&#34;: &#34;better-day-after&#34;},
      {&#34;title&#34;: &#34;Better Life&#34;, &#34;id&#34;: &#34;better-life&#34;}
  ]
}
```

Then the template can be structure something like

```
&#60;menu&#62;
${for(a_to_z)}
${if(it.href)}&#60;li&#62;&#60;a href=&#34;${it.href}&#34;&#62;${it.label}&#60;/a&#62;&#60;/li&#62;${endif}
${endfor}
&#60;/menu&#62;

${for(content)}
${if(it.letter)}

## &#60;a id=&#34;${it.letter}&#34; name=&#34;${it.letter}&#34;&#62;${it.letter}&#60;/a&#62;

${endif}
- [${it.name}](${it.id})
${endfor}

```

There is one gotcha in A to Z list generation. A YAML parser may convert a bare N to false (and presumable Y will become true). This is really annoying. The way to avoid this is to add a space to the letter in your JSON output. This will insure that the N or Y arent converted to the boolean values true and false. Pandocs template engine is smart enough to trim leading and trailing spaces.

Finally this technique can be used to produce lists and navigation that are based around years, months, or other iterative types but that is left as an exercise to the reader.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/10/18/A-to-Z-lists.md" length="3868" type="text/markdown" />
    </item>    <item>
      <title>Skimmer</title>
      <link>https://rsdoiel.github.io/blog/2023/10/06/concept.html</link>
      <description>
        <![CDATA[have a problem. I like to read my feeds in newsboat but I can't seem to get it working on a few machines I use.
        I miss having access to read feeds. Additionally there are times I would like to read my feeds in the same way
        I read twtxt feeds using `yarnc timeline | less -R`. Just get a list of all items in reverse chronological order.
        
        I am not interested in reinventing newsboat, it does a really good job, but I do want an option where newsboat isn't
        available or is not not convenient to use.  This lead me to think about an experiment I am calling skimmer
        . Something that works with RSS, Atom and jsonfeeds in the same way I use `yarnc timeline | less -R`.  
        I'm also inspired by Dave Winer's a river of news site and his outline tooling. But in this case I don't want
        an output style output, just a simple list of items in reverse chronological order. I'm thinking of a more
        ephemeral experience in reading.
        
        This has left me with some questions.
        
        ...]]>
      </description>
      <source:markdown># skimmer

By R. S. Doiel, 2023-10-06

I have a problem. I like to read my feeds in newsboat but I can&#39;t seem to get it working on a few machines I use.
I miss having access to read feeds. Additionally there are times I would like to read my feeds in the same way
I read twtxt feeds using `yarnc timeline | less -R`. Just get a list of all items in reverse chronological order.

I am not interested in reinventing newsboat, it does a really good job, but I do want an option where newsboat isn&#39;t
available or is not not convenient to use.  This lead me to think about an experiment I am calling skimmer
. Something that works with RSS, Atom and jsonfeeds in the same way I use `yarnc timeline | less -R`.  
I&#39;m also inspired by Dave Winer&#39;s a river of news site and his outline tooling. But in this case I don&#39;t want
an output style output, just a simple list of items in reverse chronological order. I&#39;m thinking of a more
ephemeral experience in reading.

This has left me with some questions.

- How simple is would it be to write skimmer?
- How much effort would be required to maintain it?
- Could this tool incorporate support for other feed types, e.g. twtxt, Gopher, Gemini?

There is a Go package called [gofeed](https://github.com/mmcdole/gofeed). The README describes it
as a &#34;universal&#34; feed reading parser. That seems like a good starting point and picking a very narrowly
focus task seems like a way to keep the experiment simple to implement.

## Design issues

The reader tool needs to output to standard out in the same manner as `yarnc timeline` does. The goal isn&#39;t
to be newsboat, or river of news, drummer, or Lynx but to present a stream of items usefully formatted to read
from standard output.

Some design ideas

1. Feeds should be fetched by the same tool as the reader but that should be done explicitly (downloads can take a while)
2. I want to honor that RSS does not require titles! I need to handle that case gracefully
3. For a given list of feed URLs I want to save the content in a SQLite3 database (useful down the road)
4. I&#39;d like the simplicity of newsboat&#39;s URL list but I want to eventually support OPML import/export

# Skimmer, a thin wrapper around gofeed

In terms of working with RSS, Atom and JSON feeds the [gofeed](https://github.com/mmcdole/gofeed) takes care of
all the heavy lifting in parsing that content. The go http package provides a reliable client.
There is a pure Go package, [go-sqlite](), for integrating with SQLite 3 database. The real task is knitting this
together and a convenient package.

Here&#39;s some ideas about behavior.

To configure skimmer you just run the command. It&#39;ll create a directory at `$HOME/.skimmer` to store configuration
much like newsboat does with `$HOME/.newsboat`.

~~~
skimmer
~~~

A default URL list to be created so when running the command you have something to fetch and read.

Since fetching feed content can be slow (this is true of all news readers I&#39;ve used) I think you should have to
explicitly say fetch.

~~~
skimmer -fetch
~~~

This would read the URLs in the URL list and populate a simple SQLite 3 database table. Then running skimmer again 
would display any harvested content (or running skimmer in another terminal session).

Since we&#39;re accumulating data in a database there are some house keep chores like prune that need to be supported.
Initial this can be very simple and if the experiment move forward I can improve them over time. I want something
like saying prune everything up to today.

~~~
skimmer -prune today
~~~

There are times I just want to limit the number of items displayed so a limit options makes sense

~~~
skimmer -limit 10
~~~

Since I am displaying to standard out I should be able to output via Pandoc to pretty print the content.

~~~
skimmer -limit 50 | pandoc -t markdown -f plain | less -R
~~~

That seems a like a good set of design features for an initial experiment.

## Proof of concept implementation

Spending a little time this evening. I&#39;ve release a proof of concept on GitHub
at &#60;https://github.com/rsdoiel/skimmer&#62;, you can read the initial documentation
at [skimmer](https://rsdoiel.github.io/skimmer).</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/10/06/concept.md" length="5508" type="text/markdown" />
    </item>    <item>
      <title>Quick recipe, compiling Pandoc (M1)</title>
      <link>https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-Pandoc-M1.html</link>
      <description>
        <![CDATA[These are my quick notes for compiling Pandoc on a M1 Mac Mini. I use a similar recipe for building Pandoc on Linux (NOTE: the challenges with libiconv and Mac Ports' libiconv below if you get a build error).
        
        1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)
            a. Use the curl example command to install it
            b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)
        2. Make sure GHCup is pointing at the "recommended" versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)
        3. Clone <https://github.com/jgm/pandoc> to your local machine
        4. Check out the version you want to build (e.g. 3.1.4)
        5. Run the "usual" Haskell build sequence with cabal per Pandoc's installation documentation for building from source
            a. `cabal clean`
            b. `cabal update`
            c. `cabal install pandoc-cli`
        
        Here's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).
        
        ...]]>
      </description>
      <source:markdown># Quick recipe, compile Pandoc (M1)

These are my quick notes for compiling Pandoc on a M1 Mac Mini. I use a similar recipe for building Pandoc on Linux (NOTE: the challenges with libiconv and Mac Ports&#39; libiconv below if you get a build error).

1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)
    a. Use the curl example command to install it
    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)
2. Make sure GHCup is pointing at the &#34;recommended&#34; versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)
3. Clone &#60;https://github.com/jgm/pandoc&#62; to your local machine
4. Check out the version you want to build (e.g. 3.1.4)
5. Run the &#34;usual&#34; Haskell build sequence with cabal per Pandoc&#39;s installation documentation for building from source
    a. `cabal clean`
    b. `cabal update`
    c. `cabal install pandoc-cli`

Here&#39;s an example of the shell commands I run (I&#39;m assuming you&#39;re installing GHCup for the first time).

~~~
curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh
source $HOME/.gchup/env
ghcup tui
mkdir -p src/github.com/jgm/pandoc
cd src/github.com/jgm/pandoc
git clone git@github.com:jgm/pandoc
cd pandoc
git checkout 3.1.4
cabal clean
cabal update
cabal install pandoc-cli
~~~

This will install Pandoc in your `$HOME/.cabal/bin` directory. Make sure
it is in your path (it should be if you&#39;ve sourced the GHCup environment after you installed GHCup).

## libiconv compile issues

If you use Mac Ports it can confuse Cabal/Haskell which one to link to. You&#39;ll get an error talking about undefined symbols and iconv.  To get a clean compile I&#39;ve typically worked around this issue by removing the Mac Ports installed libiconv temporarily (e.g. `sudo port uninstall libiconv`, an using the &#34;all&#34; option when prompted).  After I&#39;ve got a clean install of Pandoc then I re-install libiconv for those Ports based applications that need it. Putting libiconv back is important, as Mac Ports version of Git expects it.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-Pandoc-M1.md" length="3494" type="text/markdown" />
    </item>    <item>
      <title>Quick recipe, compiling PostgREST (M1)</title>
      <link>https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.html</link>
      <description>
        <![CDATA[These are my quick notes for compiling PostgREST on a M1 Mac Mini. I use a similar recipe for building PostgREST on Linux.
        
        1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)
            a. Use the curl example command to install it
            b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)
        2. Make sure GHCup is pointing at the "recommended" versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)
        3. Clone <https://github.com/PostgREST/postgrest> to your local machine
        4. Check out the version you want to build (e.g. v11.1.0)
        5. Run the "usual" Haskell build sequence with cabal
            a. `cabal clean`
            b. `cabal update`
            c. `cabal build`
            d. `cabal install`
        
        Here's an example of the shell commands I run (I'm assuming you're installing GHCup for the first time).
        
        ...]]>
      </description>
      <source:markdown># Quick recipe, compile PostgREST (M1)

These are my quick notes for compiling PostgREST on a M1 Mac Mini. I use a similar recipe for building PostgREST on Linux.

1. Install [GHCup](https://www.haskell.org/ghcup/) to get a good Haskell setup (I accept all the default choices)
    a. Use the curl example command to install it
    b. Make sure the environment is active (e.g. source `$HOME/.ghcup/env`)
2. Make sure GHCup is pointing at the &#34;recommended&#34; versions of GHC, Cabal, etc. (others may work but I prefer the stable releases)
3. Clone &#60;https://github.com/PostgREST/postgrest&#62; to your local machine
4. Check out the version you want to build (e.g. v11.1.0)
5. Run the &#34;usual&#34; Haskell build sequence with cabal
    a. `cabal clean`
    b. `cabal update`
    c. `cabal build`
    d. `cabal install`

Here&#39;s an example of the shell commands I run (I&#39;m assuming you&#39;re installing GHCup for the first time).

~~~
curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://get-ghcup.haskell.org | sh
source $HOME/.gchup/env
ghcup tui
mkdir -p src/github.com/PostgREST
cd src/github.com/PostgREST
git clone git@github.com:PostgREST/postgrest
cd postgrest
cabal clean
cabal update
cabal build
cabal install
~~~

This will install PostgREST in your `$HOME/.cabal/bin` directory. Make sure
it is in your path (it should be if you&#39;ve sourced the GHCup environment after you installed GHCup).</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/07/05/quick-recipe-compiling-PostgREST-M1.md" length="2670" type="text/markdown" />
    </item>    <item>
      <title>gsettings command</title>
      <link>https://rsdoiel.github.io/blog/2023/05/20/gsettings-commands.html</link>
      <description>
        <![CDATA[# gsettings command
        
        One of the things I find annoying about Ubuntu Desktop defaults is that when I open a new application it opens in the upper left corner. I then drag it to the center screen and start working. It's amazing how a small inconvenience can grind on you over time.  When I've search the net for changing this behavior the usual suggestions are "install gnome-tweaks". This seems ham-handed. I think continue searching and eventually find the command below. So I am making a note of the command here in my blog so I can find it latter.
        
        ~~~
        gsettings set org.gnome.mutter center-new-window true
        ~~~]]>
      </description>
      <source:markdown># gsettings command

One of the things I find annoying about Ubuntu Desktop defaults is that when I open a new application it opens in the upper left corner. I then drag it to the center screen and start working. It&#39;s amazing how a small inconvenience can grind on you over time.  When I&#39;ve search the net for changing this behavior the usual suggestions are &#34;install gnome-tweaks&#34;. This seems ham-handed. I think continue searching and eventually find the command below. So I am making a note of the command here in my blog so I can find it latter.

~~~
gsettings set org.gnome.mutter center-new-window true
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/05/20/gsettings-commands.md" length="1648" type="text/markdown" />
    </item>    <item>
      <title>First Personal Search Engine Prototype</title>
      <link>https://rsdoiel.github.io/blog/2023/03/10/first-prototype-pse.html</link>
      <description>
        <![CDATA['ve implemented a first prototype of my personal search engine which
        I will abbreviate as "pse" from here on out. I implemented it using 
        three [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) scripts
        relying on [sqlite3](https://sqlite.org), [wget](https://en.wikipedia.org/wiki/Wget) and [PageFind](https://pagefind.app) to do the heavy lifting.
        
        Both Firefox and newsboat store useful information in sqlite databases.  Firefox's `moz_places.sqlite` holds both all the URLs visited as well as those that are associated with bookmarks (i.e. the SQLite database `moz_bookmarks.sqlite`).  I had about 2000 bookmarks, less than I thought with many being stale from link rot. Stale page URLs really slow down the harvest process because of the need for wget to wait on various timeouts (e.g. DNS, server response, download times).  The "history" URLs would make an interesting collection to spider but you'd probably want to have an exclude list (e.g. there's no point in saving queries to search engines, web mail, shopping sites). Exploring that will wait for another prototype.
        
        ...]]>
      </description>
      <source:markdown># First Personal Search Engine Prototype

By R. S. Doiel, 2023-08-10

I&#39;ve implemented a first prototype of my personal search engine which
I will abbreviate as &#34;pse&#34; from here on out. I implemented it using 
three [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) scripts
relying on [sqlite3](https://sqlite.org), [wget](https://en.wikipedia.org/wiki/Wget) and [PageFind](https://pagefind.app) to do the heavy lifting.

Both Firefox and newsboat store useful information in sqlite databases.  Firefox&#39;s `moz_places.sqlite` holds both all the URLs visited as well as those that are associated with bookmarks (i.e. the SQLite database `moz_bookmarks.sqlite`).  I had about 2000 bookmarks, less than I thought with many being stale from link rot. Stale page URLs really slow down the harvest process because of the need for wget to wait on various timeouts (e.g. DNS, server response, download times).  The &#34;history&#34; URLs would make an interesting collection to spider but you&#39;d probably want to have an exclude list (e.g. there&#39;s no point in saving queries to search engines, web mail, shopping sites). Exploring that will wait for another prototype.

The `cache.db` associated with Newsboat provided a rich resource of content and much fewer stale links (not surprising because I maintain that URL list more much activity then reviewing my bookmarks).  Between the two I had 16,000 pages. I used SQLite 3 to query the url values from the various DB into sorting for unique URLs into a single text file one URL per line.

The next thing after creating a list of pages I wanted to search was to download them into a directory using wget.  Wget has many options, I choose to enable timestamping, create a protocol directory and then a domain and path directory for each item. This has the advantage of being able to transform the Path into a URL later.

Once the content was harvested I then used PageFind to index the all the harvested content. Since I started using PageFind originally the tool has gained an option called `--serve` which provides a localhost web service on port 1414.  All I needed to do was add an index.html file to the directory where I harvested the content and saved the PageFind indexes. Then I used PageFind to again to provide a localhost based personal search engine.

While the total number of pages was small (16k pages) I did find interesting results just trying out random words. This makes the prototype look promising.

## Current prototype components

I have simple Bash script that gets the URLs from both Firefox bookmarks and Newsboat&#39;s cache then generates a single text file of unique URLs I&#39;ve named &#34;pages.txt&#34;.

I then use the &#34;pages.txt&#34; file to harvest content with wget into a tree structure like 

- htdocs
    - http (all the http based URLs I harvest go in here)
    - https (all the https based URLs I harvest go in here)
    - pagefind (this holds the PageFind indexes and JavaScript to implement the search UI)
    - index.html (this holds the webpage for the search UI using the libraries in `pagefind`)

Since I&#39;m only downloaded the HTML the 16k pages does not take up significant disk space yet.

## Prototype Implementation

Here&#39;s the bash scripts I use to get the URLs, harvest content and launch my localhost search engine based on PageFind.

Get the URLs I want to be searchable. I use to environment variables
for finding the various SQLite 3 databases (i.e. PSE_MOZ_PLACES, PSE_NEWSBOAT).

~~~
#!/bin/bash

if [ &#34;$PSE_MOZ_PLACES&#34; = &#34;&#34; ]; then
    printf &#34;the PSE_MOZ_PLACES environment variable is not set.&#34;
    exit 1
fi
if [ &#34;$PSE_NEWSBOAT&#34; = &#34;&#34; ]; then
    printf &#34;the PSE_NEWSBOAT environment variable is not set.&#34;
    exit 1
fi

sqlite3 &#34;$PSE_MOZ_PLACES&#34; \
    &#39;SELECT moz_places.url AS url FROM moz_bookmarks JOIN moz_places ON moz_bookmarks.fk = moz_places.id WHERE moz_bookmarks.type = 1 AND moz_bookmarks.fk IS NOT NULL&#39; \
    &#62;moz_places.txt
sqlite3 &#34;$PSE_NEWSBOAT&#34; &#39;SELECT url FROM rss_item&#39; &#62;newsboat.txt
cat moz_places.txt newsboat.txt |
    grep -E &#39;^(http|https):&#39; |
    grep -v &#39;://127.0.&#39; |
    grep -v &#39;://192.&#39; |
    grep -v &#39;view-source:&#39; |
    sort -u &#62;pages.txt
~~~

The next step is to have the pages. I use wget for that.

~~~
#!/bin/bash
#
if [ ! -f &#34;pages.txt&#34; ]; then
    echo &#34;missing pages.txt, skipping harvest&#34;
    exit 1
fi
echo &#34;Output is logged to pages.log&#34;
wget --input-file pages.txt \
    --timestamping \
    --append-output pages.log \
    --directory-prefix htdocs \
    --max-redirect=5 \
    --force-directories \
    --protocol-directories \
    --convert-links \
    --no-cache --no-cookies
~~~

Finally I have a bash script that generates the index.html page, an Open Search Description XML file, indexes the harvested sites and launches PageFind in server mode.

~~~
#!/bin/bash
mkdir -p htdocs

cat &#60;&#60;OPENSEARCH_XML &#62;htdocs/pse.osdx
&#60;OpenSearchDescription xmlns=&#34;http://a9.com/-/spec/opensearch/1.1/&#34;
                       xmlns:moz=&#34;http://www.mozilla.org/2006/browser/search/&#34;&#62;
  &#60;ShortName&#62;PSE&#60;/ShortName&#62;
  &#60;Description&#62;A Personal Search Engine implemented via wget and PageFind&#60;/Description&#62;
  &#60;InputEncoding&#62;UTF-8&#60;/InputEncoding&#62;
  &#60;Url rel=&#34;self&#34; type=&#34;text/html&#34; method=&#34;get&#34; template=&#34;http://localhost:1414/index.html?q={searchTerms}&#34; /&#62;
  &#60;moz:SearchForm&#62;http://localhost:1414/index.html&#60;/moz:SearchForm&#62;
&#60;/OpenSearchDescription&#62;
OPENSEARCH_XML

cat &#60;&#60;HTML &#62;htdocs/index.html
&#60;html&#62;
&#60;head&#62;
&#60;link
  rel=&#34;search&#34;
  type=&#34;application/opensearchdescription+xml&#34;
  title=&#34;A Personal Search Engine&#34;
  href=&#34;http://localhost:1414/pse.osdx&#34; /&#62;
&#60;link href=&#34;/pagefind/pagefind-ui.css&#34; rel=&#34;stylesheet&#34;&#62;
&#60;/head&#62;
&#60;body&#62;
&#60;h1&#62;A personal search engine&#60;/h1&#62;
&#60;div id=&#34;search&#34;&#62;&#60;/div&#62;
&#60;script src=&#34;/pagefind/pagefind-ui.js&#34; type=&#34;text/javascript&#34;&#62;&#60;/script&#62;
&#60;script&#62;
    window.addEventListener(&#39;DOMContentLoaded&#39;, function(event) {
		let page_url = new URL(window.location.href),
    	    query_string = page_url.searchParams.get(&#39;q&#39;),
      		pse = new PagefindUI({ element: &#34;#search&#34; });
		if (query_string !== null) {
			pse.triggerSearch(query_string);
		}
    });
&#60;/script&#62;
&#60;/body&#62;
&#60;/html&#62;
HTML

pagefind \
--source htdocs \
--serve
~~~

Then I just language my web browser pointing at `http://localhost:1414/index.html`. I can even pass the URL a `?q=...` query string if I want.

From a functionality point of view this is very bare bones and I don&#39;t think 16K pages is enough to make it compelling (I think I need closer to 100K for that).

## What I learned from the prototype so far

This prototype suffers from several limitations.

1. Stale links in my pages.txt make the harvest process really really slow, I need to have a way to avoid stale links getting into the pages.txt or have them removed from the pages.txt
2. PageFind&#39;s result display uses the pages I downloaded to my local machine. It would be better if the result link was translated to point at the actual source of the pages, I think this can be done via JavaScript in my index.html when I setup the PageFind search/results element. Needs more exploration

16K pages is a very tiny corpus. I get interesting results from my testing but not good enough to make me use first.  I&#39;m guessing I need a corpus of at least 100K pages to be compelling for first search use.

It is really nice having a localhost personal search engine. It means that I can keep working with my home network connection is problematic. I like that. Since the website generated for my localhost system is a &#34;static site&#34; I could easily replicate that to net and make it available to other machines.

Right now the big time sync is harvesting content to index. I&#39;m not certain yet how much space disk space will be needed for my 100K page target corpus.

Setting up indexing and the search UI were the easiest part of the process.  PageFind is so easy to work with compare to enterprise search applications.

## Things to explore

I can think of several ways to enlarge my search corpus. The first is there are a few websites I use for reference that are small enough to mirror. Wget provides a mirror function. Working from a &#34;sites.txt&#34; list I could mirror those sites periodically and have their content available for indexing.

When experimenting with the mirror option I notice I wind up with PDF that are linked in the pages being mirrored.  If I used the Unix find command to locate all the PDF I could use another tool to extract there text.  Doing that would enlarge my search beyond plain text and HTML.  I would need to think this through as ultimately I&#39;d want to be able to recover the path to the PDF when those results are displayed.

Another approach would be to work with my full web browsers&#39; history as
well as it&#39;s bookmarks. This would significantly expand the corpus. If I did this I could also check the &#34;head&#34; of the HTML for references to feeds that could be folded into my feed link harvests. This would have the advantage of capture content from sources I find useful to read but would catch blog posts I might have skipped due to limited reading time.

I use Pocket to read the pages I find interesting in my feed reader.  Pocket has an API and I could get some additional interesting pages from it. Pocket also has various curated lists and they might have interesting pages to harvest and index. I think the trick would be to use those suggests against an exclude list of some sort. E.g. Makes not sense to try to harvest paywall stuff or commercial sites more generally. One of the values I see in pse is that it is a personal search engine not a replacement for commercial search engines generally.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/03/10/first-prototype-pse.md" length="11178" type="text/markdown" />
    </item>    <item>
      <title>Prototyping a personal search engine</title>
      <link>https://rsdoiel.github.io/blog/2023/03/07/prototyping-a-personal-search-engine.html</link>
      <description>
        <![CDATA[> Do we really need a search engine to index the "whole web"? Maybe a curated subset is better.
        
        Alex Schreoder's post [A Vision for Search](https://alexschroeder.ch/wiki/2023-03-07_A_vision_for_search) prompted me to write up an idea I call a "personal search engine".   I've been thinking about a "a personal search engine" for years, maybe a decade.
        
        With the current state of brokenness in commercial search engines, especially with the implosion of the commercial social media platforms, we have an opportunity to re-think search on a more personal level.
        
        The tooling around static site generation where a personal search is an extension of your own website suggests a path out of the quagmire of commercial search engines.  Can techniques I use for my own site search, be extended into a personal search engine?
        
        ...]]>
      </description>
      <source:markdown># Prototyping a personal search engine

By R. S. Doiel, 2023-03-07

&#62; Do we really need a search engine to index the &#34;whole web&#34;? Maybe a curated subset is better.

Alex Schreoder&#39;s post [A Vision for Search](https://alexschroeder.ch/wiki/2023-03-07_A_vision_for_search) prompted me to write up an idea I call a &#34;personal search engine&#34;.   I&#39;ve been thinking about a &#34;a personal search engine&#34; for years, maybe a decade.

With the current state of brokenness in commercial search engines, especially with the implosion of the commercial social media platforms, we have an opportunity to re-think search on a more personal level.

The tooling around static site generation where a personal search is an extension of your own website suggests a path out of the quagmire of commercial search engines.  Can techniques I use for my own site search, be extended into a personal search engine?

## A Broken Cycle

Search engines happened pretty early on in the web. If my memory is correct they showed up with the arrival of support for [CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface &#34;Common Gateway Interface&#34;) in early web server software. Remembering back through the decades I see a pattern.

1. Someone comes up with a clever way to index web content and determine relevancy
2. They index enough the web to be interesting and attract early adopters
3. They go mainstream, this compels them to have a business model, usually some form of ad-tech
4. They index an ever larger portion of the web, the results from the search engine starts to degrade
5. The business model becomes the primary focus of the company, the indexing gets exploited (e.g. content farms, page hijacking), the search results degrade.

Stage four and five can be summed up as the &#34;bad search engine stage&#34;. When things get bad enough a new search engine comes on the scene and the early adopters jump ship and the cycle repeats. This was well established by the time some graduate students at Stanford invented page rank. I think it is happening now with search integrated ChatGPT.

I think we&#39;re at the point in the cycle where there is an opportunity for something new. Maybe even break the loop entirely.

## How do I use search?

My use of search engines can be described in four broad categories.

1. Look for a specific answer queries
    - `spelling of &#60;VALUE&#62;`
    - `meaning of &#60;VALUE&#62;`
    - `location of &#60;VALUE&#62;`
    - `convert &#60;UNIT&#62; from &#60;VALUE&#62; to &#60;UNIT&#62;`
2. Shopping queries
    - pricing an item
    - finding an item
    - finding marketing material on an item
3. Look for subject information
    - a topic search
    - news event
    - algorithms
4. Look for information I know exists
    - technical documentation
    - an article I read
    - an article I want to read next

Most of my searches are either subject information or retrieving something I know exists. Both are particularly susceptible to degradation when the business model comes to dominate the search results.

A personal search engine for me would address these four types of searches before I reach for alternatives. In the mean time I&#39;m stuck attempting to mitigate the bad search experience as best I can.

## Mitigating the bad search engine experience

&#62; As commercial search engines degrade I rely on a given website&#39;s own search more

I&#39;ve noticed the follow search behavior practice changes in my own web usage.  For shopping I tend to go to the vendors I trust and use their searches on their websites.  To learn about a place, it&#39;s Wikipedia and if I trying to get a sense of going there I&#39;ll probably rely on an Open Street Map to avoid the ad-tech in commercial services. I dread using the commercial maps because usage correlates so strongly with the spam I encounter the next time I use an internet connected device.

For spelling and dictionary I can use Wiktionary. Location information I use Wikipedia and Open Street Maps. Weather info I have links into [NOAA](https://www.weather.gov/) website. Do I really need to use the commercial services?

It seems obvious that the commercial services for me are at best a fallback experience. They are no longer the &#34;go to&#34; place on the web to find stuff. I miss the convenience of using my web browsers URL box as a search box but the noise of the commercial search engines means the convenience is not worth the cost.

What I would really like is a search service that integrated **my trusted sources** with a single search box but without the noise of the commercial sites. Is this possible? How much work would it be?

I think a personal (or even small group) search engine is plausible and desirable. I think we can build a prototype with some off the shelf parts.

## Observations

1. I only need to index a tiny subset of the web, I don&#39;t want a web crawler that needs to be monitored and managed
2. The audience of the search engine is me and possibly some friends
3. There are a huge number of existing standards, protocols and structured data formats and practices I could leverage to mange building a search corpus and for indexing.
4. Static site generators have moved site search from services (often outsourced to commercial search engines) to browser based solutions (e.g. [PageFind](https://pagefind.app), [LunrJS](https://lunrjs.com))
5. A localhost site could stage pages for indexing and I could leverage my personal website to expose my indexes to my web devices (e.g. my phone).
6. Tools like wget can mirror websites and that could also be used to stage content for a personal search engine
7. There is a growing body of Open Access data, journal articles and books, these could be indexed and made available in a personal search engine with some effort

## Exploring a personal search engine concept

When I&#39;ve brought up the idea of &#34;a personal search engine&#34; over the years with colleagues I&#39;ve been consistently surprise at the opposition I encounter.  There are so many of reasons not to build something, including a personal search engine. That has left me thinking more deeply about the problem, a good thing in my experience.  I&#39;ve synthesized that resistance into three general questions. Keeping those questions in mind will be helpful in evaluating the costs in time for prototyping a personal search engine and ultimately if the prototype should turn into an open source project.

1. How would a personal search engine know/discover &#34;new&#34; content to include?
2. Search engines are hard to setup and maintain (e.g. Solr, Opensearch), why would I want to spend time doing that?
3. Indexing and search engines are resource intensive, isn&#39;t that going to bog down my computer?

Constraints can be a good thing to consider as well. Here&#39;s some constraints I think will be helpful when considering a prototype implementation.

- I maintain my personal website using a Raspberry Pi 400. The personal search engine needs to respect the limitations of that device.
- I&#39;d like to be able to access my personal search engine from all my networked devices (e.g. my phone when I am away from home)
- I have little time to prototype or code anything
- I need to explain the prototype easily if I want others to help expand on the ideas
- If it breaks I need to easily fix it

I believe recent evolution of static site generation and site search offer an adjacent technology that can be leverage to demonstrate a personal search engine as a prototype. The prototype of a personal search engine could be an extension of my existing website.

## Addressing challenges

### How can a personal search engine know about new things?

The first challenge boils down to discovering content you want to index. What I&#39;m describing is a personal search engine. I&#39;m not trying to &#34;index the whole web&#34; or even a large part of it. I suspect the corpus I regularly search probably is in the neighborhood of a 100,000 pages or so. Too big for a bookmark list but magnitudes smaller than search engine deployments commonly see in an enterprise setting. I also am NOT suggesting a personal search engine will replace commercial search engines or even compete with them. What I&#39;m thinking of is an added tool, not a replacement.

Curating content is labor intensive. This is why Yahoo evolved from a curated web directory to become a hybrid web directory plus search engine before its final demise.  I don&#39;t want to have to change how I currently find content on the web. When I do stumble on something interesting I need a mechanism to easily add it to my personal search engine. Fortunately I think my current web reading habits can function like a [mechanical Turk](https://en.wikipedia.org/wiki/Mechanical_Turk).

Most &#34;new&#34; content I find isn&#39;t from using a commercial search engine. When I look at my habits I find two avenues for content discovery dominate. I come across something via social media (today that&#39;s RSS feeds provided via Mastodon and Yarn Social/Twtxt) or from RSS, Atom and JSON feeds of blogs or websites I follow. Since the social media platforms I track support RSS I typically read all this content via newsboat which is a terminal based feed reader. I still find myself using the web browser&#39;s bookmark feature. It&#39;s just the bookmarks aren&#39;t helpful if they remain only in my web browser.  I also use [Pocket](https://getpocket.com) to read things later. I think all these can serve as a &#34;link discovery&#34; mechanism for a personal search engine. It&#39;s just a matter of collecting the URLs into a list of content I want to index, staging the content, index it and publish the resulting indexes on my personal website using a browser based search engine to query them.

This link discovery approach is different from how commercial search engines work.  Commercial engines rely on crawlers that retrieve a web page, analyze the content, find new links in the page then recursively follows those to scan whole domains and websites.  Recursive crawlers aren&#39;t automatic. It&#39;s easy for them to get trapped in link loops and often they can be a burden to the sites they are crawling (hence robot.txt files suggesting to crawlers what needs to be avoided).  I don&#39;t need to index the whole web, usually not even whole websites.  I&#39;m interested in page level content and I can get a list of web pages from by bookmarks and the feeds I follow.


A Quick digression:

Blogs, in spite of media hype, haven&#39;t &#34;gone away&#34;.  Presently we&#39;re seeing a bit of a renaissance with projects like [Micro.blog](https://micro.blog) and [FeedLand](http://docs.feedland.org/about.opml &#34;this is a cool project from Dave Winer&#34;). The &#34;big services&#34; like [WordPress](https://wordpress.com), [Medium](https://medium.com), [Substack](https://substack.com) and [Mailchimp](https://mailchimp.com/) provide RSS feeds for their content. RSS/Atom/JSON feed syndication all are alive and well at least for the sites I track and content I read. I suspect this is the case for others.  What is a challenge is knowing how to find the feed URL.  But even that I&#39;ve notice is becoming increasingly predictable. I suspect given a list of blog sites I could come up with a way of guessing the feed URL in many cases even without an advertised URL in the HTML head or RSS link in the footer.

### Search engines are hard to setup and maintain, how can that be made easier?

I think this can be addressed in several ways. First is splitting the problem of content retrieval, indexing and search UI.  [PageFind](https://pagefind.app) is the current static site search I use on my blog.  It does a really good job at indexing blog content will little configuration. PageFind is clever about the indexes it builds.  When PageFind indexes a site is builds a partitioned index. Each partition is loaded by the web browser only when the current search string suggests it is needed. This means you can index a large number of pages (e.g. 100,000 pages) before it starts to feel sluggish. Indexing is fast and can be done on demand after harvesting the new pages you come across in your feeds. If the PageFind indexes are saved in my static site directory (a Git repository) I can implement the search UI there implementing the personal search engine prototype. The web browser is the search engine and PageFind tool is the indexer. The harvester is built by extracting interesting URLs from the feeds I follow and the current state of my web browsers&#39; bookmarks and potentially from content in Pocket. Note the web browser bookmarks are synchronized across my devices so if I encounter an interesting URL in the physical world I can easily add it my personal search engine too the next time I process the synchronized bookmark file.

### Indexing and search engines are resource intensive, isn&#39;t that going to bog down my computer?

Enterprise Search Engine Software is complicated to setup, very resource intensive and requires upkeep. For me Solr, Elasticsearch, Opensearch falls into the category &#34;day job&#34; duty and I do not want that burden for my personal search engine. Fortunately I don&#39;t need to run Solr, Elasticsearch or Opensearch. I can build a decent search engine using [PageFind](https://pagefind.app).  PageFind is simple to configured, simple to index with and it&#39;s indexes scale superbly for a browser based search engine UI. Hosting is reduced to the existing effort I put into updating my personal blog and automating the link extraction from the feeds I follow and my web browsers&#39; current bookmark file.

I currently use PageFind for web content I mirror to a search directory locally for off line reading. From that experience I know it can handle at least 100,000 pages. I know it will work on my Raspberry Pi 400. I don&#39;t see a problem in a prototype personal search engine assuming a corpus in the neighborhood of 100,000 pages.


## Sketching the prototype

Here&#39;s a sketch of a prototype of &#34;a personal search engine&#34; built on PageFind.

1. Generate a list of URLs pointing at pages I want to index (this can be done by mining my bookmarks and feed reader content).
2. Harvest and stage the pages on my local file system, maintaining a way to associated their actual URL with the staged copy
3. Index with PageFind and save the resulting indexes my local copy of my personal website
4. Have a page on my personal website use these indexes to implement a search and results page

The code that I would need to be implemented is mostly around extracting URL from my browser&#39;s bookmark file and my the feeds managed in my feed reader. Since newsboat is open source and it stores it cached feeds in a SQLite3 database in principle I could use the tables in that database to generate a list of content to harvest for indexing. I could write a script that combines the content from my bookmarks file and newsboat database rendering a flat list to harvest, stage and then index with PageFind. A prototype could be done in Bash or Python without too much of an effort.

One challenge remains after harvesting and staging is solved. It would be nice to use my personal search engine as my default search engine. After all I am already curating the content. I think this can be done by supporting the [Open Search Description](https://developer.mozilla.org/en-US/docs/Web/OpenSearch) to make my personal search engine a first class citizen in my browser URL bar. Similarly I could turn the personal search engine page into a PWA so I can have it on my phone&#39;s desktop along the other apps I commonly use.

Observations that maybe helpful for a successful prototype

1. I don&#39;t need to crawl the whole web just the pages that interest me
2. I don&#39;t need to create or monitor a recursive web crawler
3. I avoid junk because I&#39;m curating the sources through my existing web reading practices
4. I am targeting a small search corpus, approximately 100,000 pages or so
5. I am only indexing HTML, Pagefind can limit the elements it indexes

A prototype of a personal search engine seems possible. The challenge will be finding the time to implement it.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2023/03/07/prototyping-a-personal-search-engine.md" length="17337" type="text/markdown" />
    </item>    <item>
      <title>SQL query to CSV, a missing datatool</title>
      <link>https://rsdoiel.github.io/blog/2023/01/03/sql-to-csv-a-missing-datatool.html</link>
      <description>
        <![CDATA[At work we maintain allot of metadata related academic and research publications in SQL databases. We use SQL to query the database and export what we need in tab delimited files. Often the exported data includes a column containing publication or article titles.  Titles in library metadata can be a bit messy. They contain a wide set of UTF-8 characters include math symbols and various types of quotation marks. The exported tab delimited data usually needs clean up before you can import it successfully into a spreadsheet.
        
        In the worst cases we debug what the problem is then write a Python script to handle the tweak to fix things.  This results in allot of extra work and slows down the turn around for getting reports out quickly. This is particularly true of data stored in MySQL 8 (though we also use SQLite 3 and Postgres).
        
        This got me thinking about how to get a clean export (tab or CSV) from our SQL databases today.  It would be nice if you provided a command line tool with a data source string (e.g. in a config file or the environment), a SQL query and the tool would use that to render a CSV or tab delimited file to standard out or a output file. It would work something like this.
        
        ...]]>
      </description>
      <source:markdown># SQL query to CSV, a missing datatool

By R. S. Doiel, 2023-01-13

Update: 2023-03-13

At work we maintain allot of metadata related academic and research publications in SQL databases. We use SQL to query the database and export what we need in tab delimited files. Often the exported data includes a column containing publication or article titles.  Titles in library metadata can be a bit messy. They contain a wide set of UTF-8 characters include math symbols and various types of quotation marks. The exported tab delimited data usually needs clean up before you can import it successfully into a spreadsheet.

In the worst cases we debug what the problem is then write a Python script to handle the tweak to fix things.  This results in allot of extra work and slows down the turn around for getting reports out quickly. This is particularly true of data stored in MySQL 8 (though we also use SQLite 3 and Postgres).

This got me thinking about how to get a clean export (tab or CSV) from our SQL databases today.  It would be nice if you provided a command line tool with a data source string (e.g. in a config file or the environment), a SQL query and the tool would use that to render a CSV or tab delimited file to standard out or a output file. It would work something like this.

```
    sql2csv -o eprint_status_report.csv -config=$HOME/.my.cnf \
	    &#39;SELECT eprintid, title, eprint_status FROM eprint&#39; 
```

The `sql2csv` would take the results of the query and write to the CSV file.

The nice thing about this approach is that I could support the three relational databases we use -- i.e. MySQL 8, Postgres and SQLite3 with one common tool so my Bash scripts that run the reports would be very simple rather than specialized to one database system or the other.

I hope to experiment with this approach in the next release of [datatools](https://github.com/caltechlibrary/datatools), an open source project maintained at work.

## update

Jon Woodring pointed out to me today that both SQLite3 and PostgreSQL clients can output to CSV without need of an external tool. Wish MySQL client did that! Instead MySQL client supports tab delimited output. I&#39;m still concidering sql2csv due to the ammount work I do with MySQL database but I&#39;m not sure if it will make it into to the datatools project or now since I suspect our MySQL usage will decline overtime as more projects are built with PostgreSQL and SQLite3.</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2023/01/03/sql-to-csv-a-missing-datatool.md" length="4113" type="text/markdown" />
    </item>    <item>
      <title>Go and MySQL timestamps</title>
      <link>https://rsdoiel.github.io/blog/2022/12/12/Go-and-MySQL-Timestamps.html</link>
      <description>
        <![CDATA[The Go [sql](https://pkg.go.dev/database/sql) package provides a nice abstraction for working with SQL databases. The underlying drivers and DBMS can present some quirks that are SQL dialect and driver specific such as the [MySQL driver](github.com/go-sql-driver/mysql).  Sometimes that is not a big deal. [MySQL](https://dev.mysql.com) can maintain a creation timestamp as well as a modified timestamp easily via the SQL schema definition for the field. Unfortunately if you need to work with the MySQL timestamp at a Go level (e.g. display the timestamp in a useful way) the int64 provided via the driver isn't compatible with the `int64` used in Go's `time.Time`. To work around this limitation I've found it necessary to convert the MySQL timestamp to a formatted string using [DATE_FORMAT](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_date-format "DATE_FORMAT is a MySQL date/time function returning a string value") and from the Go side convert the formatted string into a `time.Time` using `time.Parse()`. Below is some Golang pseudo code showing this approach.
        
        ...]]>
      </description>
      <source:markdown># Go and MySQL timestamps

By R. S. Doiel, 2022-12-12

The Go [sql](https://pkg.go.dev/database/sql) package provides a nice abstraction for working with SQL databases. The underlying drivers and DBMS can present some quirks that are SQL dialect and driver specific such as the [MySQL driver](github.com/go-sql-driver/mysql).  Sometimes that is not a big deal. [MySQL](https://dev.mysql.com) can maintain a creation timestamp as well as a modified timestamp easily via the SQL schema definition for the field. Unfortunately if you need to work with the MySQL timestamp at a Go level (e.g. display the timestamp in a useful way) the int64 provided via the driver isn&#39;t compatible with the `int64` used in Go&#39;s `time.Time`. To work around this limitation I&#39;ve found it necessary to convert the MySQL timestamp to a formatted string using [DATE_FORMAT](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_date-format &#34;DATE_FORMAT is a MySQL date/time function returning a string value&#34;) and from the Go side convert the formatted string into a `time.Time` using `time.Parse()`. Below is some Golang pseudo code showing this approach.

```
// Format used by MySQL strings representing date/times
const MySQLTimestamp = &#34;2006-01-02 15:04:05&#34;

// GetRecordUpdate takes a configuration with a db attribute previously
// opened and an id string returning a record populated with id and updated values where updated is an attribute of type time.Time. We use MySQL&#39;s
// `DATE_FORMAT()` function to convert the timestamp into a string and
// Go&#39;s `time.Parse()` to convert the string into a `time.Time` value.
func GetRecordUpdate(cfg, id string) {
	stmt := `SELECT id, DATE_FORMAT(updated, &#34;%Y-%m-%d %H:%i:%s&#34;) FROM some_tabl WHERE id = ?`
	row, err := cfg.db.Query(stmt, id)
	if err != nil {
		return nil, err
	}
	defer row.Close()
	record := new(Record)
	if row.Next() {
		var updated string
		if err := row.Scan(&#38;record.ID, &#38;updated); err != nil {
			return nil, err
		}
		record.Updated, err = time.Parse(MySQLTimestamp, updated)
		if err != nil {
			return nil, err
		}
	}
	err = row.Err()
	return record, err
}
```</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/12/12/Go-and-MySQL-Timestamps.md" length="3689" type="text/markdown" />
    </item>    <item>
      <title>Progress and time remaining</title>
      <link>https://rsdoiel.github.io/blog/2022/12/05/progress-and-time-remaining.html</link>
      <description>
        <![CDATA[I often find myself logging output when I'm developing tools.  This is typically the case where I am iterating over data and transforming it. Overtime I've come to realize I really want a few specific pieces of information for non-error logging (e.g. `-verbose` which monitors progress as well as errors).
        
        - percentage completed
        - estimated time allocated (i.e. time remaining)
        
        To do that I need three pieces of information.
        
        1. the count of the current iteration(e.g. `i`)
        2. the total number of iterations required (e.g. `tot`)
        3. The time just before I started iterating(e.g. `t0`)
        
        The values for `i` and `tot` let me compute the percent completed. The percent completed is trivial `(i/tot) * 100.0`. Note on the first pass (i.e. `i == 0`) you can skip the percentage calculation.
        
        ...]]>
      </description>
      <source:markdown># Progress and time remaining

By R. S. Doiel, 2022-11-05

I often find myself logging output when I&#39;m developing tools.  This is typically the case where I am iterating over data and transforming it. Overtime I&#39;ve come to realize I really want a few specific pieces of information for non-error logging (e.g. `-verbose` which monitors progress as well as errors).

- percentage completed
- estimated time allocated (i.e. time remaining)

To do that I need three pieces of information.

1. the count of the current iteration(e.g. `i`)
2. the total number of iterations required (e.g. `tot`)
3. The time just before I started iterating(e.g. `t0`)

The values for `i` and `tot` let me compute the percent completed. The percent completed is trivial `(i/tot) * 100.0`. Note on the first pass (i.e. `i == 0`) you can skip the percentage calculation.


```golang
import (
	&#34;time&#34;
	&#34;fmt&#34;
)

// Show progress with amount of time running
func progress(t0 time.Time, i int, tot int) string {
    if i == 0 {
        return &#34;&#34;
    }
	percent := (float64(i) / float64(tot)) * 100.0
	t1 := time.Now()
	// NOTE: Truncating the duration to seconds
	return fmt.Sprintf(&#34;%.2f%% %v&#34;, percent, t1.Sub(t0).Truncate(time.Second))
}
```

Here&#39;s how you might use it.

```golang
	tot := len(ids)
	t0 := time.Now()
	for i, id := range ids {
		// ... processing stuff here ... and display progress every 1000 records
		if (i % 1000) == 0 {
			log.Printf(&#34;%s records processed&#34;, progress(t0, i, tot))
		}
	}
```

An improvement on this is to include an time remaining. I need to calculated the estimated time allocation (i.e. ETA). I know `t0` so I can estimate that with this formula `estimated time allocation = (((current running time since t0)/ the number of items processed) * total number of items)`[^1]. ETA adjusted for time running gives us time remaining[^2]. The first pass of the function progress has a trivial optimization since we don&#39;t have enough delta t0 to compute an estimate. Calls after that are computed using our formula.

[^1]: In code `(rt/i)*tot` is estimated time allocation

[^2]: Estimated Time Remaining, in code `((rt/i)*tot) - rt`

```golang
func progress(t0 time.Time, i int, tot int) string {
	if i == 0 {
		return &#34;0.00 ETR Unknown&#34;
	}
	// percent completed
	percent := (float64(i) / float64(tot)) * 100.0
	// running time
    rt := time.Now().Sub(t0)
    // estimated time allocation - running time = time remaining
    eta := time.Duration((float64(rt)/float64(i)*float64(tot)) - float64(rt))
    return fmt.Sprintf(&#34;%.2f%% ETR %v&#34;, percent, eta.Truncate(time.Second))
}
```</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/12/05/progress-and-time-remaining.md" length="3845" type="text/markdown" />
    </item>    <item>
      <title>Pandoc, Pagefind and Make</title>
      <link>https://rsdoiel.github.io/blog/2022/11/28/pandoc-pagefind-and-make.html</link>
      <description>
        <![CDATA[Recently I've refresh my approach to website generation using three programs.
        
        - [Pandoc](https://pandoc.org)
        - [Pagefind](https://pagefind.app) for providing a full text search of documentation
        - [GNU Make](https://www.gnu.org/software/make/)
            - [website.mak](website.mak) Makefile
        
        Pandoc does the heavy lifting. It renders all the HTML pages, CITATION.cff (from the projects [codemeta.json](codemeta.github.io "codemeta.json is a metadata documentation schema for documenting software projects")) and rendering an about.md file (also from the project's codemeta.json). This is done with three Pandoc templates. Pandoc can also be used to rendering man pages following a simple page recipe.
        
        I've recently adopted Pagefind for indexing the HTML for the project's website and providing the full text search UI suitable for a static website. The Pagefind indexes can be combined with your group or organization's static website providing a rich cross project search (exercise left for another post).
        
        Finally I orchestrate the site construction with GNU Make. I do this with a simple dedicated Makefile called [website.mak](#website.mak).
        
        ...]]>
      </description>
      <source:markdown># Pandoc, Pagefind and Make

Recently I&#39;ve refresh my approach to website generation using three programs.

- [Pandoc](https://pandoc.org)
- [Pagefind](https://pagefind.app) for providing a full text search of documentation
- [GNU Make](https://www.gnu.org/software/make/)
    - [website.mak](website.mak) Makefile

Pandoc does the heavy lifting. It renders all the HTML pages, CITATION.cff (from the projects [codemeta.json](codemeta.github.io &#34;codemeta.json is a metadata documentation schema for documenting software projects&#34;)) and rendering an about.md file (also from the project&#39;s codemeta.json). This is done with three Pandoc templates. Pandoc can also be used to rendering man pages following a simple page recipe.

I&#39;ve recently adopted Pagefind for indexing the HTML for the project&#39;s website and providing the full text search UI suitable for a static website. The Pagefind indexes can be combined with your group or organization&#39;s static website providing a rich cross project search (exercise left for another post).

Finally I orchestrate the site construction with GNU Make. I do this with a simple dedicated Makefile called [website.mak](#website.mak).


## website.mak

The website.mak file is relatively simple.

```makefile
#
# Makefile for running pandoc on all Markdown docs ending in .md
#
PROJECT = PROJECT_NAME_GOES_HERE

MD_PAGES = $(shell ls -1 *.md) about.md

HTML_PAGES = $(shell ls -1 *.md | sed -E &#39;s/.md/.html/g&#39;) about.md

build: $(HTML_PAGES) $(MD_PAGES) pagefind

about.md: .FORCE
        cat codemeta.json | sed -E &#39;s/&#34;@context&#34;/&#34;at__context&#34;/g;s/&#34;@type&#34;/&#34;at__type&#34;/g;s/&#34;@id&#34;/&#34;at__id&#34;/g&#39; &#62;_codemeta.json
        if [ -f $(PANDOC) ]; then echo &#34;&#34; | pandoc --metadata title=&#34;About $(PROJECT)&#34; --metadata-file=_codemeta.json --template codemeta-md.tmpl &#62;about.md; fi
        if [ -f _codemeta.json ]; then rm _codemeta.json; fi

$(HTML_PAGES): $(MD_PAGES) .FORCE
	pandoc -s --to html5 $(basename $@).md -o $(basename $@).html \
		--metadata title=&#34;$(PROJECT) - $@&#34; \
	    --lua-filter=links-to-html.lua \
	    --template=page.tmpl
	git add $(basename $@).html

pagefind: .FORCE
	pagefind --verbose --exclude-selectors=&#34;nav,header,footer&#34; --bundle-dir ./pagefind --source .
	git add pagefind

clean:
	@if [ -f index.html ]; then rm *.html; fi
	@if [ -f README.html ]; then rm *.html; fi

.FORCE:
```

Only the &#34;PROJECT&#34; value needs to be set. Typically this is just the name of the repository&#39;s base directory.

## Pandoc, filters and templates

When write my Markdown documents I link to Markdown files instead of the HTML versions. This serves two purposes. First GitHub can use this linking directory and second if you decide to repurposed the website as a Gopher or Gemini resource
you don&#39;t linking to the Markdown file makes more sense.  To convert the &#34;.md&#34; names to &#34;.html&#34; when I render the HTML I use a simple Lua filter called [links-to-html.lua](https://stackoverflow.com/questions/40993488/convert-markdown-links-to-html-with-pandoc#49396058 &#34;see the stackoverflow answer that shows this technique&#34;).

```lua
# links-to-html.lua
function Link(el)
  el.target = string.gsub(el.target, &#34;%.md&#34;, &#34;.html&#34;)
  return el
end
```

The &#34;page.tmpl&#34; file provides a nice wrapper to the Markdown rendered as HTML by Pandoc. It includes the site navigation and project copyright information in the wrapping HTML. It is based on the default Pandoc page template with some added markup for navigation and copyright info in the footer. I also update the link to the CSS to conform with our general site branding requirements. You can generate a basic template using Pandoc.

```shell
pandoc --print-default-template=html5
```

I also use Pandoc to generate an &#34;about.md&#34; file describing the project and author info.  The content of the about.md is taken directly from the project&#39;s codemeta.json file after I&#39;ve renamed the &#34;@&#34; JSON-LD fields (those cause problems for Pandoc). You can see the preparation of a temporary &#34;_codemeta.json&#34; using `cat` and `sed` to rename the fields. This is I use a Pandoc template to render the Markdown from.

```pandoc
  ---
  title: $name$
  ---

  About this software
  ===================

  $name$ $version$
  ----------------

  $if(author)$
  ### Authors
  
  $for(author)$
  - $it.givenName$ $it.familyName$
  $endfor$
  $endif$
  
  $if(description)$
  $description$
  $endif$
  
  
  $if(license)$- License: $license$$endif$
  0$if(codeRepository)$- GitHub: $codeRepository$$endif$
  $if(issueTracker)$- Issues: $issueTracker$$endif$
  
  
  $if(programmingLanguage)$
  ### Programming languages
  
  $for(programmingLanguage)$
  - $programmingLanguage$
  $endfor$
  $endif$
  
  $if(operatingSystem)$
  ### Operating Systems
  
  $for(operatingSystem)$
  - $operatingSystem$
  $endfor$
  $endif$
  
  $if(softwareRequirements)$
  ### Software Requiremets
  
  $for(softwareRequirements)$
  - $softwareRequirements$
  $endfor$
  $endif$
  
  $if(relatedLink)$
  ### Related Links
  
  $for(relatedLink)$
  - [$it$]($it$)
  $endfor$
  $endif$
```

This same technique can be repurposed to render a CITATION.cff if needed.

## Pagefind

Pagefind provides three levels of functionality. First it will generate indexes for a full text search of your
project&#39;s HTML pages. It also builds the necessary search UI for your static site. I include the search UI via a
Markdown document that embeds the HTML markup described at [Pagefind.app](https://pagefind.app/docs/)&#39;s Getting started
page.  When I invoke Pagefind I use the `--bundle-dir` option to be &#34;pagefind&#34; rather than &#34;_pagefind&#34;.  The reason is GitHub Pages ignores the &#34;_pagefind&#34; (probably ignores all directories with &#34;_&#34; prefix).

If you need a quick static web server while you&#39;re writing and developing your documentation website Pagefind can
provide that using the `--serve` option. Assuming you&#39;re in your project&#39;s directory then something like this should do the trick.

```shell
    pagefind --source . --bundle-dir=pagefind --serve
```</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/28/pandoc-pagefind-and-make.md" length="7547" type="text/markdown" />
    </item>    <item>
      <title>Initial Impressions of Pagefind</title>
      <link>https://rsdoiel.github.io/blog/2022/11/21/initial-impressions-pagefind.html</link>
      <description>
        <![CDATA[I'm interested in site search that does not require using server side services (e.g. Solr/Elasticsearch/Opensearch). I've used [LunrJS](https://lunrjs.com) on my person blog site for several years.  The challenge with LunrJS is indexes become large and that limits how much your can index and still have a quick loading page. [Pagefind](https://pagefind.app) addresses the large index problem. The search page only downloads the portion of the indexes it needs. The index and search functionality are compiled down to WASM files. This does raise challenges if you're targeting older web browsers.
        
        Pagefind is a [rust](https://www.rust-lang.org/) application build using `cargo` and `rustc`. Unlike the documentation on the [Pagefind](https://pagefind.app) website which suggests installing via `npm` and `npx` I recommend installing it from sources using the latest release of cargo/rustic.  For me I found getting the latest cargo/rustc is easiest using [rustup](https://rustup.rs/). Pagefind will not compile using older versions of cargo/rustc (e.g. the example currently available from Mac Ports for M1 Macs).
        
        Here's the steps I took to bring Pagefind up on my M1 Mac.
        
        ...]]>
      </description>
      <source:markdown># Initial Impression of Pagefind

By R. S. Doiel, 2022-11-21

I&#39;m interested in site search that does not require using server side services (e.g. Solr/Elasticsearch/Opensearch). I&#39;ve used [LunrJS](https://lunrjs.com) on my person blog site for several years.  The challenge with LunrJS is indexes become large and that limits how much your can index and still have a quick loading page. [Pagefind](https://pagefind.app) addresses the large index problem. The search page only downloads the portion of the indexes it needs. The index and search functionality are compiled down to WASM files. This does raise challenges if you&#39;re targeting older web browsers.

Pagefind is a [rust](https://www.rust-lang.org/) application build using `cargo` and `rustc`. Unlike the documentation on the [Pagefind](https://pagefind.app) website which suggests installing via `npm` and `npx` I recommend installing it from sources using the latest release of cargo/rustic.  For me I found getting the latest cargo/rustc is easiest using [rustup](https://rustup.rs/). Pagefind will not compile using older versions of cargo/rustc (e.g. the example currently available from Mac Ports for M1 Macs).

Here&#39;s the steps I took to bring Pagefind up on my M1 Mac.

1. Install cargo/rust using rustup
2. Make sure `$HOME/.cargo/bin` is in my PATH
3. Clone the Pagefind Git repository
4. Change to the repository directory
5. Build and install pagefind

```
curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh
export PATH=&#34;$HOME/.cargo/bin:$PATH&#34;
git clone git@github.com git@github.com:CloudCannon/pagefind.git src/github.com/CloudCannon/pagefind
cd src/github.com/CloudCannon/pagefind
cargo install pagefind --features extended
```

Next steps were

1. Switch to my local copy of my website
2. Build my site in the usual page
3. Update my `search.html` page to use pagefind
4. Index my site using pagefind
5. Test my a local web server

To get the HTML/JavaScript needed to embed pagefind in your search page see [Getting Started](https://pagefind.app/docs/). The HTML/JavaScript fragment is at the top of the page. After updating `search.html` I ran the pagefind command[^1].

```
pagefind --verbose --bundle-dir ./pagefind --source .
```

The indexing is wicked fast and it gives you nice details. I verified everything worked as expected using `pttk ws` static site web server. I then published my website. You can see the results at &#60;http://rsdoiel.sdf.org/search.html&#62; and &#60;https://rsdoiel.github.io/search.html&#62;

[^1]: I specified the bundle directory because GitHub pages had a problem with the default `_pagefind`.</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/21/initial-impressions-pagefind.md" length="4293" type="text/markdown" />
    </item>    <item>
      <title>Browser based site search</title>
      <link>https://rsdoiel.github.io/blog/2022/11/18/browser-side-site-search.html</link>
      <description>
        <![CDATA[I recently read Brewster Kahles 2015 post about his vision for a [distributed web](https://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/). Many of his ideas have carried over into [DWeb](https://wiki.mozilla.org/Dweb), [Indie Web](https://indieweb.org/), [Small Web](https://benhoyt.com/writings/the-small-web-is-beautiful/), [Small Internet](https://cafebedouin.org/2021/07/28/the-small-internet/) and the like. A point he touches on is site search running in the web browser.
        
        I've use this approach in my own website relying on [LunrJS](https://lunrjs.com) by Oliver Nightingale. It is a common approach for small sites built using Markdown and [Pandoc](https://pandoc.org).  In the Brewster article he mentions [js-search](https://github.com/cebe/js-search), an implementation I was not familiar with. Like LunrJS the query engine runs in the browser via JavaScript but unlike LunrJS the indexes are built using PHP rather than JavaScript. The last couple of years I've used [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) to generating indexes for my own website site while using LunrJS for the browser side query engine. Today I check to see what the [Hugo](https://gohugo.io/tools/search/) community is using and found [Pagefind](https://github.com/cloudcannon/pagefind). Pagefind looks impressive. There was a presentation on at [Hugo Conference 2022](https://hugoconf.io/). It takes building a Lucene-like index several steps further. I appears to handle much larger indexes without requiring the full indexes to be downloaded into the browser.  It seems like a good candidate for prototyping personal search engine.
        
        ...]]>
      </description>
      <source:markdown>Browser based site search
=========================

By R. S. Doiel, 2022-11-18

I recently read Brewster Kahles 2015 post about his vision for a [distributed web](https://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/). Many of his ideas have carried over into [DWeb](https://wiki.mozilla.org/Dweb), [Indie Web](https://indieweb.org/), [Small Web](https://benhoyt.com/writings/the-small-web-is-beautiful/), [Small Internet](https://cafebedouin.org/2021/07/28/the-small-internet/) and the like. A point he touches on is site search running in the web browser.

I&#39;ve use this approach in my own website relying on [LunrJS](https://lunrjs.com) by Oliver Nightingale. It is a common approach for small sites built using Markdown and [Pandoc](https://pandoc.org).  In the Brewster article he mentions [js-search](https://github.com/cebe/js-search), an implementation I was not familiar with. Like LunrJS the query engine runs in the browser via JavaScript but unlike LunrJS the indexes are built using PHP rather than JavaScript. The last couple of years I&#39;ve used [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) to generating indexes for my own website site while using LunrJS for the browser side query engine. Today I check to see what the [Hugo](https://gohugo.io/tools/search/) community is using and found [Pagefind](https://github.com/cloudcannon/pagefind). Pagefind looks impressive. There was a presentation on at [Hugo Conference 2022](https://hugoconf.io/). It takes building a Lucene-like index several steps further. I appears to handle much larger indexes without requiring the full indexes to be downloaded into the browser.  It seems like a good candidate for prototyping personal search engine.

How long have been has browser side search been around? I do not remember when I started using. I explored seven projects on GitHub that implemented browser side site search. This is an arbitrary selection projects but even then I had no idea that this approach dates back a over decade!

| Project | Indexer | query engine | earliest commit[^1] | recent commit[^2] |
|---------|---------|--------------|:-------------------:|:-----------------:|
| [LunrJS](https://github.com/olivernn/lunr.js) | JavaScript | JavaScript | 2011 | 2020 |
| [Fuse.io](https://github.com/krisk/Fuse) | JavaScript/Typescript | JavaScript/Typescript | 2012 | 2022 |
| [search-index](https://github.com/fergiemcdowall/search-index) | JavaScript | JavaScript | 2013 | 2016 |
| [js-search](https://github.com/cebe/js-search) (cebe) | PHP | JavaScript | 2014 | 2022 |
| [js-search](https://github.com/bvaughn/js-search) (bvaughn)| JavaScript | JavaScript | 2015 | 2022 |
| [Lunr.py](https://github.com/yeraydiazdiaz/lunr.py) | Python | Python or JavaScript | 2018 | 2022 |
| [Pagefind](https://github.com/cloudcannon/pagefind) | Rust | WASM and JavaScript | 2022 | 2022 |

[^1]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.

[^2]: Years are based on checking reviewing the commit history on GitHub as of 2022-11-18.</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/18/browser-side-site-search.md" length="5300" type="text/markdown" />
    </item>    <item>
      <title>Revealing the Pandoc AST</title>
      <link>https://rsdoiel.github.io/blog/2022/11/17/revealing-pandoc-ast.html</link>
      <description>
        <![CDATA[I've used Pandoc for a number of years, probably a decade. It's been wonderful
        watching it grow in capability. When Pandoc started accepting JSON documents as
        a support metadata file things really started to click for me. Pandoc became
        my go to tool for rendering content in my writing and documentation projects.
        
        Recently I've decided I want a little bit more from Pandoc. I've become curious
        about prototyping some document conversion via Pandoc's filter mechanism. To do
        that you need to understand the AST, aka abstract syntax tree. 
        How is the AST structure? 
        
        It turns out I just wasn't thinking simply enough (or maybe just not paying
        enough attention while I skimmed Pandoc's documentation). Pandoc's processing
        model looks like
        
        ...]]>
      </description>
      <source:markdown>Revealing the Pandoc AST
========================

I&#39;ve used Pandoc for a number of years, probably a decade. It&#39;s been wonderful
watching it grow in capability. When Pandoc started accepting JSON documents as
a support metadata file things really started to click for me. Pandoc became
my go to tool for rendering content in my writing and documentation projects.

Recently I&#39;ve decided I want a little bit more from Pandoc. I&#39;ve become curious
about prototyping some document conversion via Pandoc&#39;s filter mechanism. To do
that you need to understand the AST, aka abstract syntax tree. 
How is the AST structure? 

It turns out I just wasn&#39;t thinking simply enough (or maybe just not paying
enough attention while I skimmed Pandoc&#39;s documentation). Pandoc&#39;s processing
model looks like

```
	INPUT --reader--&#62; AST --filter AST --writer--&#62; OUTPUT
```

I&#39;ve &#34;known&#34; this forever. The missing piece for me was understanding 
the AST can be an output format.  Use the `--to` option with the value
&#34;native&#34; you get the Haskell representation of the AST. It&#39;s that simple.

```
	pandoc --from=markdown --to=native \
	   learning-to-write-a-pandoc-filter.md | \
	   head -n 20
```

Output

```
[ Header
    1
    ( &#34;learning-to-write-a-pandoc-filter&#34; , [] , [] )
    [ Str &#34;Learning&#34;
    , Space
    , Str &#34;to&#34;
    , Space
    , Str &#34;write&#34;
    , Space
    , Str &#34;a&#34;
    , Space
    , Str &#34;Pandoc&#34;
    , Space
    , Str &#34;filter&#34;
    ]
, Para
    [ Str &#34;I\8217ve&#34;
    , Space
    , Str &#34;used&#34;
    , Space
```

If you prefer JSON over Haskell use `--to=json` for similar effect. Here&#39;s
an example piping through [jq](https://stedolan.github.io/jq/).

```
	pandoc --from=markdown --to=json \
	   learning-to-write-a-pandoc-filter.md | jq .
```

Writing filters makes much sense to me now. I can see the AST and see
how the documentation describes writing hooks in Lua to process it.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/17/revealing-pandoc-ast.md" length="3099" type="text/markdown" />
    </item>    <item>
      <title>Twitter&#39;s pending implosion</title>
      <link>https://rsdoiel.github.io/blog/2022/11/11/Twitter-implosion.html</link>
      <description>
        <![CDATA[It looks like Twitter continues to implode as layoffs and resignations continue. If bankers, investors and lenders call in the loans [bankruptcy appears to be possible](https://www.reuters.com/technology/twitter-information-security-chief-kissner-decides-leave-2022-11-10/). So what's next?
        
        
        Twitter has been troubled for some time. The advertising model corrodes content. Twitter is effectively a massive RSS-like distribution system. It has stagnated as the APIs became more restrictive. The Advertising Business Model via [Ad-tech](https://pluralistic.net/tag/adtech/ "per Cory Doctorow 'ad-fraud'") encourages decay regardless of system.  Non-Twitter examples include commercial search engines (e.g. Google, Bing et el). Their usefulness usefulness declines over time. I believe this due to the increase in "noise" in the signal. The "noise" is driven be business models. That usually boils down to content who's function is to attract your attention so it can be sold for money. A corollary is [fear based journalism](https://medium.com/@oliviacadby/fear-mongering-journalisms-downfall-aac1f4f5756d). That has even caught the attention of a [Pope](https://www.9news.com.au/world/fear-based-journalism-is-terrorism-pope/4860b502-5dbb-4eef-abcf-57582445fc2c). Not fun.
        
        I suspect business models don't encourage great content. Business models are generally designed to turn a profit. They tend to get refined and tuned to that purpose. The evolution of Twitter and Google's search engine would make good case studies in that regard.
        
        
        ...]]>
      </description>
      <source:markdown>Twitter&#39;s pending implosion
===========================

By R. S. Doiel, 2022-11-11

It looks like Twitter continues to implode as layoffs and resignations continue. If bankers, investors and lenders call in the loans [bankruptcy appears to be possible](https://www.reuters.com/technology/twitter-information-security-chief-kissner-decides-leave-2022-11-10/). So what&#39;s next?


The problem
-----------

Twitter has been troubled for some time. The advertising model corrodes content. Twitter is effectively a massive RSS-like distribution system. It has stagnated as the APIs became more restrictive. The Advertising Business Model via [Ad-tech](https://pluralistic.net/tag/adtech/ &#34;per Cory Doctorow &#39;ad-fraud&#39;&#34;) encourages decay regardless of system.  Non-Twitter examples include commercial search engines (e.g. Google, Bing et el). Their usefulness usefulness declines over time. I believe this due to the increase in &#34;noise&#34; in the signal. The &#34;noise&#34; is driven be business models. That usually boils down to content who&#39;s function is to attract your attention so it can be sold for money. A corollary is [fear based journalism](https://medium.com/@oliviacadby/fear-mongering-journalisms-downfall-aac1f4f5756d). That has even caught the attention of a [Pope](https://www.9news.com.au/world/fear-based-journalism-is-terrorism-pope/4860b502-5dbb-4eef-abcf-57582445fc2c). Not fun.

I suspect business models don&#39;t encourage great content. Business models are generally designed to turn a profit. They tend to get refined and tuned to that purpose. The evolution of Twitter and Google&#39;s search engine would make good case studies in that regard.


A small hope
------------

I don&#39;t know what is next but I know what I find interesting. I&#39;ve looked at Mastodon a number of times. It&#39;s not going away but the W3C activity pub spec is horribly complex. Complexity slows adoption. It reminds me of SGML. Conceptually interesting but in practice was too heavy. It did form inspiration for HTML though, and that has proven successful. What gives me hope is that Mastodon has survived. I think casting a wide net is interesting. The wider net is something I&#39;ve heard called the &#34;small web&#34;.

The small web
-------------

For a number of years there has been a slowly growing  &#34;small web&#34; movement. I think it is relatively new term, I didn&#39;t find it in [Wikipedia](https://en.wikipedia.org/w/index.php?search=small+web&#38;ns0=1 &#34;today is 2022-11-11&#34;) when I looked today. As I see it the &#34;small web&#34; has been driven by a number of things. It is not a homogeneous movement but rather a collection of various efforts and communities.  I think it likely to continue to evolve. At times evolve rapidly. Perhaps it will coalesce at some point.  Here&#39;s what appears to me to be the common motivations as of 2022-11-11.

- desire for simplicity
- desire for authenticity
- lower resource footprint
- text as a primary but not exclusive medium
- hyperlinks encouraged
- a space where you&#39;re not a product
- desire for decentralization (so you&#39;re not a product)
- a desire to have room to grow (because you&#39;re not a product)

The &#34;small web&#34; is a term I&#39;ve seen pop up in Gopherspace, among people who promote [Gemini](https://gemini.circumlunar.space/), [Micro blogging](https://micro.blog &#34;as an example of micro blogging&#34;) and in the [Public Access Unix](https://sdf.org) communities.&#34;small web&#34; as a term does not return useful results in the commercial search engines I&#39;ve checked. Elements seem to be part of [DWeb](https://getdweb.net/) which Mozilla is [championing](https://wiki.mozilla.org/Dweb). Curiously in spite of the hype and marketing I don&#39;t see &#34;small web&#34; in [web 3.0](https://www.forbes.com/advisor/investing/cryptocurrency/what-is-web-3-0/). I think blockchain has proven environmentally dangerous and tends to compile down to various forms of [grift](https://pluralistic.net/2022/05/27/voluntary-carbon-market/).


Small web
---------

What does &#34;small web&#34; mean to me?  I think it means

- simple protocols that are flexible and friendly to tool creation
- built on existing network protocols with a proven track record (e.g. IPv4, IPv6)
- decentralized by design as was the early Internet
- low barrier to participation
    - e.g. a text editor, static site providing a URL to a twtxt file
- text centric (at least for the moment)
- integrated with the larger Internet, i.e. supports hyper links
- friendly to distributed personal search engines (e.g. LunrJS running over curated set of JSONfeeds or twtxt urls)
- &#34;feed&#34; oriented discovery based on simple formats (e.g. [RSS 2.0](https://cyber.harvard.edu/rss/rss.html), [JSONfeed](https://www.jsonfeed.org/), [twtxt](https://twtxt.readthedocs.io/en/latest/), [OPML](https://en.wikipedia.org/wiki/OPML), even [Gophermaps](https://en.wikipedia.org/wiki/Gopher_(protocol) &#34;see Source code of a menu title&#34;))
- sustainable and preservation friendly
    - example characteristics
        - clone-able (e.g. as easy as cloning a Git Repo)
        - push button update to Internet Archive&#39;s way back machine
        - human and machine readable metadata

I think the &#34;small web&#34; already exists. Examples include readable personal websites hosted as &#34;static pages&#34; via GitHub and S3 buckets are a good examples of prior art in a &#34;small web&#34;.  Gopherspace is a good example of the &#34;small web&#34;. I think the various [tilde communities](https://tilde.club) hosted on [Public Access Unix](https://en.wikipedia.org/wiki/SDF_Public_Access_Unix_System) are examples. Even the venerable &#34;bloggosphere&#34; of [Wordpress](https://wordpress.com) and the newer [Micro.blog](https://micro.blog/) is evidence that the &#34;small web&#34; already is hear. [Dave Winer](https://scripting.com)&#39;s [Feedland](http://feedland.org/) is a good example of innovation in the &#34;small web&#34; happen today.  [Yarn.social](https://yarn.social) built on twtxt file format is very promising. I would argue right now the &#34;small web&#34; is the internet that already exists outside the walled gardens of Google, Meta/Facebook, Twitter, TikTok, Pinterest, Slack, Trello, Discord, etc.

I think it is significant that the &#34;small web&#34; existed before the Pandemic. It continued to thrive during it. It is likely to evolve beyond it. The pending shift has already happening as it is already populated by &#34;early adopters&#34; and appears to be growing into larger community participation.  For the &#34;main stream&#34; it is waiting to be &#34;discovered&#34; or perhaps &#34;re-discovered&#34; depending on your point of view.

How do you participate?
-----------------------

You may already be participating in the &#34;small web&#34;.  Do you blog? Do your read feeds? Do you use a non-soloed social media platform like Mastodon? Do you use Gopher? The &#34;small web&#34; is defined by choice and is characterized by simplicity. It is a general term. You&#39;re the navigator not an algorithm tuned to tune someone a profit. If you are not sure where to start you can join a communities like [sdf.org](https://sdf.org) and get started there. You can explore [Gopherspace](https://floodgap.com) via a WWW proxy. You can create a static website and host a [twtxt](https://twtxt.readthedocs.io/en/latest/) file on GitHub or a [Yarn Pod](https://yarn.social). You can create a site via [Micro.blog](https://micro.blog) or [Feedland](http://feedland.org). You can blog. You can read RSS feeds or read twtxt feed with [twtxt](https://twtxt.readthedocs.io/en/latest/user/intro.html), [twet](https://github.com/quite/twet) or [yarn.social](https://yarn.social). You participate by stepping outside the walled gardens and seeing the larger &#34;Internet&#34;.

I think the important thing is to realize the alternatives are already here, you don&#39;t need to wait for invention, invitation or permission. You can move beyond the silos today. You don&#39;t need to have your attention captured then bought and sold. It&#39;s not so much a matter of &#34;giving up&#34; a silo but rather stepping outside one and breathing some fresh air.

Things to watch
---------------

- [Feedland](https://feedland.org)
- [yarn.social](https://yarn.social) and [twtxt](https://twtxt.readthedocs.io/en/latest/)
- [Micro.blog](https://micro.blog/)
- [Mastodon](https://joinmastodon.org/)
- [Gopherspace](http://gopher.floodgap.com/gopher/gw?a=gopher%3A%2F%2Fgopher.floodgap.com%2F1%2Fworld), see [Gopherspace in 2020](https://cheapskatesguide.org/articles/gopherspace.html) as a nice orientation to see the internet through lynx and text
- Even [Project Gemini](https://gemini.circumlunar.space/)</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/11/Twitter-implosion.md" length="10550" type="text/markdown" />
    </item>    <item>
      <title>Compiling Pandoc from source</title>
      <link>https://rsdoiel.github.io/blog/2022/11/07/compiling-pandoc-from-source.html</link>
      <description>
        <![CDATA[I started playing around with Pandoc's __pandoc-server__ last Friday. I want to play with the latest version of Pandoc.  When I gave it a try this weekend I found that my Raspberry Pi 400's SD card was too small. This lead me to giving the build process a try on my Ubuntu desktop. These are my notes about how I going about building from scratch.  I am not a Haskell programmer and don't know the tool chain or language. Take everything that follows with a good dose of salt but this is what I did to get everything up and running. I am following the compile from source instructions in Pandoc's [INSTALL.md](https://github.com/jgm/pandoc/blob/master/INSTALL.md)
        
        I'm running this first on an Intel Ubuntu box because I have the disk space available there. If it works then I'll try it directly on my Raspberry Pi 400 with an upgrade SD card and running the 64bit version of Raspberry Pi OS.
        
        I did not have Haskell or Cabal installed when I started this process.
        
        ...]]>
      </description>
      <source:markdown>Compiling Pandoc from source
============================

By R. S. Doiel, 2022-11-07

I started playing around with Pandoc&#39;s __pandoc-server__ last Friday. I want to play with the latest version of Pandoc.  When I gave it a try this weekend I found that my Raspberry Pi 400&#39;s SD card was too small. This lead me to giving the build process a try on my Ubuntu desktop. These are my notes about how I going about building from scratch.  I am not a Haskell programmer and don&#39;t know the tool chain or language. Take everything that follows with a good dose of salt but this is what I did to get everything up and running. I am following the compile from source instructions in Pandoc&#39;s [INSTALL.md](https://github.com/jgm/pandoc/blob/master/INSTALL.md)

I&#39;m running this first on an Intel Ubuntu box because I have the disk space available there. If it works then I&#39;ll try it directly on my Raspberry Pi 400 with an upgrade SD card and running the 64bit version of Raspberry Pi OS.

I did not have Haskell or Cabal installed when I started this process.

Steps
-----

1. Install __stack__ (it will install GHC)
2. Clone the GitHub repo for [Pandoc](https://github.com/jgm/pandoc)
3. Setup __stack__ for Pandoc
4. Build and test with __stack__
5. Install __stack__ install
6. Make a symbolic link from __pandoc__ to __pandoc-server__

```
sudo apt update
sudo apt search &#34;haskell-stack&#34;
sudo apt install &#34;haskell-stack&#34;
stack upgrade
git clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc
cd src/github.com/jgm/pandoc
stack setup 
stack build
stack test
stack install
ln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server
```

This step takes a long time and on the Raspberry Pi it&#39;ll take allot longer.

The final installation of Pandoc was in my `$HOME/.local/bin` directory. Assuming this is early in your path this can allow you to experiment with a different version of Pandoc from the one installed on your system. 

I also wanted to try the latest of __pandoc-server__.  This was not automatically installed and is not mentioned in the INSTALL.md file explicitly. But looking at the discussion of running __pandoc-server__ in CGI mode got me thinking. I then checked the installation on my Ubuntu box for the packaged version of pandoc-server and saw that is was a symbolic link.  Adding a `ln` command to my build instruction solved the problem.

I decided to try compiling Pandoc on my M1 mac.  First I needed to get __stack__ installed. I use Mac Ports but it wasn&#39;t in the list of available packages.  Fortunately the Haskell Stack website provides a shell script for installation on Unixes. I wanted to install __stack__ in my home `bin` directory not `/usr/bin/slack`. So after reviewing the downloaded install script I found the `-d` option for changing where it installs to. It indicated I need to additional work with __xcode__.

```
curl -sSL https://get.haskellstack.org/ &#62; stack-install.sh
more stack-install.sh
sh stack-install.sh -d $HOME/bin
```

The __stack__ installation resulted in a message in this form.

```
Stack has been installed to: $HOME/bin/stack

NOTE: You may need to run &#39;xcode-select --install&#39; and/or
      &#39;open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg&#39;
      to set up the Xcode command-line tools, which Stack uses.

WARNING: &#39;$HOME/.local/bin&#39; is not on your PATH.
    Stack will place the binaries it builds in &#39;$HOME/.local/bin&#39; so
    for best results, please add it to the beginning of PATH in your profile.
```

I already had xcode setup for compiling Go so those addition step was not needed.  I only needed to add `$HOME/.local/bin` to my search path.

I then followed the steps I used on my Ubuntu Intel box.

```
git clone git@github.com:jgm/pandoc src/github.com/jgm/pandoc
cd src/github.com/jgm/pandoc
stack setup
stack build
stack test
stack install
ln $HOME/.local/bin/pandoc $HOME/.local/bin/pandoc-server
```

Now when I have a chance to update my Raspberry Pi 400 to a suitable sized SD Card (or external drive) I&#39;ll be ready to compile a current version of Pandoc from source.

Additional notes
----------------

[stack](https://docs.haskellstack.org/en/stable/) is a Haskell build tool. It setups up an Haskell environment per project. If a project requires a specific version of the Haskell compiler it&#39;ll be installed and made accessible for the project. In this way it&#39;s a bit like having a specific environment for Python. The stack website indicates that it targets cross platform development in Haskell which is nice.  Other features of stack remind me of Go &#34;go&#34; command in that it can build things or Rust&#39;s &#34;cargo&#34; command. Like __cargo__ it can update itself which is nice. That is what I did after installing the Debian package version used by Ubuntu. Configuration of a &#34;stack&#34; project uses YAML files. Stack uses __cabal__, Haskell&#39;s older build tool but subsumes __cabal-install__ for setting up __cabal__ and __ghc__. It appears from my reading that __stack__ addresses some of the short falls __cabal__ originally had and specifically focusing on reproducible compiles. This is important in sharing code as well as if you want to integrate automated compilation and testing. It maintains a project with &#34;cabal files&#34; so there is the ability to work with older non-stack code if I read the documentation correctly. Both __cabal__ and __stack__ seem to be evolving in parallel taking different approaches but influencing one another. Both systems use &#34;cabal files&#34; for describing projects and dependencies as of 2022. The short version of [Why Stack](https://docs.haskellstack.org/en/stable/#why-stack) can be found the __stack__ website.

[Hackage](https://hackage.haskell.org/) is a central repository of Haskell packages. 

[Stackage](https://www.stackage.org/) is a curated subset of Hackage packages. It appears to be the preferred place for __stack__ to pull from.</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/07/compiling-pandoc-from-source.md" length="7405" type="text/markdown" />
    </item>    <item>
      <title>feeds, formats and plain text</title>
      <link>https://rsdoiel.github.io/blog/2022/11/01/Feeds-formats-and-plain-text.html</link>
      <description>
        <![CDATA[There has been a proliferation of feed formats. My personal preferred format is RSS 2.0. It's stable and proven the test of type. Atom feeds always felt a little like, "not invented here so we're inventing it again", type of thing. The claim was they could support read/write but so can RSS 2.0 specially with the namespace possibilities. The innovative work [Dave Winer](https://scripting.com) has done in the past and is doing today with [Feedland](https://feedland.org) is remarkably impressive.
        
        In my experience the format of the feed is less critical than the how to author the metadata.  Over the last several years I've moved to static hosting as my preferred way of hosting a website. My writing is typically in Markdown or Fountain formats and frontmatter like used in RMarkdown has proven very convenient. The "blogit" command that started out from an idea in [mkpage](https://github.com/caltechlibrary/mkpage "Make Page, a Pandoc preprocessor and tool set") has been implemented in [pttk](https://github.com/rsdoiel/pttk "Plain Text Toolkit"). So for me metadata authoring makes sense in the front matter. That has the advantage that Pandoc can leverage the information in its templates (that is what I use to render HTML, man pages and the occasional PDF). It also is a food source for data to include in a feed.
        
        ...]]>
      </description>
      <source:markdown>Feeds, formats, and plain text
==============================

By R. S. Doiel, 2022-11-01

There has been a proliferation of feed formats. My personal preferred format is RSS 2.0. It&#39;s stable and proven the test of type. Atom feeds always felt a little like, &#34;not invented here so we&#39;re inventing it again&#34;, type of thing. The claim was they could support read/write but so can RSS 2.0 specially with the namespace possibilities. The innovative work [Dave Winer](https://scripting.com) has done in the past and is doing today with [Feedland](https://feedland.org) is remarkably impressive.

In my experience the format of the feed is less critical than the how to author the metadata.  Over the last several years I&#39;ve moved to static hosting as my preferred way of hosting a website. My writing is typically in Markdown or Fountain formats and frontmatter like used in RMarkdown has proven very convenient. The &#34;blogit&#34; command that started out from an idea in [mkpage](https://github.com/caltechlibrary/mkpage &#34;Make Page, a Pandoc preprocessor and tool set&#34;) has been implemented in [pttk](https://github.com/rsdoiel/pttk &#34;Plain Text Toolkit&#34;). So for me metadata authoring makes sense in the front matter. That has the advantage that Pandoc can leverage the information in its templates (that is what I use to render HTML, man pages and the occasional PDF). It also is a food source for data to include in a feed.

I&#39;ve recently become aware of a really simple text format called [twtxt](https://twtxt.readthedocs.io/en/latest/). This simple format is meant for micro blogging but is also useful as a feed source and format. Especially in terms of rendering content for Gopherspace which I&#39;ve re-engaged in recently. [Yarn.social](https://yarn.social) has built an entire ecosystem around it. Very impressive. The format is so simple it can be done with a pipe and the &#34;echo&#34; command in the shell.  It looks promising in terms for personal search ingest as well.

One of the formats that Dave Winer supports in Feedland and is used in the micro blogging community he has connected with is [jsonfeeds](https://www.jsonfeed.org/). It is lightweight and to me feels allot like RSS 2.0 without the XML-isms that go along with it.  I&#39;m playing with the idea that in pttk it&#39;ll be the standard feed format and that from it I can then render our traditional feed friends of RSS 2.0 and Atom.

I&#39;ve looked at the ActivityPub from the Mastodon community but like [James Mill](https://prologic.github.io/prologic/ &#34;aka prologic&#34;) I find it too complex. What is needed is something simple, really simple.  That&#39;s why I&#39;ve been looking closely at Gopherspace again. The Gophermap can function as a bookmark file, a &#34;home page&#34; a list of feeds. A little archaic but practical in its simplicity. The only challenges I&#39;ve run into has been figuring out that expectations of the Gopher server software. Currently I&#39;ve settled on [gophernicus](https://gophernicus.org) as that is was it supported at [sdf.org](https://sdf.org) where I have a gopher &#34;hole&#34;.

As pttk grows and I explore where I can take simple text processing I&#39;m not targeting Gopherspace, twtxt and static websites. I&#39;ve looked at [Gemini](https://gemini.circumlunar.space/docs/specification.gmi) but haven&#39;t grokked the point yet.  Their choice of yet another markup for content seems problematic at best. For me gopher solves the problems that would make me look at Gemini and I can use most any structured text I want. The text just needs to be readable easily by humans. The Gophermap provides can be enhanced menus much like &#34;index.html&#34; pages have become (a trunk that branches and eventually leads to a leaf). 

[OPML](http://home.opml.org/) remains a really nice outline data format.  It&#39;s something I&#39;d like to eventually integrate with pttk. It can be easily represented as JSON. Just need to figure what problem I am trying to solve by using it.  Share a list of feeds is the classic case but looking at twtxt as well as the [newsboat](https://newsboat.org/) URL list makes me think it is more than I need. We&#39;ll see.  It is certainly reasonable to generate from a simpler source. If I ever write a personal search engine (something I&#39;ve been thinking about to nearly a decade) it&#39;d be a good way to share curated indexes sources as well as sources to crawl.  I just need to think that through more.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/01/Feeds-formats-and-plain-text.md" length="6197" type="text/markdown" />
    </item>    <item>
      <title>Installing Cargo/Rust on Raspberry Pi 400</title>
      <link>https://rsdoiel.github.io/blog/2022/11/01/installing-cargo-rust-r400.html</link>
      <description>
        <![CDATA[On my Raspberry Pi 400 I'm running the 64bit Raspberry Pi OS.
        The version of Cargo and Rustc are not recent enough to install
        [ncgopher](https://github.com/jansc/ncgopher). What worked for
        me was to first install cargo via the instructions in the [The Cargo Book](https://doc.rust-lang.org/cargo/getting-started/installation.html). 
        
        ...]]>
      </description>
      <source:markdown>Installing Cargo/Rest on Raspberry Pi 400
=========================================

On my Raspberry Pi 400 I&#39;m running the 64bit Raspberry Pi OS.
The version of Cargo and Rustc are not recent enough to install
[ncgopher](https://github.com/jansc/ncgopher). What worked for
me was to first install cargo via the instructions in the [The Cargo Book](https://doc.rust-lang.org/cargo/getting-started/installation.html). 

~~~shell
curl https://sh.rustup.rs -sSf | sh
~~~

An important note is if you previously installed a version of Cargo/Rust
via the debian package system you should uninstall it before running the
instructions above from the Cargo Book.

With this version I was able to install __ncgopher__ using the simple
recipe of 

~~~shell
cargo install ncgopher
~~~</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/11/01/installing-cargo-rust-r400.md" length="1599" type="text/markdown" />
    </item>    <item>
      <title>7:30 AM, Oberon Language: A minimum SYSTEM module</title>
      <link>https://rsdoiel.github.io/blog/2022/10/18/Wishlist-Oberon-in-2023-2022-10-18_070730.html</link>
      <description>
        <![CDATA[# 7:30 AM, Oberon Language: A minimum SYSTEM module
        
        Post: Tuesday, October 18, 2022, 7:30 AM
        
        It occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.]]>
      </description>
      <source:markdown># 7:30 AM, Oberon Language: A minimum SYSTEM module

Post: Tuesday, October 18, 2022, 7:30 AM

It occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/10/18/Wishlist-Oberon-in-2023-2022-10-18_070730.md" length="1966" type="text/markdown" />
    </item>    <item>
      <title>Wish list for Oberon in 2023</title>
      <link>https://rsdoiel.github.io/blog/2022/10/16/Wishlist-Oberon-in-2023.html</link>
      <description>
        <![CDATA[Next year will be ten years since Prof. Wirth and Paul Reed released [Project Oberon 2013](https://www.projectoberon.com).  It took me most of that decade to stumble on the project and community.  I am left wondering if Prof. Wirth and Paul Reed sat down today what would they design? I think only minor changes are needed and those mostly around assumptions.
        
        
        ...]]>
      </description>
      <source:markdown>Wish list for Oberon in 2023
===========================

Next year will be ten years since Prof. Wirth and Paul Reed released [Project Oberon 2013](https://www.projectoberon.com).  It took me most of that decade to stumble on the project and community.  I am left wondering if Prof. Wirth and Paul Reed sat down today what would they design? I think only minor changes are needed and those mostly around assumptions.

Oberon-07 changing assumptions
------------------------------

The language of Oberon-07 doesn&#39;t need to change. I do think the assumptions of the compiler are worth revisiting.  A CHAR should not be assumed to be an eight bit byte.  A CHAR should represent a character or symbol in a language. Many if not most of the Oberon community speaks language other than American English and that which is trivially represented in seven or eight bit ASCII.  While changing the representation assumption in Oberon-07 does increase complexity I feel restrict the character presentation of a CHAR to eight bits puts us on the side of &#34;too simple&#34; in the equation of &#34;Simpler but not to simple&#34;.

I am concerned about the assumption of an INTEGER as 32 bits. Increasingly I&#39;ve seen single board computer implementations that are 64 bits.  Today feels allot like when I started in computing where personal computers were shifting from eight or sixteen bits to thirty two.  I suspect increasingly we will find that eight, sixteen and thirty two bit computers are relegated to the realm of specialized computers. While supporting these other widths will remain important I think shifting assumptions to sixty four bit machines makes sense now. Is a 32 bit machine &#34;too simple&#34; in our equation of &#34;simpler but not too simple&#34;?



Oberon as Operating System
--------------------------

The operating system I still find liberating in 2023 as when I first was able to use it.  The challenge in 2023 though is the three button mouse. I think the supporting the historic mouse remains important but that the viewers should also support navigation via the keyboard and easily support touch systems that lack a mouse.  Being backward compatibly while adopting an enhance UI would make things more complex bit if care is taken I think that it can be done while keep the equation balanced as &#34;simpler, but not too simple&#34;.

Transforming my assumptions in 2023
-----------------------------------

I think the Artemis Project should presume that the representation of CHAR and INTEGER may change and probably should change. The portable modules should support compiling Oberon-07 programs on non-Oberon 2014 Systems without change.  I am skeptical that I can create a module system that provides a base line with the historic Oberon implementations. I think the Oakwood modules are just too limited. I think the assumption is I need implementations for Project Oberon 2013 modules as the base line perhaps enhanced with a few additional modules to supporting networking, UTF-8, JSON, and XML. The goal I think is that using Artemis on a non-Oberon System should facilitate bootstrapping an Oberon System 2023 should one come to exist.

Errata
------

7:00 - 7:30; Oberon Language; A minimum SYSTEM module; It occurred to me that while the SYSTEM module will need to address the specifics of the hardware and host environment it could support a minimum set of useful constants. What would be extremely helpful would be able to rely on knowing the max size of an INTEGER, the size of CHAR (e.g. 8, 16 or 32 bits), default character encoding used by the compiler (e.g. ASCII, UTF-8). Likewise it would be extremely helpful to know the the CPU type (e.g. arm64, amd64, x86-32), Operating System/version and name/version of the compiler.  I think this would allow the modules that depend on SYSTEM directly to become slightly more portable.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/10/16/Wishlist-Oberon-in-2023.md" length="4683" type="text/markdown" />
    </item>    <item>
      <title>7:30 AM, Gopher: Setup</title>
      <link>https://rsdoiel.github.io/blog/2022/10/10/getting-things-setup-2022-10-10_070730.html</link>
      <description>
        <![CDATA[# 7:30 AM, Gopher: Setup
        
        Post: Monday, October 10, 2022, 7:30 AM
        
        Account verified, Yippee!]]>
      </description>
      <source:markdown># 7:30 AM, Gopher: Setup

Post: Monday, October 10, 2022, 7:30 AM

Account verified, Yippee!</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/10/10/getting-things-setup-2022-10-10_070730.md" length="616" type="text/markdown" />
    </item>    <item>
      <title>Getting things setup</title>
      <link>https://rsdoiel.github.io/blog/2022/10/09/getting-things-setup.html</link>
      <description>
        <![CDATA[I'm digging my [gopherhole on sdf.org](gopher://sdf.org:70/0/users/rsdoiel)
        as I wait for my validation to go through.  The plan is to migrate content
        from rsdoiel.github.io to here and host it in a Gopher context.  It's
        interesting learning my way around sdf.org. Reminds me of my student days
        when I first had access to a Unix system.  Each Unix has it own flavors and
        even for the same Unix type/version each system has it's own particular
        variation. Unix lends itself to customization and that why one system can
        "feel" or "look" different than the next.
        
        ...]]>
      </description>
      <source:markdown>Getting things setup
====================

By R. S. Doiel, 2022-10-09

I&#39;m digging my [gopherhole on sdf.org](gopher://sdf.org:70/0/users/rsdoiel)
as I wait for my validation to go through.  The plan is to migrate content
from rsdoiel.github.io to here and host it in a Gopher context.  It&#39;s
interesting learning my way around sdf.org. Reminds me of my student days
when I first had access to a Unix system.  Each Unix has it own flavors and
even for the same Unix type/version each system has it&#39;s own particular
variation. Unix lends itself to customization and that why one system can
&#34;feel&#34; or &#34;look&#34; different than the next.

I&#39;m trying to remember how to use Pico (vi isn&#39;t available yet).
Discovering how far &#34;mkgopher&#34; can go (very far it turns out).

I&#39;m looking forward to validation so I can have access to Git and
&#34;move in&#34; to this gopherspace in a more sustainable way.

Things to read and do
---------------------

- wait to be validated
- learn [gitia](https://git.sdf.org) and setup up a mirror my personal projects and blog
- read up on [gophernicus](https://www.gophernicus.org/) (the gopher server used by sdf.org)
- [twenex project](https://www.twenex.org/), sounds interesting,
  I remember accessing a TOPS-20 system at Whitesands in New Mexico
  once upon a time.
- figure out to access comp.lang.oberon if it is available on sdf.org
- figure out, after validation, if I can compile OBNC for working on
  Artemis and Oberon-07 code projects

Updates
-------

7:30 - 7:30; Gopher; Setup; Account verified, Yippee!</source:markdown>
      <author>rsdoiel@sdf.org  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/10/09/getting-things-setup.md" length="2551" type="text/markdown" />
    </item>    <item>
      <title>Thinking about Gopher</title>
      <link>https://rsdoiel.github.io/blog/2022/09/28/thinking-about-gopher.html</link>
      <description>
        <![CDATA[Last weekend I visited the [Gophersphere](gopher://gopher.floodgap.com "Floodgap is a good starting point for Gopher") for the first time in a very long time. I'm happy to report it is still alive an active. It remains fast, lower resource consuming. This resulted in finding a Gopher protocol package in Go and adding light weight Gopher server to [pttk](https://rsdoiel.github.io/pttk) my current vehicle for experimenting with plain text writing tools.
        
        I've been thinking allot this past half week about where to explore in Gopher. The biggest issue I ran into turned out to be easily solve. Gopher protocol is traditionally served over port 70 but if you're running a \*nix if you are just experimenting on localhost it is easier to use port 7000 (analogous to port 80 becoming 8000 or 8080 in the websphere). But some Gopher clients will only serve port 70. Two clients work very well at 7000 and they are Lynx (the trusty old console web browser) and one written in Rust called [ncgopher](https://github.com/jansc/ncgopher). The latter I find I use most of the time. It also supports Gemini sites though I am less interested in Gemini at the movement.  Gopher has a really nice sweet spot of straight forward implementation for both client and server. It would be a good exercise for moving from beginner to intermediate programming classes as you would be introducing network programming, a little parsing and the client server application models. It's a really good use case of looking back (Gopher is venerable in Internet age) and looking forward (a radical simplification of distributing readable material and related files).
        
        ...]]>
      </description>
      <source:markdown>Thinking about Gopher
=====================

By R. S. Doiel, 2022-09-28

Last weekend I visited the [Gophersphere](gopher://gopher.floodgap.com &#34;Floodgap is a good starting point for Gopher&#34;) for the first time in a very long time. I&#39;m happy to report it is still alive an active. It remains fast, lower resource consuming. This resulted in finding a Gopher protocol package in Go and adding light weight Gopher server to [pttk](https://rsdoiel.github.io/pttk) my current vehicle for experimenting with plain text writing tools.

I&#39;ve been thinking allot this past half week about where to explore in Gopher. The biggest issue I ran into turned out to be easily solve. Gopher protocol is traditionally served over port 70 but if you&#39;re running a \*nix if you are just experimenting on localhost it is easier to use port 7000 (analogous to port 80 becoming 8000 or 8080 in the websphere). But some Gopher clients will only serve port 70. Two clients work very well at 7000 and they are Lynx (the trusty old console web browser) and one written in Rust called [ncgopher](https://github.com/jansc/ncgopher). The latter I find I use most of the time. It also supports Gemini sites though I am less interested in Gemini at the movement.  Gopher has a really nice sweet spot of straight forward implementation for both client and server. It would be a good exercise for moving from beginner to intermediate programming classes as you would be introducing network programming, a little parsing and the client server application models. It&#39;s a really good use case of looking back (Gopher is venerable in Internet age) and looking forward (a radical simplification of distributing readable material and related files).

Constraints and creativity
--------------------------

The simplicity and limitations of Gopher are inspiring. The limitations are particularly important as they are good constraints that help focus where to innovate. Gopher is a protocol ripe for software innovation precisely because of it&#39;s constraints.

Gophermaps is a good example. The Go package [git.mills.io/prologic/go-gopjer](https://git.mills.io/prologic/go-gopher) supports easily building servers that have Gophermaps the way of structuring the Gopher menus (aka selectors in Gopher parlance). A Gophermaps is a plain text file where you have lines that start with a Gopher supported document type (see [Gopher protocol](https://en.wikipedia.org/wiki/Gopher_(protocol) for details) a label followed by a tab character, a relative path followed by a tab character, a hostname followed by a tab character and a port number.  Very simple to parse.  The problem is Gopher clients expect all the fields for them to interpret them as a linked resource (e.g. a text file, binary file, image, or another Gopher selector). When I first encountered Gopher at USC so many years ago (pre-Mosaic, pre-Netscape) Gophermaps selectors are trivial to setup and you could build a service that supported ftp and Gopher in the same directory structure. All the &#34;development&#34; of a gopher site was done directly on the server in the directories where the files would live. Putting in all values seemed natural. Today I don&#39;t develop on a &#34;production server&#34; if I can avoid it. My writing is done on a small pool of machines at home, each with its own name. Explicitly writing a hostname and port with the eye to publishing to a public site then becomes a game of running `sed` to correct hostname and ports across the updated Gophermap files.

&#62; Gopher selectors form &#34;links&#34; to navigate through a Gopher site or through the Gophersphere depending on what they point at

Without changing the protocol you could modify the go-gopher package&#39;s function for presenting a Gophermap where the hostname port is assumed to the gopher server name and port if it was missing. Another approach would be to translate a place holder value. This would facilitate keeping your Gopher site under version control (e.g. Git and GitHub) while allowing you to easily deploy a version of the site in a public setting or in your development setting.  The constraint of the Gophermap definition as needed by the protocol doesn&#39;t mean it forces a cumbersome choice on your writing process.

Similarly the spaces versus tabs (invisible by default in many editors) because a non-issue by adopting editors that support [editorconfig](https://editorconfig.org) or even making the server slightly more complex in correctly identifying when to convert spaces to tabs expanding them out to a Gopher selectors.

Client sites there are also many opportunities.  [Little Gopher Client](http://runtimeterror.com/tools/gopher/) pulls out the selectors its finds into a nice tree (like a bookmark tree) in a left panel and puts the text in the main window.  ncgopher let&#39;s you easily bookmark things and has a very clean, easy on the eyes reading experience in the console. In principle you the client could look at the retrieved selector and choose to display different file types based on the file extension as well as the selector type retrieved. This would let you include a richer experience in the Gophersphere for light weight markup like Commonmark files while still running nicely on Gopher protocol. Lots of room to innovate because the protocol is simple, limited and stable after all these years.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/09/28/thinking-about-gopher.md" length="7407" type="text/markdown" />
    </item>    <item>
      <title>Rust development notes</title>
      <link>https://rsdoiel.github.io/blog/2022/09/27/rust-development-notes.html</link>
      <description>
        <![CDATA[I recently wanted to try [ncgopher](https://github.com/jansc/ncgopher) which is a [rust](https://rust-lang.org) based application. I was working on a an M1 Mac mini. I use [Mac Ports](https://www.macports.org) for my userland applications and installed [cargo](https://doc.rust-lang.org/cargo/) to pickup the rust compiler and build tool
        
        ```shell
        sudo port install cargo
        ```
        
        All went well until I tried to build ncgopher and got an error as follows
        
        ...]]>
      </description>
      <source:markdown>Rust development notes
======================

by R. S. Doiel, 2022-09-27

I recently wanted to try [ncgopher](https://github.com/jansc/ncgopher) which is a [rust](https://rust-lang.org) based application. I was working on a an M1 Mac mini. I use [Mac Ports](https://www.macports.org) for my userland applications and installed [cargo](https://doc.rust-lang.org/cargo/) to pickup the rust compiler and build tool

```shell
sudo port install cargo
```

All went well until I tried to build ncgopher and got an error as follows

```
cargo build --release
    Updating crates.io index
error: Unable to update registry `crates-io`

Caused by:
  failed to fetch `https://github.com/rust-lang/crates.io-index`

Caused by:
  failed to authenticate when downloading repository: git@github.com:rust-lang/crates.io-index

  * attempted ssh-agent authentication, but no usernames succeeded: `git`

  if the git CLI succeeds then `net.git-fetch-with-cli` may help here
  https://doc.rust-lang.org/cargo/reference/config.html#netgit-fetch-with-cli

Caused by:
  no authentication available
make: *** [build] Error 101
```

This seemed odd as I could run `git clone git@github.com:rust-lang/crates.io-index` successfully. Re-reading the error message a dim light went on. I checked the cargo docs and the value `net.git-fetch-with-cli` defaults to false. That meant that cargo was using its own embedded git. OK, that makes sense but how do I fix it. I had no problem using cargo installed via ports on an Intel iMac so what gives? When cargo got installed on the M1 there was now `.cargo/config.toml` file. If you create this and set the value of `git-fetch-with-cli` to true then the problem resolves itself.

It was good that the error message provided a lead. It&#39;s also good that cargo has nice documentation. My experience though still left the taste of [COIK](https://www.urbandictionary.com/define.php?term=coik). Not sure how to improve the situation. It&#39;s not really a cargo bug (unless config.taml should be always created), it&#39;s not a rust bug and I don&#39;t even think it is a ports packaging bug.  If I was a new developer just getting familiar with git I don&#39;t think I would have known how to solve my problem even with the documentation provided. Git is something that has always struggled with COIK. While I like it it does make things challenging.

If I wind up playing with rust more then I&#39;ll add somemore notes here in the future.

My `$HOME/.cargo/config.toml` file looks like to have cargo use the git cli instead of the built in rust library.

```
[net]
git-fetch-with-cli = true
```</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/09/27/rust-development-notes.md" length="3495" type="text/markdown" />
    </item>    <item>
      <title>7:30 AM, Golang: pttk</title>
      <link>https://rsdoiel.github.io/blog/2022/09/26/golang-development-2022-09-26_070730.html</link>
      <description>
        <![CDATA[# 7:30 AM, Golang: pttk
        
        Post: Monday, September 26, 2022, 7:30 AM
        
        renamed "pandoc toolkit" (pdtk) to "plain text toolkit" (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.]]>
      </description>
      <source:markdown># 7:30 AM, Golang: pttk

Post: Monday, September 26, 2022, 7:30 AM

renamed &#34;pandoc toolkit&#34; (pdtk) to &#34;plain text toolkit&#34; (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/09/26/golang-development-2022-09-26_070730.md" length="1000" type="text/markdown" />
    </item>    <item>
      <title>PostgreSQL dump and restore</title>
      <link>https://rsdoiel.github.io/blog/2022/09/19/PostgreSQL-Dump-and-Restore.html</link>
      <description>
        <![CDATA[This is a quick note on easily dumping and restoring a specific database
        in Postgres 14.5.  This example has PostgreSQL running on localhost and
        [psql](https://www.postgresql.org/docs/current/app-psql.html) and
        [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) are both available.
        Our database administrator username is "admin", the database to dump is called "collections". The SQL dump
        file will be named "collections-dump-2022-09-19.sql".
        
        ...]]>
      </description>
      <source:markdown>PostgreSQL dump and restore
===========================

This is a quick note on easily dumping and restoring a specific database
in Postgres 14.5.  This example has PostgreSQL running on localhost and
[psql](https://www.postgresql.org/docs/current/app-psql.html) and
[pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) are both available.
Our database administrator username is &#34;admin&#34;, the database to dump is called &#34;collections&#34;. The SQL dump
file will be named &#34;collections-dump-2022-09-19.sql&#34;.

```shell
	pg_dump --username=admin --column-inserts \
	    collections &#62;collections-dump-2022-09-19.sql
```

For the restore process I follow these steps

1. Using `psql` create an empty database to restore into
2. Using `psql` replay (import) the dump file in the new database to restoring the data

The database we want to restore our content into is called &#34;collections_snapshot&#34;

```shell
	psql -U dbadmin
	\c postgres
	DROP DATABASE IF EXISTS collections_snapshot;
	CREATE DATABASE collections_snapshot;
	\c collections_snapshots
	\i ./collections-dump-2022-09-19.sql
	\q
```

Or if you want to stay at the OS shell level

```shell
	dropdb collections_snapshot
	createdb collections_snapshot
	psql -U dbadmin --dbname=collections_snapshot -f ./collections-dump-2022-09-19.sql
```


NOTE: During this restore process `psql` will display some output. This is normal. The two
types of lines output are shown below.

```sql
	INSERT 0 1
	ALTER TABLE
```

If you want to stop the input on error you can use the `--set` option to set the error behavior
to abort the reload if an error is encountered.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/09/19/PostgreSQL-Dump-and-Restore.md" length="2541" type="text/markdown" />
    </item>    <item>
      <title>12:30 PM, SQL: Postgres</title>
      <link>https://rsdoiel.github.io/blog/2022/09/19/rosette-notes-2022-09-19_121230.html</link>
      <description>
        <![CDATA[# 12:30 PM, SQL: Postgres
        
        Post: Monday, September 19, 2022, 12:30 PM
        
        Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users]]>
      </description>
      <source:markdown># 12:30 PM, SQL: Postgres

Post: Monday, September 19, 2022, 12:30 PM

Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/09/19/rosette-notes-2022-09-19_121230.md" length="1393" type="text/markdown" />
    </item>    <item>
      <title>Ordering front matter</title>
      <link>https://rsdoiel.github.io/blog/2022/08/30/Ordering-Frontmatter.html</link>
      <description>
        <![CDATA[A colleague of mine ran into an interesting Pandoc behavior. He was combining a JSON metadata document and a converted word document and wanted the YAML front matter to have a specific order of fields (makes it easier for us humans to quickly scan it and see what the document was about).
        
        The order he wanted in the front matter was
        
        - title
        - interviewer
        - interviewee
        - abstract
        
        This was for a collection of oral histories. When my friend use Pandoc's `--metadata-json` to read the JSON metadata it rendered the YAML fine except the attributes were listed in alphabetical order.
        
        We found a solution by getting Pandoc to treat the output not as Markdown plain text so that we could template the desired order of attributes.
        
        Here's the steps we used.
        
        ...]]>
      </description>
      <source:markdown>Ordering Front Matter
=====================

By R. S. Doiel, 2022-08-30

A colleague of mine ran into an interesting Pandoc behavior. He was combining a JSON metadata document and a converted word document and wanted the YAML front matter to have a specific order of fields (makes it easier for us humans to quickly scan it and see what the document was about).

The order he wanted in the front matter was

- title
- interviewer
- interviewee
- abstract

This was for a collection of oral histories. When my friend use Pandoc&#39;s `--metadata-json` to read the JSON metadata it rendered the YAML fine except the attributes were listed in alphabetical order.

We found a solution by getting Pandoc to treat the output not as Markdown plain text so that we could template the desired order of attributes.

Here&#39;s the steps we used.

1. create an empty file called &#34;empty.txt&#34; (this is just so you pandoc doesn&#39;t try to read standard input and processes
you metadata.json file with the template supplied)
2. Create a template with the order you want (see below)
3. Use pandoc to process your &#34;.txt&#34; file and your JSON metadata file using the template (it makes it tread it as plain text even though we&#39;re going to treat it as markdown later)
4. Append the content of the word file and run pandoc over your combined file as you would normally to generate your HTML


This is the contents of our [metadata.json](metadata.json) file.

```json
    {
        &#34;title&#34;: &#34;Interview with Mojo Sam&#34;, 
        &#34;interviewee&#34;: &#34;Mojo Sam&#34;, 
        &#34;interviewer&#34;: &#34;Tom Lopez&#34;,
        &#34;abstract&#34;: &#34;Interview in three sessions over sevaral decases, 1970 - 20020. The interview was conducted next door to reality via a portal in Old Montreal&#34;
    }
```

[frontmatter.tmpl](frontmatter.tmpl) is the template we used to render ordered front matter.

```
    ---
    $if(title)$title: &#34;$title$&#34;$endif$
    $if(interviewee)$interviewee: &#34;$interviewee$&#34;$endif$
    $if(interviewer)$interviewer: &#34;$interviewer$&#34;$endif$
    $if(abstract)$abstract: &#34;$abstract$&#34;$endif$
    ---
```

Here&#39;s the commands we used to generate a &#34;doc.txt&#34; file with the 
front matter in the desired order. Not &#34;empty.txt&#34; is just an empty
file so Pandoc will not read from standard input and just work with the
JSON metadata and our template.

```
touch empty.txt
pandoc --metadata-file=metadata.json --template=frontmatter.tmpl empty.txt
```

The output of the pandoc command looks like this.

```
    ---
    title: &#34;Interview with Mojo Sam&#34;
    interviewee: &#34;Mojo Sam&#34;
    interviewer: &#34;Tom Lopez&#34;
    abstract: &#34;Interview in three sessions over sevaral decases, 1970 -
    20020. The interview was conducted next door to reality via a portal in
    Old Montreal&#34;
    ---
```</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/30/Ordering-Frontmatter.md" length="3932" type="text/markdown" />
    </item>    <item>
      <title>10:30 AM, SQL: Postgres</title>
      <link>https://rsdoiel.github.io/blog/2022/08/26/rosette-notes-2022-08-26_101030.html</link>
      <description>
        <![CDATA[# 10:30 AM, SQL: Postgres
        
        Post: Friday, August 26, 2022, 10:30 AM
        
        If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \[Quickstart\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.]]>
      </description>
      <source:markdown># 10:30 AM, SQL: Postgres

Post: Friday, August 26, 2022, 10:30 AM

If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \[Quickstart\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/26/rosette-notes-2022-08-26_101030.md" length="1186" type="text/markdown" />
    </item>    <item>
      <title>Postgres 14 on Ubuntu 22.04 LTS</title>
      <link>https://rsdoiel.github.io/blog/2022/08/26/postgres-14-on-ubuntu-22.04-LTS.html</link>
      <description>
        <![CDATA[This is just a quick set of notes for working with Postgres 14 on an Ubuntu 22.04 LTS machine.  The goal is to setup Postgres 14 and have it available for personal work under a user account (e.g. jane.doe). 
        
        Assumptions
        
        - include `jane.doe` is in the sudo group
        - `jane.doe` is the one logged in and installing Postgres for machine wide use
        - `jane.doe` will want to work with her own database by default
        
        Steps
        
        1. Install Postgres
        2. Confirm installation
        3. Add `jane.doe` user providing access
        
        Below is the commands I typed to run to complete the three steps.
        
        ...]]>
      </description>
      <source:markdown>Postgres 14 on Ubuntu 22.04 LTS
===============================

by R. S. Doiel, 2022-08-26

This is just a quick set of notes for working with Postgres 14 on an Ubuntu 22.04 LTS machine.  The goal is to setup Postgres 14 and have it available for personal work under a user account (e.g. jane.doe). 

Assumptions

- include `jane.doe` is in the sudo group
- `jane.doe` is the one logged in and installing Postgres for machine wide use
- `jane.doe` will want to work with her own database by default

Steps

1. Install Postgres
2. Confirm installation
3. Add `jane.doe` user providing access

Below is the commands I typed to run to complete the three steps.

~~~shell
sudo apt install postgresql postgresql-contrib
sudo -u createuser --interactive
jane.doe
y
~~~

What we&#39;ve accomplished is installing Postgres, we&#39;ve create a user in Postgres DB environment called &#34;jane.doe&#34; and given &#34;jane.doe&#34; superuser permissions, i.e. the permissions to manage Postgres databases.

At this point we have a `jane.doe` Postgres admin user. This means we can run the `psql` shell from the Jane Doe account to do any database manager tasks. To confirm I want to list the databases available

~~~shell
psql 
SELECT datname FROM pg_database;
\quit
~~~

NOTE: This post is a distilation of what I learned from reading Digital Ocean&#39;s [How To Install PostgreSQL on Ubuntu 22.04 \[Quickstart\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/26/postgres-14-on-ubuntu-22.04-LTS.md" length="2614" type="text/markdown" />
    </item>    <item>
      <title>12:00 PM, SQL: Postgres</title>
      <link>https://rsdoiel.github.io/blog/2022/08/24/rosette-notes-2022-08-24_121200.html</link>
      <description>
        <![CDATA[# 12:00 PM, SQL: Postgres
        
        Post: Wednesday, August 24, 2022, 12:00 PM
        
        I miss `SHOW TABLES` it's just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE 'pg_%'
        `. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL's "SHOW TABLES". Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.]]>
      </description>
      <source:markdown># 12:00 PM, SQL: Postgres

Post: Wednesday, August 24, 2022, 12:00 PM

I miss `SHOW TABLES` it&#39;s just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE &#39;pg_%&#39;
`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL&#39;s &#34;SHOW TABLES&#34;. Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/24/rosette-notes-2022-08-24_121200.md" length="1307" type="text/markdown" />
    </item>    <item>
      <title>A Quick into to PL/pgSQL</title>
      <link>https://rsdoiel.github.io/blog/2022/08/24/plpgsql-quick-intro.html</link>
      <description>
        <![CDATA[L/pgSQL is a procedure language extended from SQL. It adds flow control and local state for procedures, functions and triggers. Procedures, functions and triggers are also the compilation unit. Visually PL/pgSQL looks similar to the MySQL or ORACLE counter parts. It reminds me of a mashup of ALGO and SQL. Like the unit of compilation, the unit of execution is also procedure, function or trigger. 
        
        The Postgres documentation defines and explains the [PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql.html) and how it works.  This document is just a quick orientation with specific examples to provide context.
        
        ...]]>
      </description>
      <source:markdown>A Quick intro to PL/pgSQL
========================

PL/pgSQL is a procedure language extended from SQL. It adds flow control and local state for procedures, functions and triggers. Procedures, functions and triggers are also the compilation unit. Visually PL/pgSQL looks similar to the MySQL or ORACLE counter parts. It reminds me of a mashup of ALGO and SQL. Like the unit of compilation, the unit of execution is also procedure, function or trigger. 

The Postgres documentation defines and explains the [PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql.html) and how it works.  This document is just a quick orientation with specific examples to provide context.

Hello World
-----------

Here is a &#34;helloworld&#34; procedure definition.

```sql
    CREATE PROCEDURE helloworld() AS $$
    DECLARE
    BEGIN
       RAISE NOTICE &#39;Hello WORLD!&#39;;
    END;
    $$ LANGUAGE plpgsql;
```

Let&#39;s take a look this line by line.

1. CREATE PROCEDURE defines the procedure and the starting and ending delimiter for the procedure (e.g. `AS $$` the procedure&#39;s text ends when `$$` is encountered an second time.
2. DECLARE is the block where you would declare the variables used in the procedure, we have none in this example
3. The BEGIN starts the actual procedure instructions
4. The `RAISE NOTICE` line is how you can display output to the console when the procedure is run
5. The END completes the procedure definition
6. the `$$ LANGUAGE plpgsql;` concludes the text defining the procedure and tells the database engine that procedure is written in PL/pgSQL.

We can run the procedure using the &#34;CALL&#34; query.

```sql
    CALL helloworld()
```

NOTE: If you want to change the procedure you can &#34;DROP&#34; it first otherwise you&#39;ll get an error that it already exists.

```sql
    DROP PROCEDURE helloworld;
```

Improving my workflow
---------------------

SQL procedures are generally stored in the RDBMs in database environment. You can think of them as records in the system&#39;s database. Procedures and functions are created and can be dropped. While they can be manually typed in the database&#39;s shell it is easier to maintain them in plain text files outside the RDBM environment.  

1. Write the procedure in a text file.
2. Load the text file (e.g. FILENAME) into Postgres 
   a. outside the Postgres shell use `psql -f FILENAME` 
   b. inside the Postgres shell used `\i FILENAME`
3. Call the procedure to test it

To turn these steps into a look I use a &#34;CREATE OR REPLACE&#34; statement and be able to reload the updated procedure easier see [43.12. Tips for Developing in PL/pgSQL](https://www.postgresql.org/docs/14/plpgsql-development-tips.html).  Note in the revised example the &#34;-- &#34; lines are comments.

Our revised [helloworld](helloworld.plpgsql).

```sql
    --
    -- Create (or replace) the new &#34;helloworld&#34; procedure.
    -- NOTE: this can be run with &#34;CALL&#34;
    --
    CREATE OR REPLACE PROCEDURE helloworld() AS $$
    DECLARE
    BEGIN
        RAISE NOTICE &#39;Hello World!&#39;;
    END;
    $$ LANGUAGE plpgsql;
```


Hi There
--------

[hithere](hithere.plpgsql) is similar to our helloworld example except it is a function that takes a parameter of the person&#39;s name. The function returns a &#34;VARCHAR&#34;, so this should work as part of a select statement.

```sql
    --
    -- This is a &#34;Hi There&#34; function. The function takes
    -- a single parameter and forms a greeting.
    --
    CREATE OR REPLACE FUNCTION hithere(name varchar) RETURNS varchar AS $$
    DECLARE
      greeting varchar;
    BEGIN
        IF name = &#39;&#39; THEN
            greeting := &#39;Hi there!&#39;;
        ELSE
            greeting := &#39;Hello &#39; || name || &#39;!&#39;;
        END IF;
        RETURN greeting;
    END;
    $$ LANGUAGE plpgsql;
```

Giving it a try.

```shell
    SELECT hithere(&#39;Mojo Sam&#39;);
```

Further reading
---------------

- [Conditionals](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONDITIONALS)
- [Loops](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-CONTROL-STRUCTURES-LOOPS)
- [Calling a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-CALLING-PROCEDURE)
- [Early return from a procedure](https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-STATEMENTS-RETURNING-PROCEDURE)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/24/plpgsql-quick-intro.md" length="5454" type="text/markdown" />
    </item>    <item>
      <title>11:30 AM, SQL: Postgres</title>
      <link>https://rsdoiel.github.io/blog/2022/08/22/rosette-notes-2022-08-22_111130.html</link>
      <description>
        <![CDATA[Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is "REPLACE" is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I've used "?" with SQLite and MySQL but with Postgres I need to use "$1", "$2", etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.
        
        
        ...]]>
      </description>
      <source:markdown># 11:30 AM, SQL: Postgres

Post: Monday, August 22, 2022, 11:30 AM

Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is &#34;REPLACE&#34; is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I&#39;ve used &#34;?&#34; with SQLite and MySQL but with Postgres I need to use &#34;$1&#34;, &#34;$2&#34;, etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/22/rosette-notes-2022-08-22_111130.md" length="2016" type="text/markdown" />
    </item>    <item>
      <title>Rosette Notes: Postgres and MySQL</title>
      <link>https://rsdoiel.github.io/blog/2022/08/19/rosette-notes.html</link>
      <description>
        <![CDATA[> A dance around two relational databases, piecing together similarities as with the tiny mosaic tiles of a guitar's rosette
        
        What follows are my preliminary notes learning Postgres 12 and 14.
        
        ...]]>
      </description>
      <source:markdown>Rosette Notes
=============

By R. S. Doiel, 2022-08-19

&#62; A dance around two relational databases, piecing together similarities as with the tiny mosaic tiles of a guitar&#39;s rosette

What follows are my preliminary notes learning Postgres 12 and 14.

Postgres &#38; MySQL
----------------

This is a short comparison of some administrative commands I commonly use. The first column describes the task followed by the SQL to execute for Postgres 14.5 and then MySQL 8. The presumption is you&#39;re using `psql` to access Postgres and `mysql` to  access MySQL. Values between `&#60;` and `&#62;` should be replaced with an appropriate value.

| Task                    | Postgres 14.5                     | MySQL 8           |
|-------------------------|------------------------------------|-------------------|
| show all databases      | `SELECT datname FROM pg_database;` | `SHOW DATABASES;` |
| select a database       | `\c &#60;dbname&#62;`                      | `USE &#60;dbname&#62;`    |
| show tables in database | `\dt`                              | `SHOW TABLES;`    |
| show columns in table   | `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = &#39;&#60;table_name&#62;&#39;;` | `SHOW COLUMNS IN &#60;table_name&#62;` |

Reflections
-----------

The Postgres shell, `psql`, provides the functionality of showing a list of tables via a short cut while MySQL choose to add the `SHOW TABLES` query. For me `SHOW ...` feels like SQL where as `\d` or `\dt` takes me out of SQL space. On the other hand given Postgres metadata structure the shortcut is appreciated and I often query for table names as I forget them. `\dt` quickly becomes second nature and is shorter to type than `SHOW TABLES`. 

Connecting to a database with `\c` in `psql` is like calling an &#34;open&#34; in programming language. The &#34;connection&#34; in `psql` is open until explicitly closed or the shell is terminated.  Like `USE ...` in the MySQL shell it make working with multiple database easy.  The difference are apparent when you execute a `DROP DATABASE ...` command. In `psql` you need to `CLOSE` the database first or the `DROP` will fail.  The MySQL shell will happily let you drop the current database you are currently using.

The challenge I&#39;ve experienced learning `psql` after knowing MySQL is my lack of familiarity with the metadata Postgres maintains about databases and structures.  On the other hand everything I&#39;ve learned about standards base SQL applies to managing Postgres once remember the database/table I need to work with.  A steeper learning curve from MySQL&#39;s `SHOW` but it also means writing external programs for managing Postgres databases and tables is far easier because everything is visible because that is how you manage Postgres. MySQL&#39;s `SHOW` is very convenient but at the cost of hiding some of its internal structures.

Both MySQL and Postgres support writing programs in SQL. They also support stored procedures, views and triggers. They&#39;ve converged in the degree in which they have both implemented SQL language standards.  The differences are mostly in approach to managing databases.  There are some differences, necessitated by implementation choices, in the `CREATE DATABASE`, `CREATE TABLE` or `ALTER` statements but you can often use the basic form described in ANSI SQL and get the results you need. When doing performance tuning the dialect differences are more important.

Dump &#38; Restore
--------------

Both Postgres and MySQL provide command line programs for dumping a database. MySQL provides a single program where as Postgres splits it in two. Check the man pages (or website docs) for details in their options. Both sets of programs are highly configurable allowing you to dump just schema, just data or both with different expectations.

| Postgres 14.5      | MySQL 8                         |
|--------------------|---------------------------------|
| `pg_dumpall`       | `mysqldump --all-databases`     |
| `pg_dump &#60;dbname&#62;` | `mysqldump --database &#60;dbname&#62;` |

The `pg_dumpall` tool is designed to restore an entire database instance. It includes account and ownership information. `pg_dump` just focuses on the database itself. If you are taking a snapshot production data to use in a test `pg_dump` output is easier to work with. It captures the specific database with out entangling things like the `template1` database or database user accounts and ownership.

You can restore a database dump in both Postgres and MySQL. The tooling is a little different.

| Postgres 14.5                   | MySQL 8                                      |
|---------------------------------|----------------------------------------------|
| `dropdb &#60;dbname&#62;`               | `mysql -execute &#39;DROP DATABASE &#60;dbname&#62;;&#39;`   |
| `createdb &#60;dbname&#62;`             | `mysql -execute &#39;CREATE DATABASE &#60;dbname&#62;;&#39;` |
| `psql -f &#60;dump_filename&#62;`       |`mysql &#60;dbname&#62; &#60; &#60;dump_filename&#62;`            |

NOTE: These instructions work for a database dumped with `pg_dump` for the Postgres example. In principle it is the same way you can restore from `pg_dumpall` but if you Postgres instance already exists then you&#39;re going to run into various problems, e.g. errors about `template1` db.

Lessons learned along the way
-----------------------------

2022-08-22

8:00 - 11:30; SQL; Postgres; Three things have turned out to be challenges in the SQL I write, first back ticks is a MySQL-ism for literal quoting of table and column names, causes problems in Postgres. Second issue is &#34;REPLACE&#34; is a none standard extension I picked up from MySQL [it wraps a DELETE and INSERT together](https://dev.mysql.com/doc/refman/8.0/en/extensions-to-ansi.html), should be using UPDATE more than I have done in the past. The third is parameter replacement in SQL statement. This appears to be [db implementation specific](http://go-database-sql.org/prepared.html). I&#39;ve used &#34;?&#34; with SQLite and MySQL but with Postgres I need to use &#34;$1&#34;, &#34;$2&#34;, etc. Challenging to write SQL once and have it work everywhere. Beginning to understand why GORM has traction.


2022-08-24

11:00 - 12:00; SQL; Postgres; I miss `SHOW TABLES` it&#39;s just muscle memory from MySQL, the SQL to show tables is `SELECT tablename FROM pg_catalog.pg_tables WHERE tablename NOT LIKE &#39;pg_%&#39;;`. I could write a SHOWTABLE in PL/pgSQL procedure implementing MySQL&#39;s &#34;SHOW TABLES&#34;. Might be a good way to learn PL/pgSQL. I could then do one for MySQL and compare the PL/SQL language implementations.

2022-08-26

9:30 - 10:30; SQL; Postgres; If you are looking for instructions on installing Postgres 14 under Ubuntu 22.04 LTS I found DigitalOcean [How To Install PostgreSQL on Ubuntu 22.04 \[Quickstart\]](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart), April 25, 2022 by Alex Garnett helpful.

2022-09-19

10:30 - 12:30; SQL; Postgres; Setting up postgres 14 on Ubuntu shell script, see [https://www.postgresql.org/download/linux/ubuntu/](https://www.postgresql.org/download/linux/ubuntu/), see [https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart](https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-22-04-quickstart) for setting up initial database and users</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/19/rosette-notes.md" length="7989" type="text/markdown" />
    </item>    <item>
      <title>5:45 PM, Golang: ptdk,  stngo</title>
      <link>https://rsdoiel.github.io/blog/2022/08/15/golang-development-2022-08-15_170545.html</link>
      <description>
        <![CDATA[# 5:45 PM, Golang: ptdk,  stngo
        
        Post: Monday, August 15, 2022, 5:45 PM
        
        Thinking through what a "post" from an simple timesheet notation file should look like. One thing occurred to me is that the entry's "end" time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog's homepage and that "title" is what I use for other posts. Maybe I should just use a command line option to provide a title?]]>
      </description>
      <source:markdown># 5:45 PM, Golang: ptdk,  stngo

Post: Monday, August 15, 2022, 5:45 PM

Thinking through what a &#34;post&#34; from an simple timesheet notation file should look like. One thing occurred to me is that the entry&#39;s &#34;end&#34; time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog&#39;s homepage and that &#34;title&#34; is what I use for other posts. Maybe I should just use a command line option to provide a title?</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/15/golang-development-2022-08-15_170545.md" length="2378" type="text/markdown" />
    </item>    <item>
      <title>PTTK and STN</title>
      <link>https://rsdoiel.github.io/blog/2022/08/15/golang-development.html</link>
      <description>
        <![CDATA[This log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the "blogit" action that will translates the simple timesheet notation (aka STN) to a short blog post.  My "short post" interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.
        
        ...]]>
      </description>
      <source:markdown>Pttk and STN
============

By R. S. Doiel, started 2022-08-15
(updated: 2022-09-26, pdtk was renamed pttk)

This log is a proof of concept in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) as a source for very short blog posts. The tooling is written in Golang (though eventually I hope to port it to Oberon-07).  The implementation combines two of my personal projects, [stngo](https://github.com/rsdoiel/stngo) and my experimental writing tool [pttk](https://github.com/rsdoiel/pttk). Updating the __pttk__ cli I added a function to the &#34;blogit&#34; action that will translates the simple timesheet notation (aka STN) to a short blog post.  My &#34;short post&#34; interest is a response to my limited writing time. What follows is the STN markup. See the [Markdown](https://raw.githubusercontent.com/rsdoiel/rsdoiel.github.io/main/blog/2022/08/15/golang-development.md) source for the unprocessed text.

2022-08-15

16:45 - 17:45; Golang; ptdk, stngo; Thinking through what a &#34;post&#34; from an simple timesheet notation file should look like. One thing occurred to me is that the entry&#39;s &#34;end&#34; time is the publication date, not the start time. That way the post is based on when it was completed not when it was started. There is an edge case of where two entries end at the same time on the same date. The calculated filename will collide. In the `BlogSTN()` function I could check for potential file collision and either issue a warning or append. Not sure of the right action. Since I write sequentially this might not be a big problem, not sure yet. Still playing with formatting before I add this type of post to my blog. Still not settled on the title question but I need something to link to from my blog&#39;s homepage and that &#34;title&#34; is what I use for other posts. Maybe I should just use a command line option to provide a title?

2022-08-14

14:00 - 17:00; Golang; pdtk, stngo; Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the &#34;right&#34; place for rendering simple timesheet notation as blog posts is in the the &#34;blogit&#34; action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.

2022-08-12

16:00 - 16:30; Golang; stngo; A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?

2022-09-26

6:30 - 7:30; Golang; pttk; renamed &#34;pandoc toolkit&#34; (pdtk) to &#34;plain text toolkit&#34; (pttk) after adding gopher support to cli. This project is less about writing tools specific to Pandoc and more about writing tools oriented around plain text.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/15/golang-development.md" length="4473" type="text/markdown" />
    </item>    <item>
      <title>5:00 PM, Golang: pdtk,  stngo</title>
      <link>https://rsdoiel.github.io/blog/2022/08/14/golang-development-2022-08-14_170500.html</link>
      <description>
        <![CDATA[# 5:00 PM, Golang: pdtk,  stngo
        
        Post: Sunday, August 14, 2022, 5:00 PM
        
        Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the "right" place for rendering simple timesheet notation as blog posts is in the the "blogit" action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.]]>
      </description>
      <source:markdown># 5:00 PM, Golang: pdtk,  stngo

Post: Sunday, August 14, 2022, 5:00 PM

Today I started an experiment. I cleaned up stngo a little today, still need to implement a general `Parse()` method that works on a `io.Reader`. After a few initial false starts I realized the &#34;right&#34; place for rendering simple timesheet notation as blog posts is in the the &#34;blogit&#34; action of [pdtk](https://rsdoiel.github.io/pttk). I think this form might be useful for both release notes in projects as well as a series aggregated from single paragraphs. The limitation of the single paragraph used in simple timesheet notation is intriguing. Proof of concept is working in v0.0.3 of pdtk. Still sorting out if I need a title and if so what it should be.</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/14/golang-development-2022-08-14_170500.md" length="1926" type="text/markdown" />
    </item>    <item>
      <title>4:30 PM, Golang: stngo</title>
      <link>https://rsdoiel.github.io/blog/2022/08/12/golang-development-2022-08-12_160430.html</link>
      <description>
        <![CDATA[# 4:30 PM, Golang: stngo
        
        Post: Friday, August 12, 2022, 4:30 PM
        
        A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?]]>
      </description>
      <source:markdown># 4:30 PM, Golang: stngo

Post: Friday, August 12, 2022, 4:30 PM

A work slack exchange has perked my interest in using [simple timesheet notation](https://rsdoiel.github.io/stngo/docs/stn.html) for very short blog posts. This could be similar to Dave Winer title less posts on [scripting](http://scripting.com). How would this actually map? Should it be a tool in the [stngo](https://rsdoiel.githubio/stngo) project?</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2022/08/12/golang-development-2022-08-12_160430.md" length="1274" type="text/markdown" />
    </item>    <item>
      <title>Turbo Oberon, the dream</title>
      <link>https://rsdoiel.github.io/blog/2022/07/30/Turbo-Oberon.html</link>
      <description>
        <![CDATA[Sometimes I have odd dreams and that was true last night through early this morning. The dream was set in the future. I was already retired. It was a dream about "Turbo Oberon".
        
        "Turbo Oberon" was an Oberon language. The language compiler was named "TO" in my dream. A module's file extension was ".tom", in honor of Tom Lopez (Meatball Fulton) of ZBS. There were allot of ZBS references in the dream.
        
        "TO" was very much a language in the Oberon-07 tradition with minor extensions when it came to bringing in modules. It allowed for a multi path search for module names. You could also express a Module import as a string allowing providing paths to the imported module.
        
        Compilation was similar to Go. Cross compilation was available out of the box by setting a few environment variables. I remember answering questions about the language and its evolution. I remember mentioning in the conversation about how I thought Go felling into the trap of complexity like Rust or C/C++ before it. The turning point for Go was generics. Complexity was the siren song to be resisted in "Turbo Oberon". Complexity is seductive to language designers and implementers. I was only an implementer.
        
        ...]]>
      </description>
      <source:markdown>Turbo Oberon, the dream
=======================

by R. S. Doiel, 2022-07-30

Sometimes I have odd dreams and that was true last night through early this morning. The dream was set in the future. I was already retired. It was a dream about &#34;Turbo Oberon&#34;.

&#34;Turbo Oberon&#34; was an Oberon language. The language compiler was named &#34;TO&#34; in my dream. A module&#39;s file extension was &#34;.tom&#34;, in honor of Tom Lopez (Meatball Fulton) of ZBS. There were allot of ZBS references in the dream.

&#34;TO&#34; was very much a language in the Oberon-07 tradition with minor extensions when it came to bringing in modules. It allowed for a multi path search for module names. You could also express a Module import as a string allowing providing paths to the imported module.

Compilation was similar to Go. Cross compilation was available out of the box by setting a few environment variables. I remember answering questions about the language and its evolution. I remember mentioning in the conversation about how I thought Go felling into the trap of complexity like Rust or C/C++ before it. The turning point for Go was generics. Complexity was the siren song to be resisted in &#34;Turbo Oberon&#34;. Complexity is seductive to language designers and implementers. I was only an implementer.

Evolution wise &#34;TO&#34; was built initially on the Go tool chain. As a result it featured easily cross-compiled binaries and had a rich standard set of Modules like Go but also included portable libraries for implementing graphic user interfaces. &#34;Turbo Oberon&#34; evolved as a conversation between Go and the clean simplicity of Oberon-07. Two example applications &#34;shipped&#34; with the &#34;TO&#34; compiler. They were an Oberon like Operating System (stand alone and hosted) and a Turbo Pascal like IDE. The IDE was called &#34;toe&#34; for Turbo Oberon Editor. I don&#39;t remember the name of the OS implementation but it might have been &#34;toos&#34;. I remember &#34;TO&#34; caused problems for search engines and catalog systems.

I remember remarking in the dream that programming in &#34;Turbo Oberon&#34; was a little like returning to my roots when I first learned to program in Turbo Pascal. Except I could run my programs regardless of the operating system or CPU architecture. &#34;&#34;TO&#34; compiler supported cross compilation for Unix, macOS, Windows on ARM, Intel, RISC-V. The targets were inherited from Go implementation roots.

In my dream I remember forking Go 1.18 and first replacing the front end of the compiler. I remember it was a challenge understanding the implementation and generate a Go compatible AST. The mapping between Oberon-07 and Go had its challenges. I remember first sticking to a strict Oberon-07 compiler targeting POSIX before enhancing module imports. I remember several failed attempts at getting module imports &#34;right&#34;. I remember being on the fence about a map data type and going with a Maps module.  I don&#39;t remember how introspection worked but saying it was based on an ETH paper for Oberon 2.  I remember the compiler, like Go, eventually became self hosting. It supported a comments based DSL to annotating RECORD types making encoding and decoding convenient, an influence of Go and it&#39;s tool chain.

I believe the &#34;Turbo Oberon Editor&#34; came first and that was followed by the operating system implementation.

I remember talking about a book that influenced me called, &#34;Operating Systems through compilers&#34; but don&#39;t know who wrote it. I remember a discussion about the debt owed to Prof. Wirth. I remember that the book showed how once you really understood building the compile you could then build the OS. There was a joke riffing on the old Lisp joke but rephrased, &#34;all applications evolve not to a Lisp but to an embedded OS&#34;.

It was a pleasant dream, in the dream I was older and already retired but still writing &#34;TO&#34; code and having fun with computers. I remember a closing video shot showing me typing away at what looked like the old Turbo Pascal IDE. As Mojo Sam said in **Somewhere Next Door to Reality**, &#34;it was a sorta a retro future&#34;.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/07/30/Turbo-Oberon.md" length="5754" type="text/markdown" />
    </item>    <item>
      <title>Artemis Project Status, 2022</title>
      <link>https://rsdoiel.github.io/blog/2022/07/27/Artemis-Status-Summer-2022.html</link>
      <description>
        <![CDATA[t's been a while since I wrote an Oberon-07 post and even longer since I've worked on Artemis. Am I done with Oberon-07 and abandoning Artemis?  No. Life happens and free time to just hasn't been available. I don't know when that will change.
        
        What's the path forward? ...]]>
      </description>
      <source:markdown>Artemis Project Status, 2022
============================

It&#39;s been a while since I wrote an Oberon-07 post and even longer since I&#39;ve worked on Artemis. Am I done with Oberon-07 and abandoning Artemis?  No. Life happens and free time to just hasn&#39;t been available. I don&#39;t know when that will change.

What&#39;s the path forward?
------------------------

Since I plan to continue working Artemis I need to find a way forward in much less available time. Time to understand some of my constraints. 

1. I work on a variety of machines, OBNC is the only compiler I&#39;ve consistently been able to use across all my machines
2. Porting between compilers takes energy and time, and those compilers don&#39;t work across all my machines
3. When I write Oberon-07 code I quickly hit a wall for the things I want to do, this is what original inspired Artemis, so there is still a need for a collection of modules
4. Oberon/Oberon-07 on Wirth RISC virtual machine is not sufficient for my development needs
5. A2, while very impressive, isn&#39;t working for me either (mostly because I need to work on ARM CPUs)

These constraints imply Artemis is currently too broadly scoped. I think I need to focus on what works in OBNC for now. Once I have a clear set of modules then I can revisit portability to other compilers.

What modules do I think I need? If I look at my person projects I tend to work allot with text, often structured text (e.g. XML, JSON, CSV). I also tend to be working with network services. Occasionally I need to interact with database (e.g. SQLite3, MySQL, Postgres).  Artemis should provide modules to make it easy to write code in Oberon-07 that works in those areas. Some of that I can do by wrapping existing C libraries. Some I can simply write from scratch in Oberon-07 (e.g. a JSON encoder/decoder). That&#39;s going to me my focus as my hobby time becomes available and then.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/07/27/Artemis-Status-Summer-2022.md" length="2682" type="text/markdown" />
    </item>    <item>
      <title>Installing Golang from source on RPi-OS for arm64</title>
      <link>https://rsdoiel.github.io/blog/2022/02/18/Installing-Go-from-Source-RPiOS-arm64.html</link>
      <description>
        <![CDATA[By R. S. Doiel, 2022-02-18
        
        This are my quick notes on installing Golang from source on the Raspberry Pi OS 64 bit.
        
        1. Get a working compiler
        	a. go to https://go.dev/dl/ and download go1.17.7.linux-arm64.tar.gz
        	b. untar the tarball in your home directory (it'll unpack to $HOME/go)
        	c. `cd go/src` and `make.bash`
        2. Move go directory to go1.17
        3. Clone go from GitHub
        4. Compile with the downloaded compiler
        	a. `cd go/src`
        	b. `env GOROOT_BOOTSTRAP=$HOME/go1.17 ./make.bash`
        	c. Make sure `$HOME/go/bin` is in the path
        	d. `go version`]]>
      </description>
      <source:markdown>Installing Golang from Source on RPi-OS for arm64
==========================================

By R. S. Doiel, 2022-02-18

This are my quick notes on installing Golang from source on the Raspberry Pi OS 64 bit.

1. Get a working compiler
	a. go to https://go.dev/dl/ and download go1.17.7.linux-arm64.tar.gz
	b. untar the tarball in your home directory (it&#39;ll unpack to $HOME/go)
	c. `cd go/src` and `make.bash`
2. Move go directory to go1.17
3. Clone go from GitHub
4. Compile with the downloaded compiler
	a. `cd go/src`
	b. `env GOROOT_BOOTSTRAP=$HOME/go1.17 ./make.bash`
	c. Make sure `$HOME/go/bin` is in the path
	d. `go version`</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2022/02/18/Installing-Go-from-Source-RPiOS-arm64.md" length="1743" type="text/markdown" />
    </item>    <item>
      <title>Notes on setting up a Mid-2010 Mac Mini</title>
      <link>https://rsdoiel.github.io/blog/2021/12/18/Notes-on-setting-up-a-2010-Mac-Mini.html</link>
      <description>
        <![CDATA[I acquired a Mid 2010 Mac Mini. It was in good condition but lacked an OS on the hard drive.  I used a previously purchased copy of Mac OS X Snow Leopard to get an OS up and running on the bare hardware. Then it was a longer effort to get the machine into a state with the software I wanted to use on it. My goal was Mac OS X High Sierra, Xcode 10.1 and Mac Ports. The process was straight forward but very time consuming but I think worth it.  I wound up with a nice machine for experimenting with and writing blog posts.
        
        The setup process was as follows:
        
        1. Install macOS Snow Leopard on the bare disk of the Mac Mini
        2. Install macOS El Capitan on the Mac Mini after manually downloading it from Apple's support site
        3. Run updates indicated by El Capitan
        4. Install macOS High Sierra on the Mac Mini after manually downloading it from the Apple's support site
        5. Run updates indicated by High Sierra 
        6. Manually download and install Xcode 10.1 command line tools 
        7. Check and run some updates again
        8. Finally install Mac Ports
        
        The OS installs took about 45 minutes to 90 minutes each. Installing Xcode took about 45 minutes to an hour. Installing Mac Ports was fast as was installing software via Mac Ports.
        
        ...]]>
      </description>
      <source:markdown>Notes on setting up a Mid-2010 Mac Mini
=======================================

By R. S. Doiel, 2021-12-18

I acquired a Mid 2010 Mac Mini. It was in good condition but lacked an OS on the hard drive.  I used a previously purchased copy of Mac OS X Snow Leopard to get an OS up and running on the bare hardware. Then it was a longer effort to get the machine into a state with the software I wanted to use on it. My goal was Mac OS X High Sierra, Xcode 10.1 and Mac Ports. The process was straight forward but very time consuming but I think worth it.  I wound up with a nice machine for experimenting with and writing blog posts.

The setup process was as follows:

1. Install macOS Snow Leopard on the bare disk of the Mac Mini
2. Install macOS El Capitan on the Mac Mini after manually downloading it from Apple&#39;s support site
3. Run updates indicated by El Capitan
4. Install macOS High Sierra on the Mac Mini after manually downloading it from the Apple&#39;s support site
5. Run updates indicated by High Sierra 
6. Manually download and install Xcode 10.1 command line tools 
7. Check and run some updates again
8. Finally install Mac Ports

The OS installs took about 45 minutes to 90 minutes each. Installing Xcode took about 45 minutes to an hour. Installing Mac Ports was fast as was installing software via Mac Ports.

Reference material
------------------

- Apple support pages that I found helpful
    - [How to get old versions of macOS](https://support.apple.com/en-us/HT211683)
    - [How to create a bootable installer for macOS](https://support.apple.com/en-us/HT201372)
    - [macOS High Sierra - Technical Specifications](https://support.apple.com/kb/SP765?locale=en_US)
- Wikipedia page on [Xcode](https://en.wikipedia.org/wiki/Xcode) is how I sorta out what version of Xcode I needed to install
- Links to old macOS and Xcode
    - Download [Mac OS X El El Capitan](http://updates-http.cdn-apple.com/2019/cert/061-41424-20191024-218af9ec-cf50-4516-9011-228c78eda3d2/InstallMacOSX.dmg)
    - Download [Mac OX X High Sierra](https://apps.apple.com/us/app/macos-high-sierra/id1246284741?mt=12)
    - Download [Xcode 10.1](https://developer.apple.com/download/all/?q=xcode), Scroll down the list until you want it.
        - [Command Line Tools (macOS 10.13) for Xcode 10.1](https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_macOS_10.13_for_Xcode_10.1/Command_Line_Tools_macOS_10.13_for_Xcode_10.1.dmg)
        - NOTE: There are two version available, you want the version for macOS 10.13 (High Sierra) NOT Mac OS 10.14.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/12/18/Notes-on-setting-up-a-2010-Mac-Mini.md" length="4315" type="text/markdown" />
    </item>    <item>
      <title>Setting up FreeDOS 1.3rc4 with Qemu</title>
      <link>https://rsdoiel.github.io/blog/2021/11/27/FreeDOS-1.3rc4-with-Qemu.html</link>
      <description>
        <![CDATA[In this article I'm going explore setting up FreeDOS with Qemu
        on my venerable Dell 4319 running Raspberry Pi Desktop OS (Debian
        GNU/Linux).  First step is to download FreeDOS "Live CD" in the
        1.3 RC4 release. See http://freedos.org/download/ for that.
        
        ## Installing Qemu
        
        I needed to install Qemu in my laptop. It runs the Raspberry Pi
        Desktop OS (i.e. Debian with Raspberry Pi UI). I choose to install
        the "qemu-system" package since I will likely use qemu for other
        things besides FreeDOS. The qemu-system package contains all the
        various systems I might want to emulate in other projects as well
        as several qemu utilities that are handy.  Here's the full sequence
        of `apt` commands I ran (NOTE: these included making sure my laptop
        was up to date before I installed qemu-system).
        
        ...]]>
      </description>
      <source:markdown>Setting up FreeDOS 1.3rc4 with Qemu
-----------------------------------

By R. S. Doiel, 2021-11-27

In this article I&#39;m going explore setting up FreeDOS with Qemu
on my venerable Dell 4319 running Raspberry Pi Desktop OS (Debian
GNU/Linux).  First step is to download FreeDOS &#34;Live CD&#34; in the
1.3 RC4 release. See http://freedos.org/download/ for that.

Installing Qemu
---------------

I needed to install Qemu in my laptop. It runs the Raspberry Pi
Desktop OS (i.e. Debian with Raspberry Pi UI). I choose to install
the &#34;qemu-system&#34; package since I will likely use qemu for other
things besides FreeDOS. The qemu-system package contains all the
various systems I might want to emulate in other projects as well
as several qemu utilities that are handy.  Here&#39;s the full sequence
of `apt` commands I ran (NOTE: these included making sure my laptop
was up to date before I installed qemu-system).

~~~
sudo apt update
sudo apt upgrade
sudo apt install qemu-system
~~~

Now that I had the software available it was time to figure out
how to actually knit things together and run FreeDOS.


Obtaining FreeDOS 1.3rc4
------------------------

Before I get started I create a folder in my home directory
for running everything. You can name it what you want
but I called mine `FreeDOS_13` and changed into that folder
for the work in this article.

~~~
mkdir FreeDOS_13
cd FreeDOS_13
~~~

I initially tried the CD images but ran into odd problems with
qemu (possibly due to my lack of experience with qemu).
After looking at that various options the USB Full release
seemed like a good choice. It comes both as an image you can
&#34;burn&#34; to your USB drive both also as a &#34;vmdk&#34; file used with
emulators.

~~~
curl -L -O https://www.ibiblio.org/pub/micro/pc-stuff/freedos/files/distributions/1.3/previews/1.3-rc4/FD13-FullUSB.zip
unzip FD13-FullUSB.zip
~~~

At this point you should see the FreeDOS &#34;vmdk&#34; file, and &#34;img&#34; file and readme files if you list the directory out. I&#39;m going to use the &#34;vmdk&#34; file to install FreeDOS on my virtual harddrive freedos.img.

~~~
ls -l 
~~~

Prepping my virtual machine
---------------------------

A virtual machine is not just a CPU and some random
access memory. A machine can include storage devices. For
the retro &#34;DOS&#34; experience you might looking virtual devices
for a &#34;harddrive&#34;, &#34;floppy drive&#34; and &#34;CD-ROM drive&#34;. 
Qemu provides a tool called `qemu-img` for creating 
these types of virtual devices.

The basic command is `qemu-img` using the &#34;create&#34; option with
some parameters.  The parameter are filename and size (see
`man qemu-img` for gory details). I am calling my virtual
harddrive &#34;freedos.img&#34;.  With `qemu-img` the size can be
specified with a suffix like &#34;K&#34; for kilobytes,  &#34;M&#34; for
megabytes and &#34;G&#34; for gigabytes. DOS is a minimal requirements
a small (by today&#39;s standards) 750 megabyte harddrive seems
appropriate.

~~~
qemu-img create freedos.img 750M
~~~

For my purposes I need a harddrive so I stopped there. You
can always create other drives and then restart your virtual
machine with the appropriate options.

Bring up my FreeDOS box
-----------------------

Now I was ready to boot from installation media and install
FreeDOS 1.3rc4 on my virtual harddrive.  For that I
use a &#34;qemu&#34; command for the system I want to emulate.
I picked `qemu-system-i386` (see can see
the gory details of that command using `man qemu-system-i386`).
To install FreeDOS I&#39;m going to boot from the vmdk file 
provided for the purpose of installation. I then use the FreeDOS
installer to make my freedos.img file bootable with all the
DOS software I want to play with.

~~~
qemu-system-i386 \
   -m 8 \
   -boot menu=on,strict=on \
   -hda freedos.img \
   -hdb FD13FULL.vmdk
~~~

At this point you should see the machine start to boot, press Esc
when prompted and select the second hard drive to boot from (that&#39;s
our vmdk drive).  The drive is then treated like the CD-ROM, follow
the programs instructions for installation. You will need to reboot
several times during the process. Until your full installation is
complete you&#39;ll need to select the second harddrive as the boot drive
and continue the installation.

The first time I successfully installed FreeDOS 1.3rc4 I just installed
the plain dos. When I re-did the process I install everything. It
fills up my 750M virtual harddrive but rc4 includes development tools
like a C compiler.  That I think made it worth it.

Here&#39;s a Bash script you can use to build your FreeDOS machine.

~~~
#!/bin/bash

if [ ! -f freedos.img ]; then
  echo &#34;Creating fresh Harddisk as drive C:&#34;
  qemu-img create freedos.img 750M
fi
echo &#34;Booting machine using FD13FULL.vmdk for installation&#34;
qemu-system-i386 \
    -m 8 \
    -boot menu=on,strict=on \
    -hda freedos.img \
    -hdb FD13FULL.vmdk
~~~

And here is one for running it.

~~~
#!/bin/bash

echo &#34;Booting machine using freedos.img as drive C:&#34;
qemu-system-i386 \
    -m 8 \
    -boot menu=on,strict=on \
    -hda freedos.img
~~~

Next step, explore FreeDOS and see what I can build.

Putting everything together
---------------------------

Below is a [script](run-freedos-1.3rc4.bash) I developed automating either building or running your FreeDOS setup.

~~~
#!/bin/bash

if [ ! -f FD13FULL.vmdk ]; then
    if [ ! -f FD13-FullUSB.zip ]; then
      echo &#34;Missing FD13FULL.vmdk, downloading FD13-FullUSB.zip&#34;
      curl -L -O https://www.ibiblio.org/pub/micro/pc-stuff/freedos/files/distributions/1.3/previews/1.3-rc4/FD13-FullUSB.zip
    fi
    echo &#34;Unzipping FD13-FullUSB.zip&#34;
    unzip FD13-FullUSB.zip
fi

if [ ! -f freedos.img ]; then
  echo &#34;Creating fresh Harddisk as drive C:&#34;
  qemu-img create freedos.img 750M
  echo &#34;Booting machine using FD13FULL.vmdk as drive C:&#34;
  echo &#34;Installing FreeDOS on drive D:&#34;
  qemu-system-i386 \
      -name FreeDOS \
      -machine pc \
      -m 32 \
      -boot order=c \
      -hda FD13FULL.vmdk \
      -hdb freedos.img \
      -parallel none \
      -vga cirrus \
      -display gtk
else
  echo &#34;Booting machine using freedos.img on drive C:&#34;
  qemu-system-i386 \
      -name FreeDOS \
      -machine pc \
      -m 32 \
      -boot menu=on,strict=on \
      -hda freedos.img \
      -parallel none \
      -vga cirrus \
      -display gtk
fi
~~~


Reference material
------------------

My inspiration for this was the description of manual install in
the FreeDOS book section of the website, [Manual Install](https://www.freedos.org/books/get-started/june14-manual-install.html).</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/11/27/FreeDOS-1.3rc4-with-Qemu.md" length="7783" type="text/markdown" />
    </item>    <item>
      <title>Portable Conversions (Integers)</title>
      <link>https://rsdoiel.github.io/blog/2021/11/26/Portable-Conversions-Integers.html</link>
      <description>
        <![CDATA[An area in working with Oberon-07 on a POSIX machine that has proven problematic is type conversion. In particular converting to and from INTEGER or REAL and ASCII.  None of the three compilers I am exploring provide a common way of handling this. I've explored relying on C libraries but that approach has it's own set of problems.  I've become convinced a better approach is a pure Oberon-07 library that handles type conversion with a minimum of assumptions about the implementation details of the Oberon compiler or hardware. I'm calling my conversion module "Types". The name is short and descriptive and seems an appropriate name for a module consisting of type conversion tests and transformations.  My initial implementation will focusing on converting integers to and from ASCII.
        
        ...]]>
      </description>
      <source:markdown>Portable conversions (Integers)
===============================

By R. S. Doiel, 2021-11-26

An area in working with Oberon-07 on a POSIX machine that has proven problematic is type conversion. In particular converting to and from INTEGER or REAL and ASCII.  None of the three compilers I am exploring provide a common way of handling this. I&#39;ve explored relying on C libraries but that approach has it&#39;s own set of problems.  I&#39;ve become convinced a better approach is a pure Oberon-07 library that handles type conversion with a minimum of assumptions about the implementation details of the Oberon compiler or hardware. I&#39;m calling my conversion module &#34;Types&#34;. The name is short and descriptive and seems an appropriate name for a module consisting of type conversion tests and transformations.  My initial implementation will focusing on converting integers to and from ASCII.

INTEGER to ASCII and back again
-------------------------------

I don&#39;t want to rely on the representation of the INTEGER value in the compiler or at the machine level. That has lead me to think in terms of an INTEGER as a signed whole number. 

The simplest case of converting to/from ASCII is the digits from zero to nine (inclusive). Going from an INTEGER to an ASCII CHAR is just looking up the offset of the character representing the &#34;digit&#34;. Like wise going from ASCII CHAR to a INTEGER is a matter of mapping in the reverse direction.  Let&#39;s call these procedures `DigitToChar` and  `CharToDigit*`.

Since INTEGER can be larger than zero through nine and CHAR can hold non-digits I&#39;m going to add two additional procedures for validating inputs -- `IsIntDigit` and `IsCharDigit`. Both return TRUE if valid, FALSE if not.

For numbers larger than one digit I can use decimal right shift to extract the ones column value or a left shift to reverse the process.  Let&#39;s called these `IntShiftRight` and `IntShiftLeft`.  For shift right it&#39;d be good to capture the ones column being lost. For shift left it would be good to be able to shift in a desired digit. That way you could shift/unshift to retrieve to extract and put back values.

A draft definition for &#34;Types&#34; should look something like this.

~~~
DEFINITION Types;

(* Check if an integer is a single digit, i.e. from 0 through 9 returns
   TRUE, otherwise FALSE *)
PROCEDURE IsIntDigit(x : INTEGER) : BOOLEAN;

(* Check if a CHAR is &#34;0&#34; through &#34;9&#34; and return TRUE, otherwise FALSE *)
PROCEDURE IsCharDigit(ch : CHAR) : BOOLEAN;

(* Convert digit 0 through 9 into an ASCII CHAR &#34;0&#34; through &#34;9&#34;,
   ok is TRUE if conversion successful, FALSE otherwise *)
PROCEDURE DigitToChar(x : INTEGER; VAR ch : CHAR; VAR ok : BOOLEAN);

(* Convert a CHAR &#34;0&#34; through &#34;9&#34; into a digit 0 through 9, ok
   is TRUE is conversion successful, FALSE otherwise *)
PROCEDURE CharToDigit(ch : CHAR; VAR x : INTEGER; VAR ok : BOOLEAN);

(* Shift an integer to the right (i.e. x * 0.1) set &#34;r&#34; to the
   value shifted out (ones column lost) and return the shifted value.
   E.g.  x becomes 12, r becomes 3.

       x := IntShiftRight(123, r);
   
 *)
PROCEDURE IntShiftRight(x : INTEGER; VAR r : INTEGER) : INTEGER;

(* Shift an integer to the left (i.e. x * 10) adding the value y
   after the shift.

   E.g. x before 123

       x := IntShiftRight(12, 3);

 *)
PROCEDURE IntShiftLeft(x, y : INTEGER) : INTEGER;

(* INTEGER to ASCII *)
PROCEDURE Itoa(src : INTEGER; VAR value : ARRAY OF CHAR; VAR ok : BOOLEAN);

(* ASCII to INTEGER *)
PROCEDURE Atoi(src : ARRAY OF CHAR; VAR value : INTEGER; VAR ok : BOOLEAN);

END Types.
~~~


NOTE: Oberon-07 provides us the ORD and CHR built as part of the
language.  These are for working with the encoding and decoding
values as integers. This is not the same thing as the meaning
of &#34;0&#34; versus the value of 0.  Getting to and from the encoding
to the meaning of the presentation can be done with some simple
arithmetic.

Putting it all together
-----------------------

~~~
(* DigitToChar converts an INTEGER less than to a character. E.g.
   0 should return &#34;0&#34;, 3 returns &#34;3&#34;, 0 returns &#34;9&#34; *)
PROCEDURE DigitToChar*(i : INTEGER) : CHAR;
BEGIN
  RETURN (CHR(ORD(&#34;0&#34;) + i))
END DigitToChar;

(* CharToDigit converts a single &#34;Digit&#34; character to an INTEGER value.
   E.g. &#34;0&#34; returns 0, &#34;3&#34; returns 3, &#34;9&#34; returns 9. *)
PROCEDURE CharToDigit(ch : CHAR) : INTEGER;
BEGIN
  RETURN (ORD(ch) - ORD(&#34;0&#34;))
END CharToDigit;
~~~

This implementation is naive. It assumes the ranges of the input values
was already checked. In practice this is going to encourage bugs.

In a language like Go or Python you can return multiple values (in
Python you can return a tuple). In Oberon-07 I could use a
RECORD type to do that but that feels a little too baroque. Oberon-07
like Oberon-2, Oberon, Modula and Pascal does support &#34;VAR&#34; parameters. 
With a slight modification to our procedure signatures I can support
easy assertions about the conversion. Let&#39;s create two functional
procedures `IsIntDigit()` and `IsCharDigit()` then update our
`DigitToChar()` and `CharToDigit()` with an a  &#34;VAR ok : BOOLEAN&#34;
parameter.

~~~
(* IsIntDigit returns TRUE is the integer value is zero through nine *)
PROCEDURE IsIntDigit(i : INTEGER) : BOOLEAN;
BEGIN 
  RETURN ((i &#62;= 0) &#38; (i &#60;= 9))
END IsIntDigit;

(* IsCharDigit returns TRUE if character is zero through nine. *)
PROCEDURE IsCharDigit(ch : CHAR) : BOOLEAN;
BEGIN
  RETURN ((ch &#62;= &#34;0&#34;) &#38; (ch &#60;= &#34;9&#34;))
END IsCharDigit;

(* DigitToChar converts an INTEGER less than to a character. E.g.
   0 should return &#34;0&#34;, 3 returns &#34;3&#34;, 0 returns &#34;9&#34; *)
PROCEDURE DigitToChar*(i : INTEGER; VAR ok : BOOLEAN) : CHAR;
BEGIN
  ok := IsIntDigit(i);
  RETURN (CHR(ORD(&#34;0&#34;) + i))
END DigitToChar;

(* CharToDigit converts a single &#34;Digit&#34; character to an INTEGER value.
   E.g. &#34;0&#34; returns 0, &#34;3&#34; returns 3, &#34;9&#34; returns 9. *)
PROCEDURE CharToDigit(ch : CHAR; VAR ok : BOOLEAN) : INTEGER;
BEGIN
  ok := IsCharDigit(ch);
  RETURN (ORD(ch) - ORD(&#34;0&#34;))
END CharToDigit;
~~~

What about values are greater nine? Here we can take advantage
of our integer shift procedures.  `IntShiftRight` will move the
INTEGER value right reducing it&#39;s magnitude (i.e. x * 0.1). It
also captures the ones column lost in the shift.  Repeatedly calling
`IntShiftRight` will let us peal off the ones columns until the
value &#34;x&#34; is zero. `IntShiftLeft` shifts the integer to the
left meaning it raises it a magnitude (i.e. x * 10). `IntShiftLeft`
also rakes a value to shift in on the right side of the number.
In this way we can shift in a zero and get `x * 10` or shift in
another digit and get `(x * 10) + y`. This means you can use
`IntShiftRight` and recover an `IntShiftLeft`.

~~~

(* IntShiftRight converts the input integer to a real, multiplies by 0.1
   and converts by to an integer. The value in the ones column is record
   in the VAR parameter r.  E.g. IntShiftRight(123) return 12, r is set to 3. *)
PROCEDURE IntShiftRight*(x : INTEGER; VAR r : INTEGER) : INTEGER;
  VAR i : INTEGER; isNeg : BOOLEAN;
BEGIN
  isNeg := (x &#60; 0);
  i := FLOOR(FLT(ABS(x)) * 0.1);
  r := ABS(x) - (i * 10);
  IF isNeg THEN
    i := i * (-1);
  END;
  RETURN i
END IntShiftRight;

(* IntShiftLeft multiples input value by 10 and adds y. E.g. IntShiftLeft(123, 4) return 1234 *)
PROCEDURE IntShiftLeft*(x, y : INTEGER) : INTEGER;
  VAR i : INTEGER; isNeg : BOOLEAN;
BEGIN
  isNeg := (x &#60; 0);
  i := (ABS(x) * 10) + y;
  IF isNeg THEN
    i := i * (-1);
  END;
  RETURN i
END IntShiftLeft;

~~~

I have what I need for implementing `Itoa` (integer to ASCII).


~~~

(* Itoa converts an INTEGER to an ASCII string setting ok BOOLEAN to
   TRUE if value ARRAY OF CHAR holds the full integer, FALSE if
   value was too small to hold the integer value.  *)
PROCEDURE Itoa*(x : INTEGER; VAR value : ARRAY OF CHAR; ok : BOOLEAN);
  VAR i, j, k, l, minL : INTEGER; tmp : ARRAY BUFSIZE OF CHAR; isNeg : BOOLEAN;
BEGIN
  i := 0; j := 0; k := 0; l := LEN(value); isNeg := (x &#60; 0);
  IF isNeg THEN
    (* minimum string length for value is 3, negative sign, digit and 0X *)
    minL := 3;
  ELSE 
    (* minimum string length for value is 2, one digit and 0X *)
    minL := 2; 
  END;
  ok := (l &#62;= minL) &#38; (LEN(value) &#62;= LEN(tmp));
  IF ok THEN
    IF IsIntDigit(ABS(x)) THEN
      IF isNeg THEN
         value[i] := &#34;-&#34;; INC(i);
      END;
      value[i] := DigitToChar(ABS(x), ok); INC(i); value[i] := 0X;
    ELSE
      x := ABS(x); (* We need to work with the absolute value of x *)
      i := 0; tmp[i] := 0X;
      WHILE (x &#62;= 10) &#38; ok DO
        (* extract the ones columns *)
        x := IntShiftRight(x, k); (* a holds the shifted value, 
                                     &#34;k&#34; holds the ones column 
                                     value shifted out. *)
        (* write append k to our temp array holding values in
           reverse number magnitude *)
        tmp[i] := DigitToChar(k, ok); INC(i); tmp[i] := 0X;
      END;
      (* We now can convert the remaining &#34;ones&#34; column. *)
      tmp[i] := DigitToChar(x, ok); INC(i); tmp[i] := 0X;
      IF ok THEN
        (* now reverse the order of tmp string append each
           character to value *)
        i := 0; j := Strings.Length(tmp) - 2;
        IF isNeg THEN
          value[i] := &#34;-&#34;; INC(i);
        END;
        j := Strings.Length(tmp) - 1;
        WHILE (j &#62; -1) DO
          value[i]:= tmp[j]; 
          INC(i); DEC(j);
          value[i] := 0X;
        END;
        value[i] := 0X;
      END;
    END; 
  ELSE
    ok := FALSE;
  END;
END Itoa;

~~~

Integers in Oberon are signed. So I&#39;ve chosen to capture the sign in the `isNeg` variable. This lets me work with the absolute value for the actual conversion.  One failing in this implementation is I don&#39;t detect an overflow.  Also notice that I am accumulating the individual column values in reverse order (lowest magnitude first).  That is what I need a temporary buffer. I can then copy the values in reverse order into the VAR ARRAY OF CHAR. Finally I also maintain the ok BOOLEAN to track if anything went wrong.

When moving from an ASCII representation I can simplified the code by having a local (to the module) procedure for generating magnitudes.

Going the other way I can simplify my `Atoi` if I have an local to the module &#34;magnitude&#34; procedure.

~~~

(* magnitude takes x and multiplies it be 10^y, If y is positive zeros
   are appended to the right side (i.e. multiplied by 10). If y is
   negative then the result is shifted left (i.e.. multiplied by
   0.1 via IntShiftRight().).  The digit(s) shift to the fractional
   side of the decimal are ignored. *)
PROCEDURE magnitude(x, y : INTEGER) : INTEGER;
  VAR z, w : INTEGER;
BEGIN
  z := 1;
  IF y &#62;= 0 THEN
    WHILE y &#62; 0 DO
      z := IntShiftLeft(z, 0);
      DEC(y);
    END;
  ELSE
    WHILE y &#60; 0 DO
      x := IntShiftRight(x, w);
      INC(y);
    END;
  END;
  RETURN (x * z)
END magnitude;

~~~

And with that I can put together my `Atoi` (ASCII to integer) procedure.  I&#39;ll need to add some sanity checks as well.

~~~

(* Atoi converts an ASCII string to a signed integer value
   setting the ok BOOLEAN to TRUE on success and FALSE on error. *)
PROCEDURE Atoi*(source : ARRAY OF CHAR; VAR value : INTEGER; VAR ok : BOOLEAN);
  VAR i, l, a, m: INTEGER; isNeg : BOOLEAN;
BEGIN
  (* &#34;i&#34; is the current CHAR position we&#39;re analyzing, &#34;l&#34; is the
     length of our string, &#34;a&#34; holds the accumulated value,
     &#34;m&#34; holds the current magnitude we&#39;re working with *)
  i := 0; l := Strings.Length(source);
  a := 0; m := l - 1; isNeg := FALSE; ok := TRUE;
  (* Validate magnitude and sign behavior *)
  IF (l &#62; 0) &#38; (source[0] = &#34;-&#34;) THEN
    INC(i); DEC(m);
    isNeg := TRUE;
  ELSIF (l &#62; 0) &#38; (source[0] = &#34;+&#34;) THEN
    INC(i); DEC(m);
  END;

  (* The accumulator should always hold a positive integer, if the
     sign flips we have overflow, ok should be set to FALSE *)
  ok := TRUE;
  WHILE (i &#60; l) &#38; ok DO
    a := a + magnitude(CharToDigit(source[i], ok), m);
    IF a &#60; 0 THEN
      ok := FALSE; (* we have an overflow condition *)
    END;
    DEC(m);
    INC(i);
  END;
  IF ok THEN
    IF (i = l) THEN
      IF isNeg THEN
        value := a * (-1);
      ELSE
        value := a;
      END;
    END;
  END;
END Atoi;

~~~

Here&#39;s an example using the procedures.

Converting an integer 1234 to an string &#34;1234&#34;.

~~~

   x := 1234; s := &#34;&#34;; ok := FALSE;
   Types.Itoa(x, s, ok);
   IF ok THEN 
     Out.String(s); Out.String(&#34; = &#34;);
     Out.Int(x,1);Out.Ln;
   ELSE
     Out.String(&#34;Something went wrong&#34;);Out.Ln;
   END;

~~~

Converting a string &#34;56789&#34; to integer 56789.

~~~

   x := 0; src := &#34;56789&#34;; ok := FALSE;
   Types.Atoi(src, x, ok);
   IF ok THEN 
     Out.Int(x,1); Out.String(&#34; = &#34;); Out.String(s); 
     Out.Ln;
   ELSE
     Out.String(&#34;Something went wrong&#34;);Out.Ln;
   END;

~~~


References and resources
------------------------

Implementations for modules for this article are linked here [Types](./Types.Mod), [TypesTest](./TypesTest.Mod) and [Tests](./Tests.Mod). 

Expanded versions of the `Types` module will be available as part of Artemis Project -- [github.com/rsdoiel/Artemis](https://github.com/rsdoiel/Artemis).

Previous
--------

- [Revisiting Files](../../11/22/Revisiting-Files.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/11/26/Portable-Conversions-Integers.md" length="14533" type="text/markdown" />
    </item>    <item>
      <title>Revisiting Files</title>
      <link>https://rsdoiel.github.io/blog/2021/11/22/Revisiting-Files.html</link>
      <description>
        <![CDATA[In October I had an Email exchange with Algojack regarding a buggy example in [Oberon-07 and the file system](../../../2020/05/09/Oberon-07-and-the-filesystem.html). The serious bug was extraneous non-printable characters appearing a plain text file containing the string "Hello World". The trouble with the example was a result of my misreading the Oakwood guidelines and how **Files.WriteString()** is required to work. The **Files.WriteString()** procedure is supposed to write every element of a string to a file. This __includes the trailing Null character__. The problem for me is **Files.WriteString()** litters plain text files with tailing nulls. What I should have done was write my own **WriteString()** and **WriteLn()**. The program [HelloworldFile](./HelloworldFile.Mod) below is a more appropriate solution to writing strings and line endings than relying directly on **Files**. In a future post I will explorer making this more generalized in a revised "Fmt" module.
        
        ...]]>
      </description>
      <source:markdown>Revisiting Files
================

By R. S. Doiel, 2021-11-22

In October I had an Email exchange with Algojack regarding a buggy example in [Oberon-07 and the file system](../../../2020/05/09/Oberon-07-and-the-filesystem.html). The serious bug was extraneous non-printable characters appearing a plain text file containing the string &#34;Hello World&#34;. The trouble with the example was a result of my misreading the Oakwood guidelines and how **Files.WriteString()** is required to work. The **Files.WriteString()** procedure is supposed to write every element of a string to a file. This __includes the trailing Null character__. The problem for me is **Files.WriteString()** litters plain text files with tailing nulls. What I should have done was write my own **WriteString()** and **WriteLn()**. The program [HelloworldFile](./HelloworldFile.Mod) below is a more appropriate solution to writing strings and line endings than relying directly on **Files**. In a future post I will explorer making this more generalized in a revised &#34;Fmt&#34; module.

~~~
MODULE HelloworldFile;

IMPORT Files, Strings;

CONST OberonEOL = 1; UnixEOL = 2; WindowsEOL = 3;

VAR
  (* holds the eol marker type to use in WriteLn() *)
  eolType : INTEGER;
  (* Define a file handle *)
    f : Files.File;
  (* Define a file rider *)
    r : Files.Rider;

PROCEDURE WriteLn(VAR r : Files.Rider);
BEGIN
  IF eolType = WindowsEOL THEN
    (* A DOS/Windows style line ending, LFCR *)
    Files.Write(r, 13);
    Files.Write(r, 10);
  ELSIF eolType = UnixEOL THEN
     (* Linux/macOS style line ending, LF *)
     Files.Write(r, 10);
  ELSE
    (* Oberon, RISC OS style line ending, CR *)
    Files.Write(r, 13);
  END;
END WriteLn;

PROCEDURE WriteString(VAR r : Files.Rider; s : ARRAY OF CHAR);
  VAR i : INTEGER;
BEGIN
  i := 0;
  WHILE i &#60; Strings.Length(s) DO
    Files.Write(r, ORD(s[i]));
    INC(i);
  END;
END WriteString;

BEGIN
  (* Set the desired eol type to use *)
  eolType := UnixEOL;
  (* Create our file, New returns a file handle *)
  f := Files.New(&#34;helloworld.txt&#34;); ASSERT(f # NIL);
  (* Register our file with the file system *)
  Files.Register(f);
  (* Set the position of the rider to the beginning *)
  Files.Set(r, f, 0);
  (* Use the rider to write out &#34;Hello World!&#34; followed by a end of line *)
  WriteString(r, &#34;Hello World!&#34;);
  WriteLn(r);
  (* Close our modified file *)
  Files.Close(f);
END HelloworldFile.
~~~

I have two new procedures &#34;WriteString&#34; and &#34;WriteLn&#34;. These mimic the parameters found in the Files module. The module body is a bit longer.

Compare this to a simple example of &#34;Hello World&#34; using the **Out** module.

~~~
MODULE HelloWorld;

IMPORT Out;

BEGIN
  Out.String(&#34;Hello World&#34;);
  Out.Ln;
END HelloWorld.
~~~

Look at the difference is in the module body. I need to setup our file and rider as well as pick the type of line ending to use in &#34;WriteLn&#34;. The procedures doing the actual work look very similar, &#34;String&#34; versus &#34;WriteString&#34; and &#34;Ln&#34; versus &#34;WriteLn&#34;.  


Line ends vary between operating systems. Unix-like systems usually use a line feed. DOS/Windows systems use a carriage return and line feed. Oberon Systems use only a carriage return. If we&#39;re going to the trouble of re-creating our &#34;WriteString&#34; and &#34;WriteLn&#34; procedures it also makes sense to handle the different line ending options.  In this case I&#39;ve chosen to use an INTEGER variable global to the module called &#34;eolType&#34;. I have a small set of constants to indicate which line ending is needed. In &#34;WriteLn&#34; I use that value as a guide to which line ending to use with the rider writing to the file.

The reason I chose this approach is because I want my writing procedures to use the same procedure signatures as the &#34;Files&#34; module. In a future post I will explore type conversion and a revised implementation of my &#34;Fmt&#34; module focusing on working with plain text files.

Aside from our file setup and picking an appropriate end of line marker the shape of the two programs look very similar.

References and resources
------------------------

You can see a definition of the [Files](https://miasap.se/obnc/obncdoc/basic/Files.def.html &#34;My example module definition is based on the on Karl created in OBNC&#34;) at Karl Landstrm&#39;s documentation for his compiler along with the definitions for [In](https://miasap.se/obnc/obncdoc/basic/In.def.html) and [Out](https://miasap.se/obnc/obncdoc/basic/Out.def.html).


Next &#38; Previous
---------------

- Next [Portable Conversions (Integers)](../../11/26/Portable-Conversions-Integers.html)
- Prev [Combining Oberon-07 with C using Obc-3](../../06/14/Combining-Oberon-07-with-C-using-Obc-3.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/11/22/Revisiting-Files.md" length="6150" type="text/markdown" />
    </item>    <item>
      <title>Combining Oberon-07 with C using Obc-3</title>
      <link>https://rsdoiel.github.io/blog/2021/06/14/Combining-Oberon-07-with-C-using-Obc-3.html</link>
      <description>
        <![CDATA[This post explores integrating C code with an Oberon-07 module use
        Mike Spivey's Obc-3 Oberon Compiler.  Last year I wrote a similar post
        for Karl Landstrm's [OBNC](/blog/2020/05/01/Combining-Oberon-and-C.html).
        This goal of this post is to document how I created a version of Karl's
        Extension Library that would work with Mike's Obc-3 compiler.
        If you want to take a shortcut you can see the results on GitHub
        in my [obc-3-libext](https://github.com/rsdoiel/obc-3-libext) repository.
        
        From my time with OBNC I've come to rely on three modules from Karl's
        extension library. When trying to port some of my code to use with
        Mike's compiler. That's where I ran into a problem with that dependency.
        Karl's modules aren't available. I needed an [extArgs](http://miasap.se/obnc/obncdoc/ext/extArgs.def.html),
        an [extEnv](http://miasap.se/obnc/obncdoc/ext/extEnv.def.html) and
        [extConvert](http://miasap.se/obnc/obncdoc/ext/extConvert.def.html).
        
        ...]]>
      </description>
      <source:markdown>Combing Oberon-07 with C using Obc-3
===================================

By R. S. Doiel, 2021-06-14

This post explores integrating C code with an Oberon-07 module use
Mike Spivey&#39;s Obc-3 Oberon Compiler.  Last year I wrote a similar post
for Karl Landstrm&#39;s [OBNC](/blog/2020/05/01/Combining-Oberon-and-C.html).
This goal of this post is to document how I created a version of Karl&#39;s
Extension Library that would work with Mike&#39;s Obc-3 compiler.
If you want to take a shortcut you can see the results on GitHub
in my [obc-3-libext](https://github.com/rsdoiel/obc-3-libext) repository.

From my time with OBNC I&#39;ve come to rely on three modules from Karl&#39;s
extension library. When trying to port some of my code to use with
Mike&#39;s compiler. That&#39;s where I ran into a problem with that dependency.
Karl&#39;s modules aren&#39;t available. I needed an [extArgs](http://miasap.se/obnc/obncdoc/ext/extArgs.def.html),
an [extEnv](http://miasap.se/obnc/obncdoc/ext/extEnv.def.html) and
[extConvert](http://miasap.se/obnc/obncdoc/ext/extConvert.def.html).

Mike&#39;s own modules that ship with Obc-3 cover allot of common ground
with Karl&#39;s. They are organized differently. The trivial solution is
to implement wrapping modules using Mike&#39;s modules for implementation.
That takes case of extArgs and extEnv.

The module extConvert is in a another category. Mike&#39;s `Conv` module is
significantly minimalist. To solve that case I&#39;ve create C code to perform
the needed tasks based on Karl&#39;s examples and used Mike&#39;s share library
compilation instructions to make it available inside his run time.

Background material
-------------------

- [Obc-3 website](https://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler)
    - [Installing Obc-3](https://spivey.oriel.ox.ac.uk/corner/Installing_OBC_release_3.1)
    - [Adding primitives to Obc-3](https://spivey.oriel.ox.ac.uk/corner/How_to_add_primitives_to_OBC), this is how you extend Obc-3 with C
    - [Obc-3.1 Manual](https://spivey.oriel.ox.ac.uk/wiki/images-corner/c/ce/Obcman-3.1.pdf)
- [Obc-3 at GitHub](http://github.com/Spivoxity/obc-3)


Differences: OBNC and Obc-3
---------------------------

The OBNC compiler written by Karl takes the approach of translating
Oberon-07 code to C and then calling the C tool chain to convert that
into a   executable.  Karl&#39;s compiler is largely written in C
with some parts written in Oberon.

Mike&#39;s takes a different approach. His compiler uses a run time JIT
and is written mostly in OCaml with some C parts and shell scripting.
When you compile an Oberon program (either Oberon-2 or Oberon-07) using
Mike&#39;s compiler you get a bunch of &#34;*.k&#34; files that the object code
for Mike&#39;s thunder virtual machine and JIT.  This can in turn be used
to create a executable.

For implementing Oberon procedures in C Karl&#39;s expects an empty
procedure body. e.g.

```oberon
PROCEDURE DoMyThing();
BEGIN
END DoMyThing;
```

While Mike has added a &#34;IS&#34; phrase to the procedure signature to
indicate what the C implementation is known as. There is no procedure
body in Mike&#39;s implementation and the parameters need to map
directly into a C data type.

```oberon
PROCEDURE DoMyThing() IS &#34;do_my_thing&#34;;
```

Of course both compilers have completely different command line options
and when you&#39;re integrating C shared libraries in Mike&#39;s you need to
call your local CC (e.g. GCC, clang) to create a share library file.
Mike has extended Oberon-07 SYSTEM to include `SYSTEM.LOADLIB()` which
takes a string containing the path to the compiler shared library.

In Karl&#39;s own Oberon-07 modules he uses the `.obn` file extension but
also accepts `.Mod`.  In Mike&#39;s he uses `.m` and also accepts `.Mod`.
In this article I will be using `.m` as that simplified the recipe
of building and integrating the shared C libraries.


Similarities of OBNC and Obc-3
------------------------------

Both compilers provide for compiling Oberon-07 code, Mike&#39;s requires
the `-07` option to be used to switch from Oberon-2. Both offer the
ability to extend reach into the host POSIX system by wrapping
C shared libraries. Both run on a wide variety of POSIX systems and
you can read the source code at your leisure. This last bit is
important.

Args, extArgs and extEnv.
-------------------------

Mike provides two features in his Args module. The first is access
to the command line arguments of the compiled program. The
second feature is to provide access to the host environment variables.
In Karl&#39;s implementation he separates Mikes `Args.GetEvn()` into
a module called `extEnv`. Here&#39;s Mike&#39;s module definition looks like ---

```oberon
DEFINITION Args;

VAR argc* : INTEGER; (* this is equavilent to extArgs.count *)

PROCEDURE GetArg*(n: INTEGER; VAR s: ARRAY OF CHAR);

PROCEDURE GetEnv*(name: ARRAY OF CHAR; VAR s: ARRAY OF CHAR);

END Args.
```

My implementation of Karl&#39;s `extArgs` needs to look like ---

```oberon
DEFINITION extArgs;

VAR count*: INTEGER; (* this is the same as Args.argc *)

PROCEDURE Get*(n: INTEGER; VAR arg: ARRAY OF CHAR; VAR res: INTEGER);

END extArgs.
```

This leaves us with a very simple module mimicking Karl&#39;s.

```oberon
MODULE extArgs;

IMPORT Args;

VAR
  count*: INTEGER;

PROCEDURE Get*(n: INTEGER; VAR arg: ARRAY OF CHAR; VAR res: INTEGER);
BEGIN
  Args.GetArg(n + 1, arg);  res := 0;
END Get;

BEGIN
  count := Args.argc - 1;
END extArgs.
```

NOTE: In Mike&#39;s approach the zero-th arg is the program name.
In Karl&#39;s the zero-th arg is the first argument after the program
name. To get Karl&#39;s behavior with Mike&#39;s `GetArg()` I need to
adjust the offsets.

So far so good. How about implementing Karl&#39;s `extEnv`?

We&#39;ve already seen Mike&#39;s Args so he doesn&#39;t have a matching
definition.  Karl&#39;s `extEnv` looks like

```oberon
DEFINITION extEnv;

PROCEDURE Get*(name: ARRAY OF CHAR; VAR value: ARRAY OF CHAR; VAR res: INTEGER);

END extEnv.
```

And again a simple mapping of features and you have

```oberon
MODULE extEnv;

IMPORT Args, Strings;

PROCEDURE Get*(name : ARRAY OF CHAR; VAR value : ARRAY OF CHAR; VAR res : INTEGER);
  VAR i, l1, l2 : INTEGER; val : ARRAY 512 OF CHAR;
BEGIN
  l1 := LEN(value) - 1; (* Allow for trailing 0X *)
  Args.GetEnv(name, val);
  l2 := Strings.Length(val);
  IF l2 &#60;= l1 THEN
    res := 0;
  ELSE
    res := l2 - l1;
  END;
  i := 0;
  WHILE (i &#60; l2) &#38; (val[i] # 0X) DO
      value[i] := val[i];
      INC(i);
  END;
  value[i] := 0X;
END Get;

END extEnv.
```

extConvert requires more work
-----------------------------

Mike provides a module called `Conv.m` for converting numbers
to strings.  It is a little minimal for my current purpose.
That is easy enough to solve as Mike, like Karl provides a means
of extending Oberon code with C.  That means I need to write
`extConvert` as both `extConvert.m` (the Oberon-07 part) and
`extConvert.c` (the C part).

Here&#39;s Karl&#39;s definition

```oberon
DEFINITION extConvert;

PROCEDURE IntToString*(i: INTEGER; VAR s: ARRAY OF CHAR; VAR done: BOOLEAN);

PROCEDURE RealToString*(x: REAL; VAR s: ARRAY OF CHAR; VAR done: BOOLEAN);

PROCEDURE StringToInt*(s: ARRAY OF CHAR; VAR i: INTEGER; VAR done: BOOLEAN);

PROCEDURE StringToReal*(s: ARRAY OF CHAR; VAR x: REAL; VAR done: BOOLEAN);

END extConvert.
```

I have implement my `extConvert` as a hybrid of Oberon-07 and calls
to a C shared library I will create called `extConvert.c`.

The Oberon file (i.e. extConvert.m)

```oberon
MODULE extConvert;

IMPORT SYSTEM;

PROCEDURE IntToString*(i: INTEGER; VAR s: ARRAY OF CHAR; VAR done: BOOLEAN);
  VAR l : INTEGER;
BEGIN
  l := LEN(s); done := TRUE;
  IntToString0(i, s, l);
END IntToString;

PROCEDURE IntToString0(i : INTEGER; VAR s : ARRAY OF CHAR; l : INTEGER) IS &#34;conv_int_to_string&#34;;

PROCEDURE RealToString*(x: REAL; VAR s: ARRAY OF CHAR; VAR done: BOOLEAN);
  VAR l : INTEGER;
BEGIN
  l := LEN(s);
  RealToString0(x, s, l);
END RealToString;

PROCEDURE RealToString0(x: REAL; VAR s: ARRAY OF CHAR; l : INTEGER) IS &#34;conv_real_to_string&#34;;

PROCEDURE StringToInt*(s: ARRAY OF CHAR; VAR i: INTEGER; VAR done: BOOLEAN);
BEGIN
  done := TRUE;
  StringToInt0(s, i);
END StringToInt;

PROCEDURE StringToInt0(s : ARRAY OF CHAR; VAR i : INTEGER) IS &#34;conv_string_to_int&#34;;

PROCEDURE StringToReal*(s: ARRAY OF CHAR; VAR x: REAL; VAR done: BOOLEAN);
BEGIN
  done := TRUE;
  StringToReal0(s, x);
END StringToReal;

PROCEDURE StringToReal0(s: ARRAY OF CHAR; VAR x : REAL) IS &#34;conv_string_to_real&#34;;

BEGIN
  SYSTEM.LOADLIB(&#34;./extConvert.so&#34;);
END extConvert.
```

If you review Mike&#39;s module code you&#39;ll see I have followed a similar pattern. Before calling out to C I take care of what house keeping I can in Oberon, then I call a &#34;0&#34; version of the function implemented in C. The C implementation are not exported only the wrapping Oberon procedures are.

Notice how the initialization block calls `SYSTEM.LOADLIB(&#34;./extConvert.so&#34;);` this loads the C shared library so that the Oberon module can call out it it.

The C code in `extConvert.c` looks very traditional without the macros
you&#39;d see in OBNC&#39;s implementation. Here&#39;s what the C code look like.

```C
#include &#60;stdlib.h&#62;
#include &#60;stdio.h&#62;

void conv_int_to_string(int i, char *s, int l) {
  snprintf(s, l, &#34;%d&#34;, i);
}

void conv_real_to_string(float r, char *s, int l) {
  snprintf(s, l, &#34;%f&#34;, r);
}

void conv_real_to_exp_string(float r, char *s, int l) {
  snprintf(s, l, &#34;%e&#34;, r);
}

void conv_string_to_int(char *s, int *i) {
    *i = atoi(s);
}

void conv_string_to_real(char *s, float *r) {
    *r = atof(s);
}
```

The dance to compile the module and C shared library is very different
between OBNC and Obc-3.  With Obc-3 we compile and skip linking
the wrapping Oberon module `extConvert.m`. We compile using CC
our C shared library. We can then put it all together to test
everything out in `ConvertTest.m`.

```shell
obc -07 -c extConvert.m
gcc -fPIC -shared extConvert.c -o extConvert.so
```

Our test code program looks like.

```oberon
MODULE ConvertTest;

IMPORT T := Tests, Convert := extConvert;

VAR ts : T.TestSet;

PROCEDURE TestIntConvs() : BOOLEAN;
  VAR test, ok : BOOLEAN;
      expectI, gotI : INTEGER;
      expectS, gotS : ARRAY 128 OF CHAR;
BEGIN test := TRUE;
  gotS[0] := 0X; gotI := 0;
  expectI := 101;
  expectS := &#34;101&#34;;

  Convert.StringToInt(expectS, gotI, ok);
  T.ExpectedBool(TRUE, ok, &#34;StringToInt(&#39;101&#39;, gotI, ok) true&#34;, test);
  T.ExpectedInt(expectI, gotI, &#34;StringToInt(&#39;101&#39;, gotI, ok)&#34;, test);

  Convert.IntToString(expectI, gotS, ok);
  T.ExpectedBool(TRUE, ok, &#34;IntToString(101, gotS, ok) true&#34;, test);
  T.ExpectedString(expectS, gotS, &#34;IntToString(101, gotS, ok)&#34;, test);

  RETURN test
END TestIntConvs;

PROCEDURE TestRealConvs() : BOOLEAN;
  VAR test, ok : BOOLEAN;
      expectR, gotR : REAL;
      expectS, gotS : ARRAY 128 OF CHAR;
BEGIN test := TRUE;
  gotR := 0.0; gotS[0] := 0X;
  expectR := 3.1459;
  expectS := &#34;3.145900&#34;;

  Convert.StringToReal(expectS, gotR, ok);
  T.ExpectedBool(TRUE, ok, &#34;StringToReal(&#39;3.1459&#39;, gotR, ok) true&#34;, test);
  T.ExpectedReal(expectR, gotR, &#34;StringToReal(&#39;3.1459&#39;, gotR, ok)&#34;, test);

  Convert.RealToString(expectR, gotS, ok);
  T.ExpectedBool(TRUE, ok, &#34;RealToString(3.1459, gotS; ok) true&#34;, test);
  T.ExpectedString(expectS, gotS, &#34;RealToString(3.1459, gotS, ok)&#34;, test);

  RETURN test
END TestRealConvs;

BEGIN
  T.Init(ts, &#34;extConvert&#34;);
  T.Add(ts, TestIntConvs);
  T.Add(ts, TestRealConvs);
  ASSERT(T.Run(ts));
END ConvertTest.
```

We compile and run our test program use the following commands
(NOTE: Using Obc-3 you list all the dependent modules to possibly
be compiled one the command line along with your program module).

```shell
obc -07 -o converttest extConvert.m Tests.m ConvertTest.m
./converttest
```

Source code for these modules is available on GitHub at
[github.com/rsdoiel/obc-3-libest](https://github.com/rsdoiel/obc-3-libext)


Next &#38; Previous
---------------

- Next [Revisiting Files](../../11/22/Revisiting-Files.html)
- Previous [Beyond Oakwood, Modules and Aliases](../../05/16/Beyond-Oakwood-Modules-and-Aliases.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/06/14/Combining-Oberon-07-with-C-using-Obc-3.md" length="13525" type="text/markdown" />
    </item>    <item>
      <title>Beyond Oakwood, Modules and Aliases</title>
      <link>https://rsdoiel.github.io/blog/2021/05/16/Beyond-Oakwood-Modules-and-Aliases.html</link>
      <description>
        <![CDATA[Oakwood is the name used to refer to an early Oberon language
        standardization effort in the late 20th century.  It's the name
        of a hotel where compiler developers and the creators of Oberon
        and the Oberon System met to discuss compatibility. The lasting
        influence on the 21st century Oberon-07 language can be seen
        in the standard set of modules shipped with POSIX based Oberon-07
        compilers like
        [OBNC](https://miasap.se/obnc/), [Vishap Oberon Compiler](https://github.com/vishaps/voc) and the 
        [Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler).
        
        The Oakwood guidelines described a minimum expectation for
        a standard set of modules to be shipped with compilers.
        The modules themselves are minimalist in implementation.
        Minimalism can assist in easing the learning curve
        and encouraging a deeper understanding of how things work.
        
        The Oberon-07 language is smaller than the original Oberon language
        and the many dialects that followed.  I think of Oberon-07 as the
        distillation of all previous innovation.  It embodies the
        spirit of "Simple but not simpler than necessary". Minimalism is
        a fit description of the adaptions of the Oakwood modules for 
        Oberon-07 in the POSIX environment.
        
        ...]]>
      </description>
      <source:markdown>Beyond Oakwood, Modules and Aliases
===================================

By R. S. Doiel, 2021-05-16

Oakwood is the name used to refer to an early Oberon language
standardization effort in the late 20th century.  It&#39;s the name
of a hotel where compiler developers and the creators of Oberon
and the Oberon System met to discuss compatibility. The lasting
influence on the 21st century Oberon-07 language can be seen
in the standard set of modules shipped with POSIX based Oberon-07
compilers like
[OBNC](https://miasap.se/obnc/), [Vishap Oberon Compiler](https://github.com/vishaps/voc) and the 
[Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler).

The Oakwood guidelines described a minimum expectation for
a standard set of modules to be shipped with compilers.
The modules themselves are minimalist in implementation.
Minimalism can assist in easing the learning curve
and encouraging a deeper understanding of how things work.

The Oberon-07 language is smaller than the original Oberon language
and the many dialects that followed.  I think of Oberon-07 as the
distillation of all previous innovation.  It embodies the
spirit of &#34;Simple but not simpler than necessary&#34;. Minimalism is
a fit description of the adaptions of the Oakwood modules for 
Oberon-07 in the POSIX environment.


When simple is too simple
-------------------------

Sometimes I want more than the minimalist module.  A good example
is standard [Strings](https://miasap.se/obnc/obncdoc/basic/Strings.def.html)
module.  Thankfully you can augment the standard modules with your own.
If you are creative you can even create a drop in replacement.
This is what I wound up doing with my &#34;Chars&#34; module.

In the spirit of &#34;Simple but no simpler&#34; I originally kept Chars 
very minimal. I only implemented what I missed most from Strings.
I got down to a handful of functions for testing characters,
testing prefixes and suffixes as well as trim procedures. It was
all I included in `Chars` was until recently.

Over the last couple of weeks I have been reviewing my own Oberon-07
code in my personal projects.  I came to understand that
in my quest for minimalism I had fallen for &#34;too simple&#34;.
This was evidenced by two observations.  Everywhere I had used
the `Strings` module I also included `Chars`. It was boiler plate.
The IMPORT sequence was invariably a form of --

~~~
    IMPORT Strings, Chars, ....
~~~

On top of that I found it distracting to see `Chars.*` and `Strings.*`
comingled and operating on the same data. If felt sub optimal. It
felt baroque. That got me thinking.

&#62; What if Chars included the functionality of Strings?

I see two advantages to merging Chars and Strings. First I
only need to include one module instead of two. The second
is my code becomes more readable. I think that is because
expanding Strings to include new procedures and constants allows
for both the familiar and for evolution. The problem is renaming
`Chars.Mod` to `Strings.Mod` implies I&#39;m supplying the standard
`Strings` module. Fortunately Oberon provides a mechanism for
solving this problem. The solution Oberon provides is to allow
module names to be aliased.  Look at my new import statement.

~~~
    IMPORT Strings := Chars, ...
~~~

It is still minimal but at the same time shows `Chars` is going
to be referenced as `Strings`. By implication `Chars` provides
the functionality `Strings` but is not the same as `Strings`.
My code reads nicely.  I don&#39;t loose the provenance of what
is being referred to by `Strings` because it is clearly 
provided in the IMPORT statement.

In my new [implementation](Chars.Mod) I support all the standard
procedures you&#39;d find in an Oakwood compliant `Strings`.  I&#39;ve
included additional additional constants and functional procedures
like `StartsWith()` and `EndsWith()` and a complement of trim
procedures like `TrimLeft()`, `TrimRight()`, `Trim()`.
`TrimPrefix()`, and `TrimSuffix()`.

Here&#39;s how `Chars` definition stacks up as rendered by the
obncdoc tool.

```
(* Chars.Mod - A module for working with CHAR and 
   ARRAY OF CHAR data types.

Copyright (C) 2020, 2021 R. S. Doiel &#60;rsdoiel@gmail.com&#62;
This Source Code Form is subject to the terms of the
Mozilla PublicLicense, v. 2.0. If a copy of the MPL was
not distributed with thisfile, You can obtain one at
http://mozilla.org/MPL/2.0/. *)
DEFINITION Chars;

(*
Chars.Mod provides a modern set of procedures for working
with CHAR and ARRAY OF CHAR. It is a drop in replacement
for the Oakwood definition 
Strings module.

Example:

    IMPORT Strings := Chars;

You now have a Strings compatible Chars module plus all the Chars
extra accessible through the module alias of Strings. *)

CONST
  (* MAXSTR is exported so we can use a common
     max string size easily *)
  MAXSTR = 1024;
  (* Character constants *)
  EOT = 0X;
  TAB = 9X;
  LF  = 10X;
  FF  = 11X;
  CR  = 13X;
  SPACE = &#34; &#34;;
  DASH  = &#34;-&#34;;
  LODASH = &#34;_&#34;;
  CARET = &#34;^&#34;;
  TILDE = &#34;~&#34;;
  QUOTE = 34X;

  (* Constants commonly used characters to quote things.  *)
  QUOT   = 34X;
  AMP    = &#34;&#38;&#34;;
  APOS   = &#34;&#39;&#34;;
  LPAR   = &#34;)&#34;;
  RPAR   = &#34;(&#34;;
  AST    = &#34;*&#34;;
  LT     = &#34;&#60;&#34;;
  EQUALS = &#34;=&#34;;
  GT     = &#34;&#62;&#34;;
  LBRACK = &#34;[&#34;;
  RBRACK = &#34;]&#34;;
  LBRACE = &#34;}&#34;;
  RBRACE = &#34;{&#34;;

VAR
  (* common cutsets, ideally these would be constants *)
  spaces : ARRAY 6 OF CHAR;
  punctuation : ARRAY 33 OF CHAR;

(* InRange -- given a character to check and an inclusive range of
    characters in the ASCII character set. Compare the ordinal values
    for inclusively. Return TRUE if in range FALSE otherwise. *)
PROCEDURE InRange(c, lower, upper : CHAR) : BOOLEAN;

(* InCharList checks if character c is in list of chars *)
PROCEDURE InCharList(c : CHAR; list : ARRAY OF CHAR) : BOOLEAN;

(* IsUpper return true if the character is an upper case letter *)
PROCEDURE IsUpper(c : CHAR) : BOOLEAN;

(* IsLower return true if the character is a lower case letter *)
PROCEDURE IsLower(c : CHAR) : BOOLEAN;

(* IsDigit return true if the character in the range of &#34;0&#34; to &#34;9&#34; *)
PROCEDURE IsDigit(c : CHAR) : BOOLEAN;

(* IsAlpha return true is character is either upper or lower case letter *)
PROCEDURE IsAlpha(c : CHAR) : BOOLEAN;

(* IsAlphaNum return true is IsAlpha or IsDigit *)
PROCEDURE IsAlphaNum (c : CHAR) : BOOLEAN;

(* IsSpace returns TRUE if the char is a space, tab, carriage return or line feed *)
PROCEDURE IsSpace(c : CHAR) : BOOLEAN;

(* IsPunctuation returns TRUE if the char is a non-alpha non-numeral *)
PROCEDURE IsPunctuation(c : CHAR) : BOOLEAN;

(* Length returns the length of an ARRAY OF CHAR from zero to first
    0X encountered. [Oakwood compatible] *)
PROCEDURE Length(source : ARRAY OF CHAR) : INTEGER;

(* Insert inserts a source ARRAY OF CHAR into a destination 
    ARRAY OF CHAR maintaining a trailing 0X and truncating if
    necessary [Oakwood compatible] *)
PROCEDURE Insert(source : ARRAY OF CHAR; pos : INTEGER; VAR dest : ARRAY OF CHAR);

(* AppendChar - this copies the char and appends it to
    the destination. Returns FALSE if append fails. *)
PROCEDURE AppendChar(c : CHAR; VAR dest : ARRAY OF CHAR) : BOOLEAN;

(* Append - copy the contents of source ARRAY OF CHAR to end of
    dest ARRAY OF CHAR. [Oakwood complatible] *)
PROCEDURE Append(source : ARRAY OF CHAR; VAR dest : ARRAY OF CHAR);

(* Delete removes n number of characters starting at pos in an
    ARRAY OF CHAR. [Oakwood complatible] *)
PROCEDURE Delete(VAR source : ARRAY OF CHAR; pos, n : INTEGER);

(* Replace replaces the characters starting at pos with the
    source ARRAY OF CHAR overwriting the characters in dest
    ARRAY OF CHAR. Replace will enforce a terminating 0X as
    needed. [Oakwood compatible] *)
PROCEDURE Replace(source : ARRAY OF CHAR; pos : INTEGER; VAR dest : ARRAY OF CHAR);

(* Extract copies out a substring from an ARRAY OF CHAR into a dest
    ARRAY OF CHAR starting at pos and for n characters
    [Oakwood compatible] *)
PROCEDURE Extract(source : ARRAY OF CHAR; pos, n : INTEGER; VAR dest : ARRAY OF CHAR);

(* Pos returns the position of the first occurrence of a pattern
    ARRAY OF CHAR starting at pos in a source ARRAY OF CHAR. If
    pattern is not found then it returns -1 *)
PROCEDURE Pos(pattern, source : ARRAY OF CHAR; pos : INTEGER) : INTEGER;

(* Cap replaces each lower case letter within source by an uppercase one *)
PROCEDURE Cap(VAR source : ARRAY OF CHAR);

(* Equal - compares two ARRAY OF CHAR and returns TRUE
    if the characters match up to the end of string,
    FALSE otherwise. *)
PROCEDURE Equal(a : ARRAY OF CHAR; b : ARRAY OF CHAR) : BOOLEAN;

(* StartsWith - check to see of a prefix starts an ARRAY OF CHAR *)
PROCEDURE StartsWith(prefix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR) : BOOLEAN;

(* EndsWith - check to see of a prefix starts an ARRAY OF CHAR *)
PROCEDURE EndsWith(suffix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR) : BOOLEAN;

(* Clear - resets all cells of an ARRAY OF CHAR to 0X *)
PROCEDURE Clear(VAR a : ARRAY OF CHAR);

(* Shift returns the first character of an ARRAY OF CHAR and shifts the
    remaining elements left appending an extra 0X if necessary *)
PROCEDURE Shift(VAR source : ARRAY OF CHAR) : CHAR;

(* Pop returns the last non-OX element of an ARRAY OF CHAR replacing
    it with an OX *)
PROCEDURE Pop(VAR source : ARRAY OF CHAR) : CHAR;

(* TrimLeft - remove the leading characters in cutset
    from an ARRAY OF CHAR *)
PROCEDURE TrimLeft(cutset : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);

(* TrimRight - remove tailing characters in cutset from
    an ARRAY OF CHAR *)
PROCEDURE TrimRight(cutset : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);

(* Trim - remove leading and trailing characters in cutset
    from an ARRAY OF CHAR *)
PROCEDURE Trim(cutset : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);

(* TrimLeftSpace - remove leading spaces from an ARRAY OF CHAR *)
PROCEDURE TrimLeftSpace(VAR source : ARRAY OF CHAR);

(* TrimRightSpace - remove the trailing spaces from an ARRAY OF CHAR *)
PROCEDURE TrimRightSpace(VAR source : ARRAY OF CHAR);

(* TrimSpace - remove leading and trailing space CHARS from an 
    ARRAY OF CHAR *)
PROCEDURE TrimSpace(VAR source : ARRAY OF CHAR);

(* TrimPrefix - remove a prefix ARRAY OF CHAR from a target 
    ARRAY OF CHAR *)
PROCEDURE TrimPrefix(prefix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);

(* TrimSuffix - remove a suffix ARRAY OF CHAR from a target
    ARRAY OF CHAR *)
PROCEDURE TrimSuffix(suffix : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);

(* TrimString - remove cutString from beginning and end of ARRAY OF CHAR *)
PROCEDURE TrimString(cutString : ARRAY OF CHAR; VAR source : ARRAY OF CHAR);

END Chars.
```

My new `Chars` module has proven to be both more readable
and more focused in my projects. I get all the functionality
of `Strings` and the additional functionality I need in my own
projects. This improved the focus in my other modules and I think
maintained the spirit of &#34;Simple but not simpler&#34;.

+ [Chars.Mod](Chars.Mod)

UPDATE: The current version of my `Chars` module can be found in 
my [Artemis](https://github.com/rsdoiel/Artemis) repository. The
repository includes additional code and modules suitable to working
with Oberon-07 in a POSIX envinronment.

### Next, Previous

+ Next [Combining Oberon-07 with C using Obc-3](/blog/2021/06/14/Combining-Oberon-07-with-C-using-Obc-3.html)
+ Prev [Dates &#38; Clocks](/blog/2020/11/27/Dates-and-Clock.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/05/16/Beyond-Oakwood-Modules-and-Aliases.md" length="13242" type="text/markdown" />
    </item>    <item>
      <title>Ofront on Raspberry Pi OS</title>
      <link>https://rsdoiel.github.io/blog/2021/04/25/Ofront-on-Rasberry-Pi-OS.html</link>
      <description>
        <![CDATA[This post is about getting Ofront up and running on Raspberry Pi OS.
        Ofront provides a Oberon-2 to C transpiler as well as a Oberon V4
        development environment. There are additional clever tools like `ocat`
        that are helpful working with the differences in text file formats between
        Oberon System 3, V4 and POSIX. The V4 implementation sits nicely on top of
        POSIX with minimal compromises that distract from the Oberon experience.
        
        ...]]>
      </description>
      <source:markdown>Ofront on Raspberry Pi OS
=========================

By R. S. Doiel, 2021-04-25

This post is about getting Ofront[^1] up and running on Raspberry Pi OS[^2].
Ofront provides a Oberon-2 to C transpiler as well as a Oberon V4[^3]
development environment. There are additional clever tools like `ocat`
that are helpful working with the differences in text file formats between
Oberon System 3, V4 and POSIX. The V4 implementation sits nicely on top of
POSIX with minimal compromises that distract from the Oberon experience.

[^1]: Ofront was developed by Joseph Templ, see http://www.software-templ.com/ 

[^2]: see https://www.raspberrypi.org/software/ (a 32 bit Debian based Linux for both i386 and ARM)

[^3]: see https://ssw.jku.at/Research/Projects/Oberon.html


An Initial Impression
---------------------

I first heard of running Ofront/V4 via the ETH Oberon Mail list[^4].
What caught my eye is the reference to running on Raspberry Pi. Prof. Templ 
provides two flavors of Ofront. One targets the Raspberry Pi OS on ARM
hardware the second Linux on i386. The Raspberry Pi OS for Intel is an
i386 variant. I downloaded the tar file, unpacked it and immediately ran
the &#34;oberon.bash&#34; script provided eager to see a V4 environment. It
renders but the fonts rendered terribly slowly. I should have read the
documentation first!  Prof. Templ provides man pages for the tools that
come with Ofront including the oberon application. Reading the
man page for oberon quickly addresses the point of slow font rendering.
It also discusses how to convert Oberon fonts to X Windows bitmap fonts.
If you use the X Window fonts the V4 environment is very snappy. It does
require that X Windows knows where to find the fonts used in V4. That is
done by appending the V4 converted fonts to the X Window font map. I had
installed the Ofront system in my home directory so the command was

```bash
xset +fp $HOME/ofront_1.4/fonts
```

Running &#34;oberon.bash&#34; after that immediately improved things. Since I didn&#39;t
need the Oberon fonts outside of V4 I added the `xset` command to the
&#34;oberon.bash&#34; script just before it invokes the `oberon` command.

[^4]: See Hans Klaver&#39;s message: http://lists.inf.ethz.ch/pipermail/oberon/2021/015514.html 


Goals in my setup
-----------------

I had three goals in wanting to play with Ofront and running the V4
Oberon.

1. I wanted to work in an Oberon System environment
2. I need a system meets my vision requirements (e.g. larger font size)
3. I wanted to understand the Linz/V4 variation in Oberon&#39;s evolution

Ofront address all three once you get the X Window side setup correctly.

Setting up Ofront and V4
------------------------

First we need to boot up a Raspberry Pi OS device (or an i386 Linux with X11).
We need to retrieve the software from Joseph Templ&#39;s [software-templ.com](https://software-templ.com).
Two 1.4 versions are available precompiled. The first is for ARM running
Raspberry Pi OS and the second is for generic Linux i386 with X11. I initially
tested this on an old laptop where running the i386 version of Raspberry Pi OS. 

What we need
------------

The following software is usually already installed on your 
Raspberry Pi OS.

+ curl to download the files[^5]
+ gunzip to uncompressed the archive file
+ tar to unpack the archive file

[^5]: If not try `sudo apt install curl` from the command line

What we do
----------

1. Download the appropriate tar file
    a. ARM: http://www.software-templ.com/shareware/ofront-1.4_raspbian-Pi3.tar.gz
    b. Intel i386: http://www.software-templ.com/shareware/ofront-1.4_linux-386-3.2.tar.gz
2. Make sure we can read the compressed archive file
3. Gunzip and untar the file

Here&#39;s the commands I used for the Raspberry Pi hardware.

```bash
    curl -O http://www.software-templ.com/shareware/ofront-1.4_raspbian-Pi3.tar.gz
    tar ztvf ofront-1.4_raspbian-Pi3.tar.gz
    tar zxvf ofront-1.4_raspbian-Pi3.tar.gz
```

Here&#39;s the commands I used for Raspberry Pi OS on Intel

```bash
    curl -O http://www.software-templ.com/shareware/ofront-1.4_linux-386-3.2.tar.gz
    tar ztvf ofront-1.4_linux-386-3.2.tar.gz
    tar zxvf ofront-1.4_linux-386-3.2.tar.gz
```

At this point there should be an `ofront_1.4` directory
where you gunziped and untared the archive file. At this point
you can test to make sure everything runs by doing the following
(remember the font rendering with be very slow).

```
    cd ofront_1.4
    ./oberon.bash
```

You can exit the V4 environment by closing the window or typing
`System.Quit ~` in an Oberon viewer and middle clicking with your
mouse[^6].

[^6]: Oberon Systems expect a three button mouse, with a two button mouse you hold the alt key and press the left button. Note that command in Oberon are case sensitive.

The reason the system is so slow is that X is having to write bitmaps
a pixel at a time in the window holding our Oberon System. What we
want X to do is render an X Window font.  Joseph as provided us with
the Oberon fonts already converted for X! We just need to let the
X Window system know where to look.

What we need
------------

+ an editor for editing `oberon.bash`

What we&#39;ll do
-------------

1. Exit the running Oberon System using `System.Quit ~` or just close the window
2. Edit `oberon.bash` to speed up font rendering
3. Try `oberon.bash` again and see the speed bump

With your favorite editor add the `xset` line before the `oberon`
command is invoked. My &#34;oberon.bash&#34; looks like this.

```
#!/bin/bash

if [ -z &#34;$OFRONT_HOME&#34; ]; then
  export OFRONT_HOME=.
  echo &#34;OFRONT_HOME set to .&#34;
fi
export OBERON=.:$OFRONT_HOME/V4_ofront:$OFRONT_HOME/V4:$OFRONT_HOME/fonts
export LD_LIBRARY_PATH=.:$OFRONT_HOME/lib
export PATH=.:$OFRONT_HOME/bin:$PATH
xset +fp $HOME/ofront_1.4/fonts
$OFRONT_HOME/bin/oberon -f ./V4/Big.Map -u 8000 -c $* &#38;
```

The `xset` command adds the provided X fonts to X Window. This
results in a huge speedup of rendering. I also add the options
for using the largest font sizes via a font map file, &#34;V4/Big.Map&#34;
and set the display units to 8000. Your vision or monitor may
not need this so you want to only add the line to include the
X fonts needed by Oberon.

Now re-launch Oberon using the updated &#34;oberon.bash&#34; and
see the improvement.

```
    vi oberon.bash
    ./oberon.bash
```

You now have a functioning V4 Oberon System to play with and
explore.

There are some additional POSIX environment setup you can
add to improve the integration with your Linux setup. These
are covered in the man pages for the tools that come with Ofront.
Additional information is also provided in the Oberon Texts
and Tools files in the V4 environment. All are worth reading.


What does this setup provide?
-----------------------------

At the point we have V4 available we have a rich development
and text environment. One which I feel is conducive to both
writing in general and programming specifically. You are running
under an adapted Oberon System so there are somethings to consider.

The Oberon V4 file system does support punctuation characters aside
from periods and slashes.  So when I tried to edit a file with hyphens
in the name Oberon assumed the filename stopped at the first hyphens.
The Oberon file systems are typically case sensitive so this can
be worked around with letter case. Of course I could modify the V4
system to allow for more letters too. That&#39;s the nice thing about
having the source code.

The second issue if file format.  In Oberon we can embed fonts
and coloring and that is treated as normal text. End of line
characters are represented as a carriage return. In POSIX environments
we have &#34;plain text&#34; without specific font directives and we use
a line feed to terminate lines. Fortunately Prof. Templ provided
a program called `ocat`[^7] that makes short work of converting an
Oberon text into a POSIX friendly format. On the Oberon side of things
it&#39;s also easy because Oberon will treat an ASCII file as a text we
only need to convert the line endings and in the Ofront implementation
of V4 it handles the differences in line endings behind the scenes.

If you create or store a file in the Oberon environment it&#39;ll become
Oberon text. If you need to have a plain text version use `ocat`.
If you only read POSIX files in the Oberon environment then they remain
plain text files but V4 takes care of translating the POSIX line ending
to ones that are displayed nicely in Oberon.


[^7]: In the `ofront_1.4` directory run `man man1/ocat.1` to find out more

What to explore next?
---------------------

Now that we have a fast running V4 system we have some choices
for development. Joseph Templ has adapted the display for X
and also the file system so the files are visible from the Unix
shell.  This is a powerful arrangement. This supports both Oberon
development and the use of Oberon language for the development of
POSIX friendly programs.  The Ofront collection provides the
`ofront` an Oberon-2 to C translator, `ocl` is a tool that will
combine `ofront` with your C compiler and linker to produce
programs and libraries for Linux. There is also `ocat` for
converting Oberon texts to POSIX plain text and `showdef` for
showing module definitions.  Finally Ofront provides the
`oberon` command so we have an Oberon System available as a
development environment.

One thing I recommend exploring is Jospeh Templ&#39;s GitHub repository.
The makefile provided with the GitHub version assuming an existing
installation of ofront. Since we have one we can compile our own copy
from scratch. If you&#39;re running i386 you&#39;ll want to look at
`V4_ofront/linix386` for Pi hardware take a look at `V4_ofront/raspbian`.

Here&#39;s how I generated a new version on my Pi hardware.

```
    git clone https://github.com/jtempl/ofront
    cd ofront/V4_ofront/raspbian
    make
    ./oberon.bash
```

There is a note in the README of that directory about finding
`libX11.so` but I did not need the symbolic link suggested. Since the
font path was previously adjusted for the original version I downloaded
from Templ&#39;s website I didn&#39;t need to add the fonts again. If I fork
Templ&#39;s version or GitHub I will probably update the &#34;oberon.bash&#34;
file included to check to see if the X fonts are available and if not
add them via `xset`. That&#39;s on a someday maybe list, for now I am
content exploring the system as is.


Someday, Maybe
--------------

Things that come to mind after initial exploration include--

- Figure out how to make Atkinson-Hyperlegible[^8] available to V4
- Replace the X11 integration with SDL 2 integration and run under macOS or Windows
- Exploring porting V4 to run natively Raspberry Pi via Clang cross compilers

Not sure I&#39;ll get the time or have the energy to do these things but
they are certainly seem feasible with Ofront as it stands now.

[^8]: See https://brailleinstitute.org/freefont</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/04/25/Ofront-on-Rasberry-Pi-OS.md" length="11805" type="text/markdown" />
    </item>    <item>
      <title>Updating Schema in SQLite3</title>
      <link>https://rsdoiel.github.io/blog/2021/04/16/Updating-Schema-in-SQLite3.html</link>
      <description>
        <![CDATA[[SQLite3](https://sqlite.org/docs.html) is a handy little
        database as single file tool.  You can interact with the file
        through largely standard SQL commands and embed it easily into
        applications via the C libraries that the project supports.
        It is also available from various popular scripting languages
        like Python, PHP, and Lua. One of the things I occasionally
        need to do and always seems to forget it how to is modify a
        table schema where I need to remove a column. So here are
        some of the basics do I can quickly find them later and avoid
        reading various articles tutorials because the search engines
        doesn't return the page in the SQLite documentation.
        
        In the next sections I'll be modeling a simple person object
        with a id, uname, display_name, role and updated fields.
        
        ...]]>
      </description>
      <source:markdown>Updating Schema in SQLite3
==========================

By R. S. Doiel, 2020-04-16

[SQLite3](https://sqlite.org/docs.html) is a handy little
database as single file tool.  You can interact with the file
through largely standard SQL commands and embed it easily into
applications via the C libraries that the project supports.
It is also available from various popular scripting languages
like Python, PHP, and Lua. One of the things I occasionally
need to do and always seems to forget it how to is modify a
table schema where I need to remove a column[^1]. So here are
some of the basics do I can quickly find them later and avoid
reading various articles tutorials because the search engines
doesn&#39;t return the page in the SQLite documentation.

[^1]: The SQL `ALTER TABLE table_name DROP COLUMN column_name` does not work in SQLite3

In the next sections I&#39;ll be modeling a simple person object
with a id, uname, display_name, role and updated fields.

Creating a person table
-----------------------


```sql

CREATE TABLE IF NOT EXISTS &#34;person&#34; 
        (&#34;id&#34; INTEGER NOT NULL PRIMARY KEY, 
        &#34;uname&#34; VARCHAR(255) NOT NULL, 
        &#34;role&#34; VARCHAR(255) NOT NULL, 
        &#34;display_name&#34; VARCHAR(255) NOT NULL, 
        &#34;updated&#34; INTEGER NOT NULL);

```

Adding a column
---------------

We will create a *junk* column which we will remove later.

```sql

.schema person
ALTER TABLE person ADD COLUMN junk VARCHAR(255) NOT NULL;
.schema person

```

Dropping a column
-----------------

To drop a column in SQLite you need to actually create
a new table, migrate the data into it then drop the old table
and finally rename it. It is best to wrap this in a transaction.

```sql

BEGIN TRANSACTION;
    CREATE TABLE IF NOT EXISTS &#34;person_new&#34; 
           (&#34;id&#34; INTEGER NOT NULL PRIMARY KEY, 
           &#34;uname&#34; VARCHAR(255) NOT NULL, 
           &#34;role&#34; VARCHAR(255) NOT NULL, 
           &#34;display_name&#34; VARCHAR(255) NOT NULL, 
           &#34;updated&#34; INTEGER NOT NULL);
    INSERT INTO person_new
           SELECT id, uname, role, display_name, updated
           FROM person;
    DROP TABLE person;
    ALTER TABLE person_new RENAME TO person;
COMMIT;

```</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/04/16/Updating-Schema-in-SQLite3.md" length="3470" type="text/markdown" />
    </item>    <item>
      <title>A2 Oberon on VirtualBox 6.1</title>
      <link>https://rsdoiel.github.io/blog/2021/04/02/A2-Oberon-on-VirtualBox-6.1.html</link>
      <description>
        <![CDATA[This is a short article documenting how I install A2 Oberon
        in VirtualBox using the [FreeDOS 1.2](https://freedos.org),
        the A2 [ISO](https://sourceforge.net/projects/a2oberon/files/) cd image and [VirtualBox 6.1](https://virtualbox.org).
        
        ...]]>
      </description>
      <source:markdown>A2 Oberon on VirtualBox 6.1
===========================

By R. S. Doiel, 2021-04-02

This is a short article documenting how I install A2 Oberon
in VirtualBox using the [FreeDOS 1.2](https://freedos.org),
the A2 [ISO](https://sourceforge.net/projects/a2oberon/files/) cd image and [VirtualBox 6.1](https://virtualbox.org).

Basic Approach
--------------

1. Download the ISO images for FreeDOS and A2
2. Create a new Virtual Machine
3. Install FreeDOS 1.2 (Base install) in the virtual machine
4. Install A2 from the ISO image over the FreeDOS installation

From working with Native Oberon 2.3.7 I&#39;ve found it very helpful
to have a FreeDOS 1.2. installed in the Virtual machine first. 
I suspect the reason I have had better luck taking this approach
is based on assumptions about the virtual hard disk being setup
with an existing known formatted, boot-able partition. In essence
making our Virtualbox look like a fresh out of the box vintage PC.

Download the ISO Images for FreeDOS and A2
------------------------------------------

You&#39;ll find FreeDOS 1.2 installation ISO image at 
[FreeDos.org](http://freedos.org/download/). Download it
where you can easily find it from the VirtualBox manager.

You&#39;ll find the A2 Oberon ISO image at [SourceForge](https://sourceforge.net/projects/a2oberon/files/) in the A2 Files section. There is a green download
button you can click and it&#39;ll take you to a downloads page and download
the ISO.  Once again move it to where you can find it from 
the VirtualBox manager easily.


Create a new Virtual Machine
----------------------------

Fire up VirtualBox.  Click the &#34;New&#34; icon. Given your machine
a descriptive name and set the Type to &#34;Other&#34; and version to &#34;DOS&#34;.
Click Next.

On the Memory Size panel select the memory size you want. I picked
2048 MB. A2 like other Oberon are frugal in resource consumption.
Click Next.

On the Hard Disk panel I accepted the default &#34;Create a virtual hard disk now&#34;
and clicked &#34;Create&#34; button at the bottom of the panel.

I accepted the default &#34;VDI (VirtualBox Disk Image)&#34; and clicked
Next.

I accepted &#34;Dynamically allocated&#34; and clicked Next.

I accepted the default name and 500 MB disk size and clicked
Create.

This returned me to the main VirtualBox manager panel. I click on 
the &#34;Settings&#34; icon. This opens the Settings panel. I Clicked on the
&#34;Display&#34; label in the left side of the panel. On the &#34;Screen&#34; tab
I increased the Video Memory from 6 MB to 128 MB.  I also checked
the &#34;Enable 3D Acceleration&#34; box.

Next I clicked  &#34;Network&#34; label in the left side of the panel.
I changed the Attached to from &#34;NAT&#34; to &#34;Bridged Adapter&#34;
before clicking &#34;OK&#34;. This should return you to the manager panel.

Scroll down the description of your virtual machine so that the
&#34;Storage&#34; section is visible. You should see &#34;IDE Secondary Device 0: 
[Optical Drive] Empty&#34;. Click the the words &#34;Optical Drive&#34;.
You be given a context menu, click on &#34;Choose a Disk file&#34;. Navigate
to where you saved the FressDOS ISO (e.g. FD12CD.iso).
Click Open. This should return you to the manager panel and you
should see the &#34;FD12CD.iso&#34; file listed.

Install FreeDOS 1.2
-------------------

Click the &#34;Start&#34; button.  This should boot the machine. By
default the search order for booting is floppy drive,
CD-ROM drive then hard disk.  Since we have the FD12CD.iso
mounted in the cd ROM drive it&#39;ll boot using it.

When you see the &#34;Welcome&#34; screen press the tab key.
You should see a line describing the image it&#39;ll boot.  Click
into the Virtual machine&#39;s window and press the space bar
then type &#34;raw&#34; (without the quotes). Press the enter key.

This should return you to the install process, select your
language (e.g. English for me). The select &#34;Yes - Continue with the
installations&#34;. On the next screen select &#34;Yes - Partition drive C:&#34;.
Then select &#34;Yes - Please reboot now&#34;. This will reboot the
machine and bring you back to the Welcome page. Once again
press the tab key, press the space bar and type in &#34;raw&#34;
(without the quotes).

As before select your language and select &#34;Yes - Continue with
the installation&#34;. This time you should see the option 
&#34;Yes - Please erase and format drive C:&#34;, select it. 
After formatting it ask you to select your keyboard type.
It will then give you the option of installing base or full
installations (with our without source).  I suggest only
selecting &#34;Base packages only&#34;.  

On the next screen select &#34;Yes - Please install FreeDOS 1.2&#34;.
After it finishes you can select &#34;Yes - Please reboot now&#34;.
When the machine reboots you&#39;ll see the welcome screen again
but rather than press tab, select &#34;Boot system from hard disk&#34;.
Press enter to select the extended memory manager and you
should now be at the DOS &#34;C:&#62;&#34; prompt.

Switch back to the VirtualBox manager panel and click on
&#34;Optical Drive&#34; and click &#34;Remove disk from virtual drive&#34;.

Installing A2 Oberon
--------------------

We now should have a Virtual Machine ready to receive A2.
Click the &#34;Optical Drive&#34; again and select the A2 ISO
image you downloaded from SourceForge previously.
Your optical drive should show the full filename of
the ISO image, e.g. &#34;A2_Rev-6498_serial-trace.iso&#34;
We can now click the &#34;Start&#34; icon in the manager panel.

A2 comes up running like a &#34;live CD&#34;.  It&#39;s the full A2
so you can play around with it if you want but we&#39;re going
to install it on our virtual hard drive. At the bottom of the
A2 desktop you should see a panel of buttons. Click the button
labeled &#34;System&#34;. This will change the panel buttons below it.
In the lower panel you should see &#34;Installer&#34;, click it. This
will bring up a &#34;Welcome to Oberon&#34; installer window. You will
see two presentations of drives. The upper one will be the hard
drive where we want to install A2 and the lower one is the 
virtual CD ROM we&#39;re running. Click on the bar for the hard disk.
Before click the drive bar was red. After clicking it was yellow.
The text label above the var says, &#34;IDE0 (VBOX HARD DISK), Size 
500 MB, Open count 0&#34;.

In the lower part of the panel click &#34;Quickinstall&#34;, then
answer Yes to the model dialog that pops up. After a few moments 
A2 should finish installing itself on the virtual hard disk.  The lower
panel&#39;s buttons will include one labeled &#34;Done&#34;, press it. This
will close the installer window.

At the bottom of the desktop you should still see the System
panel buttons. There is a red one labeled &#34;Shutdown&#34;. Press it.

The virtual machine&#39;s screen should go black. On my machine
I press the right control key (the host key) to release my
mouse and keyboard from the virtual machine. Close the window
and when it select &#34;Power of the machine&#34; in when VirtualBox
prompts how to shut it down.

Like with the FD12CD.iso we want to unmount our A2 installation
CD ROM. Click on the &#34;Optical Drive&#34; in the manager panel
and choose &#34;Remove disk from virtual Drive&#34;. 

You can now start the machine again and start exploring A2.
I recommend looking at the [Oberon Wikibook](https://en.wikibooks.org/wiki/Oberon#A2_and_UnixAOS)
 for details about how to use A2 and ideas of what to explore.

One nice feature of A2 is it includes a full &#34;NativeOberon&#34;
or ETH Oberon as an A2 Application.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/04/02/A2-Oberon-on-VirtualBox-6.1.md" length="7942" type="text/markdown" />
    </item>    <item>
      <title>ETH Oberon System 3 on VirtualBox 6.1</title>
      <link>https://rsdoiel.github.io/blog/2021/03/17/NativeOberon-VirtualBox.html</link>
      <description>
        <![CDATA[In this post I am walking through installing Native Oberon 2.3.7
        (aka ETH Oberon System 3) on a virtual machine running under
        VirtualBox 6.1. It is a follow up to my 2019 post 
        [FreeDOS to Oberon System 3](/blog/2019/07/28/freedos-to-oberon-system-3.html "Link to old blog post for bringing up Oberon System 3 in VirtualBox 6.0 using FreeDOS 1.2"). To facilitate the install I will first prepare
        my virtual machine as a FreeDOS 1.2 box. This simplifies getting the
        virtual machines' hard disk partitioned and formatted correctly.
        When Native Oberon was released back in 1990's most Intel flavored
        machines shipped with some sort Microsoft OS on them.  I believe
        that is why the tools and instructions for Native Oberon assume
        you're installing over or along side a DOS partition.
        
        ...]]>
      </description>
      <source:markdown>ETH Oberon System 3 on VirtualBox 6.1
=====================================

By R. S. Doiel, 2021-03-17

In this post I am walking through installing Native Oberon 2.3.7
(aka ETH Oberon System 3) on a virtual machine running under
VirtualBox 6.1. It is a follow up to my 2019 post 
[FreeDOS to Oberon System 3](/blog/2019/07/28/freedos-to-oberon-system-3.html &#34;Link to old blog post for bringing up Oberon System 3 in VirtualBox 6.0 using FreeDOS 1.2&#34;). To facilitate the install I will first prepare
my virtual machine as a FreeDOS 1.2 box. This simplifies getting the
virtual machines&#39; hard disk partitioned and formatted correctly.
When Native Oberon was released back in 1990&#39;s most Intel flavored
machines shipped with some sort Microsoft OS on them.  I believe
that is why the tools and instructions for Native Oberon assume
you&#39;re installing over or along side a DOS partition.

Building our machine
--------------------

Requirements
------------

1. Install VirtualBox 6.1 installed on your host computer.
2. Download and install a minimal FreeDOS 1.2 as a virtual machine
3. Downloaded a copy of Native Oberon 2.3.7 alpha from SourceForge
3. Familiarized yourself Oberon&#39;s Text User Interface
4. Boot your FreeDOS virtual machine using the Oberon0.Dsk downloaded
as part of NativeOberon_2.3.7.tar.gz
5. Mount &#34;Oberon0.Dsk&#34; and start installing Native Oberon

Before you boot &#34;Oberon0.Dsk&#34; on your virtual machine make sure
you&#39;ve looked at some online Oberon documentation. This is important.
Oberon is very different from macOS, Windows, Linux, DOS, CP/M or
Unix. It is easy to read the instructions and miss important details 
like how you use the three button mouse, particularly the selections
and execute actions of text instructions.

Virtual Machine Setup
---------------------

VirtualBox 6.1 can be obtained from [virtualbox.org](https://www.virtualbox.org/).  This involves downloading the installer for your particular host
operating system (e.g. Linux, macOS or Windows) and follow the instructions
on the VirtualBox website to complete the installation.

Once VirtualBox is installed, launch VirtualBox.

Click the &#34;New&#34; button and name your machine (e.g. &#34;Native Oberon 2.3.7 Alpha&#34;) and choose type of &#34;Other&#34; and version &#34;DOS&#34;. Click continue. I accepted
the default memory size of 32 MB. This is plenty for Oberon.  I clicked on create disk and accepted the default VDI (VirtualBox Disk Image). Press continue. I think accepted &#34;Dynamically allocated&#34;, press continue. I chose a &#34;disk size&#34; of 100.00 MB. Oberon System is tiny. Press create button.

Make sure your new machine is highlight on the left side of the VirtualBox
management panel. Click on Settings button, it looks like a gear towards
the top. Click &#34;Display&#34; on the model dialog and bump the Video Memory
up to 128 MB. I also clicked Enable 3D Acceleration (though I don&#39;t think
Oberon uses this).  Before clicking OK click on the Network icon in the 
modal dialog box. Change &#34;NAT&#34; to &#34;Bridged Adapter&#34;. Now click &#34;OK&#34; to
close the modal dialog box. 

Your VirtualBox is now ready, before pressing &#34;Start&#34; we need
to install FreeDOS 1.2.

Make a FreeDOS 1.2 machine
---------------------------

Download the [FD12CD.iso](https://www.ibiblio.org/pub/micro/pc-stuff/freedos/files/distributions/1.2/FD12CD.iso) file from the 
[FreeDOS project](https://freedos.org/download).

&#34;Insert&#34; the &#34;FD12CD.ISO&#34; file into our VirtualBox 6.1
CD-ROM drive. Go to the VirtualBox management panel. 
In the area that summarizes storage click the word &#34;Empty&#34;
in the line with &#34;[Optical Drive]&#34;. Find the &#34;FD12CD.ISO&#34;
you downloaded and select it.

Now press the green &#34;Start&#34; arrow in the VirtualBox management
panel. This should start your virtual machine and it will boot
using the CD-ROM drive.

This will display a welcome screen with installation options.
Press your &#34;tab&#34; key once. This should cause a boot string to be
displayed. Type a space and then type the word &#34;raw&#34; (without quotes).
Press enter. Next select the language you want to install with
(e.g. English). Choose &#34;Yes - Continue with installation&#34; on the
next prompt. You should then be given a dialog box that
indicates &#34;Drive C: does not appear to be partitioned.&#34;, select
&#34;Yes - Partition drive C:&#34;. Then that completes press
&#34;Yes - Please reboot now&#34;. 

This will cause the machine to reboot and you will be faced with
the &#34;Welcome to FreeDOS 1.2&#34; screen once again. Press the &#34;tab&#34;
Add a space and type &#34;raw&#34; to the boot string as before.
Select the language again then select &#34;Yes - Continue with 
installation&#34;. The screen should now say something like
&#34;Drive C: does not appear to be formatted&#34;, select 
&#34;Yes - Please erase and format drive C:&#34;.

When done it&#39;ll gather some info on the system and ask you
which keyboard you&#39;re using. Pick yours (e.g. &#34;US English (Default)&#34;).
It will then give you a choice of what to install. Since we&#39;re
going to overwrite this when we install Oberon just select
the base package, then select &#34;Yes - Please install FreeDOS 1.2&#34;

Before selecting &#34;Yes - Please reboot now&#34; when the install is
finished you want to &#34;eject&#34; your FD12CD.ISO from the virtual
CD-ROM drive.  Switch back to your VirtualBox management panel.
Click the text that says &#34;FD12CD.iso&#34; and select &#34;remove disk
from virtual drive&#34; in the popup menu. Switch back to your
Virtual machine and select &#34;Yes - Please reboot now&#34;

If all goes well the machine will boot into FreeDOS 1.2. When
you see the &#34;C:&#62;&#34; prompt type &#34;shutdown&#34; (without the quotes)
and press enter. We&#39;re now ready to start installing Native
Oberon 2.3.7.


Native Oberon 2.3.7
-------------------

Native Oberon used to be hosted at ETH where Oberon and the Oberon
System was first developed as a research and instructional project.
Unfortunately this seems to no longer be supported by ETH. Prof. Wirth
has long been retired now and they no longer choose to use such a 
useful language or Operating System. 

SourceForge has a mirror of the original sources and some of the
remaining community has put at a &#34;new&#34; release of 2.3.7 Alpha 
bringing Native Oberon a little closer to the present. It&#39;s this
version we&#39;ll use. You can read more at the [SourceForge](https://sourceforge.net/projects/nativeoberon/) as well as at the [Oberon Wikibook](https://en.wikibooks.org/wiki/Oberon). ETH also still maintains an email
list for Oberon and it is active. It can be found at
https://lists.inf.ethz.ch/mailman/listinfo/oberon. I recommend
browsing the archives of the Email list if you run into problems.
I&#39;ve found very helpful information there and the people on the
list seem happy to answer a novices question.

We are going to be downloading files from the Native Oberon Project&#39;s
Files page at SourceForge.

&#62; https://sourceforge.net/projects/nativeoberon/files/nativeoberon/

In the Files page download the instructions 
[NativeOberonInstall.pdf](https://sourceforge.net/projects/nativeoberon/files/NativeOberonInstall.pdf/download) 
or or the text version. This document by Pieter Muller (May 1999) explains
the installation process. It is good for its overview though I found
the actual process simpler than what was described for May 1999. 

On the Files page you&#39;ll also see a green button to &#34;Download Latest Version&#34;, NativeOberon_2.3.7.tar.gz. Click the button and download it.

The NativeOberon_2.3.7.tar.gz contains the files we&#39;ll need to run
NativeOberon on our VirtualBox. Ungzip and untar the file into
a location that is convenient for you. I put mine
in `src/NativeOberon-2.3.7` and I had downloaded the file into my
home directory&#39;s &#34;Downloads&#34; folder. 

```shell
    mkdir -p src/NativeOberon-2.3.7
    cd src/NativeObeorn-2.3.7
    tar zxvf ~/Downloads/NativeOberon_2.3.7.tar.gz
```

You now have the software ready to proceed in installing the system
in VirtualBox.

NativeOberon in a VirtualBox
----------------------------

Go back to your VirtualBox management panel. We need to place
he boot disk image in the virtual floppy drive. In the files
we unpacked (i.e. ungzip and untar) there is a file named
&#34;Oberon0.Dsk&#34;.  We want to mount that in the virtual floppy drive.
Click on the word &#34;Empty&#34; next to &#34;Floppy Device 0:&#34; in the management
panel. You are then given a modal dialog box and we want to select 
&#34;Choose a disk file&#34;. You can then find the files you save and 
select &#34;Oberon0.Dsk&#34;. 

Booting with Oberon0.Dsk
------------------------

We can now click &#34;Start&#34; button at the top of the VirtualBox management
panel. This will boot the virtual machine using &#34;Oberon0.Dsk&#34;. Oberon
itself loads completely into memory. 

You now have a running Oberon System but we need to install it
on the virtual hard drive. Fortunately our running system comes with
built in instructions.  It is here that people how haven&#39;t 
used Oberon before are going to run into trouble.

Oberon System uses all three buttons of a three button mouse. On
most mice I&#39;ve encountered to day there are two buttons and a
scroll wheel. The scroll wheel is click able and functions like
the middle button on an Oberon mouse.

The left mount button sets the pointer, the middle button (our
scroll wheel if your mouse is like mine) is used to execute
commands and the right mouse button is used to select text.
In our installation instructions displayed on our virtual 
machine we generally be middle clicking the blue colored text. 

In Native Oberon all text is potentially actionable.  Unlike in
Unix where you type a command press enter then have to retype
(or use the command history) to execute the next command we&#39;re
going to click on the text and sometimes select text to execute
commands.  Before we proceed I highly recommend readying
and trying a tutorial out before attempting to install Oberon
on your virtual hard drive. There is an [Oberon System 3 - Main Tutorial](https://web.archive.org/web/20171226183816/http://www.ethoberon.ethz.ch/ethoberon/tutorial/) available at the Internet Archive&#39;s Wayback machine.

Installing to our Virtual Hard disk
----------------------------------

Along the right side (in the system track) is the text &#34;Edit.Open Introduction.Text&#34;. Click with your middle button (scroll wheel on my
mouse) and this will open the text in the &#34;Edit&#34; track on the left
side.  Read this text if you haven&#39;t before. Scrolling through
the text is a little different than the scroll bars on macOS, Windows,
X Windows. They are on the left side and the middle mouse button
sets the scroll position. The left button pages down, the right
pages up.  You can close the &#34;Introduction.Text&#34; windows by 
middle clicking &#34;System.Close&#34; in the upper menu bar.

Review step 1.
--------------

We need to configure the hard drive by middle clicking on
&#34;Config.Desk Standard ATA/EIDE&#34;[^1]. In the console viewer above
you should see something like

[^1]: This blue text makes it clear the command is actionable, like a link in the web browser. But the actual text is the command not the color.

```
Disk: Standard ATA/EIDE
Static BootLinker for OM Object Files / prk
 linking Native.Bin 255388
```

Review step 2.
--------------

Middle click on &#34;Edit.Open InstallFiles.Tool&#34;.  A &#34;Tool&#34; file
is like a text file but usually contains instructions and
a sorta menu or recipe of commands.  In fact our instructions
in the lower viewer of the system track is a &#34;Tool&#34; file called
&#34;Install.Tool&#34;.  Using &#34;Edit.Open&#34; to open the tool or text file
opens a viewer on the left track, the edit track. If you had
clicked on &#34;System.Open Install.Tool&#34; it would open a viewer
on the right, or systems track. In either track by default
the viewers will tile (not overlap).  If you want to close
a view you can click on &#34;System.Close&#34; in the viewer&#39;s menu
bar. Now open our InstallFiles.Tool in the edit track.

InstallFiles.Tool
-----------------

We now are going to prepare our virtual hard drive. Like
our &#34;Install.Tool&#34; text we have a series of instructions
which commands we can click on (the ones in blue).

Middle click on &#34;Partitions.Show ~&#34;. This will open a pain
showing the partition information. You should see something
like

```
Disk: Diskette0, GetSize: no media, removable

Disk: IDE0, 99MB, VBOX HARDDISK
IDE0#00        99MB  --- (Whole disk)
IDE0#01        99MB  --- (Free)

Disk: IDE2, GetSize: no media, removable, VBOX CD-ROM
```

This tells us we have three drives in our VirtualBox
visible to Oberon.  The first is the floppy drive.
It shows &#34;no media&#34;. That might seem odd but when you
read the &#34;Oberon0.Dsk&#34; it read that into memory and the
whole OS is not running in memory, not from disk! While
the disk is still &#34;in the drive&#34; as far as VirtualBox
is concerned it isn&#39;t &#34;mounted&#34; from the point of view of
the operating system.

The second disk section describes our virtual hard drive.
The third describes the virtual CD-ROM drive.

We&#39;re interested in using the disk &#34;IDE0&#34; with the
device number of &#34;01&#34;, we express that as &#34;IDE0#01&#34;. 

In the &#34;Partitions.Text&#34; viewer where we see the
partitions information we can type the command
described in &#34;InstallFiles.Tool&#34;

```
Partitions.ChangeType IDE0#01 6 76 ~
```

We then middle mouse click on line we just type. This
should produce output in the &#34;System.Log&#34; view in the
upper right of the screen that looks like

```
IDE0#01 changed to type 76
```

I had to do a modified version of step 3 of &#34;InstallFiles.Tool&#34;
choosing option &#34;b&#34;.

In the output of &#34;Partitions.show ~&#34; (i.e. the Partitions.Text
viewer) want to middle click on the &#34;I&#34; of &#34;IDE0#01&#34;.
Then right mouse button select &#34;IDE0#01&#34;. 

From 3a in the &#34;InstallFiles.Tool&#34; viewer middle mouse button
click on &#34;Partitions.Format &#34;. This should result in 
the &#34;System.Log&#34; viewer showing 

```
IDE0#01 format successful
```

After formatting the drive I was able to complete step 3b by
middle clicking the commands as provided in the &#34;InstallFiles.Tool&#34;

&#62; NOTE: you may need to scroll that window to see all of step 3

Middle click on &#34;Partitions.UpdateBootFile &#34; 
The &#34;Systems.Log&#34; viewer should show

```
IDE#00 update successful
```

Middle click on &#34;FilesSystem.Mount DST AosFS &#34; in 3b. The
&#34;System.Log&#34; viewer should show

```
DST: mounted
```

We are ready for Step 4. This command does the brunt of the
work of coping things over. The command &#34;Configuration.DoCommands&#34;
take a list of Oberon commands and executes them one after the
other. Middle Mouse click on &#34;Configuration.DoCommands&#34;.
The &#34;System.Log&#34; viewer will show many messages that are
a result of each command taken. Make sure there are no errors.
The last series of commands renamed files so you should see
something like

```
System.RenameFiles
DST:Rel.Obeorn.Text =&#62; DST:Oberon.Text renaming
DST:Rel.Network.Tool =&#62; DST:Network.Tool renaming
DST:Rel.DOS.Tool =&#62; DST:DOS.Tool renaming
```

For step 5 of &#34;InstallFiles.Tool&#34; we can close our &#34;InstallFiles.Tool&#34;
viewer by middle clicking on &#34;System.Close&#34; in the menu bar. You can
also close the &#34;Partitions.Text&#34; viewer using its menu bar and
middle clicking &#34;System.Close&#34;.

Right now we&#39;ve formatted our hard drive and copied a bunch of 
files too it. We still need to configuration our system before
it is self hosting.

In the &#34;Install.Tool&#34; viewer we want to open our &#34;Configure.Tool&#34;.
Middle click on the &#34;Edit.Open Configure.Tool&#34;.

Configuring our Oberon System
-------------------------------

The configuration tool breaks configuration into a series of
parts. First part is configure the display in Part two
we make the hard disk bootable.

To configure out display we want to middle click on the blue text in
&#34;Config.DetectVesa (BIOS cal might hang some systems!)&#34;.
You will be presented with a list of screen resolutions. I middle
clicked the blue text in &#34;Config.Vesa 00000147H 1600 * 1200 * 32&#34;.
In the &#34;System.Log&#34; viewer this showed

```
Vesa mode 00000147H
```

&#62; NOTE: You will likely need to scroll down the page using the scroll bar

In part two we need to decide how we want to boot Oberon. In our case
I recommend Option 2, boot Oberon directly (non-FAT hosted). Middle
click the blue text &#34;Config.BootParition menu ~&#34;.
The &#34;System.Log&#34; viewer should output

```
IDE0#01 config written
```

Middle click the blue text &#34;Partitions.Show ~&#34;. Like in 
&#34;InstallFiles.Tool&#34; this will open a new &#34;Partitions.Text&#34; 
viewer with content like

```
Disk: Diskette0, GetSize: no media, removable

Disk: IDE0, 99MB, VBOX HARDDISK
IDE0#00       99MB  --- (Whole disk)
IDE0#01       99MB   76 * Native Oberon, Aos

Disk: IDE2, GetSize: no media, removable, VBOX CD-ROM
```

Using your right mouse button select &#34;IDE0#01&#34; then in the
&#34;Configure.Tool&#34; viewer middle click the blue text
&#34;Partitions.Activate &#34;. The &#34;System.Log&#34; viewer should
show

```
IDE0#01 already active
```

We don&#39;t have a partition to deactivate so we can skip the last
step of option 2. This is a good place to &#34;eject&#34; our 
floppy disk &#34;Oberon0.Dsk&#34; before we &#34;System.Reboot&#34;.

To eject the disk click on &#34;Oberon0.Dsk&#34; in the VirtualBox
manager panel. The should then change the text to &#34;Empty&#34;.

Finally we&#39;re ready to move to the last step in &#34;Configure.Tool&#34;.
Scroll down and find &#34;System.Reboot&#34;. 
Middle click on the blue text &#34;System.Reboot&#34;. At this point
the virtualbox should reboot from the virtual hard drive.
This reboot will take a little longer than the floppy boot
and the screen size of the virtualbox will be large based on the
settings you picked early. You have a minimal working Oberon
System 3. Now to install some more programs and flesh the system out.

Install some programs
---------------------

First we need to get the zip files provided in 
NativeOberon_2.3.7.tar.gz on to the hard drive.  
Historically these were done via 1.44 MB floppy disks.
We&#39;re going to make it easier. Native Oberon 2.3.7 can
read an ISO formatted CD-ROM. 

Making our virtual CD-ROM
-------------------------

Under Ubuntu 20.04.2 LTS machine creating a ISO image
is one command. Below is I am going to create an ISO 
image of the directory &#34;NativeOberon-2.3.7&#34;
and save the image as &#34;NativeOberon-2.3.7.iso&#34;.

```shell
    mkisofs -J --iso-level 3 \
        -o NativeOberon-2.3.7.iso NativeOberon-2.3.7
```

The `-J` says to use the Joliet extensions, the `--iso-level`
sets the level of ISO support, in this case to 3. See
the manpage for `mkisofs` for details.

On macOS this involves two commands. First use
the &#34;Disk Utility&#34; to create an image of the folder
where you unpacked NativeOberon_2.3.7.tar.gz.
This will result in a &#34;.dmg&#34; or disk image file common
on macOS.  Next we need to convert this to an ISO
formatted image file.  For that we use a command line
macOS tool called `hdiutil` to convert the disk
image to an ISO format. In the process you will create
the ISO file but it will have the extension of &#34;.cdr&#34;.
You can rename (i.e. mv) that file so it has a &#34;.iso&#34;
extension. This is suitable to mount in VirtualBox&#39;s 
virtual CD-ROM drive.

```shell
    hdiutil convert NativeOberon-2.3.7.dmg -format UDTO -o NativeOberon-2.3.7.iso
    mv NativeOberon-2.3.7.iso.cdr NativeOberon-2.3.7.iso
```

Go to the VirtualBox 6.1 management panel and mount the
ISO image file you created. Now we&#39;re ready to return
to our Native Oberon virtual machine.

Installing from an ISO CD-ROM
-----------------------------

I suggest create the following as it&#39;s own tool text.
But if you want you can also type in the commands and
execute one by one.

```
These are the instructions from installing the Native
Oberon 2.3.7 zip archive files. Steps:

1. See what the CD-ROM mount point by reviewing the partitions

    Paritions.Show ~

On my virtual machine the second disk is IDE2 and that
is where we&#39;ll find the CD-ROM.

2. Mount the ISO image as CD

    FileSystem.Mount CD ISOFS IDE2 ~

3. Check to see what files are on the CD-ROM

    System.Directory CD:* ~

4. Copy the files from the CD-ROM to the harddisk

    System.CopyFiles
        CD:Apps1.zip =&#62; Apps1.zip
        CD:Apps2.zip =&#62; Apps2.zip
        CD:Docu.zip =&#62; Docu.zip
        CD:Gadgets.zip =&#62; Gadgets.zip
        CD:Pr3Fonts.zip =&#62; Pr3Fonts.zip
        CD:Source1.zip =&#62; Source1.zip
        CD:Source2.zip =&#62; Source2.zip
        CD:Source3.zip =&#62; Source3.zip
        ~

5. Unzip all our archives using the ZipTool.

    ZipTool.ExtractAll \o \p SYS:
        Gadgets.zip Docu.zip Apps1.zip Apps2.zip
        Pr3Fonts.zip Pr6Fonts.zip
        Source1.zip Source2.zip Source3.zip
        ~

```

You should now have a full installed Native Oberon 2.3.7
system running under VirtualBox 6.1. Enjoy your explorations.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2021/03/17/NativeOberon-VirtualBox.md" length="21823" type="text/markdown" />
    </item>    <item>
      <title>RetroFlag GPi Case Setup</title>
      <link>https://rsdoiel.github.io/blog/2020/12/24/gpi-case-setup.html</link>
      <description>
        <![CDATA[These are my notes for setting up a RetroFlag GPi case using Recalbox
        distribution for retro gaming.
        
        + RetroFlag GPi Case Kit (including a Raspberry Pi Zero W and blank SD Card)
        + A computer to setup the SD Card  and the Raspberry Pi Imager v1.5
        
        We will be installing [Recalbox](https://www.recalbox.com/ "the all-in-one retro gaming console")
        v7.7.x for Raspberry Pi Zero W and GPi case.  Recalbox which is a Retro
        Gaming Linux distribution.]]>
      </description>
      <source:markdown>Setting up a RetroFlag GPi Case
===============================

By R. S. Doiel, 2020-12-24

These are my notes for setting up a RetroFlag GPi case using Recalbox
distribution for retro gaming.

+ RetroFlag GPi Case Kit (including a Raspberry Pi Zero W and blank SD Card)
+ A computer to setup the SD Card  and the Raspberry Pi Imager v1.5

We will be installing [Recalbox](https://www.recalbox.com/ &#34;the all-in-one retro gaming console&#34;)
v7.7.x for Raspberry Pi Zero W and GPi case.  Recalbox which is a Retro
Gaming Linux distribution.

Steps
=====

Preparing the 32 GiB SD Card
---------------------------

1. Download the appropriate Raspberry Pi Imager 1.5 from 
   https://www.raspberrypi.org/software/ for your system
2. Install and launch the Raspberry Pi Imager
3. Click &#34;Operating System&#34;
  a. Select &#34;Emulation and game OS&#34;
  b. Select &#34;Recalbox&#34;
  c. Select &#34;Recalbox 7.1.1-Reloaded (Pi 0/1/GPi Case)&#34;
4. Click &#34;SD Card&#34; &#34;, then select the bank 32 GiB SD Card
5. Click &#34;Write&#34;
6. You will be asked remove the SD Card when done, do so and and exit 
   Raspberry Pi Imager

NOTE: The current release of Recalbox (7.7.1) doesn&#39;t require patching
with &#34;GPI_Case_patch.zip&#34; or installing the shutdown scripts as suggested
on the RetroFlag website. Applying the patches will prevent the GPi
from booting. The website instructions appear to be for an earlier release
of Recalbox.


Installing the Raspberry Pi Zero W in the GPi Case
--------------------------------------------------

The RetroFlag comes with instructions to install the Raspberry Pi Zero W
in the case. I found the pictorial instructions confusing. Doing a search
for &#34;RetroFlag GPi Case Setup&#34; yielded a link to [Howchoo&#39;s YouTube
video](https://www.youtube.com/watch?v=NyJUlNifN1I&#38;feature=youtu.be &#34;RetroFlag GPi CASE Setup and Usage&#34;),  This video also talks about setting up Retro Pi software,
GPi case patches. Skip these. The instructions are now for software that
is out of date (the video dates back to 2019). 

NOTE: Howchoo describes installing RetroPie not Recalbox. Don&#39;t install a
&#34;wpa_supplicant.conf&#34; file or &#34;ssh&#34; file on the SD Card as suggested.
It is not needed and will cause problems.

The GPi case looks very much like a Game Boy. It includes a &#34;Game Pack&#34;
type module which will hold our Raspberry Pi once installed. I found the
assembly instructions confusing but searching YouTube for &#34;RetroFlag GPi
Case Setup&#34; listed several videos which describe the process of putting
the case together as well as how to install RetroPie or
Recalbox Linux Distributions.

Booting the Pi Zero W with the SD Card
--------------------------------------

1. Make sure the GPi Zero Case **IS NOT CONNECTED TO POWER**
  a. the switch the case off
  b. Disconnect the barrel to USB cable from a power source
2. Remove the &#34;game pack&#34; element where you&#39;ve installed the Raspberry Pi Zero W
3. Insert the SD Card into the SD Card slot under the soft cover on the side of
   the Game Pack case
4. Re-insert &#34;Game Pack&#34; into side of the GPi case
5. Plug the barrel USB cable into a USB Power supply , 
6. Turn the power switch to &#34;ON&#34; on the top of the GPi case
7. Wait patiently, it&#39;s going to take several minutes to boot the first time</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/12/24/gpi-case-setup.md" length="4127" type="text/markdown" />
    </item>    <item>
      <title>Dates &#38; Clock</title>
      <link>https://rsdoiel.github.io/blog/2020/11/27/Dates-and-Clock.html</link>
      <description>
        <![CDATA[The [Oakwood](http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines)
        guidelines specified a common set of modules for Oberon-2 for writing
        programs outside of an Oberon System. A missing module from the Oakwood
        guidelines is modules for working with dates and the system clock.
        Fortunately the A2 Oberon System provides a template for that
        functionality. In this article I am exploring implementing the
        [Dates](Dates.Mod) and [Clock](Clock.Mod) modules for Oberon-07. I
        also plan to go beyond the A2 implementations and provide additional
        functionality such as parsing procedures and the ability to work with
        either the date or time related attributes separately in the
        `Dates.DateTime` record.
        
        ...]]>
      </description>
      <source:markdown>Dates and Clock
===============

By R. S. Doiel, 2020-11-27

The [Oakwood](http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines)
guidelines specified a common set of modules for Oberon-2 for writing
programs outside of an Oberon System. A missing module from the Oakwood
guidelines is modules for working with dates and the system clock.
Fortunately the A2 Oberon System[^1] provides a template for that
functionality. In this article I am exploring implementing the
[Dates](Dates.Mod) and [Clock](Clock.Mod) modules for Oberon-07. I
also plan to go beyond the A2 implementations and provide additional
functionality such as parsing procedures and the ability to work with
either the date or time related attributes separately in the
`Dates.DateTime` record.

[^1]: A2 information can be found in the [Oberon wikibook](https://en.wikibooks.org/wiki/Oberon#In_A2)

Divergences
-----------

One of the noticeable differences between Oberon-07 and Active Oberon
is the types that functional procedures can return. We cannot return
an Object in Oberon-07. This is not much of a handicap as we have
variable procedure parameters.  Likewise Active Oberon provides
a large variety of integer number types. In Oberon-07 we have only
INTEGER. Where I&#39;ve create new procedures I&#39;ve used the Oberon idiom
of read only input parameters followed by variable parameters with
side effects and finally parameters for the target record or values
to be updated.


Similarities
------------

In spite of the divergence I have split the module into two.
The [Dates](Dates.html) module is the one you would include in your
program, it provides a DateTime record type which holds the integer
values for year, month, day, hour, minute and second. It provides the
means of parsing a date or time string, comparison, difference and addition
of dates.  The second module [Clock](Clock.html) provides a mechanism
to retrieve the real time clock value from the host system and map the
C based time object into our own DateTime record.  Clock is specific to
OBNC method of interfacing to the C standard libraries of the host system.
If you were to use a different Oberon compiled such as the Oxford
Oberon Compiler you would need to re-implement Clock. Dates itself
should be system independent and work with Oberon-07 compilers generally.

Clock
-----

The Clock module is built from a skeleton in Oberon-07 describing the
signatures of the procedure and an implementation in [C](Clock.c) that
is built using the technique for discussed in my post
[Combining Oberon-07 and C with OBNC](../../05/01/Combining-Oberon-and-C.html). In that article I outline Karl&#39;s three step process to create a
module that will be an interface to C code.  In Step one I create
the Oberon module. Normally I&#39;d leave all procedures empty and
develop them in C. In this specific case I went ahead and wrote
the procedure called `Get` in Oberon and left the procedure `GetRtcTime`
blank. This allowed OBNC to generate the C code for `Get` saving
me some time and create the skeleton for `GetRtcTime` which does
the work interfacing with the system clock via C library calls.

The interface Oberon module looked like this:

~~~{.oberon}

MODULE Clock;

PROCEDURE GetRtcTime*(VAR second, minute, hour, day, month, year : INTEGER);
BEGIN
END GetRtcTime;

PROCEDURE Get*(VAR time, date : INTEGER);
VAR
    second, minute, hour, day, month, year : INTEGER;
BEGIN
	GetRtcTime(second, minute, hour, day, month, year);
	time = ((hour * 4096) + (minute * 64)) + second;
	date = ((year * 512) + (month * 32)) + day;
END Get;

END Clock.

~~~

I wrote the `Get` procedure code in Oberon-07 is the OBNC
compiler will render the Oberon as C during the
compilation process. I save myself writing some C code
in by leveraging OBNC.


Step two was to write [ClockTest.Mod](ClockTest.Mod) in Oberon-07.

~~~{.oberon}

MODULE ClockTest;

IMPORT Tests, Chars, Clock; (* , Out; *)

CONST
    MAXSTR = Chars.MAXSTR;

VAR
    title : ARRAY MAXSTR OF CHAR;
    success, errors : INTEGER;

PROCEDURE TestGetRtcTime() : BOOLEAN;
VAR second, minute, hour, day, month, year : INTEGER; 
    test, expected, result: BOOLEAN;
BEGIN
    test := TRUE;
    second := 0; minute := 0; hour := 0;
    day := 0; month := 0; year := 0;
    expected := TRUE;
    Clock.GetRtcTime(second, minute, hour, day, month, year);


    result := (year &#62; 1900);
    Tests.ExpectedBool(expected, result, 
          &#34;year should be greater than 1900&#34;, test);
    result := (month &#62;= 0) &#38; (month &#60;= 11);
    Tests.ExpectedBool(expected, result,
          &#34;month should be [0, 11]&#34;, test);
    result := (day &#62;= 1) &#38; (day &#60;= 31);
    Tests.ExpectedBool(expected, result,
          &#34;day should be non-zero&#34;, test);

    result := (hour &#62;= 0) &#38; (hour &#60;= 23);
    Tests.ExpectedBool(expected, result,
          &#34;hour should be [0, 23]&#34;, test);
    result := (minute &#62;= 0) &#38; (minute &#60;= 59);
    Tests.ExpectedBool(expected, result, 
          &#34;minute should be [0, 59]&#34;, test);
    result := (second &#62;= 0) &#38; (second &#60;= 60);
    Tests.ExpectedBool(expected, result,
          &#34;second year should be [0,60]&#34;, test);
    RETURN test
END TestGetRtcTime;

PROCEDURE TestGet() : BOOLEAN;
VAR time, date : INTEGER; 
    test, expected, result : BOOLEAN;
BEGIN
    test := TRUE;
    time := 0;
    date := 0;
    Clock.Get(time, date);
    expected := TRUE;
    result := (time &#62; 0);
    Tests.ExpectedBool(expected, result,
        &#34;time should not be zero&#34;, test);
    result := (date &#62; 0);
    Tests.ExpectedBool(expected, result,
        &#34;date should not be zero&#34;, test);

    RETURN test
END TestGet;

BEGIN
    Chars.Set(&#34;Clock module test&#34;, title);
    success := 0; errors := 0;
    Tests.Test(TestGetRtcTime, success, errors);
    Tests.Test(TestGet, success, errors);
    Tests.Summarize(title, success, errors);
END ClockTest.

~~~

ClockTest is a simple test module for [Clock.Mod](Clock.Mod).
It also serves the role when compiled with OBNC to create the
template C code for [Clock.c](Clock.c). Here&#39;s the steps we
take to generate `Clock.c` with OBNC:

~~~{.shell}

obnc ClockTest.Mod
mv .obnc/Clock.c ./
vi Clock.c

~~~

After compiling `.obnc/Clock.c` I then moved `.obnc/Clock.c`
to my working directory. Filled in the C version of
`GetRtcTime` function and modified my [Clock.Mod](Clock.Mod)
to contain my empty procedure.

The finally version of Clock.c looks like (note how we need to
include &#34;Clock.h&#34; in the head of the our C source file).

~~~{.c}

/*GENERATED BY OBNC 0.16.1*/

#include &#34;.obnc/Clock.h&#34;
#include &#60;obnc/OBNC.h&#62;
#include &#60;time.h&#62;

#define OBERON_SOURCE_FILENAME &#34;Clock.Mod&#34;

void Clock__GetRtcTime_(OBNC_INTEGER *second_, OBNC_INTEGER *minute_,
     OBNC_INTEGER *hour_, OBNC_INTEGER *day_,
     OBNC_INTEGER *month_, OBNC_INTEGER *year_)
{
    time_t now;
    struct tm *time_info;
    now = time(NULL);
    time_info = localtime(&#38;now);
    *second_ = time_info-&#62;tm_sec;
    *minute_ = time_info-&#62;tm_min;
    *hour_ = time_info-&#62;tm_hour;
    *day_ = time_info-&#62;tm_mday;
    *month_ = time_info-&#62;tm_mon;
    *year_ = (time_info-&#62;tm_year) + 1900;
}


void Clock__Get_(OBNC_INTEGER *time_, OBNC_INTEGER *date_)
{
	OBNC_INTEGER second_, minute_, hour_, day_, month_, year_;

	Clock__GetRtcTime_(&#38;second_, &#38;minute_, 
                       &#38;hour_, &#38;day_, &#38;month_, &#38;year_);
	(*time_) = ((hour_ * 4096) + (minute_ * 64)) + second_;
	(*date_) = ((year_ * 512) + (month_ * 32)) + day_;
}


void Clock__Init(void)
{
}

~~~

The final version of Clock.Mod looks like

~~~{.oberon}

MODULE Clock;

PROCEDURE GetRtcTime*(VAR second, minute, 
                      hour, day, month, year : INTEGER);
BEGIN
END GetRtcTime;

PROCEDURE Get*(VAR time, date : INTEGER);
BEGIN
END Get;

END Clock.

~~~


Step three was to re-compile `ClockTest.Mod` and run the tests.

~~~{.shell}

    obnc ClockTest.Mod
    ./ClockTest

~~~

Dates
-----

The dates module provides a rich variety of
procedures for working with dates. This includes parsing
date strings into `DateTime` records, testing strings for
supported date formats, setting dates or time in a `DateTime`
record as well as comparison, difference and addition
(both addition and subtraction) of dates. Tests for the Dates
module is implemented in [DatesTest.Mod](DatesTest.Mod).

~~~{.oberon}

MODULE Dates;
IMPORT Chars, Strings, Clock, Convert := extConvert;

CONST
    MAXSTR = Chars.MAXSTR;
    SHORTSTR = Chars.SHORTSTR;

    YYYYMMDD* = 1; (* YYYY-MM-DD format *)
    MMDDYYYY* = 2; (* MM/DD/YYYY format *)
    YYYYMMDDHHMMSS* = 3; (* YYYY-MM-DD HH:MM:SS format *)

TYPE
    DateTime* = RECORD
        year*, month*, day*, hour*, minute*, second* : INTEGER
    END;

VAR
    (* Month names, January = 0, December = 11 *)
    Months*: ARRAY 23 OF ARRAY 10 OF CHAR;
    (* Days of week, Monday = 0, Sunday = 6 *)
    Days*: ARRAY 7 OF ARRAY 10 OF CHAR;
    DaysInMonth: ARRAY 12 OF INTEGER;


(* Set -- initialize a date record year, month and day values *)
PROCEDURE Set*(year, month, day, hour, minute, second : INTEGER; 
               VAR dt: DateTime);
BEGIN
    dt.year := year;
    dt.month := month;
    dt.day := day;
    dt.hour := hour;
    dt.minute := minute;
    dt.second := second;
END Set;

(* SetDate -- set a Date record&#39;s year, month and day attributes *)
PROCEDURE SetDate*(year, month, day : INTEGER; VAR dt: DateTime);
BEGIN
    dt.year := year;
    dt.month := month;
    dt.day := day;
END SetDate;

(* SetTime -- set a Date record&#39;s hour, minute, second attributes *)
PROCEDURE SetTime*(hour, minute, second : INTEGER; VAR dt: DateTime);
BEGIN
    dt.hour := hour;
    dt.minute := minute;
    dt.second := second;
END SetTime;

(* Copy -- copy the values from one date record to another *)
PROCEDURE Copy*(src : DateTime; VAR dest : DateTime);
BEGIN
    dest.year := src.year;
    dest.month := src.month;
    dest.day := src.day;
    dest.hour := src.hour;
    dest.minute := src.minute;
    dest.second := src.second;
END Copy;

(* ToChars -- converts a date record into an array of chars using
the format constant. Formats supported are YYYY-MM-DD HH:MM:SS
or MM/DD/YYYY HH:MM:SS. *)
PROCEDURE ToChars*(dt: DateTime; fmt : INTEGER;
                   VAR src : ARRAY OF CHAR);
VAR ok : BOOLEAN;
BEGIN
    Chars.Clear(src);
    IF fmt = YYYYMMDD THEN
        Chars.AppendInt(dt.year, 4, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.month, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.day, 2, &#34;0&#34;, src);
    ELSIF fmt = MMDDYYYY THEN
        Chars.AppendInt(dt.month, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;/&#34;, src);
        Chars.AppendInt(dt.day, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;/&#34;, src);
        Chars.AppendInt(dt.year, 4, &#34;0&#34;, src);
    ELSIF fmt = YYYYMMDDHHMMSS THEN
        Chars.AppendInt(dt.year, 4, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.month, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.day, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34; &#34;, src);
        Chars.AppendInt(dt.hour, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;:&#34;, src);
        Chars.AppendInt(dt.minute, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;:&#34;, src);
        Chars.AppendInt(dt.second, 2, &#34;0&#34;, src);
    END;
END ToChars;

(*
 * Date and Time functions very much inspired by A2 but
 * adapted for use in Oberon-07 and OBNC compiler.
 *)

(* LeapYear -- returns TRUE if &#39;year&#39; is a leap year *)
PROCEDURE LeapYear*(year: INTEGER): BOOLEAN;
BEGIN
	RETURN (year &#62; 0) &#38; (year MOD 4 = 0) &#38; 
           (~(year MOD 100 = 0) OR (year MOD 400 = 0))
END LeapYear;

(* NumOfDays -- number of days, returns the number of 
days in that month *)
PROCEDURE NumOfDays*(year, month: INTEGER): INTEGER;
VAR result : INTEGER;
BEGIN
    result := 0;
	DEC(month);
	IF ((month &#62;= 0) &#38; (month &#60; 12)) THEN
	    IF (month = 1) &#38; LeapYear(year) THEN
            result := DaysInMonth[1]+1;
	    ELSE
            result := DaysInMonth[month];
	    END;
    END;
    RETURN result
END NumOfDays;

(* IsValid -- checks if the attributes set in a 
DateTime record are valid *)
PROCEDURE IsValid*(dt: DateTime): BOOLEAN;
BEGIN
	RETURN ((dt.year &#62; 0) &#38; (dt.month &#62; 0) &#38;
           (dt.month &#60;= 12) &#38; (dt.day &#62; 0) &#38;
           (dt.day &#60;= NumOfDays(dt.year, dt.month)) &#38;
           (dt.hour &#62;= 0) &#38; (dt.hour &#60; 24) &#38; (dt.minute &#62;= 0) &#38;
           (dt.minute &#60; 60) &#38; (dt.second &#62;= 0) &#38; (dt.second &#60; 60))
END IsValid;

(* IsValidDate -- checks to see if a datetime record 
has valid day, month and year attributes *)
PROCEDURE IsValidDate*(dt: DateTime) : BOOLEAN;
BEGIN
	RETURN (dt.year &#62; 0) &#38; (dt.month &#62; 0) &#38;
           (dt.month &#60;= 12) &#38; (dt.day &#62; 0) &#38;
           (dt.day &#60;= NumOfDays(dt.year, dt.month))
END IsValidDate;

(* IsValidTime -- checks if the hour, minute, second
attributes set in a DateTime record are valid *)
PROCEDURE IsValidTime*(dt: DateTime): BOOLEAN;
BEGIN
	RETURN (dt.hour &#62;= 0) &#38; (dt.hour &#60; 24) &#38;
           (dt.minute &#62;= 0) &#38; (dt.minute &#60; 60) &#38;
           (dt.second &#62;= 0) &#38; (dt.second &#60; 60)
END IsValidTime;


(* OberonToDateTime -- convert an Oberon date/time 
to a DateTime structure *)
PROCEDURE OberonToDateTime*(Date, Time: INTEGER; 
                            VAR dt : DateTime);
BEGIN
	dt.second := Time MOD 64; Time := Time DIV 64;
	dt.minute := Time MOD 64; Time := Time DIV 64;
	dt.hour := Time MOD 24;
	dt.day := Date MOD 32; Date := Date DIV 32;
	dt.month := (Date MOD 16) + 1; Date := Date DIV 16;
	dt.year := Date;
END OberonToDateTime;

(* DateTimeToOberon -- convert a DateTime structure
to an Oberon date/time *)
PROCEDURE DateTimeToOberon*(dt: DateTime;
                            VAR date, time: INTEGER);
BEGIN
	IF IsValid(dt) THEN
	date := (dt.year)*512 + dt.month*32 + dt.day;
	time := dt.hour*4096 + dt.minute*64 + dt.second
    ELSE
        date := 0;
        time := 0;
    END;
END DateTimeToOberon;

(* Now -- returns the current date and time as a
DateTime record. *)
PROCEDURE Now*(VAR dt: DateTime);
VAR d, t: INTEGER;
BEGIN
	Clock.Get(t, d);
	OberonToDateTime(d, t, dt);
END Now;


(* WeekDate -- returns the ISO 8601 year number,
week number &#38; week day (Monday=1, ....Sunday=7)
Algorithm is by Rick McCarty, 
http://personal.ecu.edu/mccartyr/ISOwdALG.txt
*)
PROCEDURE WeekDate*(dt: DateTime; 
                    VAR year, week, weekday: INTEGER);
VAR doy, i, yy, c, g, jan1: INTEGER; leap: BOOLEAN;
BEGIN
	IF IsValid(dt) THEN
		leap := LeapYear(dt.year);
		doy := dt.day; i := 0;
		WHILE (i &#60; (dt.month - 1)) DO
            doy := doy + DaysInMonth[i];
            INC(i);
        END;
		IF leap &#38; (dt.month &#62; 2) THEN
            INC(doy);
        END;
		yy := (dt.year - 1) MOD 100;
        c := (dt.year - 1) - yy;
        g := (yy + yy) DIV 4;
		jan1 := 1 + (((((c DIV 100) MOD 4) * 5) + g) MOD 7);

		weekday := 1 + (((doy + (jan1 - 1)) - 1) MOD 7);
        (* does doy fall in year-1 ? *)
		IF (doy &#60;= (8 - jan1)) &#38; (jan1 &#62; 4) THEN 
			year := dt.year - 1;
			IF (jan1 = 5) OR ((jan1 = 6) &#38; LeapYear(year)) THEN
                week := 53;
			ELSE
                week := 52;
			END;
		ELSE
			IF leap THEN
                i := 366;
            ELSE
                i := 365;
            END;
			IF ((i - doy) &#60; (4 - weekday)) THEN
				year := dt.year + 1;
				week := 1;
			ELSE
				year := dt.year;
				i := doy + (7-weekday) + (jan1-1);
				week := i DIV 7;
				IF (jan1 &#62; 4) THEN
                    DEC(week);
                END;
			END;
		END;
	ELSE
		year := -1; week := -1; weekday := -1;
	END;
END WeekDate;

(* Equal -- compare to date records to see if they 
are equal values *)
PROCEDURE Equal*(t1, t2: DateTime) : BOOLEAN;
BEGIN
	RETURN ((t1.second = t2.second) &#38;
            (t1.minute = t2.minute) &#38; (t1.hour = t2.hour) &#38;
            (t1.day = t2.day) &#38; (t1.month = t2.month) &#38;
            (t1.year = t2.year))
END Equal;

(* compare -- used in Compare only for comparing
specific values, returning an appropriate -1, 0, 1 *)
PROCEDURE compare(t1, t2 : INTEGER) : INTEGER;
VAR result : INTEGER;
BEGIN
	IF (t1 &#60; t2) THEN
        result := -1;
	ELSIF (t1 &#62; t2) THEN
        result := 1;
	ELSE
        result := 0;
	END;
	RETURN result
END compare;

(* Compare -- returns -1 if (t1 &#60; t2), 
0 if (t1 = t2) or 1 if (t1 &#62;  t2) *)
PROCEDURE Compare*(t1, t2: DateTime) : INTEGER;
VAR result : INTEGER;
BEGIN
	result := compare(t1.year, t2.year);
	IF (result = 0) THEN
		result := compare(t1.month, t2.month);
		IF (result = 0) THEN
			result := compare(t1.day, t2.day);
			IF (result = 0) THEN
				result := compare(t1.hour, t2.hour);
				IF (result = 0) THEN
					result := compare(t1.minute, t2.minute);
					IF (result = 0) THEN
						result := compare(t1.second, t2.second);
					END;
				END;
			END;
		END;
	END;
	RETURN result
END Compare;

(* CompareDate -- compare day, month and year
values only *)
PROCEDURE CompareDate*(t1, t2: DateTime) : INTEGER;
VAR result : INTEGER;
BEGIN
	result := compare(t1.year, t2.year);
	IF (result = 0) THEN
		result := compare(t1.month, t2.month);
		IF (result = 0) THEN
			result := compare(t1.day, t2.day);
		END;
	END;
	RETURN result
END CompareDate;

(* CompareTime -- compare second, minute and
hour values only *)
PROCEDURE CompareTime*(t1, t2: DateTime) : INTEGER;
VAR result : INTEGER;
BEGIN
	result := compare(t1.hour, t2.hour);
	IF (result = 0) THEN
		result := compare(t1.minute, t2.minute);
		IF (result = 0) THEN
			result := compare(t1.second, t2.second);
		END;
	END;
	RETURN result
END CompareTime;



(* TimeDifferences -- returns the absolute time
difference between
t1 and t2.

Note that leap seconds are not counted,
see http://www.eecis.udel.edu/~mills/leap.html *)
PROCEDURE TimeDifference*(t1, t2: DateTime;
              VAR days, hours, minutes, seconds : INTEGER);
CONST 
    SecondsPerMinute = 60; 
    SecondsPerHour = 3600; 
    SecondsPerDay = 86400;
VAR start, end: DateTime; year, month, second : INTEGER;
BEGIN
	IF (Compare(t1, t2) = -1) THEN
        start := t1;
        end := t2;
    ELSE
        start := t2;
        end := t1;
    END;
	IF (start.year = end.year) &#38; (start.month = end.month) &#38;
       (start.day = end.day) THEN
		second := end.second - start.second + 
                  ((end.minute - start.minute) * SecondsPerMinute) +
                  ((end.hour - start.hour) * SecondsPerHour);
		days := 0;
        hours := 0;
        minutes := 0;
	ELSE
		(* use start date/time as reference point *)
		(* seconds until end of the start.day *)
		second := (SecondsPerDay - start.second) -
                  (start.minute * SecondsPerMinute) -
                  (start.hour * SecondsPerHour);
		IF (start.year = end.year) &#38;
           (start.month = end.month) THEN
			(* days between start.day and end.day *)
			days := (end.day - start.day) - 1;
		ELSE
			(* days until start.month ends excluding start.day *)
			days := NumOfDays(start.year, start.month) - start.day;
			IF (start.year = end.year) THEN
				(* months between start.month and end.month *)
				FOR month := start.month + 1 TO end.month - 1 DO
					days := days + NumOfDays(start.year, month);
				END;
			ELSE
				(* days until start.year ends (excluding start.month) *)
				FOR month := start.month + 1 TO 12 DO
					days := days + NumOfDays(start.year, month);
				END;
                (* days between start.years and end.year *)
				FOR year := start.year + 1 TO end.year - 1 DO
					IF LeapYear(year) THEN days := days + 366; 
                    ELSE days := days + 365; END;
				END;
                (* days until we reach end.month in end.year *)
				FOR month := 1 TO end.month - 1 DO
					days := days + NumOfDays(end.year, month);
				END;
			END;
			(* days in end.month until reaching end.day excluding end.day *)
			days := (days + end.day) - 1;
		END;
		(* seconds in end.day *)
		second := second + end.second +
                  (end.minute * SecondsPerMinute) +
                  (end.hour * SecondsPerHour);
	END;
	days := days + (second DIV SecondsPerDay); 
    second := (second MOD SecondsPerDay);
	hours := (second DIV SecondsPerHour); 
    second := (second MOD SecondsPerHour);
	minutes := (second DIV SecondsPerMinute);
    second := (second MOD SecondsPerMinute);
	seconds := second;
END TimeDifference;

(* AddYear -- Add/Subtract a number of years to/from date *)
PROCEDURE AddYears*(VAR dt: DateTime; years : INTEGER);
BEGIN
	ASSERT(IsValid(dt));
	dt.year := dt.year + years;
	ASSERT(IsValid(dt));
END AddYears;

(* AddMonths -- Add/Subtract a number of months to/from date.
This will adjust date.year if necessary *)
PROCEDURE AddMonths*(VAR dt: DateTime; months : INTEGER);
VAR years : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	years := months DIV 12;
	dt.month := dt.month + (months MOD 12);
	IF (dt.month &#62; 12) THEN
		dt.month := dt.month - 12;
		INC(years);
	ELSIF (dt.month &#60; 1) THEN
		dt.month := dt.month + 12;
		DEC(years);
	END;
	IF (years # 0) THEN AddYears(dt, years); END;
	ASSERT(IsValid(dt));
END AddMonths;

(* AddDays --  Add/Subtract a number of days to/from date.
This will adjust date.month and date.year if necessary *)
PROCEDURE AddDays*(VAR dt: DateTime; days : INTEGER);
VAR nofDaysLeft : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	IF (days &#62; 0) THEN
		WHILE (days &#62; 0) DO
			nofDaysLeft := NumOfDays(dt.year, dt.month) - dt.day;
			IF (days &#62; nofDaysLeft) THEN
				dt.day := 1;
				AddMonths(dt, 1);
                (* -1 because we consume the first day 
                    of the next month *)
				days := days - nofDaysLeft - 1;
			ELSE
				dt.day := dt.day + days;
				days := 0;
			END;
		END;
	ELSIF (days &#60; 0) THEN
		days := -days;
		WHILE (days &#62; 0) DO
			nofDaysLeft := dt.day - 1;
			IF (days &#62; nofDaysLeft) THEN
                (* otherwise, dt could become an invalid 
                   date if the previous month has less 
                   days than dt.day *)
				dt.day := 1; 
				AddMonths(dt, -1);
				dt.day := NumOfDays(dt.year, dt.month);
                (* -1 because we consume the last day 
                   of the previous month *)
				days := days - nofDaysLeft - 1;
			ELSE
				dt.day := dt.day - days;
				days := 0;
			END;
		END;
	END;
	ASSERT(IsValid(dt));
END AddDays;

(* AddHours -- Add/Subtract a number of hours to/from date.
This will adjust date.day, date.month and date.year if necessary *)
PROCEDURE AddHours*(VAR dt: DateTime; hours : INTEGER);
VAR days : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	dt.hour := dt.hour + hours;
	days := dt.hour DIV 24;
	dt.hour := dt.hour MOD 24;
	IF (dt.hour &#60; 0) THEN
		dt.hour := dt.hour + 24;
		DEC(days);
	END;
	IF (days # 0) THEN AddDays(dt, days); END;
	ASSERT(IsValid(dt));
END AddHours;

(* AddMinutes -- Add/Subtract a number of minutes to/from date.
This will adjust date.hour, date.day, date.month and date.year
if necessary *)
PROCEDURE AddMinutes*(VAR dt: DateTime; minutes : INTEGER);
VAR hours : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	dt.minute := dt.minute + minutes;
	hours := dt.minute DIV 60;
	dt.minute := dt.minute MOD 60;
	IF (dt.minute &#60; 0) THEN
		dt.minute := dt.minute + 60;
		DEC(hours);
	END;
	IF (hours # 0) THEN AddHours(dt, hours); END;
	ASSERT(IsValid(dt));
END AddMinutes;

(* AddSeconds -- Add/Subtract a number of seconds to/from date.
This will adjust date.minute, date.hour, date.day, date.month and
date.year if necessary *)
PROCEDURE AddSeconds*(VAR dt: DateTime; seconds : INTEGER);
VAR minutes : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	dt.second := dt.second + seconds;
	minutes := dt.second DIV 60;
	dt.second := dt.second MOD 60;
	IF (dt.second &#60; 0) THEN
		dt.second := dt.second + 60;
		DEC(minutes);
	END;
	IF (minutes # 0) THEN AddMinutes(dt, minutes); END;
	ASSERT(IsValid(dt));
END AddSeconds;


(* IsDateString -- return TRUE if the ARRAY OF CHAR is 10 characters
long and is either in the form of YYYY-MM-DD or MM/DD/YYYY where
Y, M and D are digits.
NOTE: is DOES NOT check the ranges of the digits. *)
PROCEDURE IsDateString*(inline : ARRAY OF CHAR) : BOOLEAN;
VAR
    test : BOOLEAN; i, pos : INTEGER;
    src : ARRAY MAXSTR OF CHAR;
BEGIN
    Chars.Set(inline, src);
    Chars.TrimSpace(src);
    test := FALSE;
    IF Strings.Length(src) = 10 THEN
        pos := Strings.Pos(&#34;-&#34;, src, 0);
        IF pos &#62; 0 THEN
            IF (src[4] = &#34;-&#34;) &#38; (src[7] = &#34;-&#34;) THEN
                test := TRUE;
                FOR i := 0 TO 9 DO
                    IF (i # 4) &#38; (i # 7) THEN
                       IF Chars.IsDigit(src[i]) = FALSE THEN
                           test := FALSE;
                       END;
                    END;
                END;
            ELSE
                test := FALSE;
            END;
        END;
        pos := Strings.Pos(&#34;/&#34;, src, 0);
        IF pos &#62; 0 THEN
            IF (src[2] = &#34;/&#34;) &#38; (src[5] = &#34;/&#34;) THEN
                test := TRUE;
                FOR i := 0 TO 9 DO
                    IF (i # 2) &#38; (i # 5) THEN
                        IF Chars.IsDigit(src[i]) = FALSE THEN
                            test := FALSE;
                        END;
                    END;
                END;
            ELSE
                test := FALSE;
            END;
        END;
    END;
    RETURN test
END IsDateString;

(* IsTimeString -- return TRUE if the ARRAY OF CHAR has 4 to 8
characters in the form of H:MM, HH:MM, HH:MM:SS where H, M and S
are digits. *)
PROCEDURE IsTimeString*(inline : ARRAY OF CHAR) : BOOLEAN;
VAR
    test : BOOLEAN;
    l : INTEGER;
    src : ARRAY MAXSTR OF CHAR;
BEGIN
    Chars.Set(inline, src);
    Chars.TrimSpace(src);
    (* remove any trailing am/pm suffixes *)
    IF Chars.EndsWith(&#34;m&#34;, src) THEN
        IF Chars.EndsWith(&#34;am&#34;, src) THEN
            Chars.TrimSuffix(&#34;am&#34;, src);
        ELSE
            Chars.TrimSuffix(&#34;pm&#34;, src);
        END;
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;M&#34;, src) THEN
        Chars.TrimSuffix(&#34;AM&#34;, src);
        Chars.TrimSuffix(&#34;PM&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;p&#34;, src) THEN
        Chars.TrimSuffix(&#34;p&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;P&#34;, src) THEN
        Chars.TrimSuffix(&#34;P&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;a&#34;, src) THEN
        Chars.TrimSuffix(&#34;a&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;A&#34;, src) THEN
        Chars.TrimSuffix(&#34;A&#34;, src);
        Chars.TrimSpace(src);
    END;
    Strings.Extract(src, 0, 8, src);
    test := FALSE;
    l := Strings.Length(src);
    IF (l = 4) THEN
        IF Chars.IsDigit(src[0]) &#38; (src[1] = &#34;:&#34;) &#38;
            Chars.IsDigit(src[2]) &#38; Chars.IsDigit(src[3]) THEN
            test := TRUE;
        ELSE
            test := FALSE;
        END;
    ELSIF (l = 5) THEN
        IF Chars.IsDigit(src[0]) &#38; Chars.IsDigit(src[1]) &#38;
            (src[2] = &#34;:&#34;) &#38;
            Chars.IsDigit(src[3]) &#38; Chars.IsDigit(src[4]) THEN
            test := TRUE;
        ELSE
            test := FALSE;
        END;
    ELSIF (l = 8) THEN
        IF Chars.IsDigit(src[0]) &#38; Chars.IsDigit(src[1]) &#38;
            (src[2] = &#34;:&#34;) &#38;
            Chars.IsDigit(src[3]) &#38; Chars.IsDigit(src[4]) &#38;
            (src[5] = &#34;:&#34;) &#38;
            Chars.IsDigit(src[6]) &#38; Chars.IsDigit(src[7]) THEN
            test := TRUE;
        ELSE
            test := FALSE;
        END;
    ELSE
        test := FALSE;
    END;
    RETURN test
END IsTimeString;

(* ParseDate -- parses a date string in YYYY-MM-DD or
MM/DD/YYYY format. *)
PROCEDURE ParseDate*(inline : ARRAY OF CHAR;
                     VAR year, month, day : INTEGER) : BOOLEAN;
VAR src, tmp : ARRAY MAXSTR OF CHAR; ok, b : BOOLEAN;
BEGIN
    Chars.Set(inline, src);
    Chars.Clear(tmp);
    ok := FALSE;
	IF IsDateString(src) THEN
        (* LIMITATION: Need to allow for more than 4 digit years! *)
        IF (src[2] = &#34;/&#34;) &#38; (src[5] = &#34;/&#34;) THEN
            ok := TRUE;
            Strings.Extract(src, 0, 2, tmp);
            Convert.StringToInt(tmp, month, b);
            ok := ok &#38; b;
            Strings.Extract(src, 4, 2, tmp);
            Convert.StringToInt(tmp, day, b);
            ok := ok &#38; b;
            Strings.Extract(src, 6, 4, tmp);
            Convert.StringToInt(tmp, year, b);
            ok := ok &#38; b;
        ELSIF (src[4] = &#34;-&#34;) &#38; (src[7] = &#34;-&#34;) THEN
            ok := TRUE;
            Strings.Extract(src, 0, 4, tmp);
            Convert.StringToInt(tmp, year, b);
            ok := ok &#38; b;
            Strings.Extract(src, 5, 2, tmp);
            Convert.StringToInt(tmp, month, b);
            ok := ok &#38; b;
            Strings.Extract(src, 8, 2, tmp);
            Convert.StringToInt(tmp, day, b);
            ok := ok &#38; b;
        ELSE
            ok := FALSE;
        END;
    END;
    RETURN ok
END ParseDate;

(* ParseTime -- procedure for parsing time strings into hour,
minute, second. Returns TRUE on successful parse, FALSE otherwise *)
PROCEDURE ParseTime*(inline : ARRAY OF CHAR;
                     VAR hour, minute, second : INTEGER) : BOOLEAN;
VAR src, tmp : ARRAY MAXSTR OF CHAR;
    ok : BOOLEAN; cur, pos, l : INTEGER;
BEGIN
    Chars.Set(inline, src);
    Chars.Clear(tmp);
	IF IsTimeString(src) THEN
        ok := TRUE;
        cur := 0; pos := 0;
        pos := Strings.Pos(&#34;:&#34;, src, cur);
        IF pos &#62; 0 THEN
        (* Get Hour *)
            Strings.Extract(src, cur, pos - cur, tmp);
            Convert.StringToInt(tmp, hour, ok);
            IF ok THEN
                (* Get Minute *)
                cur := pos + 1;
                Strings.Extract(src, cur, 2, tmp);
                Convert.StringToInt(tmp, minute, ok);
                IF ok THEN
                    (* Get second, optional, default to zero *)
                    pos := Strings.Pos(&#34;:&#34;, src, cur);
                    IF pos &#62; 0 THEN
                        cur := pos + 1;
                        Strings.Extract(src, cur, 2, tmp);
                        Convert.StringToInt(tmp, second, ok);
                        cur := cur + 2;
                    ELSE
                        second := 0;
                    END;
                    (* Get AM/PM, optional, adjust hour if PM *)
                    l := Strings.Length(src);
                    WHILE (cur &#60; l) &#38; Chars.IsSpace(src[cur]) DO
                        cur := cur + 1;
                    END;
                    Strings.Extract(src, cur, 2, tmp);
                    Chars.TrimSpace(tmp);
                    IF Chars.Equal(tmp, &#34;PM&#34;) OR Chars.Equal(tmp, &#34;pm&#34;) THEN
                        hour := hour + 12;
                    END;
                ELSE
                    ok := FALSE;
                END;
            END;
        ELSE
            ok := FALSE;
        END;
    ELSE
        ok := FALSE;
    END;
    IF ok THEN
        ok := ((hour &#62;= 0) &#38; (hour &#60;= 23)) &#38;
            ((minute &#62;= 0) &#38; (minute &#60;= 59)) &#38;
                ((second &#62;= 0) &#38; (second &#60;= 59));
    END;
    RETURN ok
END ParseTime;


(* Parse accepts a date array of chars in either dates, times
or dates and times separate by spaces. Date formats supported
include YYYY-MM-DD, MM/DD/YYYY. Time formats include
H:MM, HH:MM, H:MM:SS, HH:MM:SS with &#39;a&#39;, &#39;am&#39;, &#39;p&#39;, &#39;pm&#39;
suffixes.  Dates and times can also be accepted as JSON
expressions with the individual time compontents are specified
as attributes, e.g. {&#34;year&#34;: 1998, &#34;month&#34;: 12, &#34;day&#34;: 10,
&#34;hour&#34;: 11, &#34;minute&#34;: 4, &#34;second&#34;: 3}.
Parse returns TRUE on successful parse, FALSE otherwise.

BUG: Assumes a 4 digit year.
*)
PROCEDURE Parse*(inline : ARRAY OF CHAR; VAR dt: DateTime) : BOOLEAN;
VAR src, ds, ts, tmp : ARRAY SHORTSTR OF CHAR; ok, okDate, okTime : BOOLEAN;
    pos, year, month, day, hour, minute, second : INTEGER;
BEGIN
    dt.year := 0;
    dt.month := 0;
    dt.day := 0;
    dt.hour := 0;
    dt.minute := 0;
    dt.second := 0;
    Chars.Clear(tmp);
    Chars.Set(inline, src);
    Chars.TrimSpace(src);
    (* Split into Date and Time components *)
    pos := Strings.Pos(&#34; &#34;, src, 0);
    IF pos &#62;= 0 THEN
        Strings.Extract(src, 0, pos, ds);
        pos := pos + 1;
        Strings.Extract(src, pos, Strings.Length(src) - pos, ts);
    ELSE
        Chars.Set(src, ds);
        Chars.Set(src, ts);
    END;
    ok := FALSE;
    IF IsDateString(ds) THEN
        ok := TRUE;
        okDate := ParseDate(ds, year, month, day);
        SetDate(year, month, day, dt);
        ok := ok &#38; okDate;
    END;
    IF IsTimeString(ts) THEN
        ok := ok OR okDate;
        okTime := ParseTime(ts, hour, minute, second);
        SetTime(hour, minute, second, dt);
        ok := ok &#38; okTime;
    END;
    RETURN ok
END Parse;

BEGIN
    Chars.Set(&#34;January&#34;, Months[0]);
    Chars.Set(&#34;February&#34;, Months[1]);
    Chars.Set(&#34;March&#34;, Months[2]);
    Chars.Set(&#34;April&#34;, Months[3]);
    Chars.Set(&#34;May&#34;, Months[4]);
    Chars.Set(&#34;June&#34;, Months[5]);
    Chars.Set(&#34;July&#34;, Months[6]);
    Chars.Set(&#34;August&#34;, Months[7]);
    Chars.Set(&#34;September&#34;, Months[8]);
    Chars.Set(&#34;October&#34;, Months[9]);
    Chars.Set(&#34;November&#34;, Months[10]);
    Chars.Set(&#34;December&#34;, Months[11]);

    Chars.Set(&#34;Sunday&#34;, Days[0]);
    Chars.Set(&#34;Monday&#34;, Days[1]);
    Chars.Set(&#34;Tuesday&#34;, Days[2]);
    Chars.Set(&#34;Wednesday&#34;, Days[3]);
    Chars.Set(&#34;Thursday&#34;, Days[4]);
    Chars.Set(&#34;Friday&#34;, Days[5]);
    Chars.Set(&#34;Saturday&#34;, Days[6]);

    DaysInMonth[0] := 31; (* January *)
    DaysInMonth[1] := 28; (* February *)
    DaysInMonth[2] := 31; (* March *)
    DaysInMonth[3] := 30; (* April *)
    DaysInMonth[4] := 31; (* May *)
    DaysInMonth[5] := 30; (* June *)
    DaysInMonth[6] := 31; (* July *)
    DaysInMonth[7] := 31; (* August *)
    DaysInMonth[8] := 30; (* September *)
    DaysInMonth[9] := 31; (* October *)
    DaysInMonth[10] := 30; (* November *)
    DaysInMonth[11] := 31; (* December *)

END Dates.

~~~

Postscript: In this article I included a reference to the module
**[Chars](Chars.html)**. This is a non-standard module I wrote
for Oberon-07. Here is a link to [Chars](Chars.Mod). RSD, 2021-05-06

### Next, Previous

+ Next [Beyond Oakwood, Modules and Aliases](/blog/2021/05/16/Beyond-Oakwood-Modules-and-Aliases.html)
+ Previous [Assemble Pages](/blog/2020/10/19/Assemble-pages.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/11/27/Dates-and-Clock.md" length="35298" type="text/markdown" />
    </item>    <item>
      <title>Chars</title>
      <link>https://rsdoiel.github.io/blog/2020/11/27/Chars.html</link>
      <description>
        <![CDATA[Source code for Chars.Mod.
        
        ...]]>
      </description>
      <source:markdown>Chars
=====

This module provides common character oriented tests.

InRange
: Check to see if a character, c, is in an inclusive range from a lower to upper character.

IsUpper
: Check to see if a character is upper case

IsLower
: Check to see if a character is lower case

IsAlpha
: Check to see if a character is alphabetic, i.e. in the range of &#34;a&#34; to &#34;z&#34;
or &#34;A&#34; to &#34;Z&#34;.

IsDigit
: Check to see if a character is a digit, i.e. in range of &#34;0&#34; to &#34;9&#34;

IsAlphaNum
: Check to see if a character is alpha or a digit

IsSpace
: Check to see if a character is a space, tab, carriage return or line feed

AppendChar
: Append a single char to the end of an ARRAY OF CHAR adjusting the terminating null character and return TRUE on success or FALSE otherwise.

AppendChars
: Append an ARRAY OF CHAR to another the destination ARRAY OF CHAR.

Equal
: Compares two ARRAY OF CHAR and returns TRUE if they match, FALSE otherwise

Clear
: Sets all cells in an ARRAY OF CHAR to 0X.

TrimSpace
: Trim the leading and trailing space characters from an ARRAY OF CHAR

TrimLeftSpace
: Trim the leading space characters from an ARRAY OF CHAR

TrimRightSpace
: Trim the trailing space characters from an ARRAY OF CHAR

StartsWith
: Checks to see if a prefix ARRAY OF CHAR matches a target ARRAY OF CHAR return TRUE if found, FALSE otherwise

EndsWith
: Checks to see if a suffix ARRAY OF CHAR matches a target ARRAY OF CHAR return TRUE if found, FALSE otherwise

TrimPrefix
: Trim a prefix ARRAY OF CHAR from a target ARRAY OF CHAR

TrimSuffix
: Trim a suffix ARRAY OF CHAR from a target ARRAY OF CHAR




Source code for **Chars.Mod**
-----------------------------

~~~
MODULE Chars;

IMPORT Strings;

CONST
    MAXSTR* = 1024; (* or whatever *)
    (* byte constants *)
    LF* = 10;
    CR* = 13;
    (* Character constants *)
    ENDSTR* = 0X;
    NEWLINE* = 10X;
    TAB* = 9X;
    SPACE* = &#34; &#34;;
    DASH* = &#34;-&#34;;
    CARET* = &#34;^&#34;;
    TILDE* = &#34;~&#34;;
    QUOTE* = CHR(34);

(* InRange -- given a character to check and an inclusive range of
characters in the ASCII character set. Compare the ordinal values
for inclusively. Return TRUE if in range FALSE otherwise. *)
PROCEDURE InRange* (c, lower, upper : CHAR) : BOOLEAN;
VAR inrange : BOOLEAN;
BEGIN
  IF (ORD(c) &#62;= ORD(lower)) &#38; (ORD(c) &#60;= ORD(upper)) THEN
    inrange := TRUE;
  ELSE
    inrange := FALSE;
  END;
  RETURN inrange
END InRange;

(* IsUpper return true if the character is an upper case letter *)
PROCEDURE IsUpper*(c : CHAR) : BOOLEAN;
VAR isupper : BOOLEAN;
BEGIN
    IF InRange(c, &#34;A&#34;, &#34;Z&#34;) THEN
        isupper := TRUE;
    ELSE
        isupper := FALSE;
    END
    RETURN isupper
END IsUpper;


(* IsLower return true if the character is a lower case letter *)
PROCEDURE IsLower*(c : CHAR) : BOOLEAN;
VAR islower : BOOLEAN;
BEGIN
    IF InRange(c, &#34;a&#34;, &#34;a&#34;) THEN
        islower := TRUE;
    ELSE
        islower := FALSE;
    END
    RETURN islower
END IsLower;

(* IsDigit return true if the character in the range of &#34;0&#34; to &#34;9&#34; *)
PROCEDURE IsDigit*(c : CHAR) : BOOLEAN;
VAR isdigit : BOOLEAN;
BEGIN
    IF InRange(c, &#34;0&#34;, &#34;9&#34;) THEN
        isdigit := TRUE;
    ELSE
        isdigit := FALSE;
    END;
    RETURN isdigit
END IsDigit;

(* IsAlpha return true is character is either upper or lower case letter *)
PROCEDURE IsAlpha*(c : CHAR) : BOOLEAN;
VAR isalpha : BOOLEAN;
BEGIN
    IF IsUpper(c) OR IsLower(c) THEN
        isalpha := TRUE;
    ELSE
        isalpha := FALSE;
    END;
    RETURN isalpha
END IsAlpha;

(* IsAlphaNum return true is IsAlpha or IsDigit *)
PROCEDURE IsAlphaNum* (c : CHAR) : BOOLEAN;
VAR isalphanum : BOOLEAN;
BEGIN
    IF IsAlpha(c) OR IsDigit(c) THEN
        isalphanum := TRUE;
    ELSE
        isalphanum := FALSE;
    END;
    RETURN isalphanum
END IsAlphaNum;

(* IsSpace returns TRUE if the char is a space, tab, carriage return or line feed *)
PROCEDURE IsSpace*(c : CHAR) : BOOLEAN;
VAR isSpace : BOOLEAN;
BEGIN
	isSpace := FALSE;
	IF (c = SPACE) OR (c = TAB) OR (ORD(c) = CR) OR (ORD(c) = LF) THEN
    	isSpace := TRUE;
	END;
	RETURN isSpace
END IsSpace;

(* AppendChar - this copies the char and appends it to
   the destination. Returns FALSE if append fails. *)
PROCEDURE AppendChar*(c : CHAR; VAR dest : ARRAY OF CHAR) : BOOLEAN;
VAR res : BOOLEAN; l : INTEGER;
BEGIN
  l := Strings.Length(dest);
  (* NOTE: we need to account for a trailing 0X to end
     the string. *)
  IF l &#60; (LEN(dest) - 1) THEN
    dest[l] := c;
    dest[l + 1] := 0X;
    res := TRUE;
  ELSE
    res := FALSE;
  END;
  RETURN res
END AppendChar;

(* AppendChars - copy the contents of src ARRAY OF CHAR to end of
   dest ARRAY OF CHAR *)
PROCEDURE AppendChars*(src : ARRAY OF CHAR; VAR dest : ARRAY OF CHAR);
VAR i, j : INTEGER;
BEGIN
  i := 0;
  WHILE (i &#60; LEN(dest)) &#38; (dest[i] # 0X) DO
    i := i + 1;
  END;
  j := 0;
  WHILE (i &#60; LEN(dest)) &#38; (j &#60; Strings.Length(src)) DO
    dest[i] := src[j];
    i := i + 1;
    j := j + 1;
  END;
  WHILE i &#60; LEN(dest) DO
    dest[i] := 0X;
    i := i + 1;
  END;
END AppendChars;

(* Equal - compares two ARRAY OF CHAR and returns TRUE
if the characters match up to the end of string, FALSE otherwise. *)
PROCEDURE Equal*(a : ARRAY OF CHAR; b : ARRAY OF CHAR) : BOOLEAN;
VAR isSame : BOOLEAN; i : INTEGER;
BEGIN
  isSame := (Strings.Length(a) = Strings.Length(b));
  i := 0;
  WHILE isSame &#38; (i &#60; Strings.Length(a)) DO
    IF a[i] # b[i] THEN
      isSame := FALSE;
    END;
    i := i + 1;
  END;
  RETURN isSame
END Equal;


(* StartsWith - check to see of a prefix starts an ARRAY OF CHAR *)
PROCEDURE StartsWith*(prefix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR) : BOOLEAN;
VAR startsWith : BOOLEAN; i: INTEGER;
BEGIN
    startsWith := FALSE;
    IF Strings.Length(prefix) &#60;= Strings.Length(src) THEN
        startsWith := TRUE;
        i := 0;
        WHILE (i &#60; Strings.Length(prefix)) &#38; startsWith DO
            IF prefix[i] # src[i] THEN
                startsWith := FALSE;
            END;
            i := i + 1;
        END;
    END;    
    RETURN startsWith
END StartsWith;

(* EndsWith - check to see of a prefix starts an ARRAY OF CHAR *)
PROCEDURE EndsWith*(suffix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR) : BOOLEAN;
VAR endsWith : BOOLEAN; i, j : INTEGER;
BEGIN
    endsWith := FALSE;
    IF Strings.Length(suffix) &#60;= Strings.Length(src) THEN
        endsWith := TRUE;
        i := 0;
        j := Strings.Length(src) - Strings.Length(suffix);
        WHILE (i &#60; Strings.Length(suffix)) &#38; endsWith DO
            IF suffix[i] # src[j] THEN
                endsWith := FALSE;
            END;
            i := i + 1;
            j := j + 1;
        END;
    END;
    RETURN endsWith
END EndsWith;


(* Clear - resets all cells of an ARRAY OF CHAR to 0X *)
PROCEDURE Clear*(VAR a : ARRAY OF CHAR);
VAR i : INTEGER;
BEGIN
  FOR i := 0 TO (LEN(a) - 1) DO
    a[i] := 0X;
  END;
END Clear;

(* Shift returns the first character of an ARRAY OF CHAR and shifts the
remaining elements left appending an extra 0X if necessary *)
PROCEDURE Shift*(VAR src : ARRAY OF CHAR) : CHAR;
VAR i, last : INTEGER; c : CHAR;
BEGIN
    i := 0;
    c := src[i];
    Strings.Delete(src, 0, 1);
    last := Strings.Length(src) - 1;
    FOR i := last TO (LEN(src) - 1) DO
        src[i] := 0X;
    END;
    RETURN c
END Shift;

(* Pop returns the last non-OX element of an ARRAY OF CHAR replacing
   it with an OX *)
PROCEDURE Pop*(VAR src : ARRAY OF CHAR) : CHAR;
VAR i, last : INTEGER; c : CHAR;
BEGIN
	(* Move to the last non-0X cell *)
	i := 0;
	last := LEN(src);
	WHILE (i &#60; last) &#38; (src[i] # 0X) DO
	   i := i + 1;
	END;
	IF i &#62; 0 THEN
		i := i - 1;
	ELSE
		i := 0;
	END;
	c := src[i];
	WHILE (i &#60; last) DO
		src[i] := 0X;
		i := i + 1;
	END;
	RETURN c
END Pop;

(* TrimLeftSpace - remove leading spaces from an ARRAY OF CHAR *)
PROCEDURE TrimLeftSpace*(VAR src : ARRAY OF CHAR);
VAR i : INTEGER;
BEGIN
    (* find the first non-space or end of the string *)
    i := 0;
    WHILE (i &#60; LEN(src)) &#38; IsSpace(src[i]) DO
        i := i + 1;
    END;
    (* Trims the beginning of the string *)
    IF i &#62; 0 THEN
        Strings.Delete(src, 0, i);
    END;
END TrimLeftSpace;

(* TrimRightSpace - remove the trailing spaces from an ARRAY OF CHAR *)
PROCEDURE TrimRightSpace*(VAR src : ARRAY OF CHAR);
VAR i, l : INTEGER; 
BEGIN
    (* Find the first 0X, end of string *)
	l := Strings.Length(src);
	i := l - 1;
	(* Find the start of the trailing space sequence *)
	WHILE (i &#62; 0) &#38; IsSpace(src[i]) DO
		i := i - 1;
	END;
	(* Delete the trailing spaces *)
	Strings.Delete(src, i + 1, l - i);
END TrimRightSpace;

(* TrimSpace - remove leading and trailing space CHARS from an ARRAY OF CHAR *)
PROCEDURE TrimSpace*(VAR src : ARRAY OF CHAR);
BEGIN
	TrimLeftSpace(src);
	TrimRightSpace(src);    
END TrimSpace;    
    

(* TrimPrefix - remove a prefix ARRAY OF CHAR from a target ARRAY OF CHAR *)
PROCEDURE TrimPrefix*(prefix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR);
VAR l : INTEGER;
BEGIN
    IF StartsWith(prefix, src) THEN
         l := Strings.Length(prefix);
         Strings.Delete(src, 0, l);
    END;
END TrimPrefix;

(* TrimSuffix - remove a suffix ARRAY OF CHAR from a target ARRAY OF CHAR *)
PROCEDURE TrimSuffix*(suffix : ARRAY OF CHAR; VAR src : ARRAY OF CHAR);
VAR i, l : INTEGER;
BEGIN
	IF EndsWith(suffix, src) THEN
		l := Strings.Length(src) - 1;
		FOR i := ((l - Strings.Length(suffix)) + 1) TO l DO
			src[i] := 0X;
		END;
	END;
END TrimSuffix;


END Chars.

~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2020/11/27/Chars.md" length="9790" type="text/markdown" />
    </item>    <item>
      <title>Clock</title>
      <link>https://rsdoiel.github.io/blog/2020/11/27/Clock.html</link>
      <description>
        <![CDATA[This is a C time library wrapper for getting system time
        to support Dates.Mod. The procedures are read only as
        setting time is non-standard on many Unix-like systems[^1].
        The two procedures follow the A2 style procedure signatures
        adjusted for Oberon-07.
        
        ...]]>
      </description>
      <source:markdown>Clock
=====

This is a C time library wrapper for getting system time
to support Dates.Mod. The procedures are read only as
setting time is non-standard on many Unix-like systems[^1].
The two procedures follow the A2 style procedure signatures
adjusted for Oberon-07.


[^1]: Eric Raymond discusses time functions, http://www.catb.org/esr/time-programming/



Source code for **Clock.Mod**
-----------------------------

~~~
MODULE Clock;

PROCEDURE GetRtcTime*(VAR second, minute, hour, day, month, year : INTEGER);
BEGIN
END GetRtcTime;

PROCEDURE Get*(VAR time, date : INTEGER);
BEGIN
END Get;

END Clock.

~~~


The C Source generated by OBNC then modified for this module.

~~~
/*GENERATED BY OBNC 0.16.1*/

#include &#34;.obnc/Clock.h&#34;
#include &#60;obnc/OBNC.h&#62;
#include &#60;time.h&#62;

#define OBERON_SOURCE_FILENAME &#34;Clock.Mod&#34;

void Clock__GetRtcTime_(OBNC_INTEGER *second_, OBNC_INTEGER *minute_, OBNC_INTEGER *hour_, OBNC_INTEGER *day_, OBNC_INTEGER *month_, OBNC_INTEGER *year_)
{
    time_t now;
    struct tm *time_info;
    now = time(NULL);
    time_info = localtime(&#38;now);
    *second_ = time_info-&#62;tm_sec;
    *minute_ = time_info-&#62;tm_min;
    *hour_ = time_info-&#62;tm_hour;
    *day_ = time_info-&#62;tm_mday;
    *month_ = time_info-&#62;tm_mon;
    *year_ = (time_info-&#62;tm_year) + 1900;
}


void Clock__Get_(OBNC_INTEGER *time_, OBNC_INTEGER *date_)
{
	OBNC_INTEGER second_, minute_, hour_, day_, month_, year_;

	Clock__GetRtcTime_(&#38;second_, &#38;minute_, &#38;hour_, &#38;day_, &#38;month_, &#38;year_);
	(*time_) = ((hour_ * 4096) + (minute_ * 64)) + second_;
	(*date_) = ((year_ * 512) + (month_ * 32)) + day_;
}


void Clock__Init(void)
{
}
~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2020/11/27/Clock.md" length="2270" type="text/markdown" />
    </item>    <item>
      <title>Dates</title>
      <link>https://rsdoiel.github.io/blog/2020/11/27/Dates.html</link>
      <description>
        <![CDATA[This module provides minimal date time records and procedures
        for working with dates in YYYY-MM-DD and MM/DD/YYYY format and
        times in H:MM, HH:MM and HH:MM:SS formats.
        
        ...]]>
      </description>
      <source:markdown>Dates
=====

This module provides minimal date time records and procedures
for working with dates in YYYY-MM-DD and MM/DD/YYYY format and
times in H:MM, HH:MM and HH:MM:SS formats.


Set
: Set a DateTime record providing year, month, day, hour, minute and second as integers and the DateTime record to be populated.

SetDate
: Set the date portion of a DateTime record, leaves the hour, minute and second attributes unmodified. 

SetTime
: Set the time portion of a DateTime record, leaves the year, month, date attributes unmodified.

Copy
: Copy the attributes of one DateTime record into another DateTime record

ToChars
: Given a DateTime record and a format constant, render the DateTime record to an array of CHAR.

LeapYear
: Given a DateTime record check to see if it is a  leap year.

NumOfDays
: Given a year and monoth return the number of days in the month.

IsValid
: Check to see if the all attributes in a DateTime record are valid.

OberonToDateTime
: Convert oberon date and time integer values into a DateTime record

DateTimeToOberon
: Convert a DateTime record into Oberon date and time integer values.

Now
: Set a DateTime record&#39;s attributes to the current time, depends of on the implementation of Clock.Mod.

WeekDate
: Given a DateTime record calculates the year, week and weekday as integers values.

Equal
: Checks to DateTime records to see if they have equivalent attribute values.

Compare
: Compare two DateTime records, if t1 &#60; t2 then return -1, if t1 = t2 return 0 else if t1 &#62; t2 return 1.

CompareDate
: Compare the year, month, day attributes of two DateTime records following the approach used in Compare.

CompareTime
: Compare the hour, minute, second attributes of two DateTime records following the approach used in Compare.

TimeDifference
: Take the differ of two DateTime records setting the difference in integer values for days, hours, minutes and seconds.

AddYears
: Add years to a DateTime record. Years is a positive or negative integer.

AddMonths
: Add months to a DateTime record. Months is either a positive or negative integer. Months will propogate to year in the DataTime record.

AddDays
: Add days to a DateTime record. Days can be either a positive or negative integer.  Days will propogate to month and year attributes of the DateTime record.

AddHours
: Add hours to a DateTime record. Hours can be either a positive or negative integer.  Hours will propogate to day, month and year attributes of the DateTime record.

AddMinutes
: Add minutes to a DateTime record. Minutes can be either a positive or negative integer. Minutes will propogate to hour, day, month and year attributes of the DateTime record.

AddSeconds
: Add seconds to a DateTime record. Seconds can be either a positive or negatice integer.  Seconds will propogate to minute, hour, day, month, year attributes of the DateTime record.

IsValidDate
: IsValidDate checks the day, month, year attributes of a DateTime record and validates the values. Returns TRUE if everthing is ok, FALSE otherwise.

IsValidTime
: IsValidTime checks the hour, minute, second attributes of a DateTime record and validates the values. Returns TRUE if everthing is ok, FALSE otherwise.

IsDateString
: Checks to see if an ARRAY OF CHAR is a parsiable date string (e.g. in 2020-11-26 or 11/26/2020). Returns TRUE if the string is parsable, FALSE otherwise. NOTE: It does NOT check to see if the day, month or year values are valid. It only checks the format of the string.

IsTimeString
: Checks to see if an ARRAY OF CHAR is a parsible time string (e.g. 3:32, 14:55, 09:19:22). NOTE: It only checks the format and does not check the hour, minute and second values.

ParseDate
: Parse an ARRAY OF CHAR setting the values if year, month and day. Return TRUE on successful parse and FALSE otherwise.

ParseTime
: Parse an ARRAY OF CHAR setting the values of hour, minute and second. Return TRUE on succesful parse and FALSE otherwise.

Parse
: Parse an ARRAY OF CHAR setting the attributes of a DateTime record. Return TURE on success, FALSE otherwise.

Limitations
-----------

Dates are presumed to be in the YYYY-DD-MM or MM/DD/YYYY formats.
Does not handle dates with spelled out months or weekdays.

Time portion of the date object doesn&#39;t include time zone.
This will need to be rectified at some point.



Source code for **Dates.Mod**
-----------------------------

~~~
(* Dates -- this module was inspired by the A2&#39;s Dates module, adapted
   for Oberon-07 and a POSIX system. It provides an assortment of procedures
   for working with a simple datetime record.

Copyright (C) 2020 R. S. Doiel

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as
published by the Free Software Foundation, either version 3 of the
License, or (at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see &#60;https://www.gnu.org/licenses/&#62;.


@Author R. S. Doiel, &#60;rsdoiel@gmail.com&#62;
copyright (c) 2020, all rights reserved.
This software is released under the GNU AGPL
See http://www.gnu.org/licenses/agpl-3.0.html
*)
MODULE Dates;
IMPORT Chars, Strings, Clock, Convert := extConvert; (*, Out; **)

CONST
    MAXSTR = Chars.MAXSTR;
    SHORTSTR = Chars.SHORTSTR;

    YYYYMMDD* = 1; (* YYYY-MM-DD format *)
    MMDDYYYY* = 2; (* MM/DD/YYYY format *)
    YYYYMMDDHHMMSS* = 3; (* YYYY-MM-DD HH:MM:SS format *)

TYPE
    DateTime* = RECORD
        year*, month*, day*, hour*, minute*, second* : INTEGER
    END;

VAR
    (* Month names, January = 0, December = 11 *)
    Months*: ARRAY 23 OF ARRAY 10 OF CHAR; 
    (* Days of week, Monday = 0, Sunday = 6 *)
    Days*: ARRAY 7 OF ARRAY 10 OF CHAR;
    DaysInMonth: ARRAY 12 OF INTEGER;


(* Set -- initialize a date record year, month and day values *)
PROCEDURE Set*(year, month, day, hour, minute, second : INTEGER; VAR dt: DateTime);
BEGIN
    dt.year := year;
    dt.month := month;
    dt.day := day;
    dt.hour := hour;
    dt.minute := minute;
    dt.second := second;
END Set;

(* SetDate -- set a Date record&#39;s year, month and day attributes *)
PROCEDURE SetDate*(year, month, day : INTEGER; VAR dt: DateTime);
BEGIN
    dt.year := year;
    dt.month := month;
    dt.day := day;
END SetDate;

(* SetTime -- set a Date record&#39;s hour, minute, second attributes *)
PROCEDURE SetTime*(hour, minute, second : INTEGER; VAR dt: DateTime);
BEGIN
    dt.hour := hour;
    dt.minute := minute;
    dt.second := second;
END SetTime;

(* Copy -- copy the values from one date record to another *)
PROCEDURE Copy*(src : DateTime; VAR dest : DateTime);
BEGIN
    dest.year := src.year;
    dest.month := src.month;
    dest.day := src.day;
    dest.hour := src.hour;
    dest.minute := src.minute;
    dest.second := src.second;
END Copy;

(* ToChars -- converts a date record into an array of chars using
the format constant. Formats supported are YYYY-MM-DD HH:MM:SS
or MM/DD/YYYY HH:MM:SS. *)
PROCEDURE ToChars*(dt: DateTime; fmt : INTEGER; VAR src : ARRAY OF CHAR);
VAR ok : BOOLEAN;
BEGIN
    Chars.Clear(src);
    IF fmt = YYYYMMDD THEN
        Chars.AppendInt(dt.year, 4, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.month, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.day, 2, &#34;0&#34;, src);
    ELSIF fmt = MMDDYYYY THEN
        Chars.AppendInt(dt.month, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;/&#34;, src);
        Chars.AppendInt(dt.day, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;/&#34;, src);
        Chars.AppendInt(dt.year, 4, &#34;0&#34;, src);
    ELSIF fmt = YYYYMMDDHHMMSS THEN
        Chars.AppendInt(dt.year, 4, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.month, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;-&#34;, src);
        Chars.AppendInt(dt.day, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34; &#34;, src);
        Chars.AppendInt(dt.hour, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;:&#34;, src);
        Chars.AppendInt(dt.minute, 2, &#34;0&#34;, src);
        ok := Chars.AppendChar(&#34;:&#34;, src);
        Chars.AppendInt(dt.second, 2, &#34;0&#34;, src);
    END;
END ToChars;

(* 
 * Date and Time functions very much inspired by A2 but
 * adapted for use in Oberon-07 and OBNC compiler.
 *)

(* LeapYear -- returns TRUE if &#39;year&#39; is a leap year *)
PROCEDURE LeapYear*(year: INTEGER): BOOLEAN;
BEGIN
	RETURN (year &#62; 0) &#38; (year MOD 4 = 0) &#38; (~(year MOD 100 = 0) OR (year MOD 400 = 0))
END LeapYear;

(* NumOfDays -- number of days, returns the number of days in that month *)
PROCEDURE NumOfDays*(year, month: INTEGER): INTEGER;
VAR result : INTEGER;
BEGIN
    result := 0;
	DEC(month);
	IF ((month &#62;= 0) &#38; (month &#60; 12)) THEN
	    IF (month = 1) &#38; LeapYear(year) THEN 
            result := DaysInMonth[1]+1;
	    ELSE 
            result := DaysInMonth[month];
	    END;
    END;
    RETURN result
END NumOfDays;

(* IsValid -- checks if the attributes set in a DateTime record are valid *)
PROCEDURE IsValid*(dt: DateTime): BOOLEAN;
BEGIN
	RETURN ((dt.year &#62; 0) &#38; (dt.month &#62; 0) &#38; (dt.month &#60;= 12) &#38; (dt.day &#62; 0) &#38; (dt.day &#60;= NumOfDays(dt.year, dt.month)) &#38; (dt.hour &#62;= 0) &#38; (dt.hour &#60; 24) &#38; (dt.minute &#62;= 0) &#38; (dt.minute &#60; 60) &#38; (dt.second &#62;= 0) &#38; (dt.second &#60; 60))
END IsValid;

(* IsValidDate -- checks to see if a datetime record has valid day, month and year
attributes *)
PROCEDURE IsValidDate*(dt: DateTime) : BOOLEAN;
BEGIN
	RETURN (dt.year &#62; 0) &#38; (dt.month &#62; 0) &#38; (dt.month &#60;= 12) &#38; (dt.day &#62; 0) &#38; (dt.day &#60;= NumOfDays(dt.year, dt.month))
END IsValidDate;

(* IsValidTime -- checks if the hour, minute, second attributes set in a DateTime record are valid *)
PROCEDURE IsValidTime*(dt: DateTime): BOOLEAN;
BEGIN
	RETURN (dt.hour &#62;= 0) &#38; (dt.hour &#60; 24) &#38; (dt.minute &#62;= 0) &#38; (dt.minute &#60; 60) &#38; (dt.second &#62;= 0) &#38; (dt.second &#60; 60)
END IsValidTime;


(* OberonToDateTime -- convert an Oberon date/time to a DateTime 
structure *)
PROCEDURE OberonToDateTime*(Date, Time: INTEGER; VAR dt : DateTime);
BEGIN
	dt.second := Time MOD 64; Time := Time DIV 64;
	dt.minute := Time MOD 64; Time := Time DIV 64;
	dt.hour := Time MOD 24;
	dt.day := Date MOD 32; Date := Date DIV 32;
	dt.month := (Date MOD 16) + 1; Date := Date DIV 16;
	dt.year := Date;
END OberonToDateTime;

(* DateTimeToOberon -- convert a DateTime structure to an Oberon 
date/time *)
PROCEDURE DateTimeToOberon*(dt: DateTime; VAR date, time: INTEGER);
BEGIN
	IF IsValid(dt) THEN
	date := (dt.year)*512 + dt.month*32 + dt.day;
	time := dt.hour*4096 + dt.minute*64 + dt.second
    ELSE
        date := 0;
        time := 0;
    END;
END DateTimeToOberon;

(* Now -- returns the current date and time as a DateTime record. *)
PROCEDURE Now*(VAR dt: DateTime);
VAR d, t: INTEGER;
BEGIN
	Clock.Get(t, d);
	OberonToDateTime(d, t, dt);
END Now;


(* WeekDate -- returns the ISO 8601 year number, week number &#38;
week day (Monday=1, ....Sunday=7) 
Algorithm is by Rick McCarty, http://personal.ecu.edu/mccartyr/ISOwdALG.txt
*)
PROCEDURE WeekDate*(dt: DateTime; VAR year, week, weekday: INTEGER);
VAR doy, i, yy, c, g, jan1: INTEGER; leap: BOOLEAN;
BEGIN
	IF IsValid(dt) THEN
		leap := LeapYear(dt.year);
		doy := dt.day; i := 0;
		WHILE (i &#60; (dt.month - 1)) DO 
            doy := doy + DaysInMonth[i];
            INC(i);
        END;
		IF leap &#38; (dt.month &#62; 2) THEN 
            INC(doy);
        END;
		yy := (dt.year - 1) MOD 100; 
        c := (dt.year - 1) - yy; 
        g := (yy + yy) DIV 4;
		jan1 := 1 + (((((c DIV 100) MOD 4) * 5) + g) MOD 7);

		weekday := 1 + (((doy + (jan1 - 1)) - 1) MOD 7);

		IF (doy &#60;= (8 - jan1)) &#38; (jan1 &#62; 4) THEN (* falls in year-1 ? *)
			year := dt.year - 1;
			IF (jan1 = 5) OR ((jan1 = 6) &#38; LeapYear(year)) THEN 
                week := 53;
			ELSE 
                week := 52;
			END;
		ELSE
			IF leap THEN 
                i := 366;
            ELSE 
                i := 365;
            END;
			IF ((i - doy) &#60; (4 - weekday)) THEN
				year := dt.year + 1;
				week := 1;
			ELSE
				year := dt.year;
				i := doy + (7-weekday) + (jan1-1);
				week := i DIV 7;
				IF (jan1 &#62; 4) THEN 
                    DEC(week);
                END;
			END;
		END;
	ELSE
		year := -1; week := -1; weekday := -1;
	END;
END WeekDate;

(* Equal -- compare to date records to see if they are equal values *)
PROCEDURE Equal*(t1, t2: DateTime) : BOOLEAN;
BEGIN
	RETURN ((t1.second = t2.second) &#38; (t1.minute = t2.minute) &#38; (t1.hour = t2.hour) &#38; (t1.day = t2.day) &#38; (t1.month = t2.month) &#38; (t1.year = t2.year))
END Equal;

(* compare -- used in Compare only for comparing specific values,
    returning an appropriate -1, 0, 1 *)
PROCEDURE compare(t1, t2 : INTEGER) : INTEGER;
VAR result : INTEGER;
BEGIN
	IF (t1 &#60; t2) THEN 
        result := -1;
	ELSIF (t1 &#62; t2) THEN 
        result := 1;
	ELSE 
        result := 0;
	END;
	RETURN result
END compare;

(* Compare -- returns -1 if (t1 &#60; t2), 0 if (t1 = t2) or 1 if (t1 &#62;  t2) *)
PROCEDURE Compare*(t1, t2: DateTime) : INTEGER;
VAR result : INTEGER;
BEGIN
	result := compare(t1.year, t2.year);
	IF (result = 0) THEN
		result := compare(t1.month, t2.month);
		IF (result = 0) THEN
			result := compare(t1.day, t2.day);
			IF (result = 0) THEN
				result := compare(t1.hour, t2.hour);
				IF (result = 0) THEN
					result := compare(t1.minute, t2.minute);
					IF (result = 0) THEN
						result := compare(t1.second, t2.second);
					END;
				END;
			END;
		END;
	END;
	RETURN result
END Compare;

(* CompareDate -- compare day, month and year values only *)
PROCEDURE CompareDate*(t1, t2: DateTime) : INTEGER;
VAR result : INTEGER;
BEGIN
	result := compare(t1.year, t2.year);
	IF (result = 0) THEN
		result := compare(t1.month, t2.month);
		IF (result = 0) THEN
			result := compare(t1.day, t2.day);
		END;
	END;
	RETURN result
END CompareDate;

(* CompareTime -- compare second, minute and hour values only *)
PROCEDURE CompareTime*(t1, t2: DateTime) : INTEGER;
VAR result : INTEGER;
BEGIN
	result := compare(t1.hour, t2.hour);
	IF (result = 0) THEN
		result := compare(t1.minute, t2.minute);
		IF (result = 0) THEN
			result := compare(t1.second, t2.second);
		END;
	END;
	RETURN result
END CompareTime;



(* TimeDifferences -- returns the absolute time difference between 
t1 and t2.

Note that leap seconds are not counted, 
see http://www.eecis.udel.edu/~mills/leap.html *)
PROCEDURE TimeDifference*(t1, t2: DateTime; VAR days, hours, minutes, seconds : INTEGER);
CONST SecondsPerMinute = 60; SecondsPerHour = 3600; SecondsPerDay = 86400;
VAR start, end: DateTime; year, month, second : INTEGER;
BEGIN
	IF (Compare(t1, t2) = -1) THEN 
        start := t1; 
        end := t2; 
    ELSE 
        start := t2; 
        end := t1; 
    END;
	IF (start.year = end.year) &#38; (start.month = end.month) &#38; (start.day = end.day) THEN
		second := end.second - start.second + ((end.minute - start.minute) * SecondsPerMinute) + ((end.hour - start.hour) * SecondsPerHour);
		days := 0;
        hours := 0;
        minutes := 0;
	ELSE
		(* use start date/time as reference point *)
		(* seconds until end of the start.day *)
		second := (SecondsPerDay - start.second) - (start.minute * SecondsPerMinute) - (start.hour * SecondsPerHour);
		IF (start.year = end.year) &#38; (start.month = end.month) THEN
			(* days between start.day and end.day *)
			days := (end.day - start.day) - 1;
		ELSE
			(* days until start.month ends excluding start.day *)
			days := NumOfDays(start.year, start.month) - start.day;
			IF (start.year = end.year) THEN
				(* months between start.month and end.month *)
				FOR month := start.month + 1 TO end.month - 1 DO
					days := days + NumOfDays(start.year, month);
				END;
			ELSE
				(* days until start.year ends (excluding start.month) *)
				FOR month := start.month + 1 TO 12 DO
					days := days + NumOfDays(start.year, month);
				END;
				FOR year := start.year + 1 TO end.year - 1 DO (* days between start.years and end.year *)
					IF LeapYear(year) THEN days := days + 366; ELSE days := days + 365; END;
				END;
				FOR month := 1 TO end.month - 1 DO (* days until we reach end.month in end.year *)
					days := days + NumOfDays(end.year, month);
				END;
			END;
			(* days in end.month until reaching end.day excluding end.day *)
			days := (days + end.day) - 1;
		END;
		(* seconds in end.day *)
		second := second + end.second + (end.minute * SecondsPerMinute) + (end.hour * SecondsPerHour);
	END;
	days := days + (second DIV SecondsPerDay); second := (second MOD SecondsPerDay);
	hours := (second DIV SecondsPerHour); second := (second MOD SecondsPerHour);
	minutes := (second DIV SecondsPerMinute); second := (second MOD SecondsPerMinute);
	seconds := second;
END TimeDifference;

(* AddYear -- Add/Subtract a number of years to/from date *)
PROCEDURE AddYears*(VAR dt: DateTime; years : INTEGER);
BEGIN
	ASSERT(IsValid(dt));
	dt.year := dt.year + years;
	ASSERT(IsValid(dt));
END AddYears;

(* AddMonths -- Add/Subtract a number of months to/from date.
This will adjust date.year if necessary *)
PROCEDURE AddMonths*(VAR dt: DateTime; months : INTEGER);
VAR years : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	years := months DIV 12;
	dt.month := dt.month + (months MOD 12);
	IF (dt.month &#62; 12) THEN
		dt.month := dt.month - 12;
		INC(years);
	ELSIF (dt.month &#60; 1) THEN
		dt.month := dt.month + 12;
		DEC(years);
	END;
	IF (years # 0) THEN AddYears(dt, years); END;
	ASSERT(IsValid(dt));
END AddMonths;

(* AddDays --  Add/Subtract a number of days to/from date.
This will adjust date.month and date.year if necessary *)
PROCEDURE AddDays*(VAR dt: DateTime; days : INTEGER);
VAR nofDaysLeft : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	IF (days &#62; 0) THEN
		WHILE (days &#62; 0) DO
			nofDaysLeft := NumOfDays(dt.year, dt.month) - dt.day;
			IF (days &#62; nofDaysLeft) THEN
				dt.day := 1;
				AddMonths(dt, 1);
				days := days - nofDaysLeft - 1; (* -1 because we consume the first day of the next month *)
			ELSE
				dt.day := dt.day + days;
				days := 0;
			END;
		END;
	ELSIF (days &#60; 0) THEN
		days := -days;
		WHILE (days &#62; 0) DO
			nofDaysLeft := dt.day - 1;
			IF (days &#62; nofDaysLeft) THEN
				dt.day := 1; (* otherwise, dt could become an invalid date if the previous month has less days than dt.day *)
				AddMonths(dt, -1);
				dt.day := NumOfDays(dt.year, dt.month);
				days := days - nofDaysLeft - 1; (* -1 because we consume the last day of the previous month *)
			ELSE
				dt.day := dt.day - days;
				days := 0;
			END;
		END;
	END;
	ASSERT(IsValid(dt));
END AddDays;

(* AddHours -- Add/Subtract a number of hours to/from date.
This will adjust date.day, date.month and date.year if necessary *)
PROCEDURE AddHours*(VAR dt: DateTime; hours : INTEGER);
VAR days : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	dt.hour := dt.hour + hours;
	days := dt.hour DIV 24;
	dt.hour := dt.hour MOD 24;
	IF (dt.hour &#60; 0) THEN
		dt.hour := dt.hour + 24;
		DEC(days);
	END;
	IF (days # 0) THEN AddDays(dt, days); END;
	ASSERT(IsValid(dt));
END AddHours;

(* AddMinutes -- Add/Subtract a number of minutes to/from date.
This will adjust date.hour, date.day, date.month and date.year 
if necessary *)
PROCEDURE AddMinutes*(VAR dt: DateTime; minutes : INTEGER);
VAR hours : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	dt.minute := dt.minute + minutes;
	hours := dt.minute DIV 60;
	dt.minute := dt.minute MOD 60;
	IF (dt.minute &#60; 0) THEN
		dt.minute := dt.minute + 60;
		DEC(hours);
	END;
	IF (hours # 0) THEN AddHours(dt, hours); END;
	ASSERT(IsValid(dt));
END AddMinutes;

(* AddSeconds -- Add/Subtract a number of seconds to/from date.
This will adjust date.minute, date.hour, date.day, date.month and
date.year if necessary *)
PROCEDURE AddSeconds*(VAR dt: DateTime; seconds : INTEGER);
VAR minutes : INTEGER;
BEGIN
	ASSERT(IsValid(dt));
	dt.second := dt.second + seconds;
	minutes := dt.second DIV 60;
	dt.second := dt.second MOD 60;
	IF (dt.second &#60; 0) THEN
		dt.second := dt.second + 60;
		DEC(minutes);
	END;
	IF (minutes # 0) THEN AddMinutes(dt, minutes); END;
	ASSERT(IsValid(dt));
END AddSeconds;


(* IsDateString -- return TRUE if the ARRAY OF CHAR is 10 characters
long and is either in the form of YYYY-MM-DD or MM/DD/YYYY where
Y, M and D are digits. 
NOTE: is DOES NOT check the ranges of the digits. *)
PROCEDURE IsDateString*(inline : ARRAY OF CHAR) : BOOLEAN;
VAR 
    test : BOOLEAN; i, pos : INTEGER;
    src : ARRAY MAXSTR OF CHAR;
BEGIN
    Chars.Set(inline, src);
    Chars.TrimSpace(src);
    test := FALSE;
    IF Strings.Length(src) = 10 THEN
        pos := Strings.Pos(&#34;-&#34;, src, 0);
        IF pos &#62; 0 THEN
            IF (src[4] = &#34;-&#34;) &#38; (src[7] = &#34;-&#34;) THEN
                test := TRUE;
                FOR i := 0 TO 9 DO
                    IF (i # 4) &#38; (i # 7) THEN
                       IF Chars.IsDigit(src[i]) = FALSE THEN
                           test := FALSE;
                       END;
                    END;
                END;
            ELSE
                test := FALSE;
            END;
        END;
        pos := Strings.Pos(&#34;/&#34;, src, 0);
        IF pos &#62; 0 THEN
            IF (src[2] = &#34;/&#34;) &#38; (src[5] = &#34;/&#34;) THEN
                test := TRUE;
                FOR i := 0 TO 9 DO
                    IF (i # 2) &#38; (i # 5) THEN
                        IF Chars.IsDigit(src[i]) = FALSE THEN
                            test := FALSE;
                        END;
                    END;
                END;
            ELSE
                test := FALSE;
            END;
        END;
    END;
    RETURN test
END IsDateString;

(* IsTimeString -- return TRUE if the ARRAY OF CHAR has 4 to 8 
characters in the form of H:MM, HH:MM, HH:MM:SS where H, M and S
are digits. *)
PROCEDURE IsTimeString*(inline : ARRAY OF CHAR) : BOOLEAN;
VAR 
    test : BOOLEAN; 
    l : INTEGER;
    src : ARRAY MAXSTR OF CHAR;
BEGIN
    Chars.Set(inline, src);
    Chars.TrimSpace(src);
    (* remove any trailing am/pm suffixes *)
    IF Chars.EndsWith(&#34;m&#34;, src) THEN
        IF Chars.EndsWith(&#34;am&#34;, src) THEN
            Chars.TrimSuffix(&#34;am&#34;, src);
        ELSE
            Chars.TrimSuffix(&#34;pm&#34;, src);
        END;
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;M&#34;, src) THEN
        Chars.TrimSuffix(&#34;AM&#34;, src);
        Chars.TrimSuffix(&#34;PM&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;p&#34;, src) THEN
        Chars.TrimSuffix(&#34;p&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;P&#34;, src) THEN
        Chars.TrimSuffix(&#34;P&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;a&#34;, src) THEN
        Chars.TrimSuffix(&#34;a&#34;, src);
        Chars.TrimSpace(src);
    ELSIF Chars.EndsWith(&#34;A&#34;, src) THEN
        Chars.TrimSuffix(&#34;A&#34;, src);
        Chars.TrimSpace(src);
    END;
    Strings.Extract(src, 0, 8, src);
    test := FALSE;
    l := Strings.Length(src);
    IF (l = 4) THEN
        IF Chars.IsDigit(src[0]) &#38; (src[1] = &#34;:&#34;) &#38; 
            Chars.IsDigit(src[2]) &#38; Chars.IsDigit(src[3]) THEN
            test := TRUE;
        ELSE
            test := FALSE;
        END;
    ELSIF (l = 5) THEN
        IF Chars.IsDigit(src[0]) &#38; Chars.IsDigit(src[1]) &#38;
            (src[2] = &#34;:&#34;) &#38; 
            Chars.IsDigit(src[3]) &#38; Chars.IsDigit(src[4]) THEN
            test := TRUE;
        ELSE
            test := FALSE;
        END;
    ELSIF (l = 8) THEN
        IF Chars.IsDigit(src[0]) &#38; Chars.IsDigit(src[1]) &#38;
            (src[2] = &#34;:&#34;) &#38; 
            Chars.IsDigit(src[3]) &#38; Chars.IsDigit(src[4]) &#38; 
            (src[5] = &#34;:&#34;) &#38; 
            Chars.IsDigit(src[6]) &#38; Chars.IsDigit(src[7]) THEN
            test := TRUE;
        ELSE
            test := FALSE;
        END;
    ELSE
        test := FALSE;
    END;
    RETURN test
END IsTimeString;

(* ParseDate -- parses a date string in YYYY-MM-DD or
MM/DD/YYYY format. *)
PROCEDURE ParseDate*(inline : ARRAY OF CHAR; VAR year, month, day : INTEGER) : BOOLEAN;
VAR src, tmp : ARRAY MAXSTR OF CHAR; ok, b : BOOLEAN;
BEGIN
    Chars.Set(inline, src);
    Chars.Clear(tmp);
    ok := FALSE;
	IF IsDateString(src) THEN
        (* FIXME: Need to allow for more than 4 digit years! *)
        IF (src[2] = &#34;/&#34;) &#38; (src[5] = &#34;/&#34;) THEN
            ok := TRUE;
            Strings.Extract(src, 0, 2, tmp);
            Convert.StringToInt(tmp, month, b);
            ok := ok &#38; b;
            Strings.Extract(src, 4, 2, tmp);
            Convert.StringToInt(tmp, day, b);
            ok := ok &#38; b;
            Strings.Extract(src, 6, 4, tmp);
            Convert.StringToInt(tmp, year, b);
            ok := ok &#38; b;
        ELSIF (src[4] = &#34;-&#34;) &#38; (src[7] = &#34;-&#34;) THEN
            ok := TRUE;
            Strings.Extract(src, 0, 4, tmp);
            Convert.StringToInt(tmp, year, b);
            ok := ok &#38; b;
            Strings.Extract(src, 5, 2, tmp);
            Convert.StringToInt(tmp, month, b);
            ok := ok &#38; b;
            Strings.Extract(src, 8, 2, tmp);
            Convert.StringToInt(tmp, day, b);
            ok := ok &#38; b;
        ELSE
            ok := FALSE;
        END;
    END;
    RETURN ok
END ParseDate;

(* ParseTime -- procedure for parsing time strings into hour,
minute, second. Returns TRUE on successful parse, FALSE otherwise *)
PROCEDURE ParseTime*(inline : ARRAY OF CHAR; VAR hour, minute, second : INTEGER) : BOOLEAN;
VAR src, tmp : ARRAY MAXSTR OF CHAR; ok : BOOLEAN; cur, pos, l : INTEGER;
BEGIN
    Chars.Set(inline, src);
    Chars.Clear(tmp);
	IF IsTimeString(src) THEN
        ok := TRUE;
        cur := 0; pos := 0;
        pos := Strings.Pos(&#34;:&#34;, src, cur);
        IF pos &#62; 0 THEN
        (* Get Hour *)
            Strings.Extract(src, cur, pos - cur, tmp);
            Convert.StringToInt(tmp, hour, ok);
            IF ok THEN
                (* Get Minute *)
                cur := pos + 1;
                Strings.Extract(src, cur, 2, tmp);
                Convert.StringToInt(tmp, minute, ok);
                IF ok THEN
                    (* Get second, optional, default to zero *)
                    pos := Strings.Pos(&#34;:&#34;, src, cur);
                    IF pos &#62; 0 THEN
                        cur := pos + 1;
                        Strings.Extract(src, cur, 2, tmp);
                        Convert.StringToInt(tmp, second, ok);
                        cur := cur + 2;
                    ELSE
                        second := 0;
                    END;
                    (* Get AM/PM, optional, adjust hour if PM *)
                    l := Strings.Length(src);
                    WHILE (cur &#60; l) &#38; Chars.IsSpace(src[cur]) DO
                        cur := cur + 1;
                    END;
                    Strings.Extract(src, cur, 2, tmp);
                    Chars.TrimSpace(tmp);
                    IF Chars.Equal(tmp, &#34;PM&#34;) OR Chars.Equal(tmp, &#34;pm&#34;) THEN
                        hour := hour + 12;
                    END;
                ELSE
                    ok := FALSE;
                END;
            END;
        ELSE
            ok := FALSE;
        END;
    ELSE
        ok := FALSE;
    END;
    IF ok THEN
        ok := ((hour &#62;= 0) &#38; (hour &#60;= 23)) &#38;
            ((minute &#62;= 0) &#38; (minute &#60;= 59)) &#38;
                ((second &#62;= 0) &#38; (second &#60;= 59));
    END;
    RETURN ok
END ParseTime;


(* Parse accepts a date array of chars in either dates, times
or dates and times separate by spaces. Date formats supported
include YYYY-MM-DD, MM/DD/YYYY. Time formats include
H:MM, HH:MM, H:MM:SS, HH:MM:SS with &#39;a&#39;, &#39;am&#39;, &#39;p&#39;, &#39;pm&#39; 
suffixes.  Dates and times can also be accepted as JSON 
expressions with the individual time compontents are specified 
as attributes, e.g. `{&#34;year&#34;: 1998, &#34;month&#34;: 12, &#34;day&#34;: 10,
&#34;hour&#34;: 11, &#34;minute&#34;: 4, &#34;second&#34;: 3}.
Parse returns TRUE on successful parse, FALSE otherwise.

BUG: Assumes a 4 digit year.
*) 
PROCEDURE Parse*(inline : ARRAY OF CHAR; VAR dt: DateTime) : BOOLEAN;
VAR src, ds, ts, tmp : ARRAY SHORTSTR OF CHAR; ok, okDate, okTime : BOOLEAN; 
    pos, year, month, day, hour, minute, second : INTEGER;
BEGIN
    dt.year := 0;
    dt.month := 0;
    dt.day := 0;
    dt.hour := 0;
    dt.minute := 0;
    dt.second := 0;
    Chars.Clear(tmp);
    Chars.Set(inline, src);
    Chars.TrimSpace(src);
    (* Split into Date and Time components *)
    pos := Strings.Pos(&#34; &#34;, src, 0);
    IF pos &#62;= 0 THEN
        Strings.Extract(src, 0, pos, ds);
        pos := pos + 1;
        Strings.Extract(src, pos, Strings.Length(src) - pos, ts);
    ELSE
        Chars.Set(src, ds);
        Chars.Set(src, ts);
    END;
    ok := FALSE;
    IF IsDateString(ds) THEN
        ok := TRUE;
        okDate := ParseDate(ds, year, month, day);
        SetDate(year, month, day, dt);
        ok := ok &#38; okDate;
    END;
    IF IsTimeString(ts) THEN
        ok := ok OR okDate;
        okTime := ParseTime(ts, hour, minute, second);
        SetTime(hour, minute, second, dt);
        ok := ok &#38; okTime;
    END;
    RETURN ok
END Parse;

BEGIN
    Chars.Set(&#34;January&#34;, Months[0]);
    Chars.Set(&#34;February&#34;, Months[1]);
    Chars.Set(&#34;March&#34;, Months[2]);
    Chars.Set(&#34;April&#34;, Months[3]);
    Chars.Set(&#34;May&#34;, Months[4]);
    Chars.Set(&#34;June&#34;, Months[5]);
    Chars.Set(&#34;July&#34;, Months[6]);
    Chars.Set(&#34;August&#34;, Months[7]);
    Chars.Set(&#34;September&#34;, Months[8]);
    Chars.Set(&#34;October&#34;, Months[9]);
    Chars.Set(&#34;November&#34;, Months[10]);
    Chars.Set(&#34;December&#34;, Months[11]);

    Chars.Set(&#34;Sunday&#34;, Days[0]);
    Chars.Set(&#34;Monday&#34;, Days[1]);
    Chars.Set(&#34;Tuesday&#34;, Days[2]);
    Chars.Set(&#34;Wednesday&#34;, Days[3]);
    Chars.Set(&#34;Thursday&#34;, Days[4]);
    Chars.Set(&#34;Friday&#34;, Days[5]);
    Chars.Set(&#34;Saturday&#34;, Days[6]);

    DaysInMonth[0] := 31; (* January *)
    DaysInMonth[1] := 28; (* February *)
    DaysInMonth[2] := 31; (* March *)
    DaysInMonth[3] := 30; (* April *)
    DaysInMonth[4] := 31; (* May *)
    DaysInMonth[5] := 30; (* June *)
    DaysInMonth[6] := 31; (* July *)
    DaysInMonth[7] := 31; (* August *)
    DaysInMonth[8] := 30; (* September *)
    DaysInMonth[9] := 31; (* October *)
    DaysInMonth[10] := 30; (* November *)
    DaysInMonth[11] := 31; (* December *)

END Dates.

~~~</source:markdown>
      <enclosure url="https://rsdoiel.github.io/blog/2020/11/27/Dates.md" length="30668" type="text/markdown" />
    </item>    <item>
      <title>Pandoc &#38; Metadata</title>
      <link>https://rsdoiel.github.io/blog/2020/11/11/Pandoc-Metadata.html</link>
      <description>
        <![CDATA[Pandoc supports three ways of providing metadata to its template
        engine. 
        
        1. Front matter
        2. Command line optional metadata
        3. A JSON metadata file.
        
        Front matter is a community term that comes from physical world
        of paper books and articles.  It is the information that comes 
        before the primary content.  This information might be things 
        like title, author, publisher and publication date. These days 
        it'll also include things like identifiers like ISSN, ISBN possibly 
        newer identifiers like DOI or ORCID. In the library and programming
        community we refer to this type of structured information as
        metadata.  Data about the publication or article.
        
        ...]]>
      </description>
      <source:markdown>Pandoc &#38; Metadata 
=================

Pandoc supports three ways of providing metadata to its template
engine. 

1. Front matter
2. Command line optional metadata
3. A JSON metadata file.

Front matter
------------

Front matter is a community term that comes from physical world
of paper books and articles.  It is the information that comes 
before the primary content.  This information might be things 
like title, author, publisher and publication date. These days 
it&#39;ll also include things like identifiers like ISSN, ISBN possibly 
newer identifiers like DOI or ORCID. In the library and programming
community we refer to this type of structured information as
metadata.  Data about the publication or article.

Many publication systems like TeX/LaTeX support provided means of 
incorporating metadata into the document.  When simple markup formats 
like Markdown, Textile and Asciidoc became popular the practice was 
continued by including the metadata in some sort of structured encoding
at the beginning of the document. The community adopted the term from
the print world, &#34;front matter&#34;. 

Pandoc provides for several ways of working with metadata and supports
one format of front matter encoding called [YAML](https://yaml.org/). 
Other markup processors support other encoding of front matter. Two
popular alternatives of encoding are [TOML](https://toml.io/en/) and 
[JSON](https://json.org).  If you use one of the alternative encoding
for your front matter then you&#39;ll need to split the front matter
out of your document before processing with Pandoc[^1].  

[^1]: The [MkPage Project](https://caltechlibrary.github.io/mkpage/) provides a tool called [frontmatter](https://caltechlibrary.github.io/mkpage/docs/frontmatter/) that can be easy or your can easily roll your own in Python or other favorite language.


If you provide YAML formatted front matter Pandoc will pass this
metadata on and make it available to it&#39;s template engine and the
templates you create to render content with Pandoc. See the Pandoc
User Guide section [YAML metadata blocks](https://pandoc.org/MANUAL.html#extension-yaml_metadata_block) for more details. If you&#39;ve used another
encoding of front matter then the metadata file approach is probably
the ticket.

Metadata passed by command line
-------------------------------

If you only have a metadata elements you would like to
make available to the template (e.g. title, pub date) you
can easily add them using the `--metadata` command line option.
This is documented in the Pandoc User Guide under the heading
[Reader Options](https://pandoc.org/MANUAL.html). Here&#39;s a simple
example where we have a title, &#34;U. S. Constitution&#34; and a
publication date of &#34;September 28, 1787&#34;.

~~~{.shell}
    pandoc --metadata \
        title=&#34;U. S. Constitution&#34; \
        pubdate=&#34;September 28, 1787&#34; \
        --from markdown --to html --template doc1.tmpl \
        constitution.txt
~~~

The template now has two additional values available as metadata
in addition to `body`, namely `pubdate` and `title`. Here&#39;s an
example template [doc1.tmpl](doc1.tmpl).

~~~

   &#60;!DOCTYPE html&#62;
   &#60;html&#62;
   &#60;head&#62;
       &#60;title&#62;${title}&#60;/title&#62;
   &#60;/head&#62;
   &#60;body&#62;
      &#60;h1&#62;${title}&#60;/h1&#62;
      &#60;h2&#62;${pubdate}&#60;/h2&#62;
      &#60;p&#62;
      ${body}
      &#60;p&#62;
   &#60;/body&#62;
   &#60;/html&#62;

~~~

More complex metadata is better suited to creating a JSON document
with the structure you need to render your template.


Metadata file
-------------

Metadata files can be included with the option `--metadata-file`. This
like the `--metadata` option are discussed in the Pandoc User Guide under
the [Read Options(https://pandoc.org/MANUAL.html) heading.  The JSON 
document should contain an Object where each attribute corresponds to
the variable you wish to referenced in template.  Pandoc&#39;s template
engine support both single values but also objects and arrays. In this
way you can structure the elements you wish to include even elements
which are iterative (e.g. a list of links or topics). Below is a
JSON data structure that includes the page title as well as links
for the navigation.  The nav attribute holds a list of objects 
with attributes of href and label containing data that will be used
to render a list of anchor elements in the template.


~~~{.json}

    {
        &#34;title&#34;: &#34;U. S. Constitution&#34;,
        &#34;pubdate&#34;: &#34;September 28, 1787&#34;,
        &#34;nav&#34;: [
            {&#34;label&#34;: &#34;Pandoc Metadata&#34;, &#34;href&#34;: &#34;Pandoc-Metadata.html&#34; },
            {&#34;label&#34;: &#34;Magnacarta&#34;, &#34;href&#34;: &#34;magnacarta.html&#34; },
            {&#34;label&#34;: &#34;Declaration of Independence&#34;, &#34;href&#34;: &#34;independence.html&#34; },
            {&#34;label&#34;: &#34;U. S. Constitution&#34;, &#34;href&#34;: &#34;constitution.html&#34;}
        ]
    }

~~~

Here&#39;s a revised template to include the navigation,
see [doc2.tmpl](doc2.tmpl).

~~~

   &#60;!DOCTYPE html&#62;
   &#60;html&#62;
   &#60;head&#62;
       &#60;title&#62;${title}&#60;/title&#62;
   &#60;/head&#62;
   &#60;body&#62;
      &#60;nav&#62;
      ${for(nav)}&#60;a href=&#34;${nav.href}&#34;&#62;${nav.label}&#60;/a&#62;${sep}, ${endfor}
      &#60;/nav&#62;
      &#60;h1&#62;${title}&#60;/h1&#62;
      ${if(pubdate)}&#60;h2&#62;${pubdate}&#60;/h2&#62;${endif}
      &#60;p&#62;
      ${body}
      &#60;p&#62;
   &#60;/body&#62;
   &#60;/html&#62;

~~~


Combining Techniques
--------------------

It is worth noting that these approaches can be mixed and matched.
In the following example I use the same [metadata.json](metadata.json)
file which has title and pubdate attributes but override them
using the command line `--metadata` option. In this way I can use that 
file along with [doc2.tmpl](doc2.tmpl) and render each 
To render the constitution page from a Markdown version of the 
U. S. Constitution you could use the following Pandoc command:

~~~{.shell}

	pandoc --from markdown --to html --template doc2.tmpl \
        --metadata-file metadata.json \
        --metadata title=&#34;Magna Carta&#34; \
		--metadata pubdate=&#34;1215&#34; \
		-o magnacarta.html \
		magnacarta.txt

	pandoc --from markdown --to html --template doc2.tmpl \
        --metadata-file metadata.json \
        --metadata title=&#34;The Declaration of Indepenence&#34; \
		--metadata pubdate=&#34;July 4, 1776&#34; \
        -o independence.html \
        independence.txt

	pandoc --from markdown --to html --template doc2.tmpl \
        --metadata-file metadata.json \
        --metadata title=&#34;U. S. Constitution&#34; \
		--metadata pubdate=&#34;September 28, 1787&#34; \
        -o constitution.html \
        constitution.txt

~~~

See [Magna Carta](magnacarta.html), [The Declaration of Independence](independence.html), [U. S. Constitution](constitution.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/11/11/Pandoc-Metadata.md" length="7617" type="text/markdown" />
    </item>    <item>
      <title>Pandoc Partials</title>
      <link>https://rsdoiel.github.io/blog/2020/11/09/Pandoc-Partials.html</link>
      <description>
        <![CDATA[Most people know about [Pandoc](https://pandoc.org/) from its
        fantastic ability to convert various markup formats from one to
        another. A little less obvious is Pandoc can be a template engine
        for rendering static websites allowing you full control over the
        rendered content.
        
        The main Pandoc documentation of the template engine can be found
        in the [User Guide](https://pandoc.org/MANUAL.html#templates).
        The documentation is complete in terms of describing the template
        capabilities but lacks a tutorial for using as a replacement for more
        ambitious rendering systems like [Jekyll](https://jekyllrb.com/) or
        [Hugo](https://gohugo.io/). Pandoc takes a vary direct approach and
        can be deceptively simple to implement.]]>
      </description>
      <source:markdown>Pandoc Partial Templates
========================

Most people know about [Pandoc](https://pandoc.org/) from its
fantastic ability to convert various markup formats from one to
another. A little less obvious is Pandoc can be a template engine
for rendering static websites allowing you full control over the
rendered content.

The main Pandoc documentation of the template engine can be found
in the [User Guide](https://pandoc.org/MANUAL.html#templates).
The documentation is complete in terms of describing the template
capabilities but lacks a tutorial for using as a replacement for more
ambitious rendering systems like [Jekyll](https://jekyllrb.com/) or
[Hugo](https://gohugo.io/). Pandoc takes a vary direct approach and
can be deceptively simple to implement.

Use your own template
---------------------

First thing in this tutorial is to use our own template with Pandoc
when rendering a single webpage. You use the `-template` option to
provide your a template name. I think of this as the page level template.
This template, as I will show later, can then call other partial
templates as needed.

Example, render the [Pandoc-Partials.txt](Pandoc-Partials.txt) file using the
template named [index1.tmpl](index1.tmpl):

~~~{.shell}

    pandoc --from=markdown --to=html \
        --template=index1.tmpl Pandoc-Partials.txt &#62; index1.htm

~~~

This is a simple template page level template.

~~~{.html-code}

    &#60;!DOCTYPE html&#62;
    &#60;html&#62;
    &#60;head&#62;
    &#60;/head&#62;
    &#60;body&#62;
    ${body}
    &#60;/body&#62;
    &#60;/html&#62;

~~~

When we run our Pandoc command the file called
[Pandoc-Partials.txt](Pandoc-Partials.txt) is passed into the template as
the &#34;body&#34; element where it says `${body}`. See this Pandoc 
[User Guide](https://pandoc.org/MANUAL.html#templates) for the basics.

Example 1 rendered: [index1.htm](index1.htm)

Variables and metadata
----------------------

Pandoc&#39;s documentation is good at describing the
ways of referencing a variable or using the built-in
template functions. Where do the variables get their values?
The easiest way I&#39;ve found is to set the variables values in
a JSON metadata file.  While Pandoc can also use the metadata
described in YAML front matter Pandoc doesn&#39;t support some of the
other common front matter formats.  If you&#39;re using another format
like JSON or TOML for front matter there are tools which can split
the front matter from the rest of the markdown document. For
this example I have created the metadata as JSON in a file
called [metadata.json](metadata.json).

Example [metadata.json](metadata.json):

~~~{.json}

    {
        &#34;title&#34;: &#34;Pandoc Partial Examples&#34;,
        &#34;nav&#34;: [
            {&#34;label&#34;: &#34;Pandoc-Partials&#34;, &#34;href&#34;: &#34;Pandoc-Partials.html&#34; },
            {&#34;label&#34;: &#34;Version 1&#34;, &#34;href&#34;: &#34;index1.htm&#34; },
            {&#34;label&#34;: &#34;Version 2&#34;, &#34;href&#34;: &#34;index2.htm&#34; },
            {&#34;label&#34;: &#34;Version 3&#34;, &#34;href&#34;: &#34;index3.htm&#34; }
        ]
    }

~~~

Let&#39;s modify our initial template to include our simple navigation
and title.

Example [index2.tmpl](index2.tmpl):

~~~{.html-code}

    &#60;!DOCTYPE html&#62;
    &#60;html&#62;
    &#60;head&#62;
      ${if(title)}&#60;title&#62;${title}&#60;/title&#62;${endif}
    &#60;/head&#62;
    &#60;body&#62;
    &#60;nav&#62;
    ${for(nav)}&#60;a href=&#34;${it.href}&#34;&#62;${it.label}&#60;/a&#62;${sep}, ${endfor}
    &#60;/nav&#62;
    &#60;section&#62;
    ${body}
    &#60;/section&#62;
    &#60;/body&#62;
    &#60;/html&#62;

~~~

We would include our navigation metadata with a Pandoc command like

~~~{.shell}

    pandoc --from=markdown --to=html \
           --template=index2.tmpl \
           --metadata-file=metadata.json Pandoc-Partials.txt &#62; index2.htm

~~~

When we render this we now should be able to view a web page
with simple navigation driven by the JSON file as well as the
body content contained in the Pandoc-Partials.txt file.

Example 2 rendered: [index2.htm](index2.htm)

Partials
--------

Sometimes you have more complex documents. Putting this all in
one template can become tedious. Web designers use a term called
&#34;partials&#34;. This usually means a template for a &#34;part&#34; of a page.
In our initial example we can split our navigation into it&#39;s own
template.

Implementing partials
---------------------

Pandoc will look in the current directory for partials as well
as in a sub directory called &#34;templates&#34; of the current direct.
In this example I am going to include my partial template for
navigation in the current directory along side my
[index3.tmpl](index3.tmpl).  My navigation template is called
[nav.tmpl](nav.tmpl).

Here&#39;s my partial template:

~~~{.html-code}

    &#60;nav&#62;
    ${for(nav)}&#60;a href=&#34;${it.href}&#34;&#62;${it.label}&#60;/a&#62;${sep}, ${endfor}
    &#60;/nav&#62;

~~~

Here&#39;s my third iteration of our index template, [index3.tmpl](index3.tmpl).

~~~{.html-code}

    &#60;!DOCTYPE html&#62;
    &#60;html&#62;
    &#60;head&#62;
    ${if(title)}&#60;title&#62;${title}&#60;/title&#62;${endif}
    &#60;/head&#62;
    &#60;body&#62;
    ${if(nav)}
    ${nav.tmpl()}
    ${endif}
    &#60;section&#62;
    ${body}
    &#60;/section&#62;
    &#60;/body&#62;
    &#60;/html&#62;

~~~

Pandoc only requires you to reference the partial by using
its base name. Many people will name their templates with the
extension &#34;.html&#34;. I find this problematic as if you&#39;re trying
to list the templates in the directory you can not easily list
them separately. I use the &#34;.tmpl&#34; extension to identify my templates.
Since I have other documents that share the base name &#34;nav&#34; I
explicit call my navigation partial using the full filename followed
by the open and closed parenthesis. I have also chosen to wrap
the template in an &#34;if&#34; condition. That way if I don&#39;t want navigation
on a page I skip defining it in my metadata file.

Inside the partial template we inherit the parent metadata object.
You can use all the built-in Pandoc template functions and variables
provided by Pandoc in your partial templates.

Putting it all together:

~~~{.shell}

    pandoc --from=markdown --to=html \
           --template=index3.tmpl \
           --metadata-file=metadata.json Pandoc-Partials.txt &#62; index3.htm

~~~

Example 3 rendered: [index3.htm](index3.htm)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/11/09/Pandoc-Partials.md" length="7159" type="text/markdown" />
    </item>    <item>
      <title>Software Tools, Filters</title>
      <link>https://rsdoiel.github.io/blog/2020/10/31/Filters.html</link>
      <description>
        <![CDATA[This post is the second in a series revisiting the programs described in the 1981 book by Brian W. Kernighan and P. J.
        Plauger's called [Software Tools in Pascal](https://archive.org/details/softwaretoolsinp00kern). The book is available from the
        [Open Library](https://openlibrary.org/) and physical copies are still (2020) commonly available from used book sellers. The book was an late 20th century text on creating portable command line programs using ISO standard Pascal of the era.
        ...]]>
      </description>
      <source:markdown>Software Tools, Filters
=======================

Overview
--------

This post is the second in a series revisiting the programs
described in the 1981 book by Brian W. Kernighan and P. J.
Plauger&#39;s called [Software Tools in Pascal](https://archive.org/details/softwaretoolsinp00kern). The book is available from the
[Open Library](https://openlibrary.org/) and physical copies
are still (2020) commonly available from used book sellers.
The book was an late 20th century text on creating portable
command line programs using ISO standard Pascal of the era.

In this chapter K &#38; P focuses on developing the idea of filters.
Filters are programs which typically process standard input, do
some sort of transformation or calculation and write to standard
output.  They are intended to work either standalone or in a pipeline
to solve more complex problems. I like to think of filters as
software [LEGO](https://en.wikipedia.org/wiki/Lego).
Filter programs can be &#34;snapped&#34; together creating simple shapes
data shapes or combined to for complex compositions.

The programs from this chapter include:

+ **entab**, respecting tabstops, convert strings of spaces to tabs
+ **overstrike**, this is probably not useful anymore, it would allow &#34;overstriking&#34; characters on devices that supported it. From [wikipedia](https://en.wikipedia.org/wiki/Overstrike), &#34;In typography, overstrike is a method of printing characters that are missing from the printer&#39;s character set. The character was created by placing one character on another one  for example, overstriking &#34;L&#34; with &#34;-&#34; resulted in printing a &#34;&#34; (L with stroke) character.&#34;
+ **compress**, an early UNIX style compress for plain text files
+ **expand**, an early UNIX style expand for plain text files, previously run through with **compress**
+ **echo**, write echo&#39;s command line parameters to standard output, introduces working with command line parameters
+ **translit**, transliterate characters using a simple from/to substitution with a simple notation to describe character sequences and negation. My implementation diverges from K &#38; P

Implementing in Oberon-07
------------------------

With the exception of **echo** (used to introduce command line parameter processing) each program increases in complexity.  The last program **translit**is the most complex in this chapter.  It introducing what we a &#34;domain specific language&#34; or &#34;DSL&#34;.  A DSL is a notation allowing us to describe something implicitly rather than explicitly. All the programs except **translit** follow closely the original Pascal translated to Oberon-07.  **translit** book implementation is very much a result of the constraints of Pascal of the early 1980s as well as the minimalist assumption that could be made about the host operating system. I will focus on revising that program in particular bring the code up to current practice as well as offering insights I&#39;ve learned.


The program **translit** introduces what is called a &#34;Domain Specific Language&#34;.Domain specific languages or DSL for short are often simple notations to describe how to solve vary narrow problems.  If you&#39;ve used any of the popular spreadsheet programs where you&#39;ve entered a formula to compute something you&#39;ve used a domain specific language.  If you&#39;ve ever search for text in a document using a regular expression you&#39;ve used a domain specific language.  By focusing a notation on a small problem space you can often come up with simple ways of expressing or composing programmatic solutions to get a job done.

In **translit** the notation let&#39;s us describe what we want to translate. At the simplest level the **translit** program takes a character and replaces it with another character. What make increases **translit** utility is that it can take a set of characters and replace it with another.  If you want to change all lower cases letters and replace them with uppercase letters. This &#34;from set&#34; and &#34;to set&#34; are easy to describe as two ranges, &#34;a&#34; to &#34;z&#34; and &#34;A&#34; to &#34;Z&#34;.  Our domain notation allows us to express this as &#34;a-z&#34; and &#34;A-Z&#34;.  K &#38; P include several of features in there notation including characters to exclude from a translation as well as an &#34;escape notation&#34; for describing characters like new lines, tabs, or the characters that describe a range and exclusion (i.e. dash and caret).



2.1 Putting Tabs Back
=====================

[Page 31](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/31/mode/1up)

Implementing **entab** in Oberon-07 is straight forward.
Like my [Detab](Detab.Mod) implementation I am using
a second modules called [Tabs](Tabs.Mod). This removes
the need for the `#include` macros used in the K &#38; P version.
I have used the same loop structure as K &#38; P this time.
There is a difference in my `WHILE` loop. I separate the
character read from the `WHILE` conditional test.  Combining the
two is common in &#34;C&#34; and is consistent with the programming style
other books by Kernighan.  In Oberon-07 doesn&#39;t make sense at all.
Oberon&#39;s `In.Char()` is not a function returning as in the Pascal
primitives implemented for the K &#38; P book or indeed like in the &#34;C&#34;
language. In Oberon&#39;s &#34;In&#34; module the status of a read operation is
exposed by `In.Done`. I&#39;ve chosen to put the next call to
`In.Char()` at the bottom of my `WHILE` loop because it is clear
that it is the last think done before ether iterating again or
exiting the loop. Other than that the Oberon version looks much
like K &#38; P&#39;s Pascal.


Program Documentation
---------------------

[Page 32](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/32/mode/1up)


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PROGRAM

  entab	convert runs of blanks into tabs

USAGE

  entab

FUNCTION

  entab copies its input to its output, replacing strings of
  blanks by tabs so the output is visually the same as the
  input, but contains fewer characters. Tab stops are assumed
  to be set every four columns (i.e. 1, 5, 9, ...), so that
  each sequence of one to four blanks ending on a tab stop
  is replaced by a tab character

EXAMPLE

  Using -&#62; as visible tab:

    entab
      col  1   2   34  rest
    -&#62;col-&#62;1-&#62;2-&#62;34-&#62;rest

BUGS

  entab is naive about backspaces, virtical motions, and
  non-printing characters. entab will convert  a single blank
  to a tab if it occurs at a tab stop. The entab is not an
  exact inverse of detab.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Source code for **Entab.Mod**
-----------------------------

~~~

MODULE Entab;
  IMPORT In, Out, Tabs;

CONST
  NEWLINE = 10;
  TAB = 9;
  BLANK = 32;

PROCEDURE Entab();
VAR
  c : CHAR;
  col, newcol : INTEGER;
  tabstops : Tabs.TabType;
BEGIN
  Tabs.SetTabs(tabstops);
  col := 1;
  REPEAT
    newcol := col;
    In.Char(c);
    IF In.Done THEN (* NOTE: We check that the read was successful! *)
      WHILE (ORD(c) = BLANK) DO
        newcol := newcol + 1;
        IF (Tabs.TabPos(newcol, tabstops)) THEN
          Out.Char(CHR(TAB));
          col := newcol;
        END;
        (* NOTE: Get the next char, check the loop condition
           and either iterate or exit the loop *)
        In.Char(c);
      END;
      WHILE (col &#60; newcol) DO
        Out.Char(CHR(BLANK)); (* output left over blanks *)
        col := col + 1;
      END;
      (* NOTE: Since we may have gotten a new char in the first WHILE
         we need to check again if the read was successful *)
      IF In.Done THEN
        Out.Char(c);
        IF (ORD(c) = NEWLINE) THEN
          col := 1;
        ELSE
          col := col + 1;
        END;
      END;
    END;
  UNTIL In.Done # TRUE;
END Entab;

BEGIN
  Entab();
END Entab.

~~~



2.2 Overstrikes
===============


[Page 34](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/34/mode/1up)


Overstrike isn&#39;t a tool that is useful today but I&#39;ve included it
simply to be follow along the flow of the K &#38; P book. It very much
reflects an error where teletype like devices where still common and
printers printed much like typewriters did. On a 20th century
manual type writer you could underline a word or letter by backing
up the carriage then typing the underscore character. Striking out
a word was accomplished by a similar technique. The mid to late
20th century computers device retained this mechanism though by
1980&#39;s it was beginning to disappear along with manual typewriters.
This program relies on the the nature of ASCII character set and
reflects some of the non-print character&#39;s functionality. I
found it did not work on today&#39;s terminal emulators reliably. Your
mileage may very nor do I have a vintage printer to test it on.

Our module follows K &#38; P design almost verbatim. The differences
are those suggested by differences between Pascal and Oberon-07.
Like in previous examples we don&#39;t need to use an ENDFILE constant
as we can simply check the value of `In.Done` to determine
if the last read was successful. This simplifies some of
the `IF/ELSE` logic and the termination of the `REPEAT/UNTIL`
loop.  It makes the `WHILE/DO` loop a little more verbose.

One thing I would like to point out in the original Pascal of the
book is a problem often referred to as the &#34;dangling else&#34; problem.
While this is usually discussed in the context of compiler
implementation I feel like it is a bigger issue for the person
reading the source code. It is particularly problematic when you
have complex &#34;IF/ELSE&#34; sequences that are nested.  This is not
limited to the 1980&#39;s era Pascal. You see it in other languages
like C.  It is a convenience for the person typing the source code
but a problem for those who maintain it. We see this ambiguity in
the Pascal procedure **overstrike** inside the repeat loop
on [page 35](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/35/mode/1up).
It is made worse by the fact that K &#38; P have taken advantage of
omitting the semi-colons where optional. If you type in this
procedure and remove the indication if quickly becomes ambiguous
about where on &#34;IF/ELSE&#34; begins and the next ends. In Oberon-07 it
is clear when you have a dangling &#34;IF&#34; statement. This vintage
Pascal, not so much.

K &#38; P do mention the dangling &#34;ELSE&#34; problem later in the text.
Their recommend practice was include the explicit final &#34;ELSE&#34;
at a comment to avoid confusion. But you can see how easy an
omitting the comment is in the **overstrike** program.

Limitations
-----------

This is documented &#34;BUG&#34; section describes the limitations
well, &#34;**overstrike** is naive about vertical motions and non-
printing characters. It produces one over struck line for each
sequence of backspaces&#34;. But in addition to that most printing
devices these days either have their own drivers or expect to work
with a standard like Postscript. This limited the usefulness of
this program today though controlling character movement in a
&#34;vt100&#34; emulation using old fashion ASCII control codes is
still interesting if only for historical reasons.


Program Documentation
---------------------

[Page 36](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/36/mode/1up)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PROGRAM

  overstrike    replace overstrikes by multiple-lines

USAGE

  overstrike

FUNCTION

  overstrike copies in input to its output, replacing lines
  containing backspaces by multiple lines that overstrike
  to print the same as input, but containing no backspaces.
  It is assumed that the output is to be printed on a device
  that takes the first character of each line as a carriage
  control; a blank carriage control causes normal space before
  print, while a plus sign &#39;+&#39; suppresses space before print
  and hence causes the remainder of the line to overstrike
  the previous line.

EXAMPLE

  Using &#60;- as a visible backspace:

    overstrike
    abc&#60;-&#60;-&#60;-___
     abc
    +___

BUGS

  overstrike is naive about vertical motions and non-printing
  characters. It produces one over struck line for each sequence
  of backspaces.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Source code for **Overstrike.Mod**
----------------------------------

~~~

MODULE Overstrike;
IMPORT In, Out;

CONST
  NEWLINE = 10;
  BLANK = 32;
  PLUS = 43;
  BACKSPACE = 8;

PROCEDURE Max(x, y : INTEGER) : INTEGER;
VAR max : INTEGER;
BEGIN
  IF (x &#62; y) THEN
    max := x
  ELSE
    max := y
  END;
  RETURN max
END Max;

PROCEDURE Overstrike;
CONST
  SKIP = BLANK;
  NOSKIP = PLUS;
VAR
  c : CHAR;
  col, newcol, i : INTEGER;
BEGIN
  col := 1;
  REPEAT
    newcol := col;
    In.Char(c);
    (* NOTE We check In.Done on each loop evalution *)
    WHILE (In.Done = TRUE) &#38; (ORD(c) = BACKSPACE) DO (* eat the backspaces *)
      newcol := Max(newcol, 1);
      In.Char(c);
    END;
    (* NOTE: We check In.Done again, since we may have
       additional reads when eating the backspaces. If
       the previous while loop has taken us to the end of file.
       this will be also mean In.Done = FALSE. *)
    IF In.Done THEN
      IF (newcol &#60; col) THEN
        Out.Char(CHR(NEWLINE)); (* start overstrike line *)
        Out.Char(CHR(NOSKIP));
        FOR i := 0 TO newcol DO
          Out.Char(CHR(BLANK));
        END;
        col := newcol;
      ELSIF (col = 1) THEN (* NOTE: In.Done already check for end of file *)
        Out.Char(CHR(SKIP)); (* normal line *)
      END;
      (* NOTE: In.Done already was checked so we are in mid line *)
      Out.Char(c);    (* normal character *)
      IF (ORD(c) = NEWLINE) THEN
        col := 1
      ELSE
        col := col + 1
      END;
    END;
  UNTIL In.Done # TRUE;
END Overstrike;

BEGIN
  Overstrike();
END Overstrike.

~~~


2.3 Text Compression
====================

[Page 37](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/37/mode/1up)

In 20th century computing everything is expensive, memory,
persistent storage computational ability in CPU.  If you were
primarily working with text you still worried about running out of
space in your storage medium. You see it in the units
of measurement used in that era such as bytes, kilobytes, hertz and
kilohertz. To day we talk about megabytes, gigabytes, terabytes and
petabytes. Plain text files are a tiny size compared to must
digital objects today but in the late 20th century
their size in storage was still a concern.  One way to solve this
problem was to encode your plain text to use less storage space.
Early attempts at file compression took advantage of repetition to
save space. Many text documents have repeated characters
whether spaces or punctuation or other formatting. This is what
inspired the K &#38; P implementation of **compress** and **expand**.
Today we&#39;d use other approaches to save space whether we were
storing text or a digital photograph.


Program Documentation
---------------------

[Page ](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/40/mode/1up)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PROGRAM

    compress    compress input by encoding repeated characters

USAGE

    compress

FUNCTION

    compress copies its input to its output, replacing strings
    of four or more identical characters by a code sequence so
    that the output generally contains fewer characters than the
    input. A run of x&#39;s is encoded as -nx, where the count n is
    a character: &#39;A&#39; calls for a repetition of one x, &#39;B&#39; a
    repetition of two x&#39;s, and so on. Runs longer than 26 are
    broken into several shorter ones. Runs of -&#39;s of any length
    are encoded.

EXAMPLE

    compress
    Item     Name           Value
    Item-D Name-I Value
    1       car             -$7,000.00
    1-G car-J -A-$7,000.00
    &#60;ENDFILE&#62;

BUGS

    The implementation assumes 26 legal characters beginning with A.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Source code for **Compress.Mod**
--------------------------------

~~~

MODULE Compress;
IMPORT In, Out;

CONST
    TILDE = &#34;~&#34;;
    WARNING = TILDE;    (* ~ *)

(* Min -- compute minimum of two integers *)
PROCEDURE Min(x, y : INTEGER) : INTEGER;
VAR min : INTEGER;
BEGIN
    IF (x &#60; y) THEN
        min := x
    ELSE
        min := y
    END;
    RETURN min
END Min;

(* PutRep -- put out representation of run of n &#39;c&#39;s *)
PROCEDURE PutRep (n : INTEGER; c : CHAR);
CONST
    MAXREP = 26;    (* assuming &#39;A&#39; .. &#39;Z&#39; *)
    THRESH = 4;
VAR i : INTEGER;
BEGIN
    WHILE (n &#62;= THRESH) OR ((c = WARNING) &#38; (n &#62; 0)) DO
        Out.Char(WARNING);
        Out.Char(CHR((Min(n, MAXREP) - 1) + ORD(&#34;A&#34;)));
        Out.Char(c);
        n := n - MAXREP;
    END;
    FOR i := n TO 1 BY (-1) DO
        Out.Char(c);
    END;
END PutRep;

(* Compress -- compress standard input *)
PROCEDURE Compress();
VAR
    c, lastc : CHAR;
    n : INTEGER;
BEGIN
    n := 1;
    In.Char(lastc);
    WHILE (In.Done = TRUE) DO
        In.Char(c);
        IF (In.Done = FALSE) THEN
            IF (n &#62; 1) OR (lastc = WARNING) THEN
                PutRep(n, lastc)
            ELSE
                Out.Char(lastc);
            END;
        ELSIF (c = lastc) THEN
            n := n + 1
        ELSIF (n &#62; 1) OR (lastc = WARNING) THEN
            PutRep(n, lastc);
            n := 1
        ELSE
            Out.Char(lastc);
        END;
        lastc := c;
    END;
END Compress;


BEGIN
    Compress();
END Compress.

~~~



2.4 Text Expansion
==================

[Page 41](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/41/mode/1up)

Our procedures map closely to the original Pascal with a few
significant differences.  As previously I&#39;ve chosen a
`REPEAT ... UNTIL` loop structure because we are always attempting
to read at least once. The `IF THEN ELSIF ELSE` logic is a little
different. In the K &#38; P version they combine retrieving
a character and testing its value.  This is a style common in
languages like C. As previous mentioned I split the read of the
character from the test.  Aside from the choices imposed by the
&#34;In&#34; module I also feel that retrieving the value, then testing is
a simpler statement to read. There is little need to worry about a
side effect when you separate the action from the test. It does
change the structure of the inner and outer `IF` statements.



Program Documentation
---------------------

[Page 43](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/43/mode/1up)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PROGRAM

    expand  expand compressed input

USAGE

    expand

FUNCTION

    expand copies its input, which has presumably been encoded by
    compress, to its output, replacing code sequences -nc by the
    repeated characters they stand for so that the text output
    exactly matches that which was originally encoded. The
    occurrence of the warning character - in the input means that
    which was originally encoded. The occurrence of the warning
    character - in the input means that the next character is a
    repetition count; &#39;A&#39; calls for one instance of the following
    character, &#39;B&#39; calls for two, and so on up to &#39;Z&#39;.

EXAMPLE

    expand
    Item~D Name~I Value
    Item    Name        Value
    1~G car~J ~A~$7,000.00
    1       car         -$7,000.00
    &#60;ENDFILE&#62;

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Source code for **Expand.Mod**
------------------------------

~~~
MODULE Expand;
IMPORT In, Out;

CONST
    TILDE = &#34;~&#34;;
    WARNING = TILDE;    (* ~ *)
    LetterA = ORD(&#34;A&#34;);
    LetterZ = ORD(&#34;Z&#34;);

(* IsUpper -- true if c is upper case letter *)
PROCEDURE IsUpper (c : CHAR) : BOOLEAN;
VAR res : BOOLEAN;
BEGIN
    IF (ORD(c) &#62;= LetterA) &#38; (ORD(c) &#60;= LetterZ) THEN
        res := TRUE;
    ELSE
        res := FALSE;
    END
    RETURN res
END IsUpper;

(* Expand -- uncompress standard input *)
PROCEDURE Expand();
VAR
    c : CHAR;
    n, i : INTEGER;
BEGIN
    REPEAT
        In.Char(c);
        IF (c # WARNING) THEN
            Out.Char(c);
        ELSE
            In.Char(c);
            IF IsUpper(c) THEN
                n := (ORD(c) - ORD(&#34;A&#34;)) + 1;
                In.Char(c);
                IF (In.Done) THEN
                    FOR i := n TO 1 BY -1 DO
                        Out.Char(c);
                    END;
                ELSE
                    Out.Char(WARNING);
                    Out.Char(CHR((n - 1) + ORD(&#34;A&#34;)));
                END;
            ELSE
                Out.Char(WARNING);
                IF In.Done THEN
                    Out.Char(c);
                END;
            END;
        END;
    UNTIL In.Done # TRUE;
END Expand;

BEGIN
    Expand();
END Expand.

~~~


2.5 Command Arguments
=====================

[Page 44](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/44/mode/1up)


Program Documentation
---------------------

[Page 45](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/45/mode/1up)


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PROGRAM

    echo    echo arguments to standard output

USAGE

    echo [ argument ... ]

FUNCTION

    echo copies its command line arguments to its output as a line
    of text with one space
    between each argument. IF there are no arguments, no output is
    produced.

EXAMPLE

    To see if your system is alive:

        echo hello world!
        hello world!

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Source code for **Echo.Mod**
----------------------------

~~~

MODULE Echo;
IMPORT Out, Args := extArgs;

CONST
    MAXSTR = 1024; (* or whatever *)
    BLANK = &#34; &#34;;

(* Echo -- echo command line arguments to output *)
PROCEDURE Echo();
VAR
    i, res : INTEGER;
    argstr : ARRAY MAXSTR OF CHAR;
BEGIN
    i := 0;
    FOR i := 0 TO (Args.count - 1) DO
        Args.Get(i, argstr, res);
        IF (i &#62; 0) THEN
            Out.Char(BLANK);
        END;
        Out.String(argstr);
    END;
    IF Args.count &#62; 0 THEN
        Out.Ln();
    END;
END Echo;

BEGIN
    Echo();
END Echo.

~~~


2.6 Character Transliteration
=============================

[Page 47](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/47/mode/1up)


**translit** is the most complicated program so far in the book.
Most of the translation process from Pascal to Oberon-07 has
remained similar to the previous examples.

My implementation of **translit** diverges from the K &#38; P
implementation at several points. Much of this is a result of
Oberon evolution beyond Pascal. First Oberon counts arrays from
zero instead of one so I have opted to use -1 as a value to
indicate the index of a character in a string was not found.
Equally I have simplified the logic in `xindex()` to make it clear
how I am handling the index lookup described in `index()` of the
Pascal implementation. K &#38; P implemented `makeset()` and `dodash()`.
`dodash()` particularly looked troublesome. If you came across the
function name `dodash()` without seeing the code comments
&#34;doing a dash&#34; seems a little obscure.  I have chosen to name
that process &#34;Expand Sequence&#34; for clarity. I have simplified the
task of making sets of characters for translation into three cases
by splitting the test conditions from the actions. First check to
see if we have an escape sequence and if so handle it. Second check
to see if we have an expansion sequence and if so handle it else
append the char found to the end of the set being assembled. This
resulted in `dodash()` being replaced by `IsSequence()` and
`ExpandSequence()`.  Likewise `esc()` was replaced with `IsEscape()`
and `ExpandEscape()`. I renamed `addchar()` to `AppendChar()`
in the &#34;Chars&#34; module as that seemed more specific and clearer.

I choose to advance the value used when expanding a set description
in the loop inside of my `MakeSet()`. I minimized the side effects
of the expand functions to the target destination.  It is clearer
while in the `MakeSet()` loop to see the relationship of the test
and transformation and how to advance through the string. This also
allowed me to use fewer parameters to procedures which tends to
make things more readable as well as simpler.

I have included an additional procedure not included in the K &#38; P
Pascal of this program. `Error()` displays a string and halts.
K &#38; P provide this as part of their Pascal environment. I have
chosen to embed it here because it is short and trivial.

Translit suggested the &#34;Chars&#34; module because of the repetition in
previous programs. In K &#38; P the approach to code reuse is to create
a separate source file and to included via a pre-processor. In
Oberon we have the module concept.

My [Chars](Chars.Mod) module provides a useful set of test
procedures like `IsAlpha(c)`, `IsUpper(c)`, `IsLower()` in addition
to the `CharInRange()` and `IsAlphaNum()`.  It also includes
`AppendChar()` which can be used to append a single character value
to an end of an array of char.


Program Documentation
---------------------

[Page 56](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/56/mode/1up)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PROGRAM

    translit    transliterate characters

USAGE

    translit    [^]src [dest]

FUNCTION

    translit maps its input, on a character by character basis, and
    writes the translated version to its output.In the simplest case,
    each character is the argument src is translated to the
    corresponding character is the argument dest; all other characters
    are copies as is. Both the src and dest may contain substrings of
    the form c1 - c2 as shorthand for all the characters in the range
    c1..c2 and c2 must both be digits, or both be letter of the same
    case. If dest is absent, all characters represented by src are
    deleted. Otherwise, if dest is shorter than src, all characters
    is src that would map to or beyond the last character in
    dest are mapped to the last character in dest; moreover adjacent
    instances of such characters in the input are represented in the
    output by a single instance of the last character in dest. The

        translit 0-9 9

    converts each string of digits to the single digit 9.
    Finally, if src is precedded by ^, then all but the characters
    represented by src are taken as the source string; i.e., they are
    all deleted if dest is absent, or they are all collapsed if the
    last character in dest is present.

EXAMPLE

    To convert upper case to lower:

        translit A-Z a-z

    To discard punctualtion and isolate words by spaces on each line:

        translit ^a-zA-Z@n &#34; &#34;
        This is a simple-minded test, i.e., a test of translit.
        This is a simple minded test i e a test of translit

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pascal Source
-------------

[translit.p, Page 48](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/48/mode/1up)

[makeset.p, Page 52](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/52/mode/2up)


[addstr.p, Page 53](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/53/mode/1up)

[dodash.p, Page 53](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/53/mode/1up)

[isalphanum.p, Page 54](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/54/mode/1up)

[esc.p, Page 55](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/55/mode/1up)


[length.p, Page 46](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/46/mode/1up)


The impacts of having a richer language than 1980s ISO Pascal and
evolution in practice suggest a revision in the K &#38; P approach. I
have attempted to keep the spirit of their example program while
reflecting changes in practice that have occurred in the last four
decades.


Source code for **Translit.Mod**
--------------------------------

~~~
MODULE Translit;
IMPORT In, Out, Args := extArgs, Strings, Chars;

CONST
    MAXSTR = 1024; (* or whatever *)
    DASH = Chars.DASH;
    ENDSTR = Chars.ENDSTR;
    ESCAPE = &#34;@&#34;;
    TAB* = Chars.TAB;

(* Error -- write an error string to standard out and
   halt program *)
PROCEDURE Error(s : ARRAY OF CHAR);
BEGIN
    Out.String(s);Out.Ln();
    ASSERT(FALSE);
END Error;

(* IsEscape - this procedure looks to see if we have an
escape sequence at position in variable i *)
PROCEDURE IsEscape*(src : ARRAY OF CHAR; i : INTEGER) : BOOLEAN;
VAR res : BOOLEAN; last : INTEGER;
BEGIN
  res := FALSE;
  last := Strings.Length(src) - 1;
  IF (i &#60; last) &#38; (src[i] = ESCAPE) THEN
    res := TRUE;
  END;
  RETURN res
END IsEscape;

(* ExpandEscape - this procedure takes a source array, a
   position and appends the escaped value to the destintation
   array.  It returns TRUE on successuss, FALSE otherwise. *)
PROCEDURE ExpandEscape*(src : ARRAY OF CHAR; i : INTEGER; VAR dest : ARRAY OF CHAR) : BOOLEAN;
VAR res : BOOLEAN; j : INTEGER;
BEGIN
 res := FALSE;
 j := i + 1;
 IF j &#60; Strings.Length(src)  THEN
    res := Chars.AppendChar(src[j], dest)
 END
 RETURN res
END ExpandEscape;

(* IsSequence - this procedure looks at position i and checks
   to see if we have a sequence to expand *)
PROCEDURE IsSequence*(src : ARRAY OF CHAR; i : INTEGER) : BOOLEAN;
VAR res : BOOLEAN;
BEGIN
  res := Strings.Length(src) - i &#62;= 3;
  (* Do we have a sequence of alphumeric character
     DASH alpanumeric character? *)
  IF res &#38; Chars.IsAlphaNum(src[i]) &#38; (src[i+1] = DASH) &#38;
            Chars.IsAlphaNum(src[i+2]) THEN
      res := TRUE;
  END;
  RETURN res
END IsSequence;

(* ExpandSequence - this procedure expands a sequence x
   starting at i and append the sequence into the destination
   string. It returns TRUE on success, FALSE otherwise *)
PROCEDURE ExpandSequence*(src : ARRAY OF CHAR; i : INTEGER; VAR dest : ARRAY OF CHAR) : BOOLEAN;
VAR res : BOOLEAN; cur, start, end : INTEGER;
BEGIN
  (* Make sure sequence is assending *)
  res := TRUE;
  start := ORD(src[i]);
  end := ORD(src[i+2]);
  IF start &#60; end THEN
    FOR cur := start TO end DO
      IF res THEN
        res := Chars.AppendChar(CHR(cur), dest);
      END;
    END;
  ELSE
    res := FALSE;
  END;
  RETURN res
END ExpandSequence;


(* makeset -- make sets based on src expanded into destination *)
PROCEDURE MakeSet* (src : ARRAY OF CHAR; start : INTEGER; VAR dest : ARRAY OF CHAR) : BOOLEAN;
VAR i : INTEGER; makeset : BOOLEAN;
BEGIN
    i := start;
    makeset := TRUE;
    WHILE (makeset = TRUE) &#38; (i &#60; Strings.Length(src)) DO
        IF IsEscape(src, i) THEN
            makeset := ExpandEscape(src, i, dest);
            i := i + 2;
        ELSIF IsSequence(src, i) THEN
            makeset := ExpandSequence(src, i, dest);
            i := i + 3;
        ELSE
            makeset := Chars.AppendChar(src[i], dest);
            i := i + 1;
        END;
    END;
    RETURN makeset
END MakeSet;


(* Index -- find position of character c in string s *)
PROCEDURE Index* (VAR s : ARRAY OF CHAR; c : CHAR) : INTEGER;
VAR
    i, index : INTEGER;
BEGIN
    i := 0;
    WHILE (s[i] # c) &#38; (s[i] # ENDSTR) DO
        i := i + 1;
    END;
    IF (s[i] = ENDSTR) THEN
        index := -1; (* Value not found *)
    ELSE
        index := i; (* Value found *)
    END;
    RETURN index
END Index;

(* XIndex -- conditionally invert value found in index *)
PROCEDURE XIndex* (VAR inset : ARRAY OF CHAR; c : CHAR;
    allbut : BOOLEAN; lastto : INTEGER) : INTEGER;
VAR
    xindex : INTEGER;
BEGIN
    (* Uninverted index value *)
    xindex := Index(inset, c);
    (* Handle inverted index value *)
    IF (allbut = TRUE) THEN
        IF (xindex = -1)  THEN
            (* Translate as an inverted the response *)
            xindex := 0; (* lastto - 1; *)
        ELSE
            (* Indicate no translate *)
            xindex := -1;
        END;
    END;
    RETURN xindex
END XIndex;

(* Translit -- map characters *)
PROCEDURE Translit* ();
CONST
    NEGATE = Chars.CARET; (* ^ *)
VAR
    arg, fromset, toset : ARRAY MAXSTR OF CHAR;
    c : CHAR;
    i, lastto : INTEGER;
    allbut, squash : BOOLEAN;
    res : INTEGER;
BEGIN
    i := 0;
    lastto := MAXSTR - 1;
    (* NOTE: We are doing low level of string manimulation. Oberon
       strings are terminated by 0X, but Oberon compilers do not
       automatically initialize memory to a specific state. In the
       OBNC implementation of Oberon-07 assign &#34;&#34; to an assignment
       like `s := &#34;&#34;;` only writes a 0X to position zero of the
       array of char. Since we are doing position based character
       assignment and can easily overwrite a single 0X.  To be safe
       we want to assign all the positions in the array to 0X so the
       memory is in a known state.  *)
    Chars.Clear(arg);
    Chars.Clear(fromset);
    Chars.Clear(toset);
    IF (Args.count = 0) THEN
        Error(&#34;usage: translit from to&#34;);
    END;
    (* NOTE: I have not used an IF ELSE here because we have
       additional conditions that lead to complex logic.  The
       procedure Error() calls ASSERT(FALSE); which in Oberon-07
       halts the program from further execution *)
    IF (Args.count &#62; 0) THEN
        Args.Get(0, arg, res);
        allbut := (arg[0] = NEGATE);
        IF (allbut) THEN
            i := 1;
        ELSE
            i := 0;
        END;
        IF MakeSet(arg, i, fromset) = FALSE THEN
            Error(&#34;from set too long&#34;);
        END;
    END;
    (* NOTE: We have initialized our array of char earlier so we only
       need to know if we need to update toset to a new value *)
    Chars.Clear(arg);
    IF (Args.count = 2) THEN
        Args.Get(1, arg, res);
        IF MakeSet(arg, 0, toset) = FALSE THEN
            Error(&#34;to set too long&#34;);
        END;
    END;

    lastto := Strings.Length(toset);
    squash := (Strings.Length(fromset) &#62; lastto) OR (allbut);
    REPEAT
        In.Char(c);
        IF In.Done THEN
            i := XIndex(fromset, c, allbut, lastto);
            IF (squash) &#38; (i&#62;=lastto) &#38; (lastto&#62;0) THEN (* translate *)
                Out.Char(toset[lastto]);
            ELSIF (i &#62;= 0) &#38; (lastto &#62; 0) THEN    (* translate *)
                Out.Char(toset[i]);
            ELSIF i = -1 THEN                        (* copy *)
              (* Do not translate the character *)
              Out.Char(c);
              (* NOTE: No else clause needed as not writing out
			     a cut value is deleting *)
            END;
        END;
    UNTIL (In.Done # TRUE);
END Translit;

BEGIN
    Translit();
END Translit.

~~~



In closing
==========

In this chapter we interact with some of the most common features
of command line programs available on POSIX systems. K &#38; P have given
us a solid foundation on which to build more complex and ambitious
programs. In the following chapters the read will find an
accelerated level of complexity bit also programs that are
significantly more powerful.

Oberon language evolved with the Oberon System which had a very
different rich text user interface when compared with POSIX.
Fortunately Karl&#39;s OBNC comes with a set of modules that make
Oberon-07 friendly for building programs for POSIX operating systems.
I&#39;ve taken advantage of his `extArgs` module much in the way
that K &#38; P relied on a set of primitive tools to provide a common
programming environment. K &#38; P&#39;s version of
[implementation of primitives](https://archive.org/details/softwaretoolsinp00kern/page/315/mode/1up)
listed in their appendix. Karl&#39;s OBNC extensions modules are
described on [website](https://miasap.se/obnc/obncdoc/ext/).
Other Oberon compilers provide similar modules though implementation
specific. A good example is Spivey&#39;s [Oxford Oberon-2 Compiler](https://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler).
K &#38; P chose to target multiple Pascal implementations, I have the
luxury of targeting one Oberon-07 implementation. That said if you
added a pre-processor like K &#38; P did you could also take their approach
to allow you Oberon-07 code to work across many Oberon compiler
implementations. I leave that as an exercise for the reader.

I&#39;ve chosen to revise some of the code presented in K &#38; P&#39;s book. I
believe the K &#38; P implementations still contains wisdom in their
implementations. They had different constraints and thus made
different choices in implementation. Understand the trade offs and
challenges to writing portable code capable of running in very
divergent set of early 1980&#39;s operating systems remains useful today.

Compiling with OBNC:

~~~

    obnc -o entab Entab.Mod
    obnc -o overstrike Overstrike.Mod
    obnc -o compress Compress.Mod
    obnc -o expand Expand.Mod
    obnc -o echo Echo.Mod
    obnc -o translit Translit.Mod

~~~

+ [Entab](Entab.Mod)
    + [Tabs](Tabs.Mod), this one visited this one in last installment.
+ [Overstrike](Overstrike.Mod)
+ [Compress](Compress.Mod)
+ [Expand](Expand.Mod)
+ [Echo](Echo.Mod)
+ [Translit](Translit.Mod)
	+ [Chars](Chars.Mod)

&#60;!--
Next and Previous
-----------------

+ Next: [Files]()
--&#62;

Previous
--------

+ Previous: [Getting Started](../../09/29/Software-Tools-1.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/10/31/Filters.md" length="37906" type="text/markdown" />
    </item>    <item>
      <title>Assembling Pages</title>
      <link>https://rsdoiel.github.io/blog/2020/10/19/Assemble-pages.html</link>
      <description>
        <![CDATA[This is the thirteenth post in the [Mostly Oberon](https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.]]>
      </description>
      <source:markdown>Assembling pages
================

This is the thirteenth post in the [Mostly Oberon](https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.

Pandoc and JSON
---------------

I use [Pandoc](https://pandoc.org) to process Markdown documents. I like to keep my
front matter in JSON rather than Pandoc&#39;s YAML. Fortunately Pandoc
does support working with JSON as a metadata file include. Normally I would
manually split the JSON front matter and the rest of the markup into two
separate files, then process with Pandoc and other tooling like
[LunrJS](https://lunrjs.com). [AssemblePage](AssemblePage.Mod) automates this
process.

Example shell usage:

~~~

   AssemblePage MyText.txt \
      metadata=document.json \
      document=document.md
   pandoc --from markdown --to html \
      --metadata-file document.json \
      --standalone \
      document.md &#62;MyText.html

~~~

Source code for **AssemblePage.Mod**
------------------------------------

~~~

MODULE AssemblePage;
  IMPORT Out, Strings, Files, Args := extArgs;

VAR
  srcName, metaName, docName : ARRAY 1024 OF CHAR;

(* FrontMatter takes a &#34;read&#34; Rider, r, and a &#34;write&#34; Rider &#34;w&#34;.
If the first character read by r is an opening curly bracket
(the start of the front matter) it writes it out with w, until
it finds a matching closing curly bracket or the file ends. *)
PROCEDURE FrontMatter*(VAR r : Files.Rider; VAR w : Files.Rider);
  VAR c : BYTE; cCnt : INTEGER;
BEGIN
  (* Scan for opening JSON front matter *)
  cCnt := 0;
  REPEAT
    Files.Read(r, c);
    IF r.eof = FALSE THEN
      IF c = ORD(&#34;{&#34;) THEN
        cCnt := cCnt + 1;
      ELSIF c = ORD(&#34;}&#34;) THEN
        cCnt := cCnt - 1;
      END;
      Files.Write(w, c);
    END;
  UNTIL (r.eof = TRUE) OR (cCnt = 0);
  IF cCnt # 0 THEN
    Out.String(&#34;ERROR: mis matched &#39;{&#39; and &#39;}&#39; in front matter&#34;);
    ASSERT(FALSE);
  END;
END FrontMatter;

(* CopyIO copies the characters from a &#34;read&#34; Rider to a &#34;write&#34; Rider *)
PROCEDURE CopyIO*(VAR r : Files.Rider; VAR w: Files.Rider);
  VAR c : BYTE;
BEGIN
  REPEAT
    Files.Read(r, c);
    IF r.eof = FALSE THEN
      Files.Write(w, c);
    END;
  UNTIL r.eof = TRUE;
END CopyIO;

PROCEDURE ProcessParameters(VAR sName, mName, dName : ARRAY OF CHAR);
  VAR
    arg : ARRAY 1024 OF CHAR;
    i, res : INTEGER;
BEGIN
  mName := &#34;document.json&#34;;
  dName := &#34;document.txt&#34;;
  arg := &#34;&#34;;
  FOR i := 0 TO (Args.count - 1) DO
    Args.Get(i, arg, res);
    IF Strings.Pos(&#34;metadata=&#34;, arg, 0) = 0 THEN
      Strings.Extract(arg, 9, Strings.Length(arg), mName);
    ELSIF Strings.Pos(&#34;document=&#34;, arg, 0) = 0 THEN
      Strings.Extract(arg, 9, Strings.Length(arg), dName);
    ELSE
      Strings.Extract(arg, 0, Strings.Length(arg), sName);
    END;
  END;
END ProcessParameters;

PROCEDURE AssemblePage(srcName, metaName, docName : ARRAY OF CHAR);
VAR
  src, meta, doc : Files.File;
  reader, writer : Files.Rider;
BEGIN
  src := Files.Old(srcName);
  IF src # NIL THEN
    Files.Set(reader, src, 0);
    IF metaName # &#34;&#34; THEN
      meta := Files.New(metaName);
      Files.Register(meta);
      Files.Set(writer, meta, 0);
      FrontMatter(reader, writer);
      Files.Close(meta);
    END;
    IF docName # &#34;&#34; THEN
      doc := Files.New(docName);
      Files.Register(doc);
      Files.Set(writer, doc, 0);
      CopyIO(reader, writer);
      Files.Close(doc);
    END;
  ELSE
    Out.String(&#34;ERROR: Could not read &#34;);Out.String(srcName);Out.Ln();
    ASSERT(FALSE);
  END;
  Files.Close(src);
END AssemblePage;

BEGIN
  ProcessParameters(srcName, metaName, docName);
  AssemblePage(srcName, metaName, docName);
END AssemblePage.

~~~

### Next, Previous

+ Next [Dates &#38; Clock](../../11/27/Dates-and-Clock.html)
+ Previous [Oberon To Markdown](../../10/03/Oberon-to-markdown.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/10/19/Assemble-pages.md" length="4673" type="text/markdown" />
    </item>    <item>
      <title>Oberon to Markdown</title>
      <link>https://rsdoiel.github.io/blog/2020/10/03/Oberon-to-markdown.html</link>
      <description>
        <![CDATA[This is the twelfth post in the [Mostly Oberon](https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.]]>
      </description>
      <source:markdown>Oberon to Markdown
==================

This is the twelfth post in the [Mostly Oberon](https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.

A nice feature of Oberon
------------------------

Oberon source code has a very nice property in that anything
after the closing end statement is ignored by the compiler.
This makes it a nice place to write documentation, program
notes and other ideas.

I&#39;ve gotten in the habit of writing up program docs and
notes there. When I prep to make a web document I used to
copy the source file, doing a cut and paste to re-order
the module code to the bottom of the document. I&#39;d follow
that with adding headers and code fences. Not hard but
tedious. Of course if I changed the source code I&#39;d also
have to do another cut and paste edit. This program,
`ObnToMd.Mod` automates that process.

Program Documentation
---------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PROGRAM

  ObnToMd

FUNCTION

  This is a simple program that reads Oberon modules
  from standard in and re-renders that to standard output
  such that it is suitable to process with Pandoc or other
  text processing system.

EXAMPLE

  Read the source for this program and render a file
  called &#34;blog-post.md&#34;. Use Pandoc to render HTML.

    ObnToMd &#60;ObnToMd.Mod &#62; blog-post.md
    pandoc -s --metadata title=&#34;Blog Post&#34; \
        blog-post.md &#62;blog-post.html

BUGS

  It uses a naive line analysis to identify the module
  name and then the end of module statement. Might be
  tripped up by comments containing the same strings.
  The temporary file created is called &#34;o2m.tmp&#34; and
  this filename could potentially conflict with another
  file.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




Source code for **ObnToMd.Mod**
-------------------------------

~~~

(* ObnToMd.Mod - an simple filter process for reading
an Oberon-07 module source file and rendering a markdown
friendly output suitable for piping into Pandoc. The
filter reads from standard input and writes to standard
output and makes use of a temp file name o2m.tmp which
it removes after successful rendering.

@Author R. S. Doiel, &#60;rsdoiel@gmail.com&#62;
copyright (c) 2020, all rights reserved.
Released under the BSD 2-clause license
See: https://opensource.org/licenses/BSD-2-Clause
*)
MODULE ObnToMd;
  IMPORT In, Out, Files, Strings;

CONST
  MAXLENGTH = 1024;
  LF = CHR(10);

VAR
  endOfLine : ARRAY 2 OF CHAR;

(*
 * Helper methods
 *)
PROCEDURE GenTempName(prefix, suffix : ARRAY OF CHAR; VAR name : ARRAY OF CHAR);
BEGIN
  name := &#34;&#34;;
  Strings.Append(prefix, name);
  Strings.Append(&#34;.&#34;, name);
  Strings.Append(suffix, name);
END GenTempName;

PROCEDURE GenTempFile(name : ARRAY OF CHAR; VAR r : Files.Rider; VAR f : Files.File);
BEGIN
  f := Files.New(name);
  IF f = NIL THEN
    Out.String(&#34;ERROR: can&#39;t create &#34;);Out.String(name);Out.Ln();
    ASSERT(FALSE);
  END;
  Files.Register(f);
  Files.Set(r, f, 0);
END GenTempFile;


PROCEDURE StartsWith(target, source : ARRAY OF CHAR) : BOOLEAN;
  VAR res : BOOLEAN;
BEGIN
  IF Strings.Pos(target, source, 0) &#62; -1 THEN
    res := TRUE;
  ELSE
    res := FALSE;
  END;
  RETURN res
END StartsWith;

PROCEDURE ClearString(VAR s : ARRAY OF CHAR);
  VAR i : INTEGER;
BEGIN
  FOR i := 0 TO LEN(s) - 1 DO
    s[i] := 0X;
  END;
END ClearString;


PROCEDURE ProcessModuleDef(VAR r : Files.Rider; VAR modName : ARRAY OF CHAR);
  VAR
    line, endStmt : ARRAY MAXLENGTH OF CHAR;
    start, end : INTEGER;
BEGIN
  line := &#34;&#34;;
  endStmt := &#34;&#34;;
  modName := &#34;&#34;;
  (* Find the name of the module and calc the &#34;END {NAME}.&#34; statement *)
  REPEAT
    ClearString(line);
    In.Line(line);
    IF In.Done THEN
      Files.WriteString(r, line); Files.WriteString(r, endOfLine);
      (* When `MODULE {NAME};` is encountered extract the module name *)
      IF StartsWith(&#34;MODULE &#34;, line) THEN
        start := 7;
        end := Strings.Pos(&#34;;&#34;, line, 0);
        IF (end &#62; -1) &#38; (end &#62; start) THEN
            Strings.Extract(line, start, end - start, modName);
            endStmt := &#34;END &#34;;
            Strings.Append(modName, endStmt);
            Strings.Append(&#34;.&#34;, endStmt);
        END;
      END;
    END;
  UNTIL (In.Done # TRUE) OR (endStmt # &#34;&#34;);

  (* When `END {NAME}.` is encountered  stop writing tmp file *)
  REPEAT
    In.Line(line);
    IF In.Done THEN
      Files.WriteString(r, line); Files.WriteString(r, endOfLine);
    END;
  UNTIL (In.Done # TRUE) OR StartsWith(endStmt, line);
END ProcessModuleDef;

PROCEDURE WriteModuleDef(name : ARRAY OF CHAR; VAR r : Files.Rider; VAR f : Files.File);
  VAR s : ARRAY MAXLENGTH OF CHAR; res : INTEGER;
BEGIN
  Files.Set(r, f, 0);
  REPEAT
    Files.ReadString(r, s);
    IF r.eof # TRUE THEN
      Out.String(s);
    END;
  UNTIL r.eof;
  Files.Close(f);
  Files.Delete(name, res);
END WriteModuleDef;


PROCEDURE OberonToMarkdown();
VAR
  tmpName, modName, line : ARRAY MAXLENGTH OF CHAR;
  f : Files.File;
  r : Files.Rider;
  i : INTEGER;
BEGIN
  tmpName := &#34;&#34;; modName := &#34;&#34;;  line := &#34;&#34;;
  (* Open temp file *)
  GenTempName(&#34;o2m&#34;, &#34;tmp&#34;, tmpName);
  GenTempFile(tmpName, r, f);

  (* Read the Oberon source from standard input echo the lines tmp file *)
  ProcessModuleDef(r, modName);

  (* Write remainder of file to standard out *)
  REPEAT
    In.Line(line);
    IF In.Done THEN
      Out.String(line);Out.Ln();
    END;
  UNTIL In.Done # TRUE;

  (* Write two new lines *)
  Out.Ln(); Out.Ln();
  (* Write heading `Source code for {NAME}` *)
  ClearString(line);
  line := &#34;Source code for **&#34;;
  Strings.Append(modName, line);
  Strings.Append(&#34;.Mod**&#34;, line);
  Out.String(line); Out.Ln();
  FOR i := 0 TO Strings.Length(line) - 1 DO
    Out.String(&#34;-&#34;);
  END;
  Out.Ln();
  (* Write code fence *)
  Out.Ln();Out.String(&#34;~~~&#34;);Out.Ln();
  (* Reset rider to top of tmp file
     Write temp file to standard out
     cleanup demp file *)
  WriteModuleDef(tmpName, r, f);
  (* Write code fence *)
  Out.Ln();Out.String(&#34;~~~&#34;);Out.Ln();
  (* Write tailing line and exit procedure *)
  Out.Ln();
END OberonToMarkdown;

BEGIN
  endOfLine[0] := LF;
  endOfLine[1] := 0X;
  OberonToMarkdown();
END ObnToMd.

~~~

### Next, Previous

+ Next [Assembling Pages](../../10/19/Assemble-pages.html)
+ Previous [Portable Oberon-07](../../08/15/Portable-Oberon-07.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/10/03/Oberon-to-markdown.md" length="7205" type="text/markdown" />
    </item>    <item>
      <title>Software Tools, Getting Started</title>
      <link>https://rsdoiel.github.io/blog/2020/09/29/Software-Tools-1.html</link>
      <description>
        <![CDATA[This post is the first in a series revisiting the
        programs described in the 1981 book by Brian W. Kernighan and
        P. J. Plauger's called [Software Tools in Pascal](https://archive.org/details/softwaretoolsinp00kern).
        The book is available from the [Open Library](https://openlibrary.org/)
        and physical copies are still (2020) commonly available from used book
        sellers.  The book was an early text on creating portable command
        line programs.  
        
        In this series I present the K & P (i.e. Software Tools in Pascal)
        programs re-implemented in Oberon-07. I have testing my implementations
        using Karl Landstrm's [OBNC](http://miasap.se/obnc/)
        compiler and his implementation of the Oakwood Guide's modules
        for portable Oberon programs. Karl also provides a few additional
        modules for working in a POSIX environment (e.g. BSD, macOS, Linux,
        Windows 10 with Linux subsystem). I have also tested these
        programs with Mike Spivey's [Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler) an aside
        from the differences file extensions that both compilers use
        the source code works the same.]]>
      </description>
      <source:markdown># Software Tools, Getting Started

## Overview

This post is the first in a series revisiting the
programs described in the 1981 book by Brian W. Kernighan and
P. J. Plauger&#39;s called [Software Tools in Pascal](https://archive.org/details/softwaretoolsinp00kern).
The book is available from the [Open Library](https://openlibrary.org/)
and physical copies are still (2020) commonly available from used book
sellers.  The book was an early text on creating portable command
line programs.  

In this series I present the K &#38; P (i.e. Software Tools in Pascal)
programs re-implemented in Oberon-07. I have testing my implementations
using Karl Landstrm&#39;s [OBNC](http://miasap.se/obnc/)
compiler and his implementation of the Oakwood Guide&#39;s modules
for portable Oberon programs. Karl also provides a few additional
modules for working in a POSIX environment (e.g. BSD, macOS, Linux,
Windows 10 with Linux subsystem). I have also tested these
programs with Mike Spivey&#39;s [Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler) an aside
from the differences file extensions that both compilers use
the source code works the same. 

NOTE: OBNC compiler is the work of Karl Langstrm, it is portable across many systems where the C tool chain is available.

NOTE: POSIX defines a standard of compatibility inspired by [UNIX](https://en.wikipedia.org/wiki/Unix), see &#60;https://en.wikipedia.org/wiki/POSIX&#62;


## Getting Started.

Chapter one in K &#38; P is the first chapter that presents code. It introduces
some challenges and constraints creating portable Pascal suitable for use
across hardware architectures and operating systems. In 1981 this included
mainframes, minicomputers as well as the recent evolution of the microcomputer.
The programs presented build up from simple to increasingly complex as
you move through the book.  They provide example documentation and discuss
their implementation choices. It is well worth reading the book for those
discussions, while specific to the era, mirror the problems program authors
face today in spite of the wide spread success of the POXIS model, the
consolidation of CPU types and improvements made in development tools in
the intervening decades.

Through out K &#38; P you&#39;ll see the bones of many POSIX commands we have today.

Programs from this chapter include:

1. **copyprog**, this is like &#34;cat&#34; in a POSIX system
2. **charcount**, this is like the &#34;wc&#34; POSIX command using the &#34;-c&#34; option
3. **linecount**, this is like the &#34;wc&#34; POSIX command using the &#34;-l&#34; option
4. **wordcount**, this is like the &#34;wc&#34; POSIX command using the &#34;-w&#34; option
5. **detab**, converts tabs to spaces using tab stops every four characters in a line

All programs in this chapter rely solely on standard input and output.
Today&#39;s reader will notice an absence to common concepts in today&#39;s
command line programs.  First is the lack of interaction with command line
parameters, the second is no example take advantage of environment variables.
These operating system features were not always available across
operating systems of the early 1980s. Finally I&#39;d like to point out a
really nice feature included in the book. It is often left out as a topic
in programming books.  K &#38; P provide example documentation. It&#39;s structure
like an early UNIX man page. It very clear and short. This is something
I wish all programming texts at least mentioned. Documentation is important
to the program author because it clarifies scope of the problem being
tackled and to the program user so they understand what they are using.


### [1.1. File Copying](https://archive.org/details/softwaretoolsinp00kern/page/7/mode/1up)

Here&#39;s how K &#38; P describe &#34;copyprog.pas&#34; (referred to as &#34;copy&#34; in
the documentation).


~~~

PROGRAM

    copy    copy input to output

USAGE

    copy

FUNCTION

    copy copies its input to its output unchanged. It is useful for copying
    from a terminal to a file, from file to file, or even from terminal to
    terminal. It may be used for displaying the contents of a file, without
    interpretation or formatting, by copying from the file to terminal.

EXAMPLE

    To echo lines type at your terminal.

    copy
    hello there, are you listening?
    **hello there, are you listening?**
    yes, I am.
    **yes, I am.**
    &#60;ENDFILE&#62;

~~~

The source code for &#34;copyprog.pas&#34; is shown on
[page 9](https://archive.org/details/softwaretoolsinp00kern/page/9/mode/1up)
of K &#38; P.  First the authors introduce the __copy__ procedure
then a complete the section introducing it in context of the complete Pascal
program. After this first example K &#38; P leave implementation of the full
program up to the reader.

The body of the Pascal program invokes a procedure called
[copy](https://archive.org/details/softwaretoolsinp00kern/page/8/mode/1up)
which reads from standard input character by character and writes
to standard output character by character without modification.  Two
supporting procedures are introduced, &#34;getc&#34; and &#34;putc&#34;. These are shown
in the complete program listing on page 9. They are repeatedly used
through out the book. One of the really good aspects of this simple
program is relying on the idea of standard input and output. This makes
&#34;copyprog.pas&#34; a simple filter and template for writing many of the programs
that follow. K &#38; P provide a good explanation for this simple approach.
Also note K &#38; P&#39;s rational for working character by character versus
line by line.

My Oberon-07 version takes a similar approach. The module looks remarkably
similar to the Pascal but is shorter because reading and writing characters are
provided for by Oberon&#39;s standard modules &#34;In&#34; and &#34;Out&#34;.
I have chosen to use a &#34;REPEAT/UNTIL&#34; loop over the &#34;WHILE&#34;
loop used by K &#38; P is the attempt to read from standard input needs to happen
at least once. Note in my &#34;REPEAT/UNTIL&#34; loop&#39;s terminating condition.
The value of `In.Done` is true on successful read and false
otherwise (e.g. you hit an end of the file). That means our loop must
terminate on `In.Done # TRUE` rather than `In.Done = TRUE`. This appears
counter intuitive unless you keep in mind our loop stops when we having
nothing more to read, rather than when we can continue to read.
It `In.Done` means the read was successful and does not
mean &#34;I&#39;m done and can exit now&#34;. Likewise before writing out the character
we read, it is good practice to check the `In.Done` value. If `In.Done` is
TRUE, I know can safely display the character using `Out.Char(c);`.

~~~

MODULE CopyProg;
IMPORT In, Out;

PROCEDURE copy;
VAR
  c : CHAR;
BEGIN
  REPEAT
    In.Char(c);
    IF In.Done THEN
        Out.Char(c);
    END;
  UNTIL In.Done # TRUE;
END copy;

BEGIN
  copy();
END CopyProg.

~~~

#### Limitations

This program only works with standard input and output. A more generalized
version would work with named files.

### [1.2 Counting Characters](https://archive.org/details/softwaretoolsinp00kern/page/13/mode/1up)

~~~

PROGRAM

  charcount count characters in input

USAGE

  charcount

FUNCTION

  charcount counts the characters in its input and writes the total
  as a single line of text to the output. Since each line of text is
  internally delimited by a NEWLINE character, the total count is the
  number of lines plus the number of characters within each line.

EXAMPLE

  charcount
  A single line of input.
  &#60;ENDFILE&#62;
  24

~~~

[On page 13](https://archive.org/details/softwaretoolsinp00kern/page/13/mode/1up)
K &#38; P introduces their second program, **charcount**. It is based on a single
procedure that reads from standard input and counts up the number of
characters encountered then writes the total number found to standard out
followed by a newline. In the text only the procedure is shown, it is
assumed you&#39;ll write the outer wrapper of the program yourself as
was done with the **copyprog** program. My Oberon-07 version is very similar
to the Pascal. Like in the our first &#34;CopyProg&#34; we will make use of the
&#34;In&#34; and &#34;Out&#34; modules. Since we will
need to write an INTEGER value we&#39;ll also use &#34;Out.Int()&#34; procedure which
is very similar to K &#38; P&#39;s &#34;putdec()&#34;. Aside from the counting this is very
simple  like our first program.

~~~

MODULE CharCount;
IMPORT In, Out;

PROCEDURE CharCount;
VAR
  nc : INTEGER;
  c : CHAR;
BEGIN
  nc := 0;

  REPEAT
    In.Char(c);
    IF In.Done THEN
      nc := nc + 1;
    END;
  UNTIL In.Done # TRUE;
  Out.Int(nc, 1);
  Out.Ln();
END CharCount;

BEGIN
  CharCount();
END CharCount.

~~~

#### Limitations

The primary limitation in counting characters is most readers are
interested in visible character count. In our implementation
even non-printed characters are counted. Like our first program
this only works on standard input and output. Ideally this should
be written so it works on any file including standard input and
output. If the reader implements that it could become part of a
package on statistical analysis of plain text files.

### [1.3 Counting Lines](https://archive.org/details/softwaretoolsinp00kern/page/14/mode/1up)

~~~

PROGRAM

  linecount count lines in input

USAGE

  linecount

FUNCTION

  linecount counts the lines in its input and write the total as a
  line of text to the output.

EXAMPLE

  linecount
  A single line of input.
  &#60;ENDFILE&#62;
  1

~~~

**linecount**, from [page 15](https://archive.org/details/softwaretoolsinp00kern/page/15/mode/1up)
is very similar to **charcount** except adding a
conditional count in the loop for processing the file. In
our Oberon-07 implementation we&#39;ll check if the `In.Char(c)`
call was successful but we&#39;ll add a second condition to see if the
character read was a NEWLINE. If it was I increment
our counter variable.

~~~

MODULE LineCount;
IMPORT In, Out;

PROCEDURE LineCount;
CONST
  NEWLINE = 10;

VAR
  nl : INTEGER;
  c : CHAR;
BEGIN
  nl := 0;
  REPEAT
    In.Char(c);
    IF In.Done &#38; (ORD(c) = NEWLINE) THEN
      nl := nl + 1;
    END;
  UNTIL In.Done # TRUE;
  Out.Int(nl, 1);
  Out.Ln();
END LineCount;

BEGIN
  LineCount();
END LineCount.

~~~

#### Limitations

This program assumes that NEWLINE is ASCII value 10. Line delimiters
vary between operating systems.  If your OS used carriage returns
without a NEWLINE then this program would not count lines correctly.
The reader could extend the checking to support carriage returns,
new lines, and carriage return with new lines and cover most versions
of line endings.


### [1.4 Counting Words](https://archive.org/details/softwaretoolsinp00kern/page/14/mode/1up)

~~~

PROGRAM

  wordcount count words in input

USAGE

  wordcount

FUNCTION

  wordcount counts the words in its input and write the total as a
  line of text to the output. A &#34;word&#34; is a maximal sequence of characters
  not containing a blank or tab or newline.

EXAMPLE

  wordcount
  A single line of input.
  &#60;ENDFILE&#62;
  5

BUGS

  The definition of &#34;word&#34; is simplistic.

~~~

[Page 17](https://archive.org/details/softwaretoolsinp00kern/page/17/mode/1up)
brings us to the **wordcount** program. Counting words can be
very nuanced but here K &#38; P have chosen a simple definition
which most of the time is &#34;good enough&#34; for languages like English.
A word is defined simply as an run of characters separated by
a space, tab or newline characters.  In practice most documents
will work with this minimal definition. It also makes the code
straight forward.  This is a good example of taking the simple
road if you can. It keeps this program short and sweet.

If you follow along in the K &#38; P book note their rational
and choices in arriving at there solutions. There solutions
will often balance readability and clarity over machine efficiency.
While the code has progressed from &#34;if then&#34; to &#34;if then else if&#34;
logical sequence, the solution&#39;s modeled remains
clear. This means the person reading the source code can easily verify
if the approach chosen was too simple to meet their needs or it was
&#34;good enough&#34;.

My Oberon-07 implementation is again very simple. Like in previous programs
I still have an outer check to see if the read worked (i.e. &#34;In.Done = TRUE&#34;),
otherwise the conditional logic is the same as the Pascal implementation.

~~~

MODULE WordCount;
IMPORT In, Out;

PROCEDURE WordCount;
CONST
  NEWLINE = 10;
  BLANK = 32;
  TAB = 9;

VAR
  nw : INTEGER;
  c : CHAR;
  inword : BOOLEAN;
BEGIN
  nw := 0;
  inword := FALSE;
  REPEAT
    In.Char(c);
    IF In.Done THEN
      IF ((ORD(c) = BLANK) OR (ORD(c) = NEWLINE) OR (ORD(c) = TAB)) THEN
        inword := FALSE;
      ELSIF (inword = FALSE) THEN
        inword := TRUE;
        nw := nw + 1;
      END;
    END;
  UNTIL In.Done # TRUE;
  Out.Int(nw, 1);
  Out.Ln();
END WordCount;

BEGIN
  WordCount();
END WordCount.

~~~

## [1.5 Removing Tabs](https://archive.org/details/softwaretoolsinp00kern/page/20/mode/1up)

~~~

PROGRAM

  detab convert tabs into blanks

USAGE

  detab

FUNCTION

  detab copies its input to its output, expanding the horizontal
  tabs to blanks along the way, so that the output is visually
  the same as the input, but contains no tab characters. Tab stops
  are assumed to be set every four columns (i.e. 1, 5, 9, ...), so
  each tab character is replaced by from one to four blanks.

EXAMPLE

  Usaing &#34;-&#62;&#34; as a visible tab:

  detab
  -&#62;col 1-&#62;2-&#62;34-&#62;rest
      col 1   2   34  rest

BUGS

  detab is naive about backspaces, vertical motions, and
  non-printing characters.

~~~

The source code for &#34;detab&#34; can be found on
[page 24](https://archive.org/details/softwaretoolsinp00kern/page/24/mode/1up)
in the last section of chapter 1. **detab** removes
tabs and replaces them with spaces. Rather than a simple &#34;tab&#34;
replaced with four spaces **detab** preserves a concept found on
typewriters called &#34;tab stops&#34;. In 1981 typewrites were still widely
used though word processing software would become common. Supporting the
&#34;tab stop&#34; model means the program works with what office workers would
expect from older tools like the typewriter or even the computer&#39;s
teletype machine. I think this shows an important aspect of writing
programs. Write the program for people, support existing common concepts
they will likely know.

K &#38; P implementation includes separate source files
for setting tab stops and checking a tab stop.  The Pascal K &#38; P
wrote for didn&#39;t support separate source files or program modules. Recent Pascal
versions did support the concept of modularization (e.g. UCSD Pascal). Since
and significant goal of K &#38; P was portability they needed to come up
with a solution that worked on the &#34;standard&#34; Pascal compilers available on
minicomputers and mainframes and not write their solution to a specific
Pascal system like UCSD Pascal (see Appendix, &#34;IMPLEMENTATION
PRIMITIVES [page 315](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/315/mode/1up)).
Modularization facilitates code reuse and like information hiding is an
import software technique. Unfortunately the preprocessor approach doesn&#39;t
support information hiding.

To facilitate code reuse the K &#38; P book includes a preprocessor as part
of the Pascal development tools (see [page 71](https://archive.org/stream/softwaretoolsinp00kern?ref=ol#page/71/mode/1up)
for implementation). The preprocessor written
in Pascal was based on the early versions of the &#34;C&#34; preprocessor
they had available in the early UNIX systems. Not terribly Pascal
like but it worked and allowed the two files to be shared between
this program and one in the next chapter.

Oberon-07 of course benefits from all of Wirth&#39;s language improvements
that came after Pascal. Oberon-07 supports modules and as such
there is no need for a preprocessor.  Because of Oberon-07&#39;s module
support I&#39;ve implemented the Oberon version using two files
rather than three. My main program file is &#34;Detab.Mod&#34;,
the supporting library module is &#34;Tabs.Mod&#34;. &#34;Tabs&#34; is where I
define our tab stop data structure as well as the
procedures that operating on that data structure.

Let&#39;s look at the first part, &#34;Detab.Mod&#34;. This is the module
that forms the program and it features an module level &#34;BEGIN/END&#34; block.
In that block I call &#34;Detab();&#34; which implements the program&#39;s functionality.
I import &#34;In&#34;, &#34;Out&#34; as before but I also import &#34;Tabs&#34; which I will show next.
Like my previous examples I validate the read was successful before proceeding
with the logic presented in the original Pascal and deciding
what to write to standard output.

~~~

MODULE Detab;
  IMPORT In, Out, Tabs;

CONST
  NEWLINE = 10;
  TAB = 9;
  BLANK = 32;

PROCEDURE Detab;
VAR
  c : CHAR;
  col : INTEGER;
  tabstops : Tabs.TabType;
BEGIN
  Tabs.SetTabs(tabstops); (* set initial tab stops *)
  col := 1;
  REPEAT
    In.Char(c);
    IF In.Done THEN
      IF (ORD(c) = TAB) THEN
        REPEAT
          Out.Char(CHR(BLANK));
          col := col + 1;
        UNTIL Tabs.TabPos(col, tabstops);
      ELSIF (ORD(c) = NEWLINE) THEN
        Out.Char(c);
        col := 1;
      ELSE
        Out.Char(c);
        col := col + 1;
      END;
    END;
  UNTIL In.Done # TRUE;
END Detab;

BEGIN
  Detab();
END Detab.

~~~

Our second module is &#34;Tabs.Mod&#34;. It provides the supporting procedures
and definition of the our &#34;TabType&#34; data structure. For us this
is the first time we write a module which &#34;exports&#34; procedures
and type definitions. If you are new to Oberon, expected constants,
variables and procedures names have a trailing &#34;*&#34;. Otherwise the
Oberon compiler will assume a local use only. This is a very
powerful information hiding capability and what allows you to
evolve a modules&#39; internal implementation independently of the
programs that rely on it.

~~~

MODULE Tabs;

CONST
  MAXLINE = 1000; (* or whatever *)

TYPE
  TabType* = ARRAY MAXLINE OF BOOLEAN;

(* TabPos -- return TRUE if col is a tab stop *)
PROCEDURE TabPos*(col : INTEGER; VAR tabstops : TabType) : BOOLEAN;
  VAR res : BOOLEAN;
BEGIN
  res := FALSE; (* Initialize our internal default return value *)
  IF (col &#62;= MAXLINE) THEN
    res := TRUE;
  ELSE
    res := tabstops[col];
  END;
  RETURN res
END TabPos;

(* SetTabs -- set initial tab stops *)
PROCEDURE SetTabs*(VAR tabstops: TabType);
CONST
  TABSPACE = 4; (* 4 spaces per tab *)
VAR
  i : INTEGER;
BEGIN
  (* NOTE: Arrays in Oberon start at zero, we want to
     stop at the last cell *)
  FOR i := 0 TO (MAXLINE - 1) DO
    tabstops[i] := ((i MOD TABSPACE) = 0);
  END;
END SetTabs;

END Tabs.

~~~

NOTE: This module is used by &#34;Detab.Mod&#34; and &#34;Entab.Mod&#34;
and provides for common type definitions and code reuse.
We exported `TabType`, `TabPos` and `SetTabs`. Everything else
is private to this module.

## In closing

This post briefly highlighted ports of the programs
presented in Chapter 1 of &#34;Software Tools in Pascal&#34;.
Below are links to my source files of the my
implementations inspired by the K &#38; P book. Included
in each Oberon module source after the module definition
is transcribed text of the program documentation as well
as transcribed text of the K &#38; P Pascal implementations.
Each file should compiler without modification using the
OBNC compiler.  By default the OBNC compiler will use the
module&#39;s name as the name of the executable version. I
I have used mixed case module names, if you prefer lower
case executable names use the &#34;-o&#34; option with the OBNC
compiler.

~~~

    obnc -o copy CopyProg.Mod
    obnc -o charcount CharCount.Mod
    obnc -o linecount LineCount.Mod
    obnc -o wordcount WordCount.Mod
    obnc -o detab Detab.Mod

~~~

If you happen to be using The [Oxford Oberon Compiler](http://spivey.oriel.ox.ac.uk/corner/Oxford_Oberon-2_compiler)
you need to rename the files ending in &#34;.Mod&#34; to &#34;.m&#34; 
and you can compiler with the following command.

~~~
    obc -07 -o copyprog CopyProg.m
    obc -07 -o charcount CharCount.m
    obc -07 -o linecount LineCount.m
    obc -07 -o wordcount WordCount.m
    obc -07 -o detab Tabs.m Detab.m
~~~

Note the line for compiling &#34;Detab&#34; with **obc**, your
local modules need to become before the module calling them.


+ [CopyProg](CopyProg.Mod)
+ [CharCount](CharCount.Mod)
+ [LineCount](LineCount.Mod)
+ [WordCount](WordCount.Mod)
+ [Detab](Detab.Mod)
    + [Tabs](Tabs.Mod), this one we&#39;ll revisit in next installment.


# Next

+ [Filters](../../10/31/Filters.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/09/29/Software-Tools-1.md" length="21893" type="text/markdown" />
    </item>    <item>
      <title>Portable Oberon-07</title>
      <link>https://rsdoiel.github.io/blog/2020/08/15/Portable-Oberon-07.html</link>
      <description>
        <![CDATA[This is the eleventh post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.
        Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the
        various rabbit holes I will inevitably fall into.]]>
      </description>
      <source:markdown># Portable Oberon-07

## using OBNC modules

This is the eleventh post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.
Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the
various rabbit holes I will inevitably fall into.

## Working with standard input

By R. S. Doiel, 2020-08-15 (updated: 2020-09-05)

Karl Landstrm&#39;s [OBNC](https://miasap.se/obnc/), Oberon-07 compiler,
comes with an Oberon-2 inspired set of modules
described in the Oakwood Guidelines as well as
several very useful additions making Oberon-07 suitable for
writing programs in a POSIX environment.  We&#39;re going to
explore three of the Oakwood modules and two of Karl&#39;s own additions
in this post as we create a program called [SlowCat](SlowCat.Mod).
I am using the term &#34;portable&#34; to mean the code can be compiled
using OBNC on macOS, Linux, and Raspberry Pi OS and Windows 10
(i.e. wherever OBNC is available). The Oakwood Guideline modules
focus on portability between an Oberon System and other systems.
I&#39;ll leave that discussion along with
[POW!](http://www.fim.uni-linz.ac.at/pow/pow.htm)
to the end of this post.


### SlowCat

Recently while I was reviewing logs at work using [cat](https://en.wikipedia.org/wiki/Cat_(Unix)), [grep](https://en.wikipedia.org/wiki/Grep)
and [more](https://en.wikipedia.org/wiki/More_(command)) it
struck me that it would have been nice if **cat**
or **more** came with a time delay so you could use them like a
teleprompter. This would let you casually watch the file scroll
by while still being able to read the lines. The program we&#39;ll build
in this post is &#34;SlowCat&#34; which accepts a command line parameter
indicating the delay in seconds between display each line read from
standard input.

## Working with Standard Input and Output

The Oakwood guides for Oberon-2 describe two modules
particularly useful for working with standard input and output.
They are appropriately called `In` and `Out`. On many Oberon Systems
these have been implemented such that your code could run under Unix
or Oberon System with a simple re-compile.  We&#39;ve used `Out` in our
first program of this series, &#34;Hello World&#34;. It provides a means to
write Oberon system base types to standard out.  We&#39;ve used `In`
a few times too. But `In` is worth diving into a bit more.

### In

The [In](http://miasap.se/obnc/obncdoc/basic/In.def.html) module provides
a mirror of inputs to those of [Out](http://miasap.se/obnc/obncdoc/basic/Out.def.html). In Karl&#39;s implementation we are interested in one procedure
and module status variable.

+ `In.Line(VAR line: ARRAY OF CHAR)` : Read a sequence of characters from standard input from the current position in the file to the end of line.
+ `In.Done` : Is a status Boolean variable, if the last call to an procedure in `In` was successful then it is set TRUE, otherwise FALSE (e.g. we&#39;re at the end of a file)

We use Karl&#39;s `In.Line()` extension to the standard `In` implementation
before and will do so again as it simplifies our code and keeps things
easily readable.

There is one nuance with `In.Done` that is easy to get tripped up on.
`In.Done` indicates if the last operation was successful.
So if you&#39;re using `In.Line()` then `In.Done`
should be true if reading the line was successful. If you hit the end of
the file then `In.Done` should be false.  When you write your loop
this can be counter intuitive.  Here is a example of testing `In.Done`
with a repeat until loop.


~~~

    REPEAT
      In.Line(text);
      IF In.Done THEN
        Out.String(text);Out.Ln();
      END;
    UNTIL In.Done = FALSE;

~~~


So when you read this it is easy to think of `In.Done` as you&#39;re
done reading from standard input but actually we need to check for `FALSE`.
The value of `In.Done` was indicating the success of reading our line.
An unsuccessful line read, meaning we&#39;re at the end of the file, sets
`In.Done` to false!

### Out

As mention `Out` provides our output functions. We&#39;ll be using
two procedure from `Out`, namely `Out.String()` and `Out.Ln()`.
We&#39;ve seen both before.

### Input

&#34;SlowCat&#34; needs to calculate how often to write a line of
text to standard output with the `Out` module.  To do that
I need access to the system&#39;s current time.  There isn&#39;t an
Oakwood module for time. There is a module called 
`Input` which provides a &#34;Time&#34; procedure. As a result
I need to import `Input` as well as `In` even though
I am using `In` to manage reading the file I am processing
with &#34;SlowCat&#34;.

A note about Karl&#39;s implementation.  `Input` is an Oakwood
module that provides access to three system resources -- 
mouse, keyboard and system time.  Karl 
provides two versions `Input` and `Input0`, the first is
intended to be used with the `XYPlane` module for graphical
applications the second for POSIX shell based application.
In the case of &#34;SlowCat&#34; I&#39;ve stuck with `Input` as I am 
only accessing time I&#39;ve stuck with `Input` to make my source
code more portable if you&#39;re using another Oberon compiler.

## Working with Karl&#39;s extensions

This is the part of my code which is not portable
between compiler implementations and with Oberon Systems.
Karl provides a number of extension module wrapping various
POSIX calls.  We are going to use two,
[extArgs](http://miasap.se/obnc/obncdoc/ext/extArgs.def.html)
which provides access to command line arguments and
[extConvert](http://miasap.se/obnc/obncdoc/ext/extConvert.def.html)
which provides a means of converting strings to integers.
If you are using another Oberon compiler you&#39;ll need to 
find their equivalents and change my code example. I
use `extArgs` to access the command line parameters
included in my POSIX shell invocation and I&#39;ve used
`extConvert` to convert the string presentation of the
delay to an integer value for my delay.


## Our Approach

To create &#34;SlowCat&#34; we need four procedures and one
global variable.

`Usage()`
: display a help text if parameters don&#39;t make sense

`ProcessArgs()`
: to get our delay time from the command line

`Delay(count : INTEGER)`
: a busy wait procedure

`SlowCat(count : INTEGER)`
: take standard input and display like a teleprompter

`count`
: is an integer holding our delay value (seconds of waiting) which is set by ProcessArgs()

### Usage

Usage just wraps helpful text and display it to standard out.

## ProcessArgs()

This a functional procedure. It uses two of Karl&#39;s extension
modules. It uses `extArgs` to retrieve the command line parameters
and `extConvert` the string value retrieved into an integer.
`ProcessArgs()` returns TRUE if we can successful convert the
command line parameter and set the value of count otherwise return
FALSE.

## Delay(VAR count : INTEGER)

This procedure uses `Input0` to fetch the current epoch time
and counts the number of seconds until we&#39;ve reached our delay
value. It&#39;s a busy loop which isn&#39;t ideal but does keep the
program simple.

## SlowCat(VAR count: INTEGER);

This is the heart of our command line program. It reads
a line of text from standard input, if successful writes it
to standard out and then waits using delay before repeating
this process. The delay is only invoked when a reading a
line was successful.

## Putting it all together

Here&#39;s a &#34;SlowCat&#34; program.


~~~

    MODULE SlowCat;
      IMPORT In, Out, Input, Args := extArgs, Convert := extConvert;

    CONST
      MAXLINE = 1024;

    VAR
      count: INTEGER;

    PROCEDURE Usage();
    BEGIN
      Out.String(&#34;USAGE:&#34;);Out.Ln();
      Out.Ln();
      Out.String(&#34;SlowCat outputs lines of text delayed by&#34;);Out.Ln();
      Out.String(&#34;a number of seconds. It takes one parameter,&#34;);Out.Ln();
      Out.String(&#34;an integer, which is the number of seconds to&#34;);Out.Ln();
      Out.String(&#34;delay a line of output.&#34;);Out.Ln();
      Out.String(&#34;SlowCat works on standard input and output.&#34;);Out.Ln();
      Out.Ln();
      Out.String(&#34;EXAMPLE:&#34;);
      Out.Ln();
      Out.String(&#34;    SlowCat 15 &#60; README.md&#34;);Out.Ln();
      Out.Ln();
    END Usage;

    PROCEDURE ProcessArgs() : BOOLEAN;
      VAR i : INTEGER; ok : BOOLEAN; arg : ARRAY MAXLINE OF CHAR;
          res : BOOLEAN;
    BEGIN
      res := FALSE;
      IF Args.count = 1 THEN
        Args.Get(0, arg, i);
        Convert.StringToInt(arg, i, ok);
        IF ok THEN
           (* convert seconds to microseconds of clock *)
           count := (i * 1000);
           res := TRUE;
        END;
      END;
      RETURN res
    END ProcessArgs;

    PROCEDURE Delay*(count : INTEGER);
      VAR start, current, delay : INTEGER;
    BEGIN
       start := Input.Time();
       REPEAT
         current := Input.Time();
         delay := (current - start);
       UNTIL delay &#62;= count;
    END Delay;

    PROCEDURE SlowCat(count : INTEGER);
      VAR text : ARRAY MAXLINE OF CHAR;
    BEGIN
      REPEAT
        In.Line(text);
        IF In.Done THEN
          Out.String(text);Out.Ln();
          (* Delay by count *)
          Delay(count);
        END;
      UNTIL In.Done = FALSE;
    END SlowCat;

    BEGIN
      count := 0;
      IF ProcessArgs() THEN
        SlowCat(count);
      ELSE
        Usage();
      END;
    END SlowCat.

~~~


## Compiling and trying it out

To compile our program and try it out reading
our source code do the following.


~~~

    obnc SlowCat.Mod
    # If successful
    ./SlowCat 2 &#60; SlowCat.Mod

~~~



## Oakwood Guidelines and POW!

Oberon and Oberon-2 were both used in creating and enhancing the
Oberon System(s) as well as for writing programs on other operating
systems (e.g. Apple&#39;s Mac and Microsoft Windows).
Implementing Oberon compilers on non Oberon Systems required clarification
beyond the specification. The Oakwood Guidelines were an agreement
between some of the important Oberon-2 compiler implementers which
attempted to fill in that gap while encouraging portability in
source code between operating systems. Portability was desirable
because it allowed programmers (e.g. students) to compile
and run their Oberon programs with minimal modification in any
environment where an Oakwood compliant compiler was available.

Citation for Oakwood can be found in [Oberon-2 Programming with Windows](https://archive.org/details/oberonprogrammin00mhlb/page/n363/mode/2up?q=Oakwood+Guidlines).

&#62; Kirk B.(ed): The Oakwood Guidelines for Oberon-2 Compiler Developers. Available via FTP from ftp.fim.uni-linz.ac.at, /pub/soft/pow-oberon/oakwood

The FTP machine doesn&#39;t exist any more and does not appear to have been included in JKU&#39;s preservation plans. Fortunately the POW! website has been preserved.

[POW!](http://www.fim.uni-linz.ac.at/pow/pow.htm) was a
different approach. It was a compiler and IDE targeting
other than Oberon Systems (e.g. Windows and later Java). It was
intended to be used in a hybrid development environment and to
facilitate leveraging non-Oberon resources (e.g. Java classes,
native Windows API).  POW project proposed &#34;Opal&#34; which was a
super set of modules that went beyond Oakwood. Having skimmed
&#34;Oberon-2 Programming with Windows&#34; some may seem reasonable to
port to Oberon-07, others less so.

Why Oakwood and POW? These efforts are of interest to Oberon-07
developers as a well worn path to write code that is easy to
compile on POSIX systems and on systems that are based on the
more recent [Project Oberon 2013](http://www.projectoberon.com/).
It enhances the opportunity to bring forward well written modules
from prior systems like [A2](https://en.wikibooks.org/wiki/Oberon/A2)
but implemented for the next generation of Oberon Systems
like [Integrated Oberon](https://github.com/io-core/io).

### Oakwood PDF

Finding a PDF of the original Oakwood guidelines is going to become
tricky in the future. It was created by Robinson Associates and the
copy I&#39;ve read from 1995 includes a page saying not for distribution.
Which sorta makes sense in the era of closed source software
development. It is problematic for those of us who want to explore
how systems evolved.  The term &#34;Oakwood Guidelines&#34; is bandied about
after 1993 and several of the modules have had influence on the language
use via book publications.  I was able to find a PDF of the 1995
version of the guidelines at
[http://www.math.bas.bg/bantchev/place/oberon/oakwood-guidelines.pdf](http://www.math.bas.bg/bantchev/place/oberon/oakwood-guidelines.pdf).

Here&#39;s a typical explanation of Oakwood from 
[http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines](http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines)
for a description of Oakwood.

&#62; __The Oakwood Guidelines for the Oberon-2 Compiler Developers /These guidelines have been produced by a group of Oberon-2 compiler developers, including ETH developers, after a meeting at the Oakwood Hotel in Croydon, UK in June 1993__

[http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines](http://www.edm2.com/index.php/The_Oakwood_Guidelines_for_Oberon-2_Compiler_Developers#The_Oakwood_Guidelines)  
(an OS/2 developer website) was helpful for providing details about Oakwood.

It would have been nice if the Oakwood document had made its way
into either ETH&#39;s or JKU&#39;s research libraries.

Leveraging prior art opens doors to the past and future. Karl has
done with this with the modules he provides with his OBNC compiler
project.

### Next and Previous

+ Next [Oberon to Markdown](../../10/03/Oberon-to-markdown.html)
+ Previous [Procedures in records](../..//07/07/Procedures-in-records.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/08/15/Portable-Oberon-07.md" length="14284" type="text/markdown" />
    </item>    <item>
      <title>Words Matter</title>
      <link>https://rsdoiel.github.io/blog/2020/07/08/words-matter.html</link>
      <description>
        <![CDATA[**Why does software development use the vocabulary of slavery and
        Jim Crow to describe our creations?** What we call things matters.
        This is especially true of the words we use day to day without thinking.
        
        ```shell
            git pull origin master
        ```
        
        "Naming things is a hard problem in computer science." That is
        a phrase I remember from my student days. We name variables,
        programs and algorithms. We name architectures. Naming is a choice.
        The names convey meaning and intent. Names and terms are a human
        communication. They matter.
        
        ```shell
            git push origin master
        ```]]>
      </description>
      <source:markdown># Words Matter

By R. S. Doiel, 2020-07-08

UPDATE (2020-08-15, RSD): When I added a post today I was VERY pleased to 
to see that GitHub now allows me to publish my blog via the &#34;main&#34; branch.
It&#39;s nice to see the change in the words we use.

**Why does software development use the vocabulary of slavery and
Jim Crow to describe our creations?** What we call things matters.
This is especially true of the words we use day to day without thinking.

```shell
    git pull origin master
```

&#34;Naming things is a hard problem in computer science.&#34; That is
a phrase I remember from my student days. We name variables,
programs and algorithms. We name architectures. Naming is a choice.
The names convey meaning and intent. Names and terms are a human
communication. They matter.

```shell
    git push origin master
```


## Names can change

I remember the first time I encountered the terminology &#34;master/slave&#34;
describing network and database architecture. I remember cringing at
the terms. I accepted the terminology because I was a student and
naively assumed that those terms were chosen innocently and did not
mean what they did. I was wrong.

Example MySQL command:

```sql
    CHANGE MASTER TO MASTER_HOST=host1,
    MASTER_PORT=3002 FOR CHANNEL &#39;channel2&#39;;
```

When I did not challenge the use of those terms in computer science
I became complicit in the status quo of systemic racism. I am not happy
about that. Not then and not now.  We need inclusive language
in engineering. We need real diversity to find solutions to
today&#39;s challenges.  In software engineering we very much control what
we call things. Software is an explicit form of written human
communication. Words count. Words directly impact culture and our
community.

Example MySQL commands:

```sql
    START SLAVE;
    RESET SLAVE;
    STOP SLAVE;
```

As a direct benefactor of white male privilege I find the
terminology of &#34;master/slave&#34; offensive.  I am certain those
words caused injury to those who did not benefit from the same white
male privilege.  Using &#34;master/slave&#34; terminology to describe database,
replication, network architectures or as used with version control
systems like Git is like polishing statues that glorify slavery.
It endorses inhumanity in a subtle and casual way.

We have a choice about how we communicate and what we communicate
to convey meaning.  Let&#39;s use better words. It is time to change
some.


## Practice Change

**To change our community&#39;s vocabulary we need to thoughtfully choose terms**.
My friends and colleagues introduced me to using the term &#34;main&#34; as
a better descriptive word than &#34;master&#34; in Git repositories. When
I started making the branch name changes in my Git repositories I ran
across a problem at GitHub.


## Note the problem

Recently GitHub made it possible to easily change the default branch.
For a number of years you could change the published documentation
branch from &#34;gh-pages&#34; to something you prefer.
What you can&#39;t do is change the name of the branch used to publish
personal and group websites.  **GitHub explicitly states that the
publication branch must be named &#34;master&#34;.** I tested this and
confirmed the documentation is accurate as of today (2020-07-08, morning).

With the recent improvements in GitHub to allow default branches to
be named better it seems odd that you still have to publish
personal and group pages to &#34;master&#34; in order to publish a website. It
never made sense to me that person and group pages didn&#39;t use the default
of &#34;gh-pages&#34; in the first place.


## Taking action

Morning (2020-07-08): I submitted a ticket to GitHub asking to have
the option of using another word besides &#34;master&#34; for the publication
branch in publishing my personal GitHub pages (i.e. this blog).
Unfortunately the ticket doesn&#39;t have a public URL. I don&#39;t know how many
other people have submitted similar requests. If only a few people
have requested this recently it will not be changed. Such is the nature
of systemic problems.

Please help improve the words and names we use in software.
I believe it can make a difference in creating a more inclusive and
equitable community and profession. Raise the issue when it comes to
your attention. Silence becomes acceptance with systemic problems.

## A reply

Evening (2020-07-08): I got a reply today from Steve G of customer
support. Not sure if the reply is a bot or human.  It was a nice polite
reply (I wrote a nice polite ticket).  Steve mentioned that CEO Nat
Friedman had addressed this on twitter and to follow the GitHub blog
news.  Steve said dropping master was a priority for GitHub but there
is no time line for implementation.

I am not sure how you can develop software with a priority in mind and
not have a sense of time it takes to implement it.  I mentioned that in my
response to Steve G.

After some Duck Duck searching I found a [BBC article](https://www.bbc.com/news/technology-53050955) dated June 15th, 2020 where Nat made the
announcement about Microsoft&#39;s GitHub making the change away from &#34;master&#34;.
Next week is July 15th. It will be interesting to see how much of an unscheduled priority this change is.

I am skeptical.  If Steve G had indicated that they are actively working
the problem and provided a general time range I would be more inclined to
give Nat, GitHub and Microsoft the benefit of the doubt.  Given the rest
of the content I read on Nat&#39;s twitter feed I don&#39;t think this is a priority
beyond press, publicity and stock price.


## Where to go from here?

Just as many sites have adopted more gender neutral terms in documentation
practice we should encourage better descriptive terms for our algorithms,
and architectures. If you run into terms perpetuating exclusion please
speak up.  Most of the web runs on web servers and databases.  Like GitHub
those software projects frequently use the terminology of &#34;master/slave&#34;.
It is especially prevalent in documentation about replication. Blindly
perpetuating the &#34;master/slave&#34; terminology to describe distributed
software and architectures is like polishing a statue to the old
Confederacy. It can and should be change. We can communicate better
without perpetuating the vocabulary of Jim Crow, segregation, slavery
and oppression.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/07/08/words-matter.md" length="7272" type="text/markdown" />
    </item>    <item>
      <title>Procedures in records</title>
      <link>https://rsdoiel.github.io/blog/2020/07/07/Procedures-in-records.html</link>
      <description>
        <![CDATA[This is the tenth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.
        Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the 
        various rabbit holes I will inevitably fall into.
        
        In my last post I looked at how Oberon-07 supports the passing of procedures as parameters in a procedure. In this one I am looking at how we can
        include procedures as a part of an Oberon RECORD. 
        
        Let's modify our module name [Noises.Mod](Noises.Mod) to explore this.
        Copy "Noises.Mod" to "Creatures.Mod". Replace the "MODULE Noises;" line with
        "MODULE Creatures;" and the final "END Noises." statement with "END Creatures.".]]>
      </description>
      <source:markdown># Procedures in records

By R. S. Doiel, 2020-07-07

This is the tenth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.
Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the 
various rabbit holes I will inevitably fall into.

In my last post I looked at how Oberon-07 supports the passing of procedures as parameters in a procedure. In this one I am looking at how we can
include procedures as a part of an Oberon RECORD. 

Let&#39;s modify our module name [Noises.Mod](Noises.Mod) to explore this.
Copy &#34;Noises.Mod&#34; to &#34;Creatures.Mod&#34;. Replace the &#34;MODULE Noises;&#34; line with
&#34;MODULE Creatures;&#34; and the final &#34;END Noises.&#34; statement with &#34;END Creatures.&#34;.


~~~

    MODULE Creatures;
    
    (* rest of code here *)

    END Creatures.

~~~


The key to supporting records with procedures as record attributes is once again Oberon&#39;s type system.  The type `Noise` we created in the previous post can also be used to declare a record attribute similar to how we use this new type to pass the procedure. In this exploration will create a linked list of &#34;Creature&#34; types which include a &#34;MakeNoise&#34; attribute.

First let&#39;s define our &#34;Creature&#34; as a type as well as a 
`CreatureList`. Add the following under our `TYPE` 
definition in [Creatures.Mod](Creatures.Mod).


~~~

    Creature = POINTER TO CreatureDesc;
    CreatureDesc = RECORD
                     name : ARRAY 32 OF CHAR;
                     noises : Noises;
                   END;

~~~


Let&#39;s create a new `MakeCreature` procedure that will create
a populate a single `Creature` type record.


~~~

    PROCEDURE MakeCreature(name : ARRAY OF CHAR; noise : Noise; VAR creature : Creature);
    BEGIN
      NEW(creature);
      creature.name := name;
      creature.noise := noise;
    END MakeCreature;

~~~


Now lets modify `MakeNoise` to accept the `Creature` type RECORD
rather than a name and a noise procedure parameter.


~~~

    PROCEDURE MakeNoise(creature : Creator);
    BEGIN
      creature.noise(creature.name);
    END MakeNoise;

~~~


How does this all work?  The two &#34;Noise&#34; procedures 
&#34;BarkBark&#34; and &#34;ChirpChirp&#34; remain as in our original 
&#34;Noises&#34; module. But our new `MakeNoise` procedure
looks takes a `Creature` record rather than accepting a
name and procedure as parameters. This makes the code 
a little more concise as well as lets you evolve the
creature record type using an object oriented approach.

Our revised module should look like this.


~~~

    MODULE Noises;
      IMPORT Out;
    
    TYPE 
      Noise = PROCEDURE(who : ARRAY OF CHAR);

      Creature = RECORD
                   name : ARRAY 32 OF CHAR;
                   noises : Noises;
                 END;
    
    VAR
      dog, bird : Creature;

    PROCEDURE BarkBark(who : ARRAY OF CHAR);
    BEGIN
      Out.String(who);
      Out.String(&#34;: Bark, bark&#34;);Out.Ln();
    END BarkBark;
    
    PROCEDURE ChirpChirp(who : ARRAY OF CHAR);
    BEGIN
      Out.String(who);
      Out.String(&#34;: Chirp, chirp&#34;);Out.Ln();
    END ChirpChirp;
    
    PROCEDURE MakeNoise(creature : Creature);
    BEGIN
      (* Call noise with the animal name *)
      creature.noise(creature.name);
    END MakeNoise;

    PROCEDURE MakeCreature(name : ARRAY OF CHAR; noise : Noise; VAR creature : Creature);
    BEGIN
      NEW(creature);
      creature.name := name;
      creature.noise := noise;
    END MakeCreaturel
    
    BEGIN
      MakeCreature(&#34;Fido&#34;, BarkBark, dog);
      MakeCreature(&#34;Tweety&#34;, ChirpChirp, bird);
      MakeNoise(dog);
      MakeNoise(bird);
    END Noises.

~~~


Where to go from here? Think about evolving [Creatures](Creatures.Mod) so
that you can create a dynamic set of creatures that mix and match their
behaviors. Another idea would be to add a &#34;MutateCreature&#34; procedure
that would let you change the noise procedure to something new.


### Next and Previous 

+ Next [Portable Oberon-07](../../08/15/Portable-Oberon-07.html)
+ Previous [Procedures as parameters](../../06/20/Procedures-as-parameters.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/07/07/Procedures-in-records.md" length="5240" type="text/markdown" />
    </item>    <item>
      <title>Procedures as parameters</title>
      <link>https://rsdoiel.github.io/blog/2020/06/20/Procedures-as-parameters.html</link>
      <description>
        <![CDATA[This is the ninth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.
        Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the 
        various rabbit holes I will inevitably fall into.
        
        Oberon-07 supports the passing of procedures as parameters in a procedure. 
        Let's create a module name [Noises.Mod](Noises.Mod) to explore this.
        
        The key to supporting this is Oberon's type system.  We need to decide what our 
        generic procedure will look like first. In our case our procedures that will display 
        an animal noise will include the name of the animal speaking.  We'll call this type 
        of procedure "Noise". It'll accept an ARRAY OF CHAR for the name as a parameter 
        then use the standard Out module to display the animal name and noise they make.]]>
      </description>
      <source:markdown># Procedures as parameters

By R. S. Doiel, 2020-06-20

This is the ninth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series.
Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the 
various rabbit holes I will inevitably fall into.

Oberon-07 supports the passing of procedures as parameters in a procedure. 
Let&#39;s create a module name [Noises.Mod](Noises.Mod) to explore this.

The key to supporting this is Oberon&#39;s type system.  We need to decide what our 
generic procedure will look like first. In our case our procedures that will display 
an animal noise will include the name of the animal speaking.  We&#39;ll call this type 
of procedure &#34;Noise&#34;. It&#39;ll accept an ARRAY OF CHAR for the name as a parameter 
then use the standard Out module to display the animal name and noise they make.


~~~

    TYPE
      Noise = PROCEDURE (who : ARRAY OF CHAR);

~~~


The two &#34;Noise&#34; procedures will be &#34;BarkBark&#34; and &#34;ChirpChirp&#34;. They will
implement the same parameter signature as describe in the &#34;Noise&#34; type.


~~~

    PROCEDURE BarkBark(who : ARRAY OF CHAR);
    BEGIN
      Out.String(who);
      Out.String(&#34;: Bark, bark&#34;);Out.Ln();
    END BarkBark;

    PROCEDURE ChirpChirp(who : ARRAY OF CHAR);
    BEGIN
      Out.String(who);
      Out.String(&#34;: Chirp, chirp&#34;);Out.Ln();
    END ChirpChirp;

~~~


We&#39;ll also create a procedure, MakeNoise, that accepts the animal name and
our &#34;Noise&#34; procedure name and it&#39;ll call the &#34;Noise&#34; type procedure 
passing in the animal name.


~~~

    PROCEDURE MakeNoise(name : ARRAY OF CHAR; noise : Noise);
    BEGIN
      (* Call noise with the animal name *)
      noise(name);
    END MakeNoise;

~~~


If we invoke MakeNoise with our animal name and pass the name of the 
procedure we want to do the MakeNoise procedure will generate our
noise output. Here&#39; is what is looks like all together.


~~~

    MODULE Noises;
      IMPORT Out;
    
    TYPE 
      Noise = PROCEDURE(who : ARRAY OF CHAR);
    
    PROCEDURE BarkBark(who : ARRAY OF CHAR);
    BEGIN
      Out.String(who);
      Out.String(&#34;: Bark, bark&#34;);Out.Ln();
    END BarkBark;
    
    PROCEDURE ChirpChirp(who : ARRAY OF CHAR);
    BEGIN
      Out.String(who);
      Out.String(&#34;: Chirp, chirp&#34;);Out.Ln();
    END ChirpChirp;
    
    PROCEDURE MakeNoise(name : ARRAY OF CHAR; noise : Noise);
    BEGIN
      (* Call noise with the animal name *)
      noise(name);
    END MakeNoise;
    
    BEGIN
      MakeNoise(&#34;Fido&#34;, BarkBark);
      MakeNoise(&#34;Tweety&#34;, ChirpChirp);
      MakeNoise(&#34;Fido&#34;, ChirpChirp);
      MakeNoise(&#34;Tweety&#34;, BarkBark);
    END Noises.

~~~


Note when we pass the procedures we include their name **without** parenthesis.
Our type definition tells the compiler that the procedure can be a parameter,
any procedure with the same signature, e.g. `who : ARRAY OF CHAR` as the
only parameter will be treated as a &#34;Noise&#34; type procedures. 

### Next and Previous 

+ Next [Procedures in Records](../../07/07/Procedures-in-records.html)
+ Previous [Dynamic types](../../05/25/Dynamic-types.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/06/20/Procedures-as-parameters.md" length="4443" type="text/markdown" />
    </item>    <item>
      <title>Dynamic types</title>
      <link>https://rsdoiel.github.io/blog/2020/05/25/Dynamic-types.html</link>
      <description>
        <![CDATA[This is the eighth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html)
        series. Mostly Oberon documents my exploration of the Oberon Language, 
        Oberon System and the various rabbit holes I will inevitably fall into.
        
        ## Dynamic Types in Oberon
        
        Oberon-07 is a succinct systems language. It provides a minimal
        but useful set of basic static types. Relying on them addresses 
        many common programming needs. The Oberon compiler ensures 
        static types are efficiently allocated in memory. One of the 
        strengths of Oberon is this ability to extend the type system. 
        This means when the basic types fall short you can take 
        advantage of Oberon's type  extension features. This includes 
        creating dynamically allocated data structures. In Oberon-07 
        combining Oberon's `POINTER TO` and `RECORD` types allows us to
        create complex and dynamic data structures. 
        
        ...]]>
      </description>
      <source:markdown># Dynamic types

By R. S. Doiel, 2020-05-25

This is the eighth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html)
series. Mostly Oberon documents my exploration of the Oberon Language, 
Oberon System and the various rabbit holes I will inevitably fall into.

## Dynamic Types in Oberon

Oberon-07 is a succinct systems language. It provides a minimal
but useful set of basic static types. Relying on them addresses 
many common programming needs. The Oberon compiler ensures 
static types are efficiently allocated in memory. One of the 
strengths of Oberon is this ability to extend the type system. 
This means when the basic types fall short you can take 
advantage of Oberon&#39;s type  extension features. This includes 
creating dynamically allocated data structures. In Oberon-07 
combining Oberon&#39;s `POINTER TO` and `RECORD` types allows us to
create complex and dynamic data structures. 


## An example, dynamic strings 

Strings in Oberon-07 are typical declared as an `ARRAY OF CHAR` 
with a specific length. If the length of a string is not 
known a head of time this presents a challenge. One approach is 
to declare a long array but that would allocate allot of memory 
which may not get used. Another approach is to create a dynamic
data structure. An example is using a linked list of shorter 
`ARRAY OF CHAR`.  The small fixed strings can combine to 
represent much larger strings. When one fills up we add 
another. 

### Pointers and records, an Oberon idiom 

Our data model is a pointer to a record where the record 
contains an `ARRAY OF CHAR` and a pointer to the next record. 
A common idiom in Oberon for dynamic types is to declare a 
`POINTER TO` type and declare a `RECORD` type which contains
the `POINTER TO` type as an attribute.  If you see this idiom 
you are looking at some sort of dynamic data structure. The 
pointer type is usually named for the dynamic type you want 
work with and the record type is declared using the same name 
with a &#34;Desc&#34; suffix. In our case `DynamicString` will be the 
name of our `POINTER TO` type and our record type will be 
called `DynamicStringDesc` following the convention.  In our 
record structure we include a &#34;value&#34; to holding a short 
fixed length `ARRAY OF CHAR`  and a &#34;next&#34; to holding the 
pointer to our next record.

In our record the value is declared as a static type. We need
to know how long our &#34;short&#34; string should be? I.e. What length
is our `ARRAY OF CHAR`? It&#39;s a question of tuning. If it is too 
short we spend more time allocating new records, too long and 
we are wasting memory in each record. A way to make tuning a 
little simpler is to use a constant value to describe our array 
length. Then if we decide our array is too big 
or too small we can adjust the constant knowing that our record 
structure and the procedures that use that the length 
information will continue to work correctly. 

Let&#39;s take a look at actual code (NOTE: vSize is our constant value). 

~~~

    CONST
      vSize = 128; 
    
    TYPE
      DynamicString* = POINTER TO DynamicStringDesc;
      DynamicStringDesc* = RECORD 
        value : ARRAY vSize OF CHAR; 
        next : DymamicString; 
      END;

~~~

NOTE: Both `DynamicString` and `DynamicStringDesc` are defined 
using an `*`. These are public and will be available 
to other modules.  Inside our record `DynamicStringDesc` we 
have two private to our module attributes, `.value` and 
`.next`. They are private so that we can change our 
implementation in the future without requiring changes in 
modules that use our dynamic strings. Likewise our constant `vSize`
is private as that is an internal implementation detail. This
practice is called information hiding.

NOTE: The asterisk in Oberon decorates procedures, types, variables
and constants that are &#34;public&#34; to other modules.

NOTE: Variables are always exported read only.

NOTE: With information hiding some details of implementation allow us 
to keep a clean division between implementation inside the module and how
that implementation might be used. With out information hiding we often
have &#34;leaky&#34; abstractions that become brittle and hard to maintain and
rely on.



## Working with DynamicString

Our type definitions describe to the compiler how to layout our 
data in memory. The type system in Oberon-07 also ensures that 
access to that memory is restricted to assignments, operations 
and procedures compatible with that type. To be useful from 
other modules we need a few procedures to help work with
this new data type. What follows is a minimal set needed to be 
useful.

### `New*(VAR str : DynamicString)`

`New` will initialize a DynamicString object setting `.value` to 
an empty string. 


~~~

  PROCEDURE New*(VAR str : DynamicString);
  BEGIN NEW(str);
    str.value := &#34;&#34;; 
    str.next := NIL;
  END New;

~~~


### `Set*(VAR str : DynamicString; source : ARRAY OF CHAR)` 

`Set` copies an `ARRAY OF CHAR` into an existing DynamicString. 
This requires that we add and link additional records if the 
`source` is longer than our current dynamic string. Set is a 
bridge procedure between an existing datatype, `ARRAY OF CHAR` 
and our new data type, `DynamicString`.


~~~

  PROCEDURE Set*(VAR str : DynamicString; source : ARRAY OF CHAR); 
    VAR cur, next : DynamicString; tmp : ARRAY vSize OF CHAR; 
        i, l : INTEGER;
  BEGIN cur := str; cur.value := &#34;&#34;;
    l := Strings.Length(source);
    i := 0; 
    WHILE i &#60; l DO
      Strings.Extract(source, i, i + vSize, tmp);
      Strings.Append(tmp, cur.value);
      i := i + Strings.Length(tmp);
      IF (i &#60; l) THEN
        IF cur.next = NIL THEN
          New(next); cur.next := next;
        END;
        cur := cur.next;
      END; 
    END;
  END Set;

~~~

### `ToCharArray*(str : DynamicString; VAR dest : ARRAY OF CHAR; VAR ok : BOOLEAN)`

`ToCharArray` copies the contents of our dynamic string into an array 
of char setting `ok` to TRUE on success or FALSE if truncated. 
Like `Set*` it is a bridge procedure to let us move data output 
our new dynamic string type.


~~~

  PROCEDURE ToCharArray*(str : DynamicString; 
                         VAR dest : ARRAY OF CHAR; 
                         VAR ok : BOOLEAN);
    VAR cur : DynamicString; i : INTEGER;
  BEGIN 
    ok := FALSE;
    cur := str; i := 0;
    WHILE cur # NIL DO
      i := i + Strings.Length(cur.value);
      Strings.Append(cur.value, dest);
      cur := cur.next;
    END;
    ok := (i = Strings.Length(dest));
  END ToCharArray;

~~~

Two additional procedures will likely be needed-- `Append` and 
`AppendCharArray`. This first one is trivial, if we want to add 
one dynamic string onto another all we need to do is link the 
last record of the first and point it to a copy of the second string we&#39;re appending.


### `Append*(extra : DynamicString; VAR dest : DynamicString);`

`Append` adds the `extra` dynamic string to `dest` dynamic string. Our 
&#34;input&#34; is `extra` and our output is a modified dynamic string 
named `dest`. This parameter order mimics the standard 
`Strings` module&#39;s `Append`.

NOTE: Oberon idiom is often input values, modified value and 
result values. Modified and result values are declared in the parameter
definition using `VAR`.

Algorithm:

1. Move to the end of `dest` dynamic string
2. Create a new record at `cur.next`.
3. Copy `extra.value` info.value `cur.next.value`
4. Advance `extra` and `cur`, repeating steps 2 to 4 as needed.

Implemented procedure.

~~~

  PROCEDURE Append*(extra: DynamicString; VAR dest : DynamicString);
    VAR cur : DynamicString;  
  BEGIN
    (* Move to the end of the dest DynamicString *)
    cur := dest;
    WHILE cur.next # NIL DO cur := cur.next; END;
    (* Starting initial pointer of `extra` copy its records
       input new records created in `cur`. *)
    WHILE extra # NIL DO
      (* Create a new record *)
      NEW(cur.next);
      cur.next.value := &#34;&#34;;
      cur.next.next := NIL;
      (* Copy extra.value into new record *)
      Strings.Extract(extra.value, 0, vSize, cur.next.value);
      (* Advance to next record for both cur and extra *)
      extra := extra.next;
      cur := cur.next;
    END;
  END Append;

~~~

A second procedure for appending an `ARRAY OF CHAR` also 
becomes trivial. First convert the `ARRAY OF CHAR` to a dynamic 
string then append it with the previous procedure.

### `AppendCharArray*(src : ARRAY OF CHAR; VAR str : DynamicString);`

This procedure appends an ARRAY OF CHAR to an existing dynamic string.

~~~

  PROCEDURE AppendCharArray*(extra: ARRAY OF CHAR; VAR dest : DynamicString);
    VAR extraStr : DynamicString;    
  BEGIN
    (* Convert our extra ARRAY OF CHAR into a DynamicString *)
    New(extraStr); Set(extraStr, extra);
    (* Now we can append. *)
    Append(extraStr, dest);
  END AppendCharArray;

~~~

At some point we will want to know the length of our dynamic string.

### `Length(str : DynamicString) : INTEGER`

Our `Length` needs to go through our linked list and total up 
the length of each value. We will use a variable called `cur` 
to point at the current record and add up our total length as 
we walk through the list.

~~~

  PROCEDURE Length*( source : DynamicString) : INTEGER;
    VAR cur : DynamicString; total : INTEGER;
  BEGIN
    total := 0;
    cur := source;
    WHILE cur # NIL DO
      total := total + Strings.Length(cur.value);
      cur := cur.next;
    END; 
    RETURN total
  END Length;

~~~

## Extending DynamicStrings module

With these few procedures we have a basic means of working with 
dynamic strings. Moving beyond this we can look at the standard 
Oberon `Strings` module for inspiration.  If we use similar 
procedure signatures we can create a drop in replacement 
for `Strings` with `DynamicStrings`.

NOTE: Procedure signatures refer to procedures type along 
with the order and types of parameters. A quick review of the 
procedure signatures for the standard module [Strings](https://miasap.se/obnc/obncdoc/basic/Strings.def.html) is 
provided by the [OBNC](https://miasap.se/obnc) compiler docs. 

Let&#39;s look at recreating `Insert` as a potential guide to
a more fully featured [&#34;DynamicStrings.Mod&#34;](DynamicStrings.Mod). 
In our `Insert` we modify the procedure signature so the 
source and destinations are dynamic strings.


### `Insert(source : DynamicString; pos : INTEGER; VAR dest : DynamicString)`

The `Insert` procedure inserts a `source` dynamic string at the 
position provided into our `dest` dynamic string. We are implementing
the same signature  for our `DynamicStrings.Insert()` as 
`Strings.Insert()`. Only the parameters for source and destination
are changed to `DynamicString`.

Internally our procedure for `Insert` is a more complicated than
the ones we&#39;ve written so far. It needs to do all the housing 
keeping for making sure we add the right content in the correct
spot.  The general idea is to find the record holding the split 
point. Split that record into two records. The first retains 
the characters before the insert position. The second holds the 
characters after the insert position and points to next record 
in the dynamic string. Once the split is accomplished it then 
is a matter of linking everything up. The record before the 
insert position is set to point at the dynamic string to be 
inserted, the inserted dynamic string is set to point at the 
record that contained the rest of the characters after the 
split.

It is easy to extract a sub-string from an `ARRAY OF CHAR` 
using the standard `Strings` module.  We can store the characters
in the `.value` of the record after the split in a temporary 
`ARRAY OF CHAR`.  The temporary `ARRAY OF CHAR` can be used to 
create a new dynamic string record which will be linked to the 
rest of our destination dynamic string. The record which held 
the characters before the insert position needs to be truncated 
and it needs to be linked to the dynamic string we want to 
insert. NOTE: This will leave a small amount of unused 
memory.

NOTE: If conserving memory is critical then re-packing the 
dynamic string could be implemented as another procedure. The 
cost would be complexity and time to shift characters between 
later records and earlier ones replacing excess NULL values.

We need to find the record where the split will occur. In the 
record to be split we need to calculate a relative 
split point. We then can copy the excess characters in that 
split record to a new record and truncate the `.value`&#39;s 
`ARRAY OF CHAR` to create our split point. Truncating is easy 
in that we replace the CHAR in the `.values` that are not 
needed with a NULL character. We can do that with a 
simple loop. Likewise calculating the relative insertion 
position can be done by taking the modulo of the `vSize` of 
`.value`.

NOTE: In Oberon stings are terminated with a NULL 
character. A NULL character holds the ASCII value `0X`.

Our algorithm:

1. Set `cur` to point to the start of our destination dynamic string
2. Move `cur` to the record in the link list where the insertion will take place
3. Calculate the relative split point in `cur.value`
4. Copy the characters in `cur.value` from relative split point to end of `.value` into a temporary `ARRAY OF CHAR`
5. Make a new record, `rest`, using the temporary `ARRAY OF CHAR` and update the value of `.next` to match that of `cur.next`
6. Truncate the record (cur) at the relative split point
7. Set `cur.next` to point to our `extra` dynamic string.
8. Move to the end of extra with `cur`
9. Set the `cur.next` to point at `rest`

Our procedure:

~~~

  PROCEDURE Insert*(extra : DynamicString; 
                    pos : INTEGER; 
                    VAR dest : DynamicString);
    VAR cur, rest : DynamicString;
        tmp : ARRAY vSize OF CHAR;
        i, splitPos : INTEGER; continue : BOOLEAN;
  BEGIN
    (* 1. Set `cur` to the start of our `dest` dynamic string *)
    cur := dest;

    (* 2. Move to the record which holds the split point *)
    i := 0;
    continue := (i &#60; pos);
    WHILE continue DO
      i := i + Strings.Length(cur.value);
      continue := (i &#60; pos);
      IF continue &#38; (cur.next # NIL) THEN
        cur := cur.next;
      ELSE
        continue := FALSE;
      END;
    END;

    (* 3. Copy the characters in `cur.value` from relative
          split point to end of `.value` into a 
          temporary `ARRAY OF CHAR` *)
    splitPos := pos MOD vSize;
    Strings.Extract(cur.value, splitPos,
                    Strings.Length(cur.value), tmp);

    (* 4. Make a new record, `rest`, using the temporary 
          `ARRAY OF CHAR` and update the value of `.next` to
          match that of `cur.next` *)
    New(rest); Set(rest, tmp);
    rest.next := cur.next;

    (* 5. Truncate `cur.value` at the relative split point *)
    i := splitPos;
    WHILE i &#60; LEN(cur.value) DO
      cur.value[i] := 0X;
      INC(i);
    END;

    (* 6. Set `cur.next` to point to our `extra`
          dynamic string. *)
    cur.next := extra;

    (* 7. Move to the end of extra with `cur` *)
    WHILE cur.next # NIL DO cur := cur.next; END;

    (* 8. Set the `cur.next` to point at `rest` *)
    cur.next := rest;
  END Insert;

~~~

While our `Insert` is the longest procedure so far the steps 
are mostly simple. Additionally we can easily extend this to 
support inserting a more traditional `ARRAY OF CHAR` using our
previously established design pattern of converting a basic type
into our dynamic type before calling the dynamic version of the
function.

~~~

  PROCEDURE InsertCharArray*(source : ARRAY OF CHAR; 
                             pos : INTEGER; 
                             VAR dest : DynamicString);
    VAR extra : DynamicString;
  BEGIN
    New(extra); Set(extra, source);
    Insert(extra, pos, dest);
  END InsertCharArray;

~~~

## Where to go next

It is possible to extend our &#34;DynamicStrings.Mod&#34; into a drop 
in replacement for the standard `Strings`.  I&#39;ve included a 
skeleton of that module as links at the end of this article 
with stubs for the missing implementations such as `Extract`, 
`Replace`, `Pos`, and `Cap`.  I&#39;ve also included a 
&#34;DynamicStringsTest.Mod&#34; for demonstrating how it works.

The procedure I suggest is to mirror `Strings` replacing the 
parameters that are `ARRAY OF CHAR` with `DynamicString`. It 
will be helpful to include some bridging procedures that accept 
`ARRAY OF CHAR` as inputs too. These will use similar names 
with a suffix of `CharArray`.

## Parameter conventions and order

Oberon is highly composable. The trick to creating a drop in 
replacement module is use the same parameter signatures so 
you only need to make minor changes like updating the `IMPORT` 
statement and using a module alias to map the old module to the
new one.  The parameter signatures in `Strings` follow a 
convention you&#39;ll see in other Oberon modules. The parameter
order is based on the &#34;inputs&#34;, &#34;modify parameters&#34;, and 
&#34;output parameters&#34;. Inputs are non-`VAR` parameters. The 
remaining are `VAR` parameters. I think of &#34;modify parameters&#34; 
as those objects who reflect side effects. I think of &#34;output&#34; 
as values that in other languages would be returned by 
functions.  This is only a convention. A variation I&#39;ve 
read in other Oberon modules is &#34;object&#34;, &#34;inputs&#34;, &#34;outputs&#34;. 
&#34;object&#34; and &#34;outputs&#34; are `VAR` parameters and &#34;inputs&#34; are 
not. This ordering makes sense when we think of records as 
holding an object. In both cases ordering is a convention 
and not enforced by the language.  Convention and consistency is 
helpful but readability is the most important.  Oberon is a 
readable language. It does not reward obfuscation. Readability is 
a great virtue in a programming language. When creating your own 
modules choose readability based on the concepts you want to
emphasize in the module (e.g. procedural, object oriented).

## The modules so far

You can read the full source for the module discussed along
with a test module in the links that follow.

+ [DynamicStrings.Mod](DynamicStrings.Mod)
+ [DynamicStringsTest.Mod](DynamicStringsTest.Mod)


### Next and Previous 

+ Next [Procedures as parameters](../../06/20/Procedures-as-parameters.html)
+ Previous [Oberon-07 and the file system](../09/Oberon-07-and-the-filesystem.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/05/25/Dynamic-types.md" length="19655" type="text/markdown" />
    </item>    <item>
      <title>Oberon-07 and the file system</title>
      <link>https://rsdoiel.github.io/blog/2020/05/09/Oberon-07-and-the-filesystem.html</link>
      <description>
        <![CDATA[This is the seventh post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.]]>
      </description>
      <source:markdown># Oberon-07 and the file system

By R. S. Doiel, 2020-05-09 (updated: 2021-10-29)

This is the seventh post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.

## Working with files in Oberon-07

In a POSIX system we often talk of opening files,
writing and reading files and close files. The Oberon
language reflects a more Oberon System point of view.

The Oberon System generally avoids modality in favor
of action. Modality is where a context must be set
before a set of actions are possible. The `vi` 
text editor is a &#34;modal&#34; editor. You are in either
edit (typing) mode or command mode. At the function
level POSIX&#39;s `open()`, is also modal. You can 
open a file for reading, open a file for writing,
you can open a file for appending, etc. The Oberon
language and base modules avoids modality.

The Oberon System is highly interactive but
has a very different idea about code, data and computer
resources. In POSIX the basic unit of code is a program
and the basic unit of execution is a program. In Oberon
the basic unit of code is the module and the basic unit
of execution is the procedure.  Modules are brought into 
memory and persist. As a result it is common in 
the Oberon System to need to have file representations 
that can persist across procedure calls. It provides
a set of abstractions that are a little bit like views
and cursors found in database systems. In taking
this approach Oberon language eschews modality at the
procedure level. 

NOTE: Modules can be explicitly unload otherwise they persist until the computer is turned off

## Key Oberon Concepts

The following are exported in the `Files` module.

File
: is a handle to the representation of a file, a File and Rider form a view into a file.

Rider
: similar to a database cursor, it is the mechanism that lets you navigate in a file

New
: Creates a new file (in memory but not on disc).

Registration
: Associates a file handle created with New with the file system. A file must be registered to persist in the file system.

Old
: Opens an existing file for use.

Set
: Set the position of a rider in a file

Pos
: Gets the position of a rider in a file

Close
: Writes out unwritten buffers in file to disc, file handle is still value as is the rider.

Purge
: Sets a file&#39;s length to zero.

Delete
: Unregister the filename with the file system.

In the Oberon Systems a file can be &#34;opened&#34; many
times with only one copy maintained in memory. This allows
efficient operations across a module&#39;s procedures.
Likewise a file can have one or more Riders associated with
it. Each rider can move through the file independently operating on
the common in memory file. If a file is created with `New` but
not registered it can be treated like an in-memory temp file.
Closing a file writes its buffers but the file remains accessible
through it handle and riders. If a file is not modified it
doesn&#39;t need to be closed.

In POSIX we generally want to explicitly close the file when
we&#39;re through. In the Oberon language we only need to close
a file if we&#39;ve modified it.

The behavior of files and riders in Oberon creates interesting
nuances around deleting files.  The Delete operation can in
principle happen multiple times before the file is deleted on
disc.  That is because the file handles and riders may still
be operating on it.  To know when a file is finally deleted 
when `Delete` procedure is called it includes a results
parameter. When that value is set to zero by the `Delete`
procedure you know your file has been deleted.

The `Files` module provides a number of methods
to read and write basic Oberon types. These use the rider
rather than the file handle. Calling them automatically
updates the riders position. The procedures themselves
map to what we&#39;ve seen in the modules `In` and `Out`.  
There are a few additional commands for file system house 
keeping such as `Length`, `GetDate`, `Base`.
See the OBNC documentation for the `Files` module for
details &#60;https://miasap.se/obnc/obncdoc/basic/Files.def.html&#62;.

In the following examples we&#39;ll be using the `Files`
module to create, update and delete a file called 
`HelloWorld.txt`.

### Creating a file

The recipe we want to follow for creating a file is
New (creates an empty file in memory), Register
(associations the filename with the file system), 
Set the rider position, with the rider write our
content and with the file call close because we&#39;ve
have changed the file.

Like our origin `SayingHi` we&#39;ll demonstrate this code
in a new module called `TypingHi.Mod`. Below is
a procedure called `WriteHelloWorld`. It shows how
to create, write and close the new file called
&#34;HelloWorld.txt&#34;.


~~~

  PROCEDURE WriteHelloWorld;
    VAR
      (* Define a file handle *)
      f : Files.File;
      (* Define a file rider *)
      r : Files.Rider;
  BEGIN
    (* Create our file, New returns a file handle *)
    f := Files.New(&#34;HelloWorld.txt&#34;);
    (* Register our file with the file system *)
    Files.Register(f);
    (* Set the position of the rider to the beginning *)
    Files.Set(r, f, 0);
    (* Use the rider to write out &#34;Hello World!&#34; *)
    Files.WriteString(r, &#34;Hello World!&#34;);
    (* Write a end of line *)
    Files.Write(r, 10);
    (* Close our modified file *)
    Files.Close(f);
  END WriteHelloWorld;

~~~


#### Receipt in review

+ New, creates our file
+ Register, associates the file handle with the file system 
+ Set initializes the rider&#39;s position
+ WriteString, writes our &#34;Hello World!&#34; to the file
+ Close, closes the file, flushing unwritten content to disc


### Working with an existing file

If we&#39;re working with an existing file we swap `New` for 
a procedure named `Old`. We don&#39;t need to register the
file because it already exists.  We still need to set
our rider and we want to read back the string we previously wrote.
We don&#39;t need to close it because we haven&#39;t
modified it. To demonstrate a new procedure is added to
our module called `ReadHelloWorld`.


~~~

  PROCEDURE ReadHelloWorld;
    VAR
      f : Files.File;
      r : Files.Rider;
      (* We need a string to store what we have read *)
      src : ARRAY 256 OF CHAR;
  BEGIN
    (* Create our file, New returns a file handle *)
    f := Files.Old(&#34;HelloWorld.txt&#34;);
    (* Set the position of the rider to the beginning *)
    Files.Set(r, f, 0);
    (* Use the rider to write out &#34;Hello World!&#34; *)
    Files.ReadString(r, src);
    (* Check the value we read, if it is not the name, 
       halt the program with an error *)
    ASSERT(src = &#34;Hello World!&#34;);
  END ReadHelloWorld;

~~~


#### Receipt in review

+ Old, gets returns a file handle for an existing file
+ Set initializes the rider&#39;s position
+ ReadString, read our &#34;Hello World!&#34; string from the file
+ Check the value we read with an ASSERT

### Removing a file

Deleting the file only requires knowing the name of the file.
Like in `ReadHelloWorld` we&#39;ll use the built-in ASSERT
procedure to check if the file was successfully removed.


~~~

  PROCEDURE DeleteHelloWorld;
    VAR
      result : INTEGER;
  BEGIN
    (* Delete our file *)
    Files.Delete(&#34;HelloWorld.txt&#34;, result);
    (* Check our result, if not zero then halt program with error *)
    ASSERT(result = 0);
  END DeleteHelloWorld;

~~~


#### Receipt in review

+ Delete the file setting a result value
+ Check the value with ASSERT to make sure it worked

## Putting it all together.

Here is the full listing of our module.


~~~

    MODULE TypingHi;
      IMPORT Files;
    
      PROCEDURE WriteHelloWorld;
        VAR
          (* Define a file handle *)
          f : Files.File;
          (* Define a file rider *)
          r : Files.Rider;
      BEGIN
        (* Create our file, New returns a file handle *)
        f := Files.New(&#34;HelloWorld.txt&#34;);
        (* Register our file with the file system *)
        Files.Register(f);
        (* Set the position of the rider to the beginning *)
        Files.Set(r, f, 0);
        (* Use the rider to write out &#34;Hello World!&#34; *)
        Files.WriteString(r, &#34;Hello World!&#34;);
        (* Write a end of line *)
        Files.Write(r, 10);
        (* Close our modified file *)
        Files.Close(f);
      END WriteHelloWorld;
    
      PROCEDURE ReadHelloWorld;
        VAR
          f : Files.File;
          r : Files.Rider;
          (* We need a string to store what we have read *)
          src : ARRAY 256 OF CHAR;
      BEGIN
        (* Create our file, New returns a file handle *)
        f := Files.Old(&#34;HelloWorld.txt&#34;);
        (* Set the position of the rider to the beginning *)
        Files.Set(r, f, 0);
        (* Use the rider to write out &#34;Hello World!&#34; *)
        Files.ReadString(r, src);
        (* Check the value we read, if it is not the name, 
           halt the program with an error *)
        ASSERT(src = &#34;Hello World!&#34;);
      END ReadHelloWorld;
    
      PROCEDURE DeleteHelloWorld;
        VAR
          result : INTEGER;
      BEGIN
        (* Delete our file *)
        Files.Delete(&#34;HelloWorld.txt&#34;, result);
        (* Check our result, if not zero then halt program with error *)
        ASSERT(result = 0);
      END DeleteHelloWorld;
    
    BEGIN
        WriteHelloWorld();
        ReadHelloWorld();
        DeleteHelloWorld();
    END TypingHi.

~~~

## Post Script, 2021-10-29

On October 29, 2021 I had an email conversation with Jules. It pointed out a problem in this implementation of Hello World.  I had incorrectly coded the end of line with `Files.WriteString(r, 0AX);` this is wrong.  At best it should have been `Files.Write(r, 10);`. There remains an issues with `Files.WriteString(&#34;Hello World!&#34;);`. The Oakwood module `Files` defines &#34;WriteString&#34; to include the trailing NULL character. This will be output in the file. It all depends on how close to the Oakwood specification that your compiler implements the `Files` module.



### Next and Previous

+ Next [Dynamic Types](../25/Dynamic-types.html)
+ Previous [Compiling OBNC on macOS](../06/Compiling-OBNC-on-macOS.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/05/09/Oberon-07-and-the-filesystem.md" length="10898" type="text/markdown" />
    </item>    <item>
      <title>Compiling OBNC on macOS</title>
      <link>https://rsdoiel.github.io/blog/2020/05/06/Compiling-OBNC-on-macOS.html</link>
      <description>
        <![CDATA[This is the sixth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.
        
        Compiling OBNC v0.16.1 on macOS (10.13.6) using MacPorts (2.6.2) 
        is straight forward if you have the required dependencies and 
        environment setup up. Below are my notes to get everything working.
        
        ...]]>
      </description>
      <source:markdown>Compiling OBNC on macOS 
=======================

By R. S. Doiel, 2020-05-06

This is the sixth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.

Compiling OBNC v0.16.1 on macOS (10.13.6) using MacPorts (2.6.2) 
is straight forward if you have the required dependencies and 
environment setup up. Below are my notes to get everything working.

## Prerequisites

+ OBNC v0.16.1
+ SDL v1.2
+ Boehm-Demers-Weiser GC
+ A C compiler and linker (OBNC uses this to generate machine specific code)

### SDL 1.2

MacPorts has libsdl 1.2 available as a package called &#34;libsdl&#34;
(not surprisingly). There are other versions of the SDL available
in ports but this is the one we&#39;re using.


~~~

 sudo port install libsdl

~~~


### The Boehm-Demers-Weiser GC

You need to install the Boehm-Demers-Weiser GC installed. Using
MacPorts it is almost as easy as installing under Debian. The
package is less obviously named than under Debian. The package
you want is &#34;beohmgc&#34;.


~~~

  sudo port install boehmgc

~~~


More info on the GC.

+ [The Boehm-Demers-Weiser GC](https://www.hboehm.info/gc/)
+ [MacPorts page](https://ports.macports.org/port/boehmgc/summary)

### C compiler and linker

XCode is provides a C compiler and linker. That is what is installed on my
machine. It can be a bit of a pain at times with obscure errors, particularly with regards to the linker. Your milleage may very. I know you can
install other C compilers (e.g. Clang) but I haven&#39;t tried them yet.

## Setting up my environment

You need to update your CC variables to find the header and
shared library files for compilation of obnc with `build`. (I added
these to my `.bash_profile`). New Macs ships with zsh and
your settings might be in a different location.MacPorts puts 
its libraries under the `/opt/local` directory.


~~~

  export C_INCLUDE_PATH=&#34;/usr/include:/usr/local/include:/opt/local/include&#34;
  export LIBRARY_PATH=&#34;/usr/lib:/usr/local/lib:/opt/local/lib&#34;
  export LD_LIBRARY_PATH=&#34;/usr/lib:/usr/local/lib:/opt/local/lib&#34;

~~~


## OBNC environment variables

This follows&#39; Karl&#39;s docs. Additionally if you install OBNC extlib so
you can do POSIX shell programs you&#39;ll need to set your
`OBNC_IMPORT_PATH` environment.  An example of including a default
install location and local home directory location.


~~~

  export OBNC_IMPORT_PATH=&#34;/usr/local/lib/obnc:$HOME/lib/obnc&#34;

~~~


### Next and Previous

+ Next [Oberon-07 and the file system](../09/Oberon-07-and-the-filesystem.html)
+ Previous [Combining Oberon-07 and C](../01/Combining-Oberon-and-C.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/05/06/Compiling-OBNC-on-macOS.md" length="3649" type="text/markdown" />
    </item>    <item>
      <title>Combining Oberon-07 and C with OBNC</title>
      <link>https://rsdoiel.github.io/blog/2020/05/01/Combining-Oberon-and-C.html</link>
      <description>
        <![CDATA[This is the fifth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html)
        series. Mostly Oberon documents my exploration of the Oberon
        Language, Oberon System and the various rabbit holes I will
        inevitably fall into.
        
        In my day job I write allot of code in Go and
        orchestration code in Python.  It's nice having
        the convenience of combining code written one
        language with an another.  You can do the same
        with [OBNC](https://miasap.se/obnc/).  The OBNC
        compiler supports inclusion of C code in a
        straight forward manner. In fact Karl's compiler
        will generate the C file for you!
        
        In learning how to combine C code and Oberon-07
        I started by reviewing Karl's [manual page](https://miasap.se/obnc/man/obnc.txt).
        The bottom part of that manual page describes
        the steps I will repeat below. The description
        sounds more complicated but when you walk through
        the steps it turns out to be pretty easy.
        
        ...]]>
      </description>
      <source:markdown># Combining Oberon-07 and C with OBNC

By R. S. Doiel, 2020-05-01

This is the fifth post in the [Mostly Oberon](../../04/11/Mostly-Oberon.html)
series. Mostly Oberon documents my exploration of the Oberon
Language, Oberon System and the various rabbit holes I will
inevitably fall into.

In my day job I write allot of code in Go and
orchestration code in Python.  It&#39;s nice having
the convenience of combining code written one
language with an another.  You can do the same
with [OBNC](https://miasap.se/obnc/).  The OBNC
compiler supports inclusion of C code in a
straight forward manner. In fact Karl&#39;s compiler
will generate the C file for you!

In learning how to combine C code and Oberon-07
I started by reviewing Karl&#39;s [manual page](https://miasap.se/obnc/man/obnc.txt).
The bottom part of that manual page describes
the steps I will repeat below. The description
sounds more complicated but when you walk through
the steps it turns out to be pretty easy.

## Basic Process

Creating a C extension for use with OBNC is very
straight forward.

1. Create a Oberon module with empty exported procedures
2. Create a Oberon test module that uses your module
3. Compile your test module with OBNC
4. Copy the generated module `.c` file to the same directory as your Oberon module source
5. Edit the skeleton `.c`,  re-compile and test

Five steps may sound complicated but in practice is
straight forward.

## Fmt, an example

In my demonstration of Karl&#39;s instructions I will be
creating a module named `Fmt` that includes two
procedures `Int()` and `Real()` that let you use
a C-style format string to format an INTEGER
or REAL as an ARRAY OF CHAR. We retain the idiomatic
way Oberon works with types but allow a little more
flexibility in how the numbers are converted and
rendered as strings.

### Step 1

Create [Fmt.Mod](Fmt.Mod) defining two exported procedures
`Int*()` and `Real*()`. The procedures body should be
empty. Karl&#39;s practice is to use exported comments to
explain the procedures.


~~~ {.oberon}

    MODULE Fmt;

    	PROCEDURE Int*(value : INTEGER; fmt: ARRAY OF CHAR;
                       VAR dest : ARRAY OF CHAR);
    	END Int;

    	PROCEDURE Real*(value : REAL; fmt: ARRAY OF CHAR;
                        VAR dest : ARRAY OF CHAR);
    	END Real;

    BEGIN
    END Fmt.

~~~


### Step 2

Create a test module, [FmtTest.Mod](FmtTest.Mod), for
[Fmt.Mod](Fmt.Mod).


~~~ {.oberon}

    MODULE FmtTest;
      IMPORT Out, Fmt;

    PROCEDURE TestInt(): BOOLEAN;
      VAR
        fmtString : ARRAY 24 OF CHAR;
        dest : ARRAY 128 OF CHAR;
        i : INTEGER;
    BEGIN
        i := 42;
        fmtString := &#34;%d&#34;;
        Fmt.Int(i, fmtString, dest);
        Out.String(dest);Out.Ln;
        RETURN TRUE
    END TestInt;

    PROCEDURE TestReal(): BOOLEAN;
      VAR
        fmtString : ARRAY 24 OF CHAR;
        dest : ARRAY 128 OF CHAR;
        r : REAL;
    BEGIN
        r := 3.145;
        fmtString := &#34;%d&#34;;
        Fmt.Real(r, fmtString, dest);
        Out.String(dest);Out.Ln;
        RETURN TRUE
    END TestReal;

    BEGIN
      ASSERT(TestInt());
      ASSERT(TestReal());
      Out.String(&#34;Success!&#34;);Out.Ln;
    END FmtTest.

~~~


### Step 3

Generate a new [Fmt.c](Fmt.c) by using the
OBNC compiler.


~~~ {.shell}

    obnc FmtTest.Mod
    mv .obnc/Fmt.c ./

~~~


the file `.obnc/Fmt.c` is your C template file. Copy it
to the directory where Fmt.Mod is.

### Step 4

Update the skeleton `Fmt.c` with the necessary C code.
Here&#39;s what OBNC generated version.


~~~ {.c}

    /*GENERATED BY OBNC 0.16.1*/

    #include &#34;Fmt.h&#34;
    #include &#60;obnc/OBNC.h&#62;

    #define OBERON_SOURCE_FILENAME &#34;Fmt.Mod&#34;

    void Fmt__Int_(OBNC_INTEGER value_, const char fmt_[], 
                   OBNC_INTEGER fmt_len, char dest_[], 
                   OBNC_INTEGER dest_len)
    {
    }


    void Fmt__Real_(OBNC_REAL value_, const char fmt_[],
                    OBNC_INTEGER fmt_len, char dest_[],
                    OBNC_INTEGER dest_len)
    {
    }


    void Fmt__Init(void)
    {
    }

~~~


Here&#39;s the skeleton revised with do what we need to be done.


~~~ {.c}

    #include &#34;.obnc/Fmt.h&#34;
    #include &#60;obnc/OBNC.h&#62;
    #include &#60;stdio.h&#62;

    #define OBERON_SOURCE_FILENAME &#34;Fmt.Mod&#34;

    void Fmt__Int_(OBNC_INTEGER value_, 
                   const char fmt_[], OBNC_INTEGER fmt_len,
                   char dest_[], OBNC_INTEGER dest_len)
    {
        sprintf(dest_, fmt_, value_);
    }


    void Fmt__Real_(OBNC_REAL value_, const char fmt_[],
                    OBNC_INTEGER fmt_len, char dest_[],
                    OBNC_INTEGER dest_len)
    {
        sprintf(dest_, fmt_, value_);
    }


    void Fmt__Init(void)
    {
    }

~~~


NOTE: You need to change the path for the `Fmt.h` file reference.
I also add the `stdio.h` include so I have access to the C
function I wish to use. Also notice how OBNC the signature
for the functions use the `_` character to identify mapped values
as well as the char arrays being provided with a length parameter.
If you are doing more extensive string work you&#39;ll want to take
advantage of these additional parameters so insure that the
as strings are terminated properly for Oberon&#39;s reuse.


### Step 5

Recompile and test.


~~~ {.shell}

    obnc FmtTest.Mod
    ./FmtTest

~~~


### Next and Previous

+ Next [Compiling OBNC on macOS](../06/Compiling-OBNC-on-macOS.html)
+ Previously [Oberon Loops and Conditions](../../04/19/Mostly-Oberon-Loops-and-Conditions.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/05/01/Combining-Oberon-and-C.md" length="6929" type="text/markdown" />
    </item>    <item>
      <title>Oberon Loops and Conditions</title>
      <link>https://rsdoiel.github.io/blog/2020/04/19/Mostly-Oberon-Loops-and-Conditions.html</link>
      <description>
        <![CDATA[This is the four post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.
        
        ## Data Flow
        
        Oberon is a small systems language and while it is minimalist.
        It provides you with the necessary primitives to get things done.
        I've touched on code organization, basic types and basic type
        extensions in the previous articles.  I have shown the basic
        control statements but have not talked about them yet.
        
        Oberon offers four basic control statements. 
        
        ...]]>
      </description>
      <source:markdown>Oberon Loops and Conditions
===========================

By R. S. Doiel, 2020-04-19

This is the four post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit holes I will inevitably fall into.

## Data Flow

Oberon is a small systems language and while it is minimalist.
It provides you with the necessary primitives to get things done.
I&#39;ve touched on code organization, basic types and basic type
extensions in the previous articles.  I have shown the basic
control statements but have not talked about them yet.

Oberon offers four basic control statements. 

IF, ELSIF, ELSE
: Basic condition test and execution

ASSERT
: A mechanism to trigger a program halt

WHILE DO, ELSIF DO
: The Loop structure in the language (aside from recursive procedures)

FOR TO, FOR TO BY
: A counting Loop where incrementing a counter by an integer value (e.g. 1 or by a specified constant).

## IF, ELSIF, ELSE

The first two provide for conditional statements of the form
if a condition is true do something. Almost ever computer language
has some form of a conditional express and the Oberon IF, ELSIF,
ELSE typical of what you find is more computer languages today.
Both ELSIF and ELSE are optional.

```Oberon
    IF (s = &#34;Freda&#34;) OR (s = &#34;Mojo&#34;) THEN
      Out.String(&#34;Wowie Zowie! I remember you from ZBS stories.&#34;);Out.Ln;
    ELSIF (s = &#34;Bilbo&#34;) OR (s = &#34;Gandolf&#34;) THEN
      Out.String(&#34;Greets, I remember from the Hobbit.&#34;);Out.Ln;
    ELSE
      Out.String(&#34;Glad to meet you &#34;);Out.String(s);Out.Ln;
    END;
```

## ASSERT

The second expression, ASSERT, is a little different. If ASSERT
evaluates an expression that is FALSE then your program is halted.
This is like combining an &#34;if EXPR is false then system exit&#34;.

```Oberon
    Out.String(&#34;Should I continue? Y/n &#34;);
    In.Line(s);
    Out.Ln;
    ASSERT((s = &#34;Y&#34;) OR (s = &#34;y&#34;));
    (* If you didn&#39;t enter Y or y the program will halt *)
```


## WHILE DO, ELSIF DO

Oberon-07 also provides two loop structures. These are very 
similar to other languages as well. The only expectation is that
a while loop may contain an ELSIF which continues the loop
execution until both clauses return FALSE.

The basic while loop, counting 1 to 10.

```Oberon
    i := 0;
    WHILE i &#60; 10 DO
       i := i + 1;
       Out.Int(i, 1);Out.String(&#34; &#34;);
    END;
```

A while, elsif loop, counting 1 to 10, then 10 to 100 by 10.

```Oberon
    i := 0;
    WHILE i &#60; 10 DO
       i := i + 1;
       Out.Int(i, 1); Out.String(&#34; &#34;);
    ELSIF i &#60; 100 DO
       i := i + 10;
       Out.Int(i, 1);Out.String(&#34; &#34;);
    END;
```


## FOR Loops

The FOR loop in Oberon is very similar to modern FOR loops.
The FOR loop increments an integer value with in a range.
You the default increments the start value by 1 but if a 
BY clause is included you can control how the increment value
works.

Regular for loop, `i` is incremented by 1.

```Oberon
    FOR i := 1 TO 10 DO
       Out.Int(i, 1);Out.String(&#34; &#34;);
    END;
```

Using a BY clause incrementing `i` by 2.

```Oberon
    FOR i := 0 TO 20 BY 2  DO
       Out.Int(i, 1);Out.String(&#34; &#34;);
    END;
```


## Putting it all together

The following [module](LoopsAndConditions.Mod) demonstrates
the conditional and loop syntax.

```Oberon
    MODULE LoopsAndConditions;
      IMPORT In, Out;
    
    PROCEDURE IfElsifElseDemo;
      VAR s : ARRAY 128 OF CHAR;
    BEGIN
      Out.String(&#34;Enter your name: &#34;);
      In.Line(s);
      Out.Ln;
      IF (s = &#34;Freda&#34;) OR (s = &#34;Mojo&#34;) THEN
        Out.String(&#34;Wowie Zowie! I remember you from ZBS stories.&#34;);Out.Ln;
      ELSIF (s = &#34;Bilbo&#34;) OR (s = &#34;Gandolf&#34;) THEN
        Out.String(&#34;Greets, I remember from the Hobbit.&#34;);Out.Ln;
      ELSE
        Out.String(&#34;Glad to meet you &#34;);Out.String(s);Out.Ln;
      END;
    END IfElsifElseDemo;
    
    PROCEDURE AssertDemo;
      VAR s : ARRAY 128 OF CHAR;
    BEGIN
      Out.String(&#34;Should I continue? Y/n &#34;);
      In.Line(s);
      Out.Ln;
      ASSERT((s = &#34;Y&#34;) OR (s = &#34;y&#34;));
    END AssertDemo;
    
    PROCEDURE WhileDemo;
      VAR i : INTEGER;
    BEGIN
      Out.String(&#34;Basic WHILE counting from 1 to 10&#34;);Out.Ln;
      i := 0;
      WHILE i &#60; 10 DO
         i := i + 1;
         Out.Int(i, 1);Out.String(&#34; &#34;);
      END;
      Out.Ln;
      Out.String(&#34;WHILE ELSIF, count 1 to 10 THEN 10 to 100&#34;);Out.Ln;
      i := 0;
      WHILE i &#60; 10 DO
         i := i + 1;
         Out.Int(i, 1); Out.String(&#34; &#34;);
      ELSIF i &#60; 100 DO
         i := i + 10;
         Out.Int(i, 1);Out.String(&#34; &#34;);
      END;
      Out.Ln;
      Out.String(&#34;Demo of while loop counting one to ten, then by tenths.&#34;);
    END WhileDemo;
    
    PROCEDURE ForDemo;
      VAR i : INTEGER;
    BEGIN
      Out.String(&#34;Basic FOR LOOP counting from 1 to 10&#34;);Out.Ln;
      FOR i := 1 TO 10 DO
         Out.Int(i, 1);Out.String(&#34; &#34;);
      END;
      Out.Ln;
      Out.String(&#34;FOR loop counting by twos 1 to 20&#34;);Out.Ln;
      FOR i := 0 TO 20 BY 2  DO
         Out.Int(i, 1);Out.String(&#34; &#34;);
      END;
      Out.Ln;
    END ForDemo;
    
    BEGIN
      IfElsifElseDemo;
      AssertDemo;
      WhileDemo;
      ForDemo;
    END LoopsAndConditions.
```


### Next and Previous

+ Next [Combining Oberon-07 and C with OBNC](../../05/01/Combining-Oberon-and-C.html)
+ Previous [Basic Types](../18/Mostly-Oberon-Basic-Types.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/04/19/Mostly-Oberon-Loops-and-Conditions.md" length="6504" type="text/markdown" />
    </item>    <item>
      <title>Oberon Basic Types</title>
      <link>https://rsdoiel.github.io/blog/2020/04/18/Mostly-Oberon-Basic-Types.html</link>
      <description>
        <![CDATA[This is the third post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.
        
        ## Simple Types
        
        Oberon is a small systems language. It provides a useful but 
        limited umber of basic types. These can be be
        thought of as simple types mapping to specific memory locations
        and more complex types composed of multiple memory locations.
        
        ...]]>
      </description>
      <source:markdown>Oberon Basic Types
==================


By R. S. Doiel, 2020-04-18

This is the third post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.

## Simple Types

Oberon is a small systems language. It provides a useful but 
limited umber of basic types. These can be be
thought of as simple types mapping to specific memory locations
and more complex types composed of multiple memory locations.

NOTE: __basic types__, INTEGER, REAL, CHAR, ARRAY, RECORD and POINTER TO

### INTEGER

Integers are easiest to be thought of as whole numbers. They may be
positive numbers or negative numbers. Declaring an integer
variable `i` it would look something like


~~~{.oberon}

    VAR i : INTEGER;

~~~


Setting `i`&#39;s value to seven would look like


~~~{.oberon}

    i := 7;

~~~



### REAL

Real holds real numbers. Real numbers contain a fractional 
component. We normally notate them with
a decimal value e.g. &#34;0.01&#34;. Like integers they can also be 
positive or negative.

Declaring a real number variable `a` would look like


~~~{.oberon}

    VAR a : REAL;

~~~


Setting the value of `a` to seven and one tenth (7.1) would
look like


~~~{.oberon}

    a := 7.1;

~~~


### CHAR

A CHAR is a single ASCII character. Oberon, unlike more recent
languages like Go or Julia, predates the wide adoption of UTF-8.
The character is represented in memory as one 8 bit byte.
If you need to work with an extended character set then you need
to either re-encode the values into ASCII. At this time[^now] there
is no standard way of handling None ASCII character systems natively.
If you need to work directly with an encoding such as UTF-8 you&#39;ll
need to develop your own modules and procedures for handily their
encoding, decoding and other operations.

Declaring a CHAR variable `c` would look like


~~~{.oberon}

    VAR c: CHAR;

~~~


Setting the value of `c` to capital Z would look like


~~~{.oberon}

    c := &#34;Z&#34;;

~~~


Note: Oberon expects double quotes to notate a character.


### More complex types

The simplest types would prove problematic when addressing
more complex data representations if Oberon lacked two three built-in
types - ARRAY, RECORD and POINTER TO. 

### ARRAY

An array is a sequence of memory locations which contain a common
type.  In Oberon-07 all arrays have to have a known link. This is
because the Oberon compiler is responsible for pre-allocating
memory when the program starts to hold the array.  While this
seems restrictive our next data type, RECORD, lets us move
into more dynamic memory structures.  Pre-allocating the array
size also has the advantage that we can re-use those locations
easily in a type safe manner.

Declaring a variable &#34;name&#34; as an array of twelve characters would 
look like and declaring a variable &#34;scores&#34; as an array of ten
integers would look like


~~~{.oberon}

    VAR 
      name : ARRAY 24 OF CHAR;
      scores : ARRAY 10 OF INTEGER;

~~~


The length of the array immediately follows the keyword &#34;ARRAY&#34; and
the &#34;OF CHAR&#34; or &#34;OF INTEGER&#34; phrases describes the types that can be 
contained in the array. In the &#34;OF CHAR&#34; the type is &#34;CHAR&#34; the 
&#34;OF INTEGER&#34; is the type &#34;INTEGER&#34;. 

Setting an array value can be done using an index. In this example
the zero-th element (first element of the array) is set to the value
102. 


~~~{.oberon}

    scores[0] := 102;

~~~


In the case of CHAR arrays the whole array can be set in a simple 
single assignment.


~~~{.oberon}

    name := &#34;Ada Lovelace&#34;;

~~~


Two key points of arrays in Oberon are a known length and a single 
type of data associated with them. Arrays can have more than
one dimension but the cells of the array most contain the same type.

NOTE: __type safety__, Type safe means the compiler or run time verify that the data stored at that location conforms to the program defined, this is helpful in maintaining program correctness.

### RECORD

The RECORD is Oberon&#39;s secret sauce. The record is used to
create new types if data representations. It extend Oberon&#39;s basic 
types creating structured data representation. In this example we&#39;ll 
create a record that holds an game&#39;s name, a list of three player names 
and a list of three scores. We&#39;ll call this record type 
&#34;TopThreeScoreboard&#34;. 


~~~{.oberon}

    TYPE
      TopThreeScoreboard = RECORD
        gameName : ARRAY 24 OF CHAR;
        playerNames : ARRAY 3, 24 OF CHAR;
        scores : ARRAY 3 OF INTEGER
      END;

~~~


Now that we have describe a record of type &#34;TopThreeScoreboard&#34; we can
declare it with our &#34;VAR&#34; statement.


~~~{.oberon}

    VAR
      scoreboard : TopThreeScoreboard;

~~~


Setting the element values in a record uses a dot notation
and if those elements are themselves. In this case we&#39;ll set
the game name to &#34;Basketball&#34;, the three players are
&#34;Ada Lovelace&#34;, &#34;Blaise Pascal&#34;, and &#34;John McCarthy&#34;, with
the scores 102, 101, 100.


~~~{.oberon}

   scoreboard.gameName := &#34;Basketball&#34;;
   scoreboard.playerNames[0] := &#34;Ada Lovelace&#34;;
   scoreboard.scores[0] := 102;
   scoreboard.playerNames[1] := &#34;Blaise Pascal&#34;;
   scoreboard.scores[0] := 101;
   scoreboard.playerNames[2] := &#34;John McCarthy&#34;;
   scoreboard.scores[0] := 100;

~~~


Records are also used to create dynamic memory structures such as lists, trees and maps (see note on &#34;AD&#34;).  The dynamic nature of records is achieved with
our next type &#34;POINTER TO&#34;.

NOTE: __AD__, Prof. Wirth wrote an excellent text on [Algorithms and Data structures](https://inf.ethz.ch/personal/wirth/AD.pdf) available in PDF format.
### POINTER TO

Oberon is a type safe language. To keep things safe in a type
safe language you need to place constraints around random
memory access. Memory can be thought of a list of locations and
we can go to those locations if we know their address. A pointer
in most languages holds an address. Oberon has pointers but they
must point at specific data types. So like array you have to indicate
the type of the thing you are pointing at in a declaration. 
E.g. `VAR a : POINTER TO CHAR;` would declare a variable &#39;a&#39; 
that points to a memory location that holds a CHAR. The more common 
case is we use &#34;POINTER TO&#34; in records to create dynamic data 
structures such as linked lists.

Here&#39;s a simple data structure representing a dynamic list
of characters. Let&#39;s call it a DString and we will implement
it using a single link list. The list can be implemented by
defining a RECORD type that holds a single character and a pointer
to the next record. We can then also define a pointer to this type
of record.  If there is no next character record
we assume we&#39;re at the end of the string.


~~~{.oberon}

    TYPE
      DStringDesc = RECORD
        value : CHAR;
        next : POINTER TO DStringDesc
      END;

      DString : POINTER TO DStringDesc;

~~~


RECORD types are permitted to use recursive definition so our 
&#34;next&#34; value is itself a type &#34;DStringDesc&#34;.  Declaring a 
DString variable is as easy as declaring our scoreboard type variable.


~~~{.oberon}

  VAR
    VAR s : DString;

~~~


Setting our DString is a little trickier. This is where
Oberon&#39;s procedures come into play. We can pass our variable &#34;s&#34;
of type DString to a procedure to build out our DString from an simple
array of characters. Note &#34;s&#34; is declared as a &#34;VAR&#34; parameter
in our procedure heading. Our `SetDString` will also need to handle
creating new elements in our dynamic string. That is what Oberon&#39;s
built-in `NEW()` procedure does. It allocates new memory for our
list of records.


~~~{.oberon}

    PROCEDURE SetDString(VAR s : DString; buf : ARRAY OF CHAR);
        VAR i : INTEGER; cur, tmp : DString;
    BEGIN
      (* Handle the case where s is NIL *)
      IF s = NIL THEN
        NEW(s);
        s.value := 0X;
        s.next := NIL;
      END;
      cur := s;
      i := 0;
      (* check to see if we are at end of string or array *)
      WHILE (buf[i] # 0X) &#38; (i &#60; LEN(buf)) DO
        cur.value := buf[i];
        IF cur.next = NIL THEN
          NEW(tmp);
          tmp.value := 0X;
          tmp.next := NIL;
          cur.next := tmp;
        END;
        (* Advance our current pointer to the next element *)
        cur := cur.next;
        i := i + 1;
      END;
    END SetDString;

~~~


We can move our string back into a fixed length array of char
with a similar procedure.


~~~{.oberon}

    PROCEDURE DStringToCharArray(s : DString; VAR buf : ARRAY OF CHAR);
      VAR cur : DString; i, l : INTEGER;
    BEGIN
      l := LEN(buf);
      i := 0;
      cur := s;
      WHILE (i &#60; l) &#38; (cur # NIL) DO
        buf[i] := cur.value; 
        cur := cur.next;
        i := i + 1;
      END;
      (* Zero out the rest of the string. *)
      WHILE (i &#60; l) DO
        buf[i] := 0X;
        i := i + 1;
      END;
    END DStringToCharArray;

~~~


At this stage we have the basics of data organization. Modules
allow us to group operations and data into cohesive focused units.
Procedures allow us to define consistent ways of interacting with
out data, and types singularly and collectively allow us to structure
data in a way that is useful to solving problems.

## Putting it all together

Here is a [module demoing our basic type](BasicTypeDemo.Mod). In it
we can define procedures to demo our assignments, display their results
all called from inside the module&#39;s initialization block.


~~~{.oberon}

    MODULE BasicTypeDemo;
      IMPORT Out;
    
      (* These are our custom data types definitions. *)
      TYPE
          TopThreeScoreboard = RECORD
            gameName : ARRAY 24 OF CHAR;
            playerNames : ARRAY 3, 24 OF CHAR;
            scores : ARRAY 3 OF INTEGER
          END;
    
          DStringDesc = RECORD
            value : CHAR;
            next : POINTER TO DStringDesc
          END;
    
          DString = POINTER TO DStringDesc;
    
      (* Here are our private variables. *)
      VAR 
        i : INTEGER;
        a : REAL;
        c: CHAR;
        name : ARRAY 24 OF CHAR;
        scores : ARRAY 10 OF INTEGER;
        scoreboard : TopThreeScoreboard;
        s : DString;
    
    
      PROCEDURE SimpleTypes;
      BEGIN
        i := 7;
        a := 7.1;
        c := &#34;Z&#34;;
      END SimpleTypes;
    
      PROCEDURE DisplaySimpleTypes;
      BEGIN
        Out.String(&#34; i: &#34;);Out.Int(i, 1);Out.Ln;
        Out.String(&#34; a: &#34;);Out.Real(a, 1);Out.Ln;
        Out.String(&#34; c: &#34;);Out.Char(c);Out.Ln;
      END DisplaySimpleTypes;
    
    
      PROCEDURE MoreComplexTypes;
      BEGIN
        scores[0] := 102;
        name := &#34;Ada Lovelace&#34;;
        scoreboard.gameName := &#34;Basketball&#34;;
        scoreboard.playerNames[0] := &#34;Ada Lovelace&#34;;
        scoreboard.scores[0] := 102;
        scoreboard.playerNames[1] := &#34;Blaise Pascal&#34;;
        scoreboard.scores[0] := 101;
        scoreboard.playerNames[2] := &#34;John McCarthy&#34;;
        scoreboard.scores[0] := 100;
      END MoreComplexTypes;
    
      PROCEDURE DisplayMoreComplexTypes;
        VAR i : INTEGER;
      BEGIN
        i := 0;
        Out.String(&#34; Game: &#34;);Out.String(scoreboard.gameName);Out.Ln;
        WHILE i &#60; LEN(scoreboard.playerNames) DO
          Out.String(&#34;    player, score: &#34;);
          Out.String(scoreboard.playerNames[i]);Out.String(&#34;, &#34;);
          Out.Int(scoreboard.scores[i], 1);
          Out.Ln;
          i := i + 1;
        END;
      END DisplayMoreComplexTypes;
    
      PROCEDURE SetDString(VAR s : DString; buf : ARRAY OF CHAR);
          VAR i : INTEGER; cur, tmp : DString;
      BEGIN
        (* Handle the case where s is NIL *)
        IF s = NIL THEN
          NEW(s);
          s.value := 0X;
          s.next := NIL;
        END;
        cur := s;
        i := 0;
        (* check to see if we are at end of string or array *)
        WHILE (buf[i] # 0X) &#38; (i &#60; LEN(buf)) DO
          cur.value := buf[i];
          IF cur.next = NIL THEN
            NEW(tmp);
            tmp.value := 0X;
            tmp.next := NIL;
            cur.next := tmp;
          END;
          cur := cur.next;
          i := i + 1;
        END;
      END SetDString;
    
      PROCEDURE DStringToCharArray(s : DString; VAR buf : ARRAY OF CHAR);
        VAR cur : DString; i, l : INTEGER;
      BEGIN
        l := LEN(buf);
        i := 0;
        cur := s;
        WHILE (i &#60; l) &#38; (cur # NIL) DO
          buf[i] := cur.value; 
          cur := cur.next;
          i := i + 1;
        END;
        (* Zero out the rest of the string. *)
        WHILE (i &#60; l) DO
          buf[i] := 0X;
          i := i + 1;
        END;
      END DStringToCharArray;
    
    BEGIN
      SimpleTypes;
      DisplaySimpleTypes;
      MoreComplexTypes;
      DisplayMoreComplexTypes;
      (* Demo our dynamic string *)
      Out.String(&#34;Copy the phrase &#39;Hello World!&#39; into our dynamic string&#34;);Out.Ln;
      SetDString(s, &#34;Hello World!&#34;);
      Out.String(&#34;Copy the value of String s into &#39;name&#39; our array of char&#34;);Out.Ln;
      DStringToCharArray(s, name);
      Out.String(&#34;Display &#39;name&#39; our array of char: &#34;);Out.String(name);Out.Ln;
    END BasicTypeDemo.

~~~


## Reading through the code

There are some nuances in Oberon syntax that can creep up on you.
First while most statements end in a semi-colon there are noticeable
exceptions. Look at the record statements in particular.  The last
element of your record before the `END` does not have a semicolon.
In that way it is a little like a `RETURN` value in a function
like procedure.

In creating our `DString` data structure the Oberon idiom is to first
create a description record, `DStringDesc` then create a pointer to
the descriptive type, i.e. `DString`. This is a very common
idiom in building out complex data structures. A good place to learn
about implementing algorithms and data structures in Oberon-07 is 
Prof. Wirth&#39;s 2004 edition of [Algorithms and Data Structures](https://inf.ethz.ch/personal/wirth/AD.pdf) which
is available from his personal website in PDF.


### Next and Previous

+ Next [Loops and Conditions](../19/Mostly-Oberon-Loops-and-Conditions.html)
+ Previous [Modules and Procedures](../12/Mostly-Oberon-Modules.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/04/18/Mostly-Oberon-Basic-Types.md" length="15156" type="text/markdown" />
    </item>    <item>
      <title>Oberon Modules and Procedures</title>
      <link>https://rsdoiel.github.io/blog/2020/04/12/Mostly-Oberon-Modules.html</link>
      <description>
        <![CDATA[This is the second post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.
        
        ## Modules
        
        The module is a primary code unit of Oberon language. Modules allow you to focus on functional units of code and can be readily composed into larger solutions.
        A module's name should match the filename you are saving it under. A module starts with declaring it's name and ends the declaration with a semicolon
        the statement separator in Oberon. Our simple "Hello World" example 
        shows the basic code shape.
        
        ...]]>
      </description>
      <source:markdown>Oberon Modules and Procedures
=============================

By R. S. Doiel, 2020-04-12

This is the second post in the [Mostly Oberon](../11/Mostly-Oberon.html) series. Mostly Oberon documents my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.

## Modules

The module is a primary code unit of Oberon language. Modules allow you to focus on functional units of code and can be readily composed into larger solutions.
A module&#39;s name should match the filename you are saving it under. A module starts with declaring it&#39;s name and ends the declaration with a semicolon
the statement separator in Oberon. Our simple &#34;Hello World&#34; example 
shows the basic code shape.


~~~{.oberon}

    MODULE HelloWorld;
      IMPORT Out;
    BEGIN
      Out.String(&#34;Hello World!&#34;); Out.Ln;
    END HelloWorld.

~~~


Modules end with a `END` followed by the module&#39;s name and a period.
Any text following the `END` statement is ignored by the compiler. This
turns out to be very useful as a place to write up ideas about the code
you&#39;re working on. You can also write any additional special instructions 
there (e.g. document usage). You can even use it as a scratch pad knowing 
that the compiler will ignore it.

Here&#39;s an example


~~~{.oberon}

    MODULE HelloWorld;
      IMPORT Out;
    BEGIN
      Out.String(&#34;Hello World!&#34;); Out.Ln;
    END HelloWorld.

    This program isn&#39;t very useful. It has no interactive ability.
    It&#39;d be nice if it could be more specific about who it was saying
    hello to.

~~~


For a module to be really useful you want to have the capability
of including both private and public code. Public code
allows us to reuse our code in other modules while the private code 
keeps internal things inside the module safe from colliding with other
modules private code. This technique is classically known as 
&#34;information hiding&#34; and in computer sciences texts as &#34;scope&#34;. Lets 
create a a more composable module called `SayingHi.Mod`.  In 
addition to display &#34;Hello World!&#34; we want a public method 
(procedure in Oberon terminology) that can ask for a name and print 
out a salutation. We will use the `SayingHi.Mod` module along with 
a newer version of `HelloWorld.Mod` named `HelloWorld2.Mod`.


## Procedures

How do we write methods in Oberon?  Methods are declared
using the keyword `PROCEDURE` followed by their name, a 
declaration of any parameters and if the procedure returns a
value (i.e. is a function) it also includes that declaration. 
Next we declare any internal variables needed by the procedure.
This is followed by the procedure&#39;s body.  The body of the 
procedure is defined by a `BEGIN` and `END` statement structure. 
The body contains the steps the procedure needs to execute.

We&#39;ll create a procedure called &#34;HelloWorld&#34; in our new module.
Since we will use this procedure from our new `HelloWorld2.Mod` 
our new &#34;HelloWorld&#34; procedure needs to be public.  A public 
procedure in `SayingHi.Mod` is available for use in our new 
`HelloWorld2.Mod` (or by another module).  Marking a procedure 
public in Oberon is a little different than in other languages. 
A Module&#39;s procedure is public if its name ends with an asterisk. 
Below is a sketch of our module `SayingHi.Mod` so far.

NOTE: This technique is also used to mark variables, records and constants as public and available to other modules. Public variables are &#34;read only&#34; in other modules.


~~~{.oberon}

    MODULE SayingHi;
      IMPORT Out;
    
      PROCEDURE HelloWorld*;
      BEGIN
        Out.String(&#34;Hello World!&#34;); Out.Ln;
      END HelloWorld;
    END SayingHi.

~~~


This modules looks allot like `HelloWorld.Mod` with a couple key
differences. Rather than relying on the module&#39;s begin and end 
statements we declare a procedure with its own begin and end statements.
Notice the procedures end statement includes the procedure name and
is terminated by semicolon rather than a period.  Like `HelloWorld.Mod`
we import the `Out` module to display our greeting.

## Putting it all together

Let&#39;s create a new &#34;Hello World&#34; module called `HelloWorld2.Mod` and
use our `SayingHi` module instead of directly importing `Out`.


~~~{.oberon}

    MODULE HelloWorld2;
      IMPORT SayingHi;
    BEGIN
      SayingHi.HelloWorld;
    END HelloWorld2.

~~~


We can compile our module with OBNC using the command


~~~

    obnc HelloWorld2.Mod

~~~


We can run our new &#34;Hello World&#34; with the command


~~~

    ./HelloWorld2

~~~


At this point we have a way of saying &#34;Hello World!&#34; whenever
we need in our Oberon programs. But just printing &#34;Hello World!&#34;
to the screen isn&#39;t very interactive. It&#39;d be nice if we could
have the computer ask our name and then respond with a greeting.

We&#39;ll modify our SayingHi to include a new procedure called &#34;Greetings&#34;
and that procedure needs to ask us our name and then display
an appropriate greeting. &#34;Greetings&#34; will be a public procedure
marked by an asterisk like &#34;HelloWorld&#34;. 

&#34;Greetings&#34; has three tasks

1. Ask politely for our name
2. Get the name typed in with our keyboard
3. Assemble and display a polite greeting

To keep our &#34;Greeting&#34; procedure short we&#39;ll split this
up into some private procedures. These will not be available
outside `SayingHi.Mod`. Here&#39;s a sketch of our improved module.


~~~{.oberon}

    MODULE SayingHi;
      IMPORT In, Out;
    
      PROCEDURE HelloWorld*;
      BEGIN
        Out.String(&#34;Hello World!&#34;); Out.Ln;
      END HelloWorld;
    
      PROCEDURE AskOurName;
      BEGIN
        Out.String(&#34;Excuse me, may I ask your name? &#34;);
      END AskOurName;
    
      PROCEDURE GetName(VAR ourName : ARRAY OF CHAR);
      BEGIN
        In.Line(ourName);
      END GetName;
    
      PROCEDURE AssembleGreeting(ourName : ARRAY OF CHAR);
      BEGIN
        Out.String(&#34;Hello &#34;);Out.String(ourName);
        Out.String (&#34;, very nice to meeting you.&#34;); Out.Ln;
      END AssembleGreeting;
    
      PROCEDURE Greetings*;
        VAR ourName : ARRAY 256 OF CHAR;
      BEGIN
        AskOurName;
        GetName(ourName);
        AssembleGreeting(ourName);
      END Greetings;
    END SayingHi.

~~~


Now let&#39;s add our Greetings procedure to `HelloWorld2.Mod`.


~~~{.oberon}

    MODULE HelloWorld2;
      IMPORT SayingHi;
    BEGIN
      SayingHi.HelloWorld;
      SayingHi.Greetings;
    END HelloWorld2.

~~~


We compile and run it the same way as before


~~~

    obnc HelloWorld2
    ./HelloWorld2

~~~


When you run `HelloWorld2` you should now see something like
(I&#39;ve answered &#34;Robert&#34; and pressed return after the second line.


~~~

   Hello World!
   Excuse me, may I ask your name? Robert
   Hello Robert, very nice to meeting you.

~~~



## Reading our code

While our revised modules are still short they actually exercise
a number of language features. Let&#39;s walk through the code 
block by block and see what is going.

`HelloWorld2.Mod` is responsible for the general management of
our program namely say &#34;Hello World!&#34; and also for initiating
and responding with a more personal greeting.  It does this by
first importing our `SayingHi.Mod` module.


~~~

    IMPORT SayingHi;

~~~


[HelloWorld2.Mod](HelloWorld2.Mod) doesn&#39;t have any of its own 
procedures and like our original [HelloWorld.Mod](HelloWorld.Mod)
relies on the module&#39;s initialization block to run our two public 
procedures from `SayingHi`. It calls first `SayingHi.HelloWorld;` 
then `SayingHi.Greetings&#39;` before existing. Other than using the 
`SayingHi` module it is similar in spirit to our first 
[HelloWorld.Mod](HelloWorld.Mod).

Our second module [SayingHi.Mod](SayingHi.Mod) does the heavy lifting.
It contains both public and private procedures.  If you tried to
use `GetName` from `SayingHi` in `HelloWorld2.Mod` you would get a
compiler error. As far as `HelloWorld2.Mod` is concerned `GetName`
does not exist. This is called information hiding and is an important
capability provided by Oberon&#39;s Modules system. 

### explore `SayingHi` more deeply

In `SayingHi.Mod` we introduce two important concepts.

1. Public and Private procedures
2. variables to hold user input

`SayingHi.Mod` imports two module, `In` which is for getting
text input from the keyboard, and `Out` which is used for displaying
text to standard output.


~~~{.oberon}

    IMPORT In, Out;

~~~


`In` and `Out` are to modules you will commonly use to either
receive input (`In`) from the keyboard or display output (`Out`)
to the terminal or shell. They provide simple methods for working
with variables and constants and built-in Oberon data types. 
This is a very useful as it lets us focus our procedures
on operating on data rather than the low level steps needed to
interact with the operating system and hardware.

NOTE: __basic types__, Oberon has a number of basic types, BYTE holds a byte as a series of bit, CHAR holds a single ASCII character, INTEGER holds a signed integer value, REAL holds a floating point number and BOOLEAN holds a True/False value.

The first procedure is `HelloWorld` and it&#39;s pretty straight forward.
It displays a &#34;Hello World!&#34; message in our terminal. It uses `Out`.
`Out.String` to display the &#34;Hello World!&#34; and `Out.Ln` to force a new
line. `Out.String` is responsible for displaying values that are of type
`ARRAY OF CHAR`. This includes text we provided in double quotes.


~~~{.oberon}

    PROCEDURE HelloWorld*;
    BEGIN
      Out.String(&#34;Hello World!&#34;); Out.Ln;
    END HelloWorld;

~~~


The notable thing about `HelloWorld*` is its annotation `*`.
This asterisk indicates to the compiler that this is
a public procedure and should be made available to other modules.
Procedures, variables, constants, records (data structures) can be
made public with this simple annotation.  If we left off the `*`
then we would not be able to use `HelloWorld` procedure from other
module.

Our second procedure is `AskOurName`. It&#39;s private because it lacks
the `*`. It is invisible to `HelloWorld2.Mod`. It is visible within
`SayingHi` module and we&#39;ll use it later in `Greetings*`. Before
a procedure, variable, constant or record can be used it must be
declared. That is why we most define `AskOurName` before we define
`Greetings*`. `AskOurName` is in other respects very similar to 
`HelloWorld*`.


~~~{.oberon}

    PROCEDURE AskOurName;
    BEGIN
      Out.String(&#34;Excuse me, may I ask your name? &#34;);
    END AskOurName;

~~~


Our third procedure `GetName` is a little more interesting.
It demonstrates several features of the Oberon language. Most
obvious is that it is the first procedure which contains a
parameter list.


~~~{.oberon}

    PROCEDURE GetName(VAR ourName: ARRAY OF CHAR);

~~~


There is allot packed in this single statement in addition
to putting a name to our procedure. Specifically it uses
a `VAR` in the parameter.  Oberon provides two kinds of parameters
in declaring procedures. The two are `VAR` and static.  A `VAR` 
parameter means that the procedure is allowed to up date the value 
in the memory location indicated by the name. A static variable 
(a parameter without the `VAR` prefix passes in a read only value. 
This allows us to distinguish between those procedures and variables
where that can be modified by the procedure and those which
will be left the same. Inside of `GetName` we call the 
`In` module using the `Line`. This retrieves a line of text
(a sequence of keyboard strokes ended with the return key).


~~~{.oberon}

    In.Line(ourName);

~~~


Because `ourName` was a variable parameter in `GetName` it
can be modified by `In.Line`.

Our next procedure `AssembleGreeting` is private like
`AskOurName` and `GetName`. Like `HelloWorld*` and `AskOurName`
it makes use of the `Out` module to display content.
Unlike `HelloWorld*` it has a parameter but this time
a static one. Notice the missing `VAR`. This indicates that
`AssembleGreeting` doesn&#39;t modify, cannot modify `ourName`.


~~~{.oberon}

    PROCEDURE AssembleGreeting(ourName : ARRAY OF CHAR);
    BEGIN
      Out.String(&#34;Hello &#34;);Out.String(ourName);
      Out.String (&#34;, very nice to meeting you.&#34;); Out.Ln;
    END AssembleGreeting;

~~~


The use of `Out.String` is more elaborate then before. Notice how
we use trailing spaces to make the output more readable.

Our final procedure is public, `Greetings*`. It does not
have any parameters.  Importantly it does include a
variable for use inside the procedure called `ourName`. 
The `VAR` line declares `ourName` as an `ARRAY 256 OF CHAR`. 
This declaration tells the compiler to allocate memory 
for storing `ourName` while `Greetings*` is being executed. 
The declaration tells us three things. First the storage
is continuous block of memory, that is what `ARRAY` means.
The second is the size of this memory block is 256 `CHAR`
long and the that we will be storing `CHAR` values in it.

The memory for `ourName` will be populated when we pass
the variable to `GetName` based on what we type at the
keyboard. If we type more than 256 ASCII characters they
will be ignored. After `GetName` records the typed character
we use the memory associated with the `ourName` variable
we read that memory to display what we typed in 
the procedure named `AssembleGreeting`.


### Going a little deeper

Oberon is a typed language meaning that 
variables are declared, allocated and checked during compile time
for specific characteristics. The one variable we created `ourName`
in the `Greetings` procedure reserves the space for 256 
[ASCII](https://en.wikipedia.org/wiki/ASCII) characters. 
In Oberon we call a single ASCII character a `CHAR`.  Since it
would be useful to work with more than one `CHAR` in relationship
to others Oberon also supports a variable type called `ARRAY`. 
An `ARRAY` is represented as a block of memory that is allocated
by the Oberon run time. Because it is allocated ahead of time we
need to know its size (i.e. how many `CHAR` are we storing). In
our case we have declared `ARRAY 256 OF CHAR`. That means we can
hold names up to 256 ASCII characters. 

`Greetings*` does three things and the second thing, `GetName` 
receives the characters typed at the keyboard.  `GetName` has
a parameter list. In this case the only one parameter is declared
`VAR ourName : ARRAY OF CHAR`. Notice the similarity and
difference between the `VAR` statement in `Greetings` versions
the parameter list.  Our `GetName` can accept **any** length of
`ARRAY OF CHAR` and it **only** can accept an `ARRAY OF CHAR`.
If you try to pass another type of variable to `GetName` the
compiler will stop with an error message.

Why is this important?

We&#39;ve minimized the memory we&#39;ve used in our program.  Memory is 
typically allocated on the stack (a block of memory made available 
by the operating system to the program). We&#39;ve told the operating 
system we need 256 `CHAR` worth of consecutive memory locations 
when we allocated room the variable `ourName` in `Greetings`. When 
we invoke `GetName` Oberon knows to use that same memory location 
for the value of `ourName` defined in the parameter.  In turn
when `In.String(ourName);` is called the module `In` knows
to store the name typed on the keyboard in that location of memory.
When `Out.String(outName);` is called the compiler knows to use
the same location of memory to send the contents to the display.
When we finally finish the `Greetings*` procedure the memory is 
released back to the operating system for re-use by this or
other programs.

### What we&#39;ve explored

1. Using a module to break down a simple problem
2. Using a module&#39;s ability to have public and private procedures 
3. Touched on how memory is used in a simple interactive program



### Next and Previous

+ Next [Basic Types](../18/Mostly-Oberon-Basic-Types.html)
+ Previous [Mostly Oberon](../11/Mostly-Oberon.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/04/12/Mostly-Oberon-Modules.md" length="16948" type="text/markdown" />
    </item>    <item>
      <title>Mostly Oberon</title>
      <link>https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.html</link>
      <description>
        <![CDATA[**Mostly Oberon** is a series of blog posts documenting my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.
        
        ## Overview
        
        Oberon is a classical computer language and operating system originated by Professors Niklaus Wirth and Jrg Gutknecht at [ETH](https://en.wikipedia.org/wiki/ETH_Zurich) circa 1987.  It was inspired by their experiences in California at the [Xerox Palo Alto Research Center](https://en.wikipedia.org/wiki/PARC_\(company\)).  This series of blog posts are my meandering exploration of Oberon-07 language based on [Project Oberon 2013](http://www.projectoberon.com/).
        
        ...]]>
      </description>
      <source:markdown>Mostly Oberon
=============

By R. S. Doiel, 2020-04-11

**Mostly Oberon** is a series of blog posts documenting my exploration of the Oberon Language, Oberon System and the various rabbit wholes I inevitably fell into.

## Overview

Oberon is a classical computer language and operating system originated by Professors Niklaus Wirth and Jrg Gutknecht at [ETH](https://en.wikipedia.org/wiki/ETH_Zurich) circa 1987.  It was inspired by their experiences in California at the [Xerox Palo Alto Research Center](https://en.wikipedia.org/wiki/PARC_\(company\)).  This series of blog posts are my meandering exploration of Oberon-07 language based on [Project Oberon 2013](http://www.projectoberon.com/).

NOTE: Oberon grew from Wirth&#39;s Modula, which grew from Pascal, which grew from his experiences with Algol.

### My Voyage

I am new to both Oberon and the Oberon System.  Oberon language is in the tradition of ALGOL, Pascal, Modula 1 and 2 as well as incorporating ideas from the parent of Object Oriented languages Simula. The Oberon language reminds me of my first programming language [Turbo Pascal](https://en.wikipedia.org/wiki/Turbo_Pascal).  Oberon&#39;s language shape is more Pascal than C. For that reason I think it has largely been overlooked.

Oberon-07 is Wirth&#39;s most recent refinement of the Oberon language.  It is a terse and powerful systems language.  It strikes a different computing path then many popular programming languages used in 2020.  You find its influence along with Simula in more recent popular languages like [Go](https://golang.org).

While Wirth conceived of Oberon in the context of a whole system it&#39;s use in research and instruction means it is also well suited [POSIX](https://en.wikipedia.org/wiki/POSIX) based systems (e.g. BSD, Linux, macOS).  The difference in programming in Oberon for a POSIX system versus a native Oberon System is primarily in the modules you import. These posts will focus on using Oberon language in a POSIX environment.

NOTE: Oberon was initially a project including the CERES Hardware, Oberon compiler and Oberon operating system for networked workstations.

The latest Oberon is Prof. Niklaus Wirth and Paul Reeds&#39; Project Oberon 2013. If you want to explore it I suggest using Peter De Wachter&#39;s [emulator](https://github.com/pdewacht/oberon-risc-emu). Project Oberon also his links to the updated books and articles in PDF format which are easy to read (or print) on most computing devices.


## A starting point

I am starting my exploration with Karl Landstrm&#39;s [OBNC](https://miasap.se/obnc/) compiler. I am focusing on getting comfortable using and writing in the Oberon language.

Here&#39;s an example of a simple &#34;Hello World&#34; program in Oberon written for a POSIX system. I&#39;ve named the [source code](HelloWorld.Mod) `HelloWorld.Mod`.

NOTE: In 2020 common POSIX systems include [Linux](https://en.wikipedia.org/wiki/Linux), [BSD](https://en.wikipedia.org/wiki/Berkeley_Software_Distribution) and [macOS](https://en.wikipedia.org/wiki/MacOS).


~~~

    MODULE HelloWorld;
      IMPORT Out;
    BEGIN
      Out.String(&#34;Hello World!&#34;); Out.Ln;
    END HelloWorld.

~~~


While this is longer than a Python &#34;hello world&#34; program it is much shorter than I remember writing in Java and about the same number of lines as in C. `BEGIN` and `END` are similar to our opening and closing curly braces in C and the module is the basic unit of source code in Oberon. `IMPORT` includes the module `Out` (modules are similar to a included library in C) for sending values to the console (stdout in POSIX). One thing to note, Oberon language(s) are case sensitive. All language terms are capitalized. This makes it easy to distinguish between source code written in Oberon versus the Oberon language itself.

The `Out` module includes methods for displaying various data types native
to Oberon. There is a corresponding `In` for receiving input as well as
some additional modules provided with our chosen compiler implementation.

Modules in Oberon can include a module wide initialization block. The
`BEGIN` through `END HelloWorld.` are an initialization block. This is
similar to C or Go&#39;s &#34;main&#34; function for our POSIX environment.

### OBNC

If you want to run my &#34;Hello World&#34; you need to compile it.  I have found that [OBNC](https://miasap.se/obnc/) compiler runs well on Linux, macOS and [Raspberry Pi](https://www.raspberrypi.org). Karl has also made a precompiled version that runs on Windows available too. It is the Oberon compiler I plan to use in this series of posts.

OBNC compiles Oberon source into C then into machine code for the computer system you are running on. Because it is compiling to C it can function as a [cross compiler](https://en.wikipedia.org/wiki/Cross_compiler). This opens the door to [bare metal programming](https://en.wikipedia.org/wiki/Bare_machine).

If you&#39;re following along please install OBNC on your computer.  Instructions are found at https://maisap.se/obnc. Karl also has excellent documentation and is responsive to questions about his implementation. His contact methods are included on his website.


### Running OBNC

OBNC provides a Oberon-07 compiler with some optional modules for working in a POSIX environment.  Compiling our &#34;Hello World&#34; is easy from your shell or terminal.


~~~

    obnc HelloWorld.Mod

~~~


If all goes well this should produce an executable file named `HelloWorld` (or `HelloWorld.exe` on Windows). You can now run this program with a command like `./HelloWorld` (or `HelloWorld` on Windows).

### Learning more about Oberon

I have faced two challenges in my exploration of Oberon, finding a compiler I was happy with (thank you Karl for OBNC) and sorting out the literature around Oberon language implementations and system versions.

Oberon has a rich history though it was not well known in Southern California in 2020. Oberon&#39;s history is primarily academic and European. It was commonly used in college level instruction in Europe from it&#39;s inception at ETH in the late 80&#39;s through the early 2000s. The Oberon System is an Open Source system (predating the term by a decade) and was created in the spirit of other academic systems such as BSD. There are many books (physical books as opposed to ebooks) dating from that era.  They covered the Oberon language and system of their time.  From a historical computing perspective they remain very interesting. But running Oberon on modern 2020 hardware is a little more challenging. Fortunately Prof. Emeritus Wirth and Paul Reed brought things up to date in 2013. I recommend Reed&#39;s [www.projectoberon.com](http://www.projectoberon.com) as a good place to start. He includes links to revised versions of the classic Oberon and Oberon System texts written by Wirth et el. Prof. Wirth&#39;s [website](https://inf.ethz.ch/personal/wirth/) is still maintained and he features links to most of his major publications. His is the canonical source of information on Oberon.

NOTE: Prof. Wirth&#39;s personal website at ETH was available as of 2020-04-11. 

I have found the ACM [Digital Library](https://dl.acm.org/) and the ETH [Research Collection](https://www.research-collection.ethz.ch/?locale-attribute=en) very helpful.  While much of the material is now historic it remains useful for both techniques and inspiration.  Today&#39;s hardware, even a Raspberry Pi Zero, is more resource rich than the original systems Oberon ran on.

The online community for Oberon and Oberon System seems mostly centered around a [mail list](https://lists.inf.ethz.ch/mailman/listinfo/oberon) at ETH and net news group [comp.lang.oberon](https://groups.google.com/forum/#!forum/comp.lang.oberon)








### Next

+ Next [Modules and Procedures](../12/Mostly-Oberon-Modules.html)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2020/04/11/Mostly-Oberon.md" length="8975" type="text/markdown" />
    </item>    <item>
      <title>FreeDOS 1.2 to Oberon System 3</title>
      <link>https://rsdoiel.github.io/blog/2019/07/28/freedos-to-oberon-system-3.html</link>
      <description>
        <![CDATA[What follows are notes on getting a FreeDOS 1.2[^1] and 
        then Native Oberon[^2] running under VirtualBox 6.0. You might 
        wonder why these two are together. While it was
        easy to run the Native Oberon installation process that process
        assumes you have a properly partitioned hard disk and VirtualBox
        seems to skip that process. I found taking advantage of FreeDOS
        simplified things for me.]]>
      </description>
      <source:markdown>FreeDOS to Oberon System 3
==========================

By R. S. Doiel, 2019-07-28

&#62;    UPDATE: (2021-02-26, RSD) Under VirtualBox 6.1 these
&#62;    instructions still fail. My hope is to revise these 
&#62;    instructions when I get it all sorted out.
&#62;
&#62;    Many links such as the ftp site at ETH Oberon are 
&#62;    no more. I&#39;ve updated this page to point at Wayback machine
&#62;    or included content in here where I cannot find it else where.
&#62;
&#62;    UPDATE: (2021-02-19, RSD) Under VirtualBox 6.1 these instructions 
&#62;    fail. For VirtualBox Ive used FreeDOS 1.3rc3 Live CD installing 
&#62;    the Plain DOS without problems.
&#62;
&#62;    UPDATE: (2021-03-16, RSD) After reviewing my post, correcting
&#62;    some mistakes I finally was able to get FreeDOS up and running
&#62;    on VirtualBox 6.1. This allows NativeOberon 2.3.6 to be brought
&#62;    up by booting the &#34;oberon0.dsk&#34; virtual floppy and following
&#62;    the instructions included. You need to know how to use
&#62;    the Oberon mouse and the way commands work in Oberon.

What follows are notes on getting a FreeDOS 1.2[^1] and 
then Native Oberon[^2] running under VirtualBox 6.0. You might 
wonder why these two are together. While it was
easy to run the Native Oberon installation process that process
assumes you have a properly partitioned hard disk and VirtualBox
seems to skip that process. I found taking advantage of FreeDOS
simplified things for me.

My goal was running Oberon System 3, but setting up a Virtual Box
with FreeDOS 1.2 gave me a virtual machine that functions like a 
1999 era PC. From there all the steps in the Oberon instructions
just worked.

## Creating FreeDOS 1.2 Virtual Box

I&#39;ve been doing a bit if computer history reading and decided to
bring up some older systems as a means to understand where
things were.  The first computers I had access to were 8080, 8086
machines running MS DOS based. My first computer programming language
was Turbo Pascal. Feeling a bit nostalgic I thought it would be
interesting to use it again and see what I remembered from the days
of old. While PC and MS DOS no longer exist as commercial productions
an wonderful group of Open Source hackers have brought new life into
DOS with FreeDOS 1.2[^3]. You&#39;ll find many of your old familiar commands
but also some nice improvements. You can even run it under VirtualBox
which is what I proceeded to do.

### VirtualBox 6.0 setup

The [FreeDOS](https://freedos.org) website includes a CD ROM image
that you can use to install it. There are couple small hitches though
to get it working under VirtualBox. First go to the [download](https://freedos.org/download) page and download the [CDROM &#34;standard&#34; installer&#34;](http://www.freedos.org/download/download/FD12CD.iso).

While that is downloading you can setup your VirtualBox machine.
First to remember is DOS compared to today&#39;s operating systems is
frugal in its hardware requirements. As a result I picked very modest
settings for my virtual machine. 

1. Launch VirtualBox
2. From the menu, pick Machine then pick new
3. Name your machine (e.g. &#34;FreeDOS 1.2&#34;), select the type: &#34;Other&#34; and Operating system of &#34;DOS&#34;
4. Set memory size as you like, I just accepted the default 32MB
5. Hard disk, pick &#34;Create a virtual hard disc now&#34;
6. Hard disk file type, pick &#34;VHD (Virtual Hard Disk)&#34;
7. Storage on physical hard disk, I picked Dynamically allocated both either is fine
8. File location and size, I accepted the default location and size
9. Before starting my FreeDOS box I made a couple of changes using &#34;settings&#34; menu icon
    a. Display, I picked bumped memory up to 128M and picked VBoxSVGA with 33D acceleration (for games)
    b. Storage, I added a second floppy drive (empty)
    c. Network, I picked attached to NAT
10. When looking at my virtual machine&#39;s detail page I clicked on the Optical drive (empty), click &#34;choose disc image&#34; and pointed at the downloaded installed CD
11. Click Start.
12. At &#34;Welcome to FreeDOS 1.2&#34; blue screen, hit TAB key
13. You will see a line that begins with a boot instruction. Add a space than add the word &#34;raw&#34; (without quotes) press enter
14. Follow the install instructions, when you get to &#34;Drive C: does not appear to be partitioned&#34; dialog, pick &#34;Yes - Partition drive C:&#34;
15. On the next screen pick &#34;Yes - Please reboot now&#34;
16. When at the &#34;Welcome to FreeDOS 1.2&#34; screen hit TAB again
17. Once again add a space and type &#34;raw&#34; to the command then press enter
18. Pick &#34;Yes - continue with the installation&#34;
19. Pick &#34;Yes - Please erase and format drive C:&#34;
20. At this point its a normal FreeDOS install
21. When the install is done and reboots &#34;eject&#34; the virtual CD form the &#34;Optical Drive&#34; in the VirtualBox panel, then choose &#34;boot from system disk&#34;,you now should have a working FreeDOS on VirtualBox

## Native Oberon System 3 on Virtual Box

Native Oberon can be found at http://www.ethoberon.ethz.ch/native/.
There is a related ftp site[^4] where you can download the necessary
files for the stand alone version. 

Here&#39;s the steps I used in my Mac to download Native Oberon and
into a file on my desktop called &#34;NativeOberon-Standalone&#34;. Open
the macOS Terminal application. I assume you&#39;ve got a Unix
command called [wget](https://en.wikipedia.org/wiki/Wget)
already installed[^5].

&#62; NOTE: The ETH ftp server is no more. I&#39;ve included Web Archive
&#62; links and links to my own copies of the files needed to
&#62; install Native Oberon 2.3.6 in the paragraphs that follow.
&#62; RSD, 2021-03-16

```bash

    cd
    mkdir -p Desktop/NativeOberon-Standalone
    cd Desktop/NativeOberon-Standalone
    wget ftp://ftp.ethoberon.ethz.ch/ETHOberon/Native/StdAlone/

```

Clone your FreeDOS Box first. You&#39;ll want to do a &#34;Full Clone&#34;. You&#39;ll
also want to &#34;remove&#34; any optical disks or floppies. You do that from
the virtual boxes&#39; detail page and clicking on the drive and picking the
&#34;Remove disk from virtual drive&#34; in the popup menu.

At this point we have a a virtual machine that is very similar to an 
1999 era PC installed with MS DOS.  [Native Oberon](http://web.archive.org/web/20190929033749/http://www.ethoberon.ethz.ch/native/) Normally you&#39;d
install [Native Oberon via 1.44MB floppy disks](/blog/2019/07/28/NativeOberon-StnAlone-2.3.6.zip &#34;Zip file of individual floppies&#34;). 
We can simulate that with our Virtual machine.
In the folder of you downloaded there is disc called &#34;oberon0.dsk&#34;. That
can go in our first floppy drive. But how to we get the rest of the 
files onto a virtual floppies? This wasn&#39;t obvious to me at first.

The Oberon install disks were organized as follows

| PACKAGE    | FILENAME     | SIZE  | DSK   |
| ---------- | ------------ | ----- | ----- |
| Oberon-0      | [oberon0.dsk](oberon0.dsk &#34;boot disk&#34;)  |          | 0 | 
| Gadgets       | [gadgets.arc](gadgets1.arc &#34;a modified gadgets.arc to fit 1.4 floppy&#34;)  | 1.4  2.9 | 1 | 
| Documentation | [docu.arc](docu.arc &#34;documentation&#34;)     | 1.3  2.5 | 2 | 
| Applications  | [apps.arc](apps.arc &#34;applications&#34;)     | 1.3  2.8 | 3 | 
| Tutorials     | [tutorial.arc](tutorial.arc &#34;tutorial&#34;) | 0.3  0.8 | 4 | 
| Pr3Fonts      | [pr3fonts.arc](pr3fonts.arc &#34;fonts&#34;) | 0.3  0.6 | 4 | 
| Pr6Fonts      | [pr6fonts.arc](pr6fonts.arc &#34;fonts&#34;) | 0.5  1.8 | 4 | 
| Source1       | [source1.arc](source1.arc &#34;Source Code&#34;)  | 0.9  2.5 | 5 | 
| Source2       | [source2.arc](source2.arc &#34;Source Code&#34;)  | 1.2  3.5 | 6 | 
| Source3       | [source3.arc](source3.arc &#34;Source Code&#34;)  | 0.6  1.7 | 7 | 


It turns out you can create 1.44MB Fat16 disc images from the
Virtual Box 6.0 floppy drive link.  When you click on the floppy
drive in the details page you have a choice that includes &#34;create a new floppy disc&#34;. Select this, find the disc a filename like &#34;disk1&#34;. Click
on the virtual floppy disk in the Virtual Box and &#34;remove&#34;
the disc then create disk2, disk3, etc. In each the empty disc image
files places the files from the table above. These image files can then
be opened on your host operating system and files copied to them. 
It&#39;s a tedious process but this gives you something the Oberon System 
can read and install from. Originally I just put all the files into an 
ISO CD ROM image but I could not figure out how to mount that from this
version of Oberon. Now when you start up your Oberon V3 virtual machine
you can install the rest of the software like Gadgets.


[^1]: FreeDOS is an Open Source implementation of PC/MS DOC

[^2]: Native Oberon is a 1990&#39;s version of Oberon System running on i386

[^3]: Download FreeDOS from http://freedos.org/download

[^4]: Download Native Oberon Stand Alone from [ftp://ftp.ethoberon.ethz.ch/ETHOberon/Native/StdAlone](NativeOberon-StdAlone-2.3.6.zip &#34;Zip of what used to be available in that directory at ftp.ethoberon.ethz.ch&#34;)

[^5]: wget is easily installed with [HomeBrew](https://brew.sh/) or [Mac Ports](https://www.macports.org/)</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2019/07/28/freedos-to-oberon-system-3.md" length="9720" type="text/markdown" />
    </item>    <item>
      <title>Review: Software Tools in Pascal</title>
      <link>https://rsdoiel.github.io/blog/2018/07/22/software-tools-in-pascal.html</link>
      <description>
        <![CDATA[Book Review: This book is by Brian W. Kernighan and P. J. Plauger. It is an
        example of the type of books I find I re-read and want in my
        personal library. The book covers software construction through 
        a series of programs written in pascal. It is about how these 
        programs work, how to approach problems and write sound software.
        I was surprised I did not know about this book when I was browsing 
        the [Open Library](https://openlibrary.org) this weekend.  While 
        Pascal was a popular in the 1980's it has faded for most people in the 
        early 21st century.  This review maybe a small bit of nostalgia. 
        On the other hand I suspect 
        ["Software Tools in Pascal"](https://openlibrary.org/books/OL4258115M/Software_tools_in_Pascal)
        is one of the short list of computer books that will remain useful
        over the long run.]]>
      </description>
      <source:markdown># Review: Software Tools in Pascal

By R. S. Doiel, 2018-07-22
(updated: 2018-07-22, 1:39 pm, PDT)


This book is by Brian W. Kernighan and P. J. Plauger. It is an
example of the type of books I find I re-read and want in my
personal library. The book covers software construction through 
a series of programs written in pascal. It is about how these 
programs work, how to approach problems and write sound software.
I was surprised I did not know about this book when I was browsing 
the [Open Library](https://openlibrary.org) this weekend.  While 
Pascal was a popular in the 1980&#39;s it has faded for most people in the 
early 21st century.  This review maybe a small bit of nostalgia. 
On the other hand I suspect 
[&#34;Software Tools in Pascal&#34;](https://openlibrary.org/books/OL4258115M/Software_tools_in_Pascal)
is one of the short list of computer books that will remain useful
over the long run.


## What&#39;s covered

The book is organized around specific programs and their implementations.
The implementations provided are simple and straight forward. Each
section is followed by a set of &#34;exercises&#34; that extend the ideas
shown in the section. In this way you could derive the modern equivalent
of these tools.

The topics you build tools for in the text are
filters, files, sorting, text patterns, editing, formatting, 
and macro processing.

If you want to follow the book along in Pascal then I think Free Pascal
available in many Debian distributions including Raspbian on the Raspberry
Pi is a good choice.  Likewise Wirth&#39;s Pascal is easy enough to port
to other languages and indeed this would be a useful exercise when I
re-read the book the next time.

The book presents a very nice set of text oriented programs to explore
programming or re-connect with your programming roots.

## Read the book

&#60;iframe width=&#34;165&#34; frameBorder=&#34;0&#34; height=&#34;400&#34; src=&#34;https://openlibrary.org/books/OL4258115M/Software_tools_in_Pascal/widget&#34;&#62;&#60;/iframe&#62;</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2018/07/22/software-tools-in-pascal.md" length="3272" type="text/markdown" />
    </item>    <item>
      <title>Accessing Go from Julia</title>
      <link>https://rsdoiel.github.io/blog/2018/03/11/accessing-go-from-julia.html</link>
      <description>
        <![CDATA[The problem: I've started exploring Julia and I would like to leverage existing
        code I've written in Go. Essentially this is a revisit to the problem in my
        last post [Go based Python Modules](https://rsdoiel.github.io/blog/2018/02/24/go-based-python-modules.html) 
        but with the language pairing of Go and Julia.]]>
      </description>
      <source:markdown># Accessing Go from Julia

By R. S. Doiel, 2018-03-11

The problem: I&#39;ve started exploring Julia and I would like to leverage existing
code I&#39;ve written in Go. Essentially this is a revisit to the problem in my
last post [Go based Python Modules](https://rsdoiel.github.io/blog/2018/02/24/go-based-python-modules.html) 
but with the language pairing of Go and Julia.


## Example 1, libtwice.go, libtwice.jl and libtwice_test.jl

In out first example we send an integer value from
Julia to Go and back via a C shared library (written in Go). While Julia doesn&#39;t
require type declarations I will be using those for clarity. Like in my previous post
I think this implementation this is a good starting point to see how Julia interacts with
C shared libraries. Like before I will present our Go code, an explanation 
followed by the Julia code and commentary.

On the Go side we create a _libtwice.go_ file with an empty `main()` 
function.  Notice that we also import the *C* package and use 
a comment decoration to indicate the function we are exporting
(see https://github.com/golang/go/wiki/cgo and 
https://golang.org/cmd/cgo/
for full story about Go&#39;s _C_ package and _cgo_).
Part of the what _cgo_ and the *C* package does is use the 
comment decoration to build the signatures for the function calls
in the shared C library.  The Go toolchain does all the heavy 
lifting in making a *C* shared library based on comment 
directives like &#34;//export&#34;. We don&#39;t need much for our twice
function.

```Go
    package main
    
    import (
    	&#34;C&#34;
    )
    
    //export twice
    func twice(i int) int {
    	return i * 2
    }
    
    func main() {}
```

Like in our previous Python implementation we need to build the C shared
library before using it from Julia. Here are some example Go build commands
for Linux, Windows and Mac OS X. You only need to run the one that applies
to your operating system.

```shell
    go build -buildmode=c-shared -o libtwice.so libtwice.go
    go build -buildmode=c-shared -o libtwice.dll libtwice.go
    go build -buildmode=c-shared -o libtwice.dynlib libtwice.go
```

Unlike the Python implementation our Julia code will be split into two files. _libtwice.jl_ will
hold our module definition and _libtwice_test.jl_ will hold our test code. In the
case of _libtwice.jl_ we will access the C exported function via a function named *ccall*. 
Julia doesn&#39;t require a separate module to be imported in order to access a C shared library.
That makes our module much simpler. We still need to be mindful of type conversion.  Both 
Go and Julia provide for rich data types and structs.  But between Go and Julia we have C 
and C&#39;s basic type system.  On the Julia side *ccall* and Julia&#39;s type system help us
managing C&#39;s limitations.

Here&#39;s the Julia module we&#39;ll call _libtwice.jl_.

```Julia
    module libtwice
            
    # We write our Julia idiomatic function
    function twice(i::Integer)
        ccall((:twice, &#34;./libtwice&#34;), Int32, (Int32,), i)
    end

    end
```

We&#39;re will put the test code in a file named _libtwice\_test.jl_. Since this isn&#39;t
an establish &#34;Package&#34; in Julia we will use Julia&#39;s *include* statement to get bring the
code in then use an *import* statement to bring the module into our current name space.

```Julia
    include(&#34;libtwice.jl&#34;)
    import libtwice
    # We run this test code for libtwice.jl
    println(&#34;Twice of 2 is &#34;, libtwice.twice(2))
```

Our test code can be run with

```shell
    julia libtwice_test.jl
```

Notice the amount of lifting that Julia&#39;s *ccall* does. The Julia code is much more compact
as a result of not having to map values in a variable declaration. We still have the challenges 
that Julia and Go both support richer types than C. In a practical case we should consider 
the over head of running to two runtimes (Go&#39;s and Julia&#39;s) as well as whether or not 
implementing as a shared library even makes sense. But if you want to leverage existing 
Go based code this approach can be useful.

Example 1 is our base recipe. The next examples focus on handling
other data types but follow the same pattern.


## Example 2, libsayhi.go, libsayhi.jl and libsayhi_test.jl

Like Python, passing strings passing to or from Julia and Go is nuanced. Go is expecting 
UTF-8 strings. Julia also supports UTF-8 but C still looks at strings as a pointer to an
address space that ends in a null value. Fortunately in Julia the *ccall* function combined with
Julia&#39;s rich type system gives us straight forward ways to map those value. 
Go code remains unchanged from our Python example in the previous post. 
In this example we use Go&#39;s *fmt* package to display the string. In the next example
we will round trip our string message.

```go
    package main
    
    import (
    	&#34;C&#34;
    	&#34;fmt&#34;
    )
    
    //export say_hi
    func say_hi(msg *C.char) {
    	fmt.Println(C.GoString(msg))
    }
    
    func main() { }
```

The Go source is the similar to our first recipe. No change from our
previous posts&#39; Python example. It will need to be compiled to create our
C shared library just as before. Run the go build line that applies to
your operating system (i.e., Linux, Windows and Mac OS X).

```shell
    go build -buildmode=c-shared -o libsayhi.so libsayhi.go
    go build -buildmode=c-shared -o libsayhi.dll libsayhi.go
    go build -buildmode=c-shared -o libsayhi.dylib libsayhi.go
```

Our Julia module looks like this.

```julia
    module libsayhi

    # Now write our Julia idiomatic function using *ccall* to access the shared library
    function say_hi(txt::AbstractString)
        ccall((:say_hi, &#34;./libsayhi&#34;), Int32, (Cstring,), txt)
    end

    end
```

This code is much more compact than our Python implementation.

Our test code looks like

```julia
    include(&#34;./libsayhi.jl&#34;)
    import libsayhi
    libsayhi.say_hi(&#34;Hello again!&#34;)
```

We run our tests with

```shell
    julia libsayhi_test.jl
```


## Example 3, libhelloworld.go and librhelloworld.cl and libhelloworld_test.jl

In this example we send a string round trip between Julia and Go. 
Most of the boiler plate we say in Python is gone due to Julia&#39;s type system. In
addition to using Julia&#39;s *ccall* we&#39;ll add a *convert* and *bytestring* function calls
to bring our __Cstring__ back to a __UTF8String__ in Julia.

The Go implementation remains unchanged from our previous Go/Python implementation. 
The heavy lifting is done by the *C* package and the comment 
`//export`. We are using `C.GoString()` and `C.CString()` to flip between 
our native
Go and C datatypes.

```go
    package main
    
    import (
    	&#34;C&#34;
    	&#34;fmt&#34;
    )
    
    //export helloworld
    func helloworld(name *C.char) *C.char {
    	txt := fmt.Sprintf(&#34;Hello %s&#34;, C.GoString(name))
    	return C.CString(txt)
    }
    
    func main() { }
```

As always we must build our C shared library from the Go code. Below is
the go build commands for Linux, Windows and Mac OS X. Pick the line that
applies to your operating system to build the C shared library.

```shell
    go build -buildmode=c-shared -o libhelloworld.so libhelloworld.go
    go build -buildmode=c-shared -o libhelloworld.dll libhelloworld.go
    go build -buildmode=c-shared -o libhelloworld.dylib libhelloworld.go
```

In our Julia, _libhelloworld.jl_, the heavy lifting of type conversion
happens in Julia&#39;s type system and in the *ccall* function call. Additionally we need
to handle the conversion from __Cstring__ Julian type to __UTF8String__ explicitly
in our return value via a functions named *convert* and *bytestring*.

```julia
    module libhelloworld

    # Now write our Julia idiomatic function
    function helloworld(txt::AbstractString)
        value = ccall((:helloworld, &#34;./libhelloworld&#34;), Cstring, (Cstring,), txt)
        convert(UTF8String, bytestring(value))
    end

    end
```

Our test code looks similar to our Python test implementation.

```julia
    include(&#34;libhelloworld.jl&#34;)
    import libhelloworld
 
    if length(ARGS) &#62; 0
        println(libhelloworld.helloworld(join(ARGS, &#34; &#34;)))
    else
        println(libhelloworld.helloworld(&#34;World&#34;))
    end
```

As before we see the Julia code is much more compact than Python&#39;s.


## Example 4, libjsonpretty.go, libjsonpretty.jl and libjsonpretty_test.jl

In this example we send JSON encode text to the Go package,
unpack it in Go&#39;s runtime and repack it using the `MarshalIndent()`
function in Go&#39;s JSON package before sending it back to Julia
in C string form.  You&#39;ll see the same encode/decode patterns as 
in our *libhelloworld* example.

Go code

```go
    package main
    
    import (
    	&#34;C&#34;
    	&#34;encoding/json&#34;
    	&#34;fmt&#34;
    	&#34;log&#34;
    )
    
    //export jsonpretty
    func jsonpretty(rawSrc *C.char) *C.char {
    	data := new(map[string]interface{})
    	err := json.Unmarshal([]byte(C.GoString(rawSrc)), &#38;data)
    	if err != nil {
    		log.Printf(&#34;%s&#34;, err)
    		return C.CString(&#34;&#34;)
    	}
    	src, err := json.MarshalIndent(data, &#34;&#34;, &#34;    &#34;)
    	if err != nil {
    		log.Printf(&#34;%s&#34;, err)
    		return C.CString(&#34;&#34;)
    	}
    	txt := fmt.Sprintf(&#34;%s&#34;, src)
    	return C.CString(txt)
    }
    
    func main() {}
```

Build commands for Linux, Windows and Mac OS X are as before, pick the one that matches
your operating system.

```shell
    go build -buildmode=c-shared -o libjsonpretty.so libjsonpretty.go
    go build -buildmode=c-shared -o libjsonpretty.dll libjsonpretty.go
    go build -buildmode=c-shared -o libjsonpretty.dylib libjsonpretty.go
```

Our Julia module code

```Julia
    module libjsonpretty

    # Now write our Julia idiomatic function
    function jsonpretty(txt::AbstractString)
        value = ccall((:jsonpretty, &#34;./libjsonpretty&#34;), Cstring, (Cstring,), txt)
        convert(UTF8String, bytestring(value))
    end
    
    end
```

Our Julia test code

```Julia
    include(&#34;./libjsonpretty.jl&#34;)
    import libjsonpretty

    src = &#34;&#34;&#34;{&#34;name&#34;:&#34;fred&#34;,&#34;age&#34;:25,&#34;height&#34;:75,&#34;units&#34;:&#34;inch&#34;,&#34;weight&#34;:&#34;239&#34;}&#34;&#34;&#34;
    println(&#34;Our origin JSON src&#34;, src)
    value = libjsonpretty.jsonpretty(src)
    println(&#34;And out pretty version\n&#34;, value)
```

As before you can run your tests with `julia libjsonpretty_test.jl`.

In closing I would like to note that to use these examples I am assuming your
Julia code is in the same directory as your shared C library. Julia, like Python3,
has a feature rich module and Package system. If you are creating a serious Julia
project then you need to be familiar with how Julia&#39;s package and module system works
and place your code and shared libraries appropriately.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2018/03/11/accessing-go-from-julia.md" length="11356" type="text/markdown" />
    </item>    <item>
      <title>Go based Python modules</title>
      <link>https://rsdoiel.github.io/blog/2018/02/24/go-based-python-modules.html</link>
      <description>
        <![CDATA[I have written a number of Go packages at work.
        My colleagues know Python and I'd like them to be able to use the
        packages without resorting to system calls from Python to the
        command line implementations. The solution is create a C-Shared
        library from my Go packages, using Go's _C_ package and combine it
        with Python's _ctypes_ package.  What follows is a series of 
        simple recipes I used to understand the details of how that worked.]]>
      </description>
      <source:markdown># Go based Python modules

By R. S. Doiel, 2018-02-24

The problem: I have written a number of Go packages at work.
My colleagues know Python and I&#39;d like them to be able to use the
packages without resorting to system calls from Python to the
command line implementations. The solution is create a C-Shared
library from my Go packages, using Go&#39;s _C_ package and combine it
with Python&#39;s _ctypes_ package.  What follows is a series of 
simple recipes I used to understand the details of how that worked.


## Example 1, libtwice.go and twice.py

Many of the the examples I&#39;ve come across on the web start by 
showing how to run a simple math operation on the Go side with
numeric values traveling round trip via the C shared library layer. 
It is a good place to start as you only need to consider type 
conversion between both Python&#39;s runtime and Go&#39;s runtime.  It 
provides a simple illustration of how the Go *C* package, Python&#39;s
*ctypes* module and the toolchain work together.

In this example we have a function in Go called &#34;twice&#34; it takes
a single integer, doubles it and returns the new value.  On
the Go side we create a _libtwice.go_ file with an empty `main()` 
function.  Notice that we also import the *C* package and use 
a comment decoration to indicate the function we are exporting
(see https://github.com/golang/go/wiki/cgo and 
https://golang.org/cmd/cgo/
for full story about Go&#39;s _C_ package and _cgo_).
Part of the what _cgo_ and the *C* package does is use the 
comment decoration to build the signatures for the function calls
in the shared C library.  The Go toolchain does all the heavy 
lifting in making a *C* shared library based on comment 
directives like &#34;//export&#34;. We don&#39;t need much for our twice
function.

```Go
    package main
    
    import (
    	&#34;C&#34;
    )
    
    //export twice
    func twice(i int) int {
    	return i * 2
    }
    
    func main() {}
```

On the python side we need to wrap our calls to our shared library
bringing them into the Python runtime in a useful and idiomatically
Python way. Python provides a few ways of doing this. In my examples
I am using the *ctypes* package.  _twice.py_ looks like this--

```python
    import ctypes
    import os
    
    # Set our shared library&#39;s name
    lib_name=&#39;libtwice&#39;
    
    # Figure out shared library extension
    uname = os.uname().sysname
    ext = &#39;.so&#39;
    if uname == &#39;Darwin&#39;:
        ext = &#39;.dylib&#39;
    if uname == &#39;Windows&#39;:
        ext = &#39;.dll&#39;
    
    # Find our shared library and load it
    dir_path = os.path.dirname(os.path.realpath(__file__))
    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))
    
    # Setup our Go functions to be nicely wrapped
    go_twice = lib.twice
    go_twice.argtypes = [ctypes.c_int]
    go_twice.restype = ctypes.c_int
    
    # Now write our Python idiomatic function
    def twice(i):
        return go_twice(ctypes.c_int(i))
    
    # We run this test code if with: python3 twice.py
    if __name__ == &#39;__main__&#39;:
        print(&#34;Twice of 2 is&#34;, twice(2))
```

Notice the amount of lifting Python&#39;s *ctypes* does for us. It provides
for converting C based types to their Python counter parts. Indeed the
additional Python source here is focused around using that functionality
to create a simple Python function called twice. This pattern of 
bringing in a low level version of our desired function and then 
presenting in a Pythonic one is common in more complex C based Python
modules.  In general we need *ctypes* to access and wrapping our 
shared library. The *os* module is used so we can find our C 
shared library based on the naming conventions of our host OS. 
For simplicity I&#39;ve kept the shared library (e.g. _libtwice.so_ 
under Linux) in the same directory as the python module 
code _twice.py_.

The build command for Linux looks like---

```shell
    go build -buildmode=c-shared -o libtwice.so libtwice.go
```

Under Windows it would look like---

```shell
    go build -buildmode=c-shared -o libtwice.dll libtwice.go
```

and Mac OS X---

```shell
    go build -buildmode=c-shared -o libtwice.dynlib libtwice.go
```

You can test the Python module with---

```shell
    python3 twice.py
```

Notice the filename choices. I could have called the Go shared
library anything as long as it wasn&#39;t called `twice.so`, `twice.dll`
or `twice.dylib`. This constraint is to avoid a module name collision
in Python.  If we had a Python script named `twice_test.py` and 
import `twice.py` then Python needs to make a distinction between
`twice.py` and our shared library. If you use a Python package
approach to wrapping the shared library you would have other options
for voiding name collision.

Here is an example of `twice_test.py` to make sure out import is
working.

```python
    import twice
    print(&#34;Twice 3&#34;, twice.twice(3))
```

Example 1 is our base recipe. The next examples focus on handling
other data types but follow the same pattern.


## Example 2, libsayhi.go and sayhi.py

I found working with strings a little more nuanced. Go&#39;s concept of
strings are oriented to utf-8. Python has its own concept of strings 
and encoding.  Both need to pass through the C layer which assumes 
strings are a char pointer pointing at contiguous memory ending 
in a null. The *sayhi* recipe is focused on moving a string from 
Python, to C, to Go (a one way trip this time). The example uses 
Go&#39;s *fmt* package to display the string. 

```go
    package main
    
    import (
    	&#34;C&#34;
    	&#34;fmt&#34;
    )
    
    //export say_hi
    func say_hi(msg *C.char) {
    	fmt.Println(C.GoString(msg))
    }
    
    func main() { }
```

The Go source is similar to our first recipe but our Python modules
needs to use *ctypes* to get you Python string into shape to be
unpacked by Go.

```python
   import ctypes
   import os
   
   # Set the name of our shared library
   lib_name = &#39;libsayhi&#39;

   # Figure out shared library extension
   uname = os.uname().sysname
   ext = &#39;.so&#39;
   if uname == &#39;Darwin&#39;:
       ext = &#39;.dylib&#39;
   if uname == &#39;Windows&#39;:
       ext = &#39;.dll&#39;
   
   # Find our shared library and load it
   dir_path = os.path.dirname(os.path.realpath(__file__))
   lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))
   
   # Setup our Go functions to be nicely wrapped
   go_say_hi = lib.say_hi
   go_say_hi.argtypes = [ctypes.c_char_p]
   # NOTE: we don&#39;t have a return type defined here, the message is 
   # displayed from Go
   
   # Now write our Python idiomatic function
   def say_hi(txt):
       return go_say_hi(ctypes.c_char_p(txt.encode(&#39;utf8&#39;)))
   
   if __name__ == &#39;__main__&#39;:
       say_hi(&#39;Hello!&#39;)
```

Putting things together (if you are using Windows or Mac OS X
you&#39;ll adjust name output name, `libsayhi.so`, to match the
filename extension suitable for your operating system).

```bash
    go build -buildmode=c-shared -o libsayhi.so libsayhi.go
```

and testing.

```bash
    python3 sayhi.py
```


## Example 3, libhelloworld.go and helloworld.py

In this example we send a Python string to Go (which expects utf-8)
build our &#34;hello world&#34; message and then send it back to Python
(which needs to do additional conversion and decoding).

Like in previous examples the Go side remains very simple. The heavy
lifting is done by the *C* package and the comment `//export`. We
are using `C.GoString()` and `C.CString()` to flip between our native
Go and C datatypes.

```go
    package main
    
    import (
    	&#34;C&#34;
    	&#34;fmt&#34;
    )
    
    //export helloworld
    func helloworld(name *C.char) *C.char {
    	txt := fmt.Sprintf(&#34;Hello %s&#34;, C.GoString(name))
    	return C.CString(txt)
    }
    
    func main() { }
```

In the python code below the conversion process is much more detailed.
Python isn&#39;t explicitly utf-8 like Go. Plus we&#39;re sending our Python 
string via C&#39;s char arrays (or pointer to chars). Finally when we 
comeback from Go via C we have to put things back in order for Python. 
Of particular note is checking how the byte arrays work then 
encoding/decoding everything as needed. We also explicitly set the result 
type from our Go version of the helloworld function.

```python
    import ctypes
    import os
    
    # Set the name of our shared library
    lib_name = &#39;libhelloworld&#39;

    # Figure out shared library extension
    uname = os.uname().sysname
    ext = &#39;.so&#39;
    if uname == &#39;Darwin&#39;:
        ext = &#39;.dylib&#39;
    if uname == &#39;Windows&#39;:
        ext = &#39;.dll&#39;
    
    # Find our shared library and load it
    dir_path = os.path.dirname(os.path.realpath(__file__))
    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))
    
    # Setup our Go functions to be nicely wrapped
    go_helloworld = lib.helloworld
    go_helloworld.argtypes = [ctypes.c_char_p]
    go_helloworld.restype = ctypes.c_char_p
    
    # Now write our Python idiomatic function
    def helloworld(txt):
        value = go_helloworld(ctypes.c_char_p(txt.encode(&#39;utf8&#39;)))
        if not isinstance(value, bytes):
            value = value.encode(&#39;utf-8&#39;)
        return value.decode()
    
    
    if __name__ == &#39;__main__&#39;:
        import sys
        if len(sys.argv) &#62; 1:
            print(helloworld(sys.argv[1]))
        else:
            print(helloworld(&#39;World&#39;))
```

The build recipe remains the same as the two previous examples.

```bash
    go build -buildmode=c-shared -o libhelloworld.so libhelloworld.go
```

Here are two variations to test.

```bash
     python3 helloworld.py
     python3 helloworld.py Jane
```


## Example 4, libjsonpretty.go and jsonpretty.py

In this example we send JSON encode text to the Go package,
unpack it in Go&#39;s runtime and repack it using the `MarshalIndent()`
function in Go&#39;s JSON package before sending it back as Python
in string form.  You&#39;ll see the same encode/decode patterns as 
in our *helloworld* example.

Go code

```go
    package main
    
    import (
    	&#34;C&#34;
    	&#34;encoding/json&#34;
    	&#34;fmt&#34;
    	&#34;log&#34;
    )
    
    //export jsonpretty
    func jsonpretty(rawSrc *C.char) *C.char {
    	data := new(map[string]interface{})
    	err := json.Unmarshal([]byte(C.GoString(rawSrc)), &#38;data)
    	if err != nil {
    		log.Printf(&#34;%s&#34;, err)
    		return C.CString(&#34;&#34;)
    	}
    	src, err := json.MarshalIndent(data, &#34;&#34;, &#34;    &#34;)
    	if err != nil {
    		log.Printf(&#34;%s&#34;, err)
    		return C.CString(&#34;&#34;)
    	}
    	txt := fmt.Sprintf(&#34;%s&#34;, src)
    	return C.CString(txt)
    }
    
    func main() {}
```

Python code

```python
    import ctypes
    import os
    import json
    
    # Set the name of our shared library
    lib_name = &#39;libjsonpretty&#39;

    # Figure out shared library extension
    uname = os.uname().sysname
    ext = &#39;.so&#39;
    if uname == &#39;Darwin&#39;:
        ext = &#39;.dylib&#39;
    if uname == &#39;Windows&#39;:
        ext = &#39;.dll&#39;

    dir_path = os.path.dirname(os.path.realpath(__file__))
    lib = ctypes.cdll.LoadLibrary(os.path.join(dir_path, lib_name+ext))
    
    go_jsonpretty = lib.jsonpretty
    go_jsonpretty.argtypes = [ctypes.c_char_p]
    go_jsonpretty.restype = ctypes.c_char_p
    
    def jsonpretty(txt):
        value = go_jsonpretty(ctypes.c_char_p(txt.encode(&#39;utf8&#39;)))
        if not isinstance(value, bytes):
            value = value.encode(&#39;utf-8&#39;)
        return value.decode()
    
    if __name__ == &#39;__main__&#39;:
        src = &#39;&#39;&#39;
    {&#34;name&#34;:&#34;fred&#34;,&#34;age&#34;:25,&#34;height&#34;:75,&#34;units&#34;:&#34;inch&#34;,&#34;weight&#34;:&#34;239&#34;}
    &#39;&#39;&#39;
        value = jsonpretty(src)
        print(&#34;Pretty print&#34;)
        print(value)
        print(&#34;Decode into dict&#34;)
        o = json.loads(value)
        print(o)
```

Build command

```shell
    go build -buildmode=c-shared -o libjsonpretty.so libjsonpretty.go
```

As before you can run your tests with `python3 jsonpretty.py`.

In closing I would like to note that to use these examples you Python3
will need to be able to find the module and shared library. For 
simplicity I&#39;ve put all the code in the same directory. If your Python
code is spread across multiple directories you&#39;ll need to make some 
adjustments.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2018/02/24/go-based-python-modules.md" length="12920" type="text/markdown" />
    </item>    <item>
      <title>Go, Bleve and Library oriented software</title>
      <link>https://rsdoiel.github.io/blog/2018/02/19/go-bleve-and-libraries.html</link>
      <description>
        <![CDATA[In 2016, Stephen Davison, asked me, "Why use Go and Blevesearch for
        our library projects?" After our conversation I wrote up some notes so
        I would remember. It is now 2018 and I am revising these notes. I
        think our choice paid off.  What follows is the current state of my
        reflection on the background, rational, concerns, and risk mitigation
        strategies so far for using [Go](https://golang.org) and
        [Blevesearch](https://blevesearch.com) for Caltech Library projects.]]>
      </description>
      <source:markdown># Go, Bleve and Library oriented software

By R. S. Doiel, 2018-02-19
(updated: 2018-02-22)

In 2016, Stephen Davison, asked me, &#34;Why use Go and Blevesearch for
our library projects?&#34; After our conversation I wrote up some notes so
I would remember. It is now 2018 and I am revising these notes. I
think our choice paid off.  What follows is the current state of my
reflection on the background, rational, concerns, and risk mitigation
strategies so far for using [Go](https://golang.org) and
[Blevesearch](https://blevesearch.com) for Caltech Library projects.

## Background

I first came across Go a few years back when it was announced as an
Open Source project by Google at an Google I/O event (2012). The
original Go authors were Robert Griesemer, Rob Pike, and Ken
Thompson. What I remember from that presentation was Go was a rather
consistent language with the features you need but little else.  Go
developed at Google as a response to high development costs for C/C++
and Java in addition to challenges with performance and slow
compilation times.  As a language I would put Go between C/C++ and
Java. It comes the ease of writing and reading you find in languages
like Python. Syntax is firmly in the C/C++ family but heavily
simplified. Like Java it provides many modern features including rich basic
data structures and garbage collection. It has a very complete standard
library and provides very good tooling.  This makes it easy to
generate code level documentation, format code, test, efficiently profile, 
and debug.

Often programming languages develop around a specific set of needs.
This is true for Go. Given the Google origin it should not be
surprising to find that Go&#39;s primary strengths are working with 
structured data, I/O and concurrency. The rich standard
library is organized around a package concept. These include packages
supporting network protocols, file and socket I/O as well as various
encoding and compression scheme. It has particularly strong support
for XML, JSON, CSV formatted data out of the box. It has a template
library for working with plain text formats as well as generating safe
HTML. You can browse Go&#39;s standard library https://golang.org/pkg/.

An additional feature is Go&#39;s consistency. Go code that compiles under
version 1.0 still compiles under 1.10. Even before 1.0 code changes
that were breaking came with tooling to automatically updates existing
code.  Running code is a strong element of Go&#39;s evolution.

Go is unsurprising and has even been called boring.  This turns out to
be a strength when building sustainable projects in a small team.


## Why do I write Go?

For me Go is a good way to write web services, assemble websites,
create search appliances and write command line (cli) utilities. When
a shell script becomes unwieldy Go is often what I turn to.  Go is
well suited to building tools as well as systems.  Go based command
line tools are very easy to orchestrate with shell and Python.

Go runs on all the platforms I actively use - Windows, Mac OS X, Linux
on both Intel and ARM (e.g. Raspberry Pi, Pine64). It has experimental
support for Android and iOS.  I&#39;ve used a tool called
[GopherJS](http://gopherjs.org) to write web browser applications that
transform my command line tools into web tools with a friendlier user
interface (see our [BibTeX Tools](https://caltechlibrary.github.io/bibtex/webapp/)).

Go supports cross compiling out of the box. This means a production
system running on AWS, Google&#39;s compute engine or Microsoft&#39;s Azure
can be compiled from Windows, Mac OS or even a Raspberry Pi!
Deployment is a matter of copying the (self contained) compiled binary
onto the production system. This contrasts with other
platforms like Perl, PHP, Python, NodeJS and Ruby where you need to
install not only your application code but all dependencies. While
interpretive languages retain an advantage of having a REPL, Go
based programs have advantages of fast compile times and easy deployment.

In many of the projects I&#39;ve written in Go I&#39;ve only required a few
(if any) 3rd party libraries (packages in Go&#39;s nomenclature). This is
quite a bit different from my experience with Perl, PHP, Python,
NodeJS and Ruby. This is in large part a legacy of having grown up at
Google before become an open source project. While the Go standard
packages are very good there is a rich ecosystem for 3rd party
packages for specialized needs. I&#39;ve found I tend to rely only on a
few of them. The one I&#39;ve used the most is
[Bleve](http://blevesearch.com).

Bleve is a Go package for building search engines. When I originally
came across Bleve (around 2014), it was described as &#34;Lucene lite&#34;. 
&#34;Lucene lite&#34; was an apt description, but I find it easier
to use than Lucene. When I first used Bleve I embedded its
functionality into the tools I used to process data and present web
services. It did not have much in the way of stand alone command line
tooling.  Today I increasingly think of Bleve as &#34;Elastic Search
lite&#34;. It ships with a set of command line tools that include support
for building Bleve&#39;s indexes.  My current practice is to only embed the search
portion of the packages. I can use the Bleve command line for the
rest.  In 2018, Bleve is being actively developed, has a small vibrant
community and is used by [Couchbase](https://couchbase.com), a well
established NoSQL player.


## Who is using Go?

Many companies use Go. The short list includes
Google, Amazon, Netflix, Dropbox, Box, eBay, Pearsons and even
Walmart and Microsoft. This came to my attention at developer conferences
back in 2014.  People from many of these companies started
presenting at conferences on pilot projects that had been successful
and moved to production. Part of what drove adoption was the ease
of development in Go along with good system performance. I also think
there was a growing disenchantment with alternatives like C++, C sharp
and Java as well as the weight of the LAMP, Tomcat, and OpenStack.

Highly visible Go based projects include

+ [Docker](http://docker.org) and [Rocket](http://www.docker.com) - Containerization for running process in the cloud
+ [Kubernettes](http://kubernetes.io/) and [Terraform](https://www.terraform.io/) - Container orchestration systems
+ [Hugo](http://hugo.io) - the fast/popular static website generator, an alternative to Jekyll, for those who want speed
+ [Caddy](https://caddyserver.com/) - a Go based web server trying to unseat Apache/NGinX focusing on easy of use plus speed
+ [IPFS](http://ipfs.io) - a cutting edge distributed storage system based on block chains


### Who is using Blevesearch?

Here&#39;s some larger projects using Bleve.

+ [Couchbase](http://www.couchbase.com), a NoSQL database platform are replacing Lucene with Bleve.  Currently the creator of Bleve works for them.
+ [Hugo](http://hugo.io) can integrate with Bleve for search and index generation
+ [Caddy](https://caddyserver.com/) integrates with Bleve to provide an embedded search capability


## Managing risks

In 2014 Go was moving from bleeding to leading edge. Serious capital
was behind its adoption and it stopped being an exotic conference
item. In 2014 Bleve was definitely bleeding edge. By late 2015 and early
2016 the program level API stabilized. People were piloting projects
with it. This included our small group at Caltech Library. In 2015
non-English language support appeared followed by a growing list
of non-European languages in 2016. By mid 2016 we started to see 
missing features like alternative sorting added. While Bleve isn&#39;t
yet 1.0 (Feb. 2018) it is reliable. The primary challenge for the Bleve
project is documentation targeting the novice and non-Programmer users.
Bleve has proven effective as an indexing and search platform for 
archival, library, and data repository content.

Adopting new software comes with risk. We have mitigated this in two ways.

1. Identify alternative technology (a plan B)
2. Architect our systems for easy decomposition and re-composition

In the case of Go, packages can be compiled to a C-Shared
library. This allows us to share working Go packages with languages
like Python, R, and PHP. We have included shared Go/Python modules
on our current road map for projects.

For Blevesearch the two alternatives are Solr and Elastic
Search. Both are well known, documented, and solid.  The costs would be
recommitting to a Java stack and its resource requirements. We have
already identified what we want to index and that could be converted
to either platform if needed.  If we stick with Go but dropped 
Blevesearch we would swap out the Bleve specific code for Go packages 
supporting Solr and Elastic Search.


The greatest risk in adopting Go for library and archive projects was 
knowledge transfer. We addressed this 
by knowledge sharing and insuring the Go codebase can 
be used via command line programs.  Additionally 
we are adding support for Go based Python modules.
Training also is available in the form of books, websites and
online courses ([lynda.com](https://www.lynda.com/Go-tutorials/Up-Running-Go/412378-2.html) offers a &#34;Up Running Go&#34; course).


## What are the benefits?

For library and archives software we have found Go&#39;s benefits include
improved back end systems performance at a lower cost, ease of development, 
ease of deployment, a rich standard library focused on the types of things 
needed in library and archival software.  Go plays nice with
other systems (e.g. I create an API based service in Go that can easily
be consumed by a web browser running JavaScript or Perl/PHP/Python
code running under LAMP). In the library and archives setting Go 
can become a high performance duck tape. We get the performance and 
reliability of C/Java type systems with code simplicity 
similar to Python.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2018/02/19/go-bleve-and-libraries.md" length="10747" type="text/markdown" />
    </item>    <item>
      <title>Raspbian Stretch on DELL E4310 Laptop</title>
      <link>https://rsdoiel.github.io/blog/2017/12/18/raspbian-stretch-on-amd64.html</link>
      <description>
        <![CDATA[This post talks about a used Dell E4310 I purchased. It covers setting it up with [Raspbian Stretch](https://www.raspberrypi.org/blog/raspbian-stretch/) and configuring it so I can share it with my family.]]>
      </description>
      <source:markdown># Raspbian Stretch on DELL E4310 Laptop

by R. S. Doiel 2017-12-18

Today I bought a used Dell E4310 laptop. The E4310 is an &#34;old model&#34; now
but certainly not vintage yet.  It has a nice keyboard and reasonable 
screen size and resolution. I bought it as a writing machine. I mostly
write in Markdown or Fountain depending on what I am writing these days.

## Getting the laptop setup

The machine came with a minimal bootable Windows 7 CD and an blank 
internal drive. Windows 7 installed fine but was missing the network 
drivers for WiFi.  I had previously copied the new [Raspbian Stretch](https://www.raspberrypi.org/blog/raspbian-stretch/) ISO to a USB drive. While
the E4310 didn&#39;t support booting from the USB drive Windows 7 does make
it easy to write to a DVRW. After digging around and finding a blank disc
I could write to it was a couple of mouse clicks and a bit of waiting 
and I had new bootable Raspbian Stretch CD.

Booting from the Raspbian Stretch CD worked like a charm. I selected 
the graphical install which worked well though initially the trackpad 
wasn&#39;t visible so I just used keyboard navigation to setup the install.
After the installation was complete and I rebooted without the install
disc everything worked except the internal WiFi adapter.

I had several WiFi dongles that I use with my Raspberry Pis so I 
borrowed one and with that was able to run the usual `sudo apt update 
&#38;&#38; sudo apt upgrade`.

While waiting for the updates I did a little web searching and found 
what I needed to know on the Debian Wiki (see
https://wiki.debian.org/iwlwifi?action=show&#38;redirect=iwlagn).  Following
the instructions for *Debian 9 &#34;Stretch&#34;* ---

```shell
    sudo vi /etc/apt/sources.list.d/non-free.list 
    # adding the deb source line from the web page
    sudo apt update &#38;&#38; sudo apt install fireware-iwlwifi
    sudo modprobe -r iwlwifi; sudo modprobe iwlwifi
    sudo shutdown -r now
```

After that everything came up fine.

## First Impressions

First, I like Raspbian Pixel. It was fun on my Pi but on an Intel box
with 4Gig RAM it is wicked fast.  Pixel is currently my favorite flavor 
of Debian GNU/Linux. It is simple, minimal with a consistent UI for 
an X based system. Quite lovely. 

If you&#39;ve got an old laptop you&#39;d like to breath some life into 
Raspbian Stretch is the way to go.


### steps for my install process

+ Booted from a minimal Windows 7 CD to get a basic OS minus networking
+ Used Windows 7 and the internal DVD-RW to create a Raspbian Stretch CD
+ Booted from the Raspbian Stretch CD and installed Raspbian replacing Windows 7
+ Used a spare WiFi dongle initially to fetch the non-free iwlwifi modules
+ Updated my source list, re-run apt update and upgrade
+ Rebooted and everything came up and is running</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2017/12/18/raspbian-stretch-on-amd64.md" length="3482" type="text/markdown" />
    </item>    <item>
      <title>Harvesting my Gists from GitHub</title>
      <link>https://rsdoiel.github.io/blog/2017/12/10/harvesting-my-gists-from-github.html</link>
      <description>
        <![CDATA[This is a just quick set of notes on harvesting my Gists on GitHub so I
        have an independent copy for my own website.]]>
      </description>
      <source:markdown># Harvesting my Gists from GitHub

By R. S. Doiel 2017-12-10

This is a just quick set of notes on harvesting my Gists on GitHub so I
have an independent copy for my own website. 

## Assumptions

In this gist I assume you are using Bash on a POSIX system (e.g. Raspbian 
on a Raspberry Pi) with the standard compliment of Unix utilities (e.g. cut, 
sed, curl). I also use Stephen Dolan&#39;s [jq](https://github.com/stedolan/jq)
as well as Caltech Library&#39;s [datatools](https://github.com/caltechlibrary/datatools).
See the respective GitHub repositories for installation instructions.
The gist harvest process was developed against GitHub&#39;s v3 API
(see developer.github.com). 

In the following examples &#34;USER&#34; is assumed to hold your GitHub user id 
(e.g. rsdoiel for https://github.com/rsdoiel).

## Getting my basic profile

This retrieves the public view of your profile.

```shell
    curl -o USER &#34;https://api.github.com/users/USER&#34;
```

## Find the urL for your gists

Get the gists url from `USER.json.

```shell
    GISTS_URL=$(jq &#34;.gists_url&#34; &#34;USER.json&#34; | sed -E &#39;s/&#34;//g&#39; | cut -d &#39;{&#39; -f 1)
    curl -o gists.json &#34;${GISTS_URL}&#34;
```

Now `gists.json` should hold a JSON array of objects representing your Gists.

## Harvesting the individual Gists.

When you look at _gists.json_ you&#39;ll see a multi-level JSON structure.  It has been
formatted by the API so be easy to scrape.  But since this data is JSON and Caltech Library
has some nice utilities for working with JSON I&#39;ll use *jsonrange* and *jq* to pull out a list
of individual Gists URLS.

```shell
    jsonrange -i gists.json | while read I; do 
        jq &#34;.[$I].files&#34; gists.json | sed -E &#39;s/&#34;//g&#39;
    done
```

Expanding this we can now curl each individual gist metadata to find URL to the raw file.


```shell
    jsonrange -i gists.json | while read I; do 
        jq &#34;.[$I].files&#34; gists.json | jsonrange -i - | while read FNAME; do
            jq &#34;.[$I].files[\&#34;$FNAME\&#34;].raw_url&#34; gists.json | sed -E &#39;s/&#34;//g&#39;; 
        done;
    done
```

Now that we have URLs to the raw gist files we can use curl again to fetch each.

What do we want to store with our harvested gists?  The raw files, metadata
about the Gist (e.g. when it was created), the Gist ID. Putting it all together
we have the following script.

```shell
    #!/bin/bash
    if [[ &#34;$1&#34; = &#34;&#34; ]]; then
        echo &#34;USAGE: $(basename &#34;$0&#34;) GITHUB_USERNAME&#34;
        exit 1
    fi

    USER=&#34;$1&#34;
    curl -o &#34;$USER.json&#34; &#34;https://api.github.com/users/$USER&#34;
    if [[ ! -s &#34;$USER.json&#34; ]]; then
        echo &#34;Someting went wrong getting https://api.github.cm/users/${USER}&#34;
        exit 1
    fi

    GISTS_URL=$(jq &#34;.gists_url&#34; &#34;$USER.json&#34; | sed -E &#39;s/&#34;//g&#39; | cut -d &#39;{&#39; -f 1)
    curl -o gists.json &#34;${GISTS_URL}&#34;
    if [[ ! -s gists.json ]]; then
        echo &#34;Someting went wrong getting ${GISTS_URL}&#34;
        exit 1
    fi

    # For each gist harvest our file
    jsonrange -i gists.json | while read I; do
        GIST_ID=$(jq &#34;.[$I].id&#34; gists.json | sed -E &#39;s/&#34;//g&#39;)
        mkdir -p &#34;gists/$GIST_ID&#34;
        echo &#34;Saving gists/$GIST_ID/metadata.json&#34;
        jq &#34;.[$I]&#34; gists.json &#62; &#34;gists/$GIST_ID/metadata.json&#34;
        jq &#34;.[$I].files&#34; gists.json | jsonrange -i - | while read FNAME; do
            URL=$(jq &#34;.[$I].files[\&#34;$FNAME\&#34;].raw_url&#34; gists.json | sed -E &#39;s/&#34;//g&#39;)
            echo &#34;Saving gist/$GIST_ID/$FNAME&#34;
            curl -o &#34;gists/$GIST_ID/$FNAME&#34; &#34;$URL&#34;
        done;
    done
```</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2017/12/10/harvesting-my-gists-from-github.md" length="4023" type="text/markdown" />
    </item>    <item>
      <title>NodeJS, NPM, Electron</title>
      <link>https://rsdoiel.github.io/blog/2017/10/20/node-npm-electron.html</link>
      <description>
        <![CDATA[This post discusses nodeJS and Electron.]]>
      </description>
      <source:markdown># NodeJS, NPM, Electron

By R. S. Doiel 2017-10-20

Electron is an app platform leveraging web technologies. Conceptually it is a
mashup of NodeJS and Chrome browser. [Electron](https://electron.atom.io/) site
has a nice starter app. It displays a window with Electron version info and
&#39;hello world&#39;.

Before you can get going with _Electron_ you need to have a
working _NodeJS_ and _NPM_. I usually compile from source and this
was my old recipe (adjusted for v8.7.0).

```shell
    cd
    git clone https://github.com/nodejs/node.git
    cd node
    git checkout v8.7.0
    ./configure --prefix=$HOME
    make &#38;&#38; make install
```

To install an _Electron Quick Start_ I added the additional steps.

```shell
    cd
    git clone https://github.com/electron/electron-quick-start
    cd electron-quick-start
    npm install
    npm start
```

Notice _Electron_ depends on a working _node_ and _npm_.  When I
tried this recipe it failed on `npm install` with errors regarding
internal missing node modules.

After some fiddling I confirmed my node/npm install failed because
I had install the new version of over a partially installed previous
version. This causes the node_modules to be populated with various
conflicting versions of internal modules.

Sorting that out allowed me to test the current version of
*electron-quick-start* cloned on 2017-10-20 under _NodeJS_ v8.7.0.

## Avoiding Setup Issues in the future

The *Makefile* for _NodeJS_ includes an &#39;uninstall&#39; option. Revising
my _NodeJS_ install recipe above I now do the following to setup a machine
to work with _NodeJS_ or _Electron_.

```shell
    git clone git@github.com:nodejs/node.git
    cd node
    ./configure --prefix=$HOME
    make uninstall
    make clean
    make -j 5
    make install
```

If I am on a device with a multi-core CPU (most of the time) you can speed
up the make process using a `-j CPU_CORE_COUNT_PLUS_ONE` option (e.g. `-j 5`
for my 4 core x86 laptop).

Once _node_ and _npm_ were working normally the instructions in the
*electron-quick-start* worked flawlessly on my x86.

I have tested the node install recipe change on my Pine64 Pinebook, on 
several Raspberry Pi 3s as well as my x86 Ubuntu Linux laptop.

I have not gotten Electron up on my Pine64 Pinebook or Raspberry Pi&#39;s yet. 
`npm install` outputs errors suggesting that it is expecting an x86 architecture.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2017/10/20/node-npm-electron.md" length="2850" type="text/markdown" />
    </item>    <item>
      <title>Cross compiling Go 1.8.3 for Pine64 Pinebook</title>
      <link>https://rsdoiel.github.io/blog/2017/06/16/cross-compiling-go.html</link>
      <description>
        <![CDATA[This is a post on setting cross compilation for a Pine64 Pinebook.]]>
      </description>
      <source:markdown># Cross compiling Go 1.8.3 for Pine64 Pinebook

By R. S. Doiel 2017-06-16

Pine64&#39;s Pinebook has a 64-bit Quad-Core ARM Cortex A53 which is 
not the same ARM processor found on a Raspberry Pi 3. As a 
result it needs its own compiled version of Go. Fortunately cross 
compiling Go is very straight forward. I found two helpful Gists
on GitHub discussing compiling Go for a 64-Bit ARM processor. 

+ [conoro&#39;s gist](https://gist.github.com/conoro/4fca191fad018b6e47922a21fab499ca)
+ [truedat101&#39;s gist](https://gist.github.com/truedat101/5898604b1f7a1ec42d65a75fa6a0b802)

I am using a Raspberry Pi 3, raspberrypi.local, as my cross compile 
host. Go 1.8.3 is already compiled and available.  Inspired by the 
gists I worked up with this recipe to bring a Go 1.8.3 to my Pinebook.

```shell
    cd
    mkdir -p gobuild
    cd gobuild
    git clone https://github.com/golang/go.git go1.8.3
    cd go1.8.3
    git checkout go1.8.3
    export GOHOSTARCH=arm
    export GOARCH=arm64
    export GOOS=linux
    cd src
    ./bootstrap.bash
```

After the bootstrap compile is finished I switch to my Pinebook,
copy the bootstrap compiler to my Pinebook and use it to compile
a new go1.8.3 for Pine64.

```shell
    cd
    scp -r raspberrypi.local:gobuild/*.tbz ./
    tar jxvf go-linux-arm64-bootstrap.tbz
    export GOROOT=go-linux-arm64-bootstrap
    go-linux-arm64-bootstrap/bin/go version
    unset GOROOT
    git clone https://github.com/golang/go
    cd go
    git checkout go1.8.3
    export GOROOT_BOOTSTRAP=$HOME/go-linux-arm64-bootstrap
    cd src
    ./all.bash
```

_all.bash_ will successfully compile _go_ and _gofmt_ but fail on 
the tests. It&#39;s not perfect but appears to work as I explore
building Go applications on my Pinebook. Upcoming Go releases should
provide better support for 64 bit ARM.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2017/06/16/cross-compiling-go.md" length="2329" type="text/markdown" />
    </item>    <item>
      <title>Exploring Bash for Windows 10 Pro</title>
      <link>https://rsdoiel.github.io/blog/2016/08/15/Setting-up-Go-under-Bash-for-Windows-10.html</link>
      <description>
        <![CDATA["_Exploring Bash for Windows 10 Pro_" coverse the process of setting up and configuring Bash on Windows 10 Pro within a Virtual Box environment. The setup involves enabling 
        developer mode, activating the Linux Subsystem Beta, and installing Bash on Ubuntu on Windows. Also covered is setting Go for this environment.]]>
      </description>
      <source:markdown># Exploring Bash for Windows 10 Pro

By R. S. Doiel 2016-08-15

    UPDATE (2016-10-27, RSD): Today trying to compile Go 1.7.3 under 
    Windows 10 Pro I&#39;ve am getting compile errors when the 
    assembler is being built.  I can compile go1.4.3 but see errors 
    in some of the tests results.

## Initial Setup and configuration

I am running Windows 10 Pro (64bit) Anniversary edition under Virtual Box. The VM was upgraded from an earlier version of Windows 10 Pro (64bit). The VM was allocated 4G or ram, 200G disc and simulating 2 cores.  After the upgrade I took the following steps

+ Search with Bing for &#34;Bash for Windows&#34; 
    + Bing returns http://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/
+ Switch on developer mode for Windows
+ Turned on Linux Subsystem Beta (searched for &#34;Turning on Features&#34;)
+ Reboot
+ Search for &#34;Bash&#34; and clicked on &#34;Run Bash command&#34;
+ Answered &#34;y&#34;
+ Waited for download and extracted file system
+ When prompted setup developer account with username/password
    + Documentation can be found at https://aka.ms/wsldocs
+ Exit root install shell
+ Search for &#34;Bash&#34; locally
+ Launched &#34;Bash on Ubuntu on Windows&#34;
+ Authenticate with your username/password


## Setting up Go under Bash for Windows 10

With Bash installed these are the steps I took to compile Go
under Bash on Ubuntu on Windows.

```shell
    sudo apt-get update &#38;&#38; sudo apt-get upgrade -y
    sudo apt-get autoremove
    sudo apt-get install build-essential clang git-core unzip zip -y
    export CGO_ENABLE=0
    git clone https://github.com/golang/go go1.4
    git clone https://github.com/golang/go go
    cd go1.4
    git checkout go1.4.3
    cd src
    ./all.bash
    cd
    export PATH=$PATH:$HOME/go1.4/bin
    cd go
    git checkout go1.7
    cd src
    ./all.bash
    cd
    export PATH=$HOME/go/bin:$HOME/bin:$PATH
    export GOPATH=$HOME
```

Note some tests failing during compilation in both 1.4.3 and 1.7. They mostly failed
around network sockets.  This is probably a result of the limitations in the Linux subsystem
under Windows.

If successful you should be able to run `go version` as well as install additional Go based software
with the usual `go get ...` syntax.

In your `.bashrc` or `.profile` add the following

```shell
    export PATH=$HOME/go/bin:$HOME/bin:$PATH
    export GOPATH=$HOME
```


## Improved vim setup

I like the vim-go packages for editing Go code in vim. They are easy to setup.

```shell
     mkdir -p ~/.vim/autoload ~/.vim/bundle 
     curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim
     git clone https://github.com/fatih/vim-go.git ~/.vim/bundle/vim-go
```

Example $HOME/.vimrc

```vimrc
    execute pathogen#infect()
    syntax on
    filetype plugin on
    set ai
    set nu
    set smartindent
    set tabstop=4
    set shiftwidth=4
    set expandtab
    let &#38;background = ( &#38;background == &#34;dark&#34;? &#34;light&#34; : &#34;dark&#34; )
    let g:vim_markdown_folding_disabled=1
```

Color schemes are browsable at [vimcolors.com](http://vimcolors.com). They can be installed in
$HOME/.vim/colors.

1. git clone and place the colorscheme
2. place the *.vim file holding the color scheme into $HOME/.vim/colors
3. start vim and at the : do colorscheme NAME where NAME is the scheme you want to try

You can find the default shipped color schemes in /usr/share/vim/vimNN/colors where vimNN is the version number
e.g. /usr/share/vim/vim74/colors.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2016/08/15/Setting-up-Go-under-Bash-for-Windows-10.md" length="4266" type="text/markdown" />
    </item>    <item>
      <title>How to make a Pi-Top more Raspbian</title>
      <link>https://rsdoiel.github.io/blog/2016/07/04/How-To-Make-A-PiTop-More-Raspbian.html</link>
      <description>
        <![CDATA[This post explore making a PiTop a little more Raspbian.]]>
      </description>
      <source:markdown>How to make a Pi-Top more Raspbian
==================================

By R. S. Doiel, 2016-07-04

I have a first generation Pi-Top.  I like the idea but found I didn&#39;t use it much due to a preference for
basic Raspbian. With the recent Pi-TopOS upgrades I realized getting back to basic Raspbian was relatively
straight forward.

## The recipe

1. Make sure you&#39;re running the latest Pi-TopOS based on Jessie
2. Login into your Pi-Top normally
3. From the Pi-Top dashboard select the &#34;Desktop&#34; icon
4. When you see the familiar Raspbian desktop click on the following things
	+ Click on the Raspberry Menu (upper left corner)
	+ Click on Preferences
	+ Click on Raspberry Pi Configuration
5. I made the following changes to my System configuration
	+ Under *Boot* I selected &#34;To CLI&#34;
	+ I unchecked *login as user &#34;pi&#34;*
6. Restart your Pi Top
	+ Click on Raspberry Menu in the upper left of the desktop
	+ Click on shutdown
	+ Select *reboot*
7. When you restart you&#39;ll see an old school console login, login as the pi user using your Pi-Top password
8. Remove the following program use the *apt* command
	+ ceed-universe
	+ pt-dashboard
	+ pt-splashscreen

```
    sudo apt purge ceed-universe pt-dashboard pt-splashscreen
```

Note: pi-battery, pt-hub-controller, pt-ipc, pt-speaker are hardware drivers specific to your Pi-Top so you probably
want to keep them.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2016/07/04/How-To-Make-A-PiTop-More-Raspbian.md" length="1938" type="text/markdown" />
    </item>    <item>
      <title>Instant Articles, Accelerated Mobile Pages, Twitter Cards and Open Graph</title>
      <link>https://rsdoiel.github.io/blog/2016/05/30/amp-cards-and-open-graph.html</link>
      <description>
        <![CDATA[This post explores Twitter Cards, Facebook Open Graph, AMP and what implementations they have for page weight and usage.]]>
      </description>
      <source:markdown># Instant Articles, Accelerated Mobile Pages, Twitter Cards and Open Graph

By R. S. Doiel 2016-05-30

## The problem

The web has gotten slow. In [2016](http://httparchive.org/trends.php) the 
average page weight is in multi-megabytes and the average number of network 
requests needed to deliver the content is counted in 
the hundreds. In the mix are saturated networks and a continued public 
expectation of responsiveness (web wisdom suggests you have about 3 seconds 
before people give up).  The odd thing is we&#39;ve known how to build fast 
websites for a [decade](https://www.stevesouders.com/) or so.  
Collectively we don&#39;t build them [fast](https://www.sitepoint.com/average-page-weight-increased-another-16-2015/). 


## Meet the new abstractions

Corporations believe they have the answer and they are providing us 
with another set of abstractions. In a few years maybe these will 
get distilled down to a shared common view but in the mean time disc 
costs remain reasonably priced and generating these new forms of 
pages or feeds is a template or so away.

+ [Twitter Cards](https://dev.twitter.com/cards/overview) and [Open Graph](http://ogp.me/)
  + Exposing your content via social media, search results or embedded in pages via an aside element
+ [Accelerated Mobile Pages](https://www.ampproject.org/) (also called AMP)
  + A simplification in content delivery to improve web reading experience
  + Its usefulness is it proscribes an approach to leverage what we have
  + AMP works well with Twitter Cards, Open Graph and can leverage Web Components
+ [Instant Articles](https://instantarticles.fb.com/)
  + a format play to feed the walled garden of Facebook for iOS and Android devices


## The players 

### Twitter Cards and Open Graph

Twitter&#39;s Titter Cards and Facebook&#39;s Open Graph offer approaches to 
build off of our existing meta elements in an HTML page&#39;s document 
head.  They are named space to avoid collisions but supporting both 
will still result in some content duplication. The k-weight 
difference in the resulting HTML pages isn&#39;t too bad. 

Adopting either or both is a matter of adjusting how your render your 
web page&#39;s head block.  It is easy enough to do manually but easier 
still using some sort of template system that renders the appropriate 
meta elements based on the content type and contents in the page 
being rendered.  

Google and other search engines can leverage this richer meta 
data and integrate it into their results. Google&#39;s Now application can 
render content cards based on either semantic. It also appears that 
content cards are being leverage selectively for an aside and related 
content on Google search results pages. You could even built this into 
your own indexing process for use with the Solr or Elasticsearch.

Content Cards offer intriguing opportunity for web crawlers and search 
engines.  This is particularly true when combined with mature feed 
formats like RSS, OPML, Atom and the maturing efforts in the linked 
data community around JSON-LD.


### AMP - Accelerated Mobile Pages

The backers of AMP (not to be confused with Apache+MySQL+PHP) are largely
publishers including major news outlets and web media
companies in the US and Europe. This is an abridged list from 2015--

+ BBC
+ Atlantic Media
+ Vox Media
+ Conde Nast
+ New York Times
+ Wall Street Journal
+ The Daily Mail
+ Huffington Post
+ Gannet
+ The Guardian
+ The Economist
+ The Financial Times

In additional to the publishers there is participation by tech companies
such as Google, Pinterest, Twitter, LinkedIn and Wordpress.com.  Accelerated
Mobile Pages offer benefits for web crawlers and search engines supporting
surfacing content is clearly and enabling easier distinction from 
advertisements. 


### Instant Articles

In additional to Open Graph Facebook has put forward [Instant Articles](https://developers.facebook.com/docs/instant-articles).
Like AMP it is targeting content delivery for mobile. Unlike AMP Instant Articles is an
explicit binding into Facebook&#39;s walled garden only exposing the content on supported
versions of iOS and Android. You don&#39;t see Instant Articles in your Facebook timeline or when  
you browse from a desktop web browser.  Unlike the previous
examples you actually need to sign up to participate in the Instant Article publishing
process.  Sign up cost is having a Facebook account, being approved by Facebook and compliance
with their terms of service. Facebook does provide some publishing tools, publishing controls
as well as some analytics. They do allow 3rd party ads as well as encourage access to
their advertising network.  Once approved the burden on your content manage process 
appears manageable.  

You can submit Instant Articles via a modified RSS feed or directly through their API. 
In this sense the overhead is about the same as that for implementing support for Twitter Cards
Open Graph, and AMP. Facebook does a good job of quickly propagating changes to your
Instant Articles across their platform. That&#39;s nice.

Why go through the trouble? If you&#39;re a content producer and your audience lives on Facebook
Facebook commands the attention of a lot of eye balls.  Instant Articles provides 
another avenue to reach them.  For some Facebook effectively controls the public view of the 
web much as America Online and Prodigy did decades ago. [Dave Winer](https://twitter.com/davewiner) 
has written extensively on how he implemented Instant Article support along with 
some very reasoned pros and cons for doing so. The landscape is evolving and 
[Dave&#39;s river of news](http://scripting.com) is worth following.


## Impact on building content

These approaches require changes in your production of your HTML and RSS sent to the browser.
Twitter Cards and Open Graph change what you put in the HEAD element of the HTML
pages.  AMP proscribes what you should put in the BODY element of the webpage.
Instant Articles tweaks your RSS output.  Not surprisingly the major content management 
systems Wordpress and Drupal have plugins for this.  All can be implemented via your template 
system or page generation process.


## Whither adopt?

Because these approaches boil down to content assembly the adoption risk 
is low.  If your audience views Twitter, Facebook or Google search results 
then it is probably worth doing.  All allow you to continue to publish your 
own content and own your URLs as opposed to being a tenant on one or another 
platform. That benefits the open web.</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2016/05/30/amp-cards-and-open-graph.md" length="7190" type="text/markdown" />
    </item>    <item>
      <title>OPML to Markdown and back</title>
      <link>https://rsdoiel.github.io/blog/2016/05/28/OPML-to-Markdown-and-back.html</link>
      <description>
        <![CDATA[This post covers developing a Go package for [OPML](http://dev.opml.org/spec2.html) and it's features.]]>
      </description>
      <source:markdown>OPML to Markdown and back
=========================

By R. S. Doiel 2016-05-28

## Overview

I wrote a Go language package to sort [OPML](http://dev.opml.org/spec2.html) outlines. 
I wrote this because my preferred [feed reader ](http://goread.io) supports manual 
sorting but not automatic alpha sorting by the _outline_ element&#39;s _text_ attribute. 

## Observations

Out of the box the OPML 2 Spec provides attributes indicating inclusion of other OPML files,
scripts, basic metadata (create, modified, authorship), and even directory structures.

[Fargo](http://fargo.io) allows user defined attributes to be applied to the _outline_ 
element in OPML. This could be used in support some of the 
[Scrivener](https://www.literatureandlatte.com/scrivener.php)
features I miss such as describing how to render a project to various formats such as
rtf, pdf, ePub, web pages or even [Final Draft fdx](https://www.finaldraft.com/) files.

I write allot of Markdown formatted text.  Markdown is simple to index, 
search and convert into useful formats. Markdown is not good at expressing more
complex structures such as metadata. Website generators that use markdown often
require a preamble or _front matter_ in the markdown to provide any metadata. This
leaves your document head cluttered and less human readable.

Another approach is to include a parallel document with the metadata.  It occurred to me 
that an OPML file could easily hold that metadata. It can even hold Markdown content.
The trouble with OPML is that it is not quick to edit by hand.

    Is there a round trip semantic mapping between OPML and Markdown?


## Germination of an idea

Entering a web link in Fargo the link is URL encoded and saved in the _text_ attribute of the 
_outline_ element.

The source view of a web links in Fargo&#39;s _outline_ element looks like

```OPML
    &#60;outline text=&#34;&#38;gt; href=&#38;quot;http://example.org&#38;quot;&#38;lt;My example.org&#38;gt;/a&#38;lt;&#34; /&#62;
```

That _outline_ element might render in Markdown as

```
    + [My element.org](http://example.org)
```

The steps to create the Markdown view are simple

1. URL decode the _text_ attribute
2. Convert HTML to Markdown

Making a round trip could be done by

3. Convert Markdown into HTML
4. For each _li_ element covert to an _outline_ element URL encoding the inner HTML of the _li_

So far so good. What about something more complex?


Here&#39;s an _outline_ element example from http://hosting.opml.org/dave/spec/directory.opml 

```OPML
    &#60;outline text=&#34;Scripting News sites&#34; created=&#34;Sun, 16 Oct 2005 05:56:10 GMT&#34; type=&#34;link&#34; url=&#34;http://hosting.opml.org/dave/mySites.opml&#34;/&#62;
```

To me that should look like 

```
    + [Scripting News Sites](http://hosting.opml.org/dave/mySites.opml)
```

What about the _created_ attribute? Could we render this case as an additional set of anchors using data uri?

This suggest a rule like

+ if the _text_ attribute contains HTML markup
    + URL decode into HTML
    + Convert HTML to Markdown
+ else render attributes as additional anchors using data URI

This might work as follows. 

```OPML
    &#60;outline text=&#34;Scripting News sites&#34; 
        created=&#34;Sun, 16 Oct 2005 05:56:10 GMT&#34; 
        type=&#34;link&#34; 
        url=&#34;http://hosting.opml.org/dave/mySites.opml&#34;/&#62;
```

Would become 

```Markdown
    + [Scripting News Sites](http://hosting.opml.org/dave/mySites.opml) [type](data:text/plain;link) [created](data:text/date;Sun, 16 Oct 2005 05:56:10 GMT)
```

In HTML this would look like

```HTML
    &#60;li&#62;&#60;a href=&#34;http://histing.opml.org/dave/mySites.opml&#34;&#62;Scripting News Sites&#60;/a&#62;
        &#60;a href=&#34;data:text/plain;link&#34;&#62;type&#60;/a&#62;
        &#60;a href=&#34;data:text/date;Sun, 16 Oct 2005 05:56:10 GMT&#34;&#62;created&#60;/a&#62;&#60;/li&#62;
```

### Markdown to OPML

Coming back to OPML from Markdown then becomes

+ Convert Markdown to HTML
+ For each _li_ element inspect anchors, 
    + if anchors contain data URI then map _outline_ element
    + else URL encode and embed in _outline_ _text_ attribute

Is this viable? Does it have any advantages?</source:markdown>
      <author>rsdoiel@gmail.com  (R. S. Doiel))</author>
      <enclosure url="https://rsdoiel.github.io/blog/2016/05/28/OPML-to-Markdown-and-back.md" length="4564" type="text/markdown" />
    </item>
  </channel>
</rss>
